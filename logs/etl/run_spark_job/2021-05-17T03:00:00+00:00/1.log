[2021-05-17 01:23:11,722] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T03:00:00+00:00 [queued]>
[2021-05-17 01:23:11,814] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T03:00:00+00:00 [queued]>
[2021-05-17 01:23:11,815] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 01:23:11,816] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-17 01:23:11,816] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 01:23:11,880] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-17T03:00:00+00:00
[2021-05-17 01:23:11,916] {standard_task_runner.py:52} INFO - Started process 33577 to run task
[2021-05-17 01:23:11,985] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-17T03:00:00+00:00', '--job-id', '876', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpe8ne46ph', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpnxz03l6d']
[2021-05-17 01:23:12,002] {standard_task_runner.py:77} INFO - Job 876: Subtask run_spark_job
[2021-05-17 01:23:12,520] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-17T03:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-17 01:23:12,799] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-17T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-17T03:00:00+00:00
[2021-05-17 01:23:12,830] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-17 01:23:18,366] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-17 01:23:18,418] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-17 01:23:18,422] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-17 01:23:18,512] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-17 01:23:36,562] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-17 01:23:38,559] {docker.py:276} INFO - 21/05/17 04:23:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-17 01:23:42,468] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-17 01:23:42,495] {docker.py:276} INFO - 21/05/17 04:23:42 INFO SparkContext: Running Spark version 3.1.1
[2021-05-17 01:23:42,620] {docker.py:276} INFO - 21/05/17 04:23:42 INFO ResourceUtils: ==============================================================
[2021-05-17 01:23:42,621] {docker.py:276} INFO - 21/05/17 04:23:42 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-17 01:23:42,622] {docker.py:276} INFO - 21/05/17 04:23:42 INFO ResourceUtils: ==============================================================
[2021-05-17 01:23:42,624] {docker.py:276} INFO - 21/05/17 04:23:42 INFO SparkContext: Submitted application: spark.py
[2021-05-17 01:23:42,721] {docker.py:276} INFO - 21/05/17 04:23:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-17 01:23:42,753] {docker.py:276} INFO - 21/05/17 04:23:42 INFO ResourceProfile: Limiting resource is cpu
[2021-05-17 01:23:42,755] {docker.py:276} INFO - 21/05/17 04:23:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-17 01:23:42,892] {docker.py:276} INFO - 21/05/17 04:23:42 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-17 01:23:42,892] {docker.py:276} INFO - 21/05/17 04:23:42 INFO SecurityManager: Changing modify acls to: jovyan
21/05/17 04:23:42 INFO SecurityManager: Changing view acls groups to:
[2021-05-17 01:23:42,893] {docker.py:276} INFO - 21/05/17 04:23:42 INFO SecurityManager: Changing modify acls groups to:
[2021-05-17 01:23:42,893] {docker.py:276} INFO - 21/05/17 04:23:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-17 01:23:43,540] {docker.py:276} INFO - 21/05/17 04:23:43 INFO Utils: Successfully started service 'sparkDriver' on port 34521.
[2021-05-17 01:23:43,610] {docker.py:276} INFO - 21/05/17 04:23:43 INFO SparkEnv: Registering MapOutputTracker
[2021-05-17 01:23:43,691] {docker.py:276} INFO - 21/05/17 04:23:43 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-17 01:23:43,752] {docker.py:276} INFO - 21/05/17 04:23:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-17 01:23:43,753] {docker.py:276} INFO - 21/05/17 04:23:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-17 01:23:43,764] {docker.py:276} INFO - 21/05/17 04:23:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-17 01:23:43,814] {docker.py:276} INFO - 21/05/17 04:23:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-667954f3-06c4-458d-a251-ed09b30b53fa
[2021-05-17 01:23:43,865] {docker.py:276} INFO - 21/05/17 04:23:43 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-17 01:23:43,910] {docker.py:276} INFO - 21/05/17 04:23:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-17 01:23:44,389] {docker.py:276} INFO - 21/05/17 04:23:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-17 01:23:44,523] {docker.py:276} INFO - 21/05/17 04:23:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://aec6a2f73954:4040
[2021-05-17 01:23:44,982] {docker.py:276} INFO - 21/05/17 04:23:44 INFO Executor: Starting executor ID driver on host aec6a2f73954
[2021-05-17 01:23:45,051] {docker.py:276} INFO - 21/05/17 04:23:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33925.
21/05/17 04:23:45 INFO NettyBlockTransferService: Server created on aec6a2f73954:33925
[2021-05-17 01:23:45,056] {docker.py:276} INFO - 21/05/17 04:23:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-17 01:23:45,075] {docker.py:276} INFO - 21/05/17 04:23:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aec6a2f73954, 33925, None)
[2021-05-17 01:23:45,087] {docker.py:276} INFO - 21/05/17 04:23:45 INFO BlockManagerMasterEndpoint: Registering block manager aec6a2f73954:33925 with 934.4 MiB RAM, BlockManagerId(driver, aec6a2f73954, 33925, None)
[2021-05-17 01:23:45,098] {docker.py:276} INFO - 21/05/17 04:23:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aec6a2f73954, 33925, None)
[2021-05-17 01:23:45,101] {docker.py:276} INFO - 21/05/17 04:23:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aec6a2f73954, 33925, None)
[2021-05-17 01:23:45,990] {docker.py:276} INFO - 21/05/17 04:23:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-17 01:23:45,991] {docker.py:276} INFO - 21/05/17 04:23:45 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-17 01:23:47,549] {docker.py:276} INFO - 21/05/17 04:23:47 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-17 01:23:47,613] {docker.py:276} INFO - 21/05/17 04:23:47 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/17 04:23:47 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-17 02:23:50,150] {docker.py:276} INFO - Traceback (most recent call last):
  File "/home/jovyan/spark.py", line 55, in <module>
[2021-05-17 02:23:50,151] {docker.py:276} INFO - data = spark.read.options(delimiter=";").csv(file, header=True).dropDuplicates()
  File "/usr/local/spark/python/pyspark/sql/readwriter.py", line 737, in csv
[2021-05-17 02:23:50,152] {docker.py:276} INFO - return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2021-05-17 02:23:50,153] {docker.py:276} INFO - File "/usr/local/spark/python/pyspark/sql/utils.py", line 111, in deco
[2021-05-17 02:23:50,154] {docker.py:276} INFO - return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError
[2021-05-17 02:23:50,155] {docker.py:276} INFO - : An error occurred while calling o77.csv.
: java.nio.file.AccessDeniedException: s3a://udac-forex-project/2/2021-05-17_01_06_56: getFileStatus on s3a://udac-forex-project/2/2021-05-17_01_06_56: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: RB4PMVR2PHFVACJY; S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=; Proxy: null), S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1903)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1882)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1882)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:149)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2016)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:2963)
	at org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:253)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$3(DataSource.scala:783)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: RB4PMVR2PHFVACJY; S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=; Proxy: null), S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 27 more
[2021-05-17 02:23:50,369] {docker.py:276} INFO - 21/05/17 04:23:56 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-17 02:23:50,404] {docker.py:276} INFO - 21/05/17 04:23:56 INFO SparkUI: Stopped Spark web UI at http://aec6a2f73954:4040
[2021-05-17 02:23:50,444] {docker.py:276} INFO - 21/05/17 04:23:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-17 02:23:50,470] {docker.py:276} INFO - 21/05/17 04:23:57 INFO MemoryStore: MemoryStore cleared
[2021-05-17 02:23:50,471] {docker.py:276} INFO - 21/05/17 04:23:57 INFO BlockManager: BlockManager stopped
[2021-05-17 02:23:50,490] {docker.py:276} INFO - 21/05/17 04:23:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-17 02:23:50,501] {docker.py:276} INFO - 21/05/17 04:23:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-17 02:23:50,514] {docker.py:276} INFO - 21/05/17 04:23:57 INFO SparkContext: Successfully stopped SparkContext
[2021-05-17 02:23:50,516] {docker.py:276} INFO - 21/05/17 04:23:57 INFO ShutdownHookManager: Shutdown hook called
[2021-05-17 02:23:50,518] {docker.py:276} INFO - 21/05/17 04:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef6fe23-1fce-4e8d-914f-b9fc024fd0da
[2021-05-17 02:23:50,523] {docker.py:276} INFO - 21/05/17 04:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6a7b826-3eff-48c2-ba43-5d679a5be4b7/pyspark-bd90686f-5575-4ae4-a3bd-91283b3a69a1
[2021-05-17 02:23:50,528] {docker.py:276} INFO - 21/05/17 04:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6a7b826-3eff-48c2-ba43-5d679a5be4b7
[2021-05-17 02:23:50,542] {docker.py:276} INFO - 21/05/17 04:23:57 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-17 02:23:50,544] {docker.py:276} INFO - 21/05/17 04:23:57 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-17 02:23:50,546] {docker.py:276} INFO - 21/05/17 04:23:57 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-17 02:23:50,797] {taskinstance.py:1482} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1138, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 321, in execute
    return self._run_image()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 281, in _run_image
    raise AirflowException('docker container failed: ' + repr(result) + f"lines {res_lines}")
airflow.exceptions.AirflowException: docker container failed: {'Error': None, 'StatusCode': 1}lines WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
21/05/17 04:23:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/05/17 04:23:42 INFO SparkContext: Running Spark version 3.1.1
21/05/17 04:23:42 INFO ResourceUtils: ==============================================================
21/05/17 04:23:42 INFO ResourceUtils: No custom resources configured for spark.driver.
21/05/17 04:23:42 INFO ResourceUtils: ==============================================================
21/05/17 04:23:42 INFO SparkContext: Submitted application: spark.py
21/05/17 04:23:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/05/17 04:23:42 INFO ResourceProfile: Limiting resource is cpu
21/05/17 04:23:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/05/17 04:23:42 INFO SecurityManager: Changing view acls to: jovyan
21/05/17 04:23:42 INFO SecurityManager: Changing modify acls to: jovyan
21/05/17 04:23:42 INFO SecurityManager: Changing view acls groups to:
21/05/17 04:23:42 INFO SecurityManager: Changing modify acls groups to:
21/05/17 04:23:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
21/05/17 04:23:43 INFO Utils: Successfully started service 'sparkDriver' on port 34521.
21/05/17 04:23:43 INFO SparkEnv: Registering MapOutputTracker
21/05/17 04:23:43 INFO SparkEnv: Registering BlockManagerMaster
21/05/17 04:23:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/05/17 04:23:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/05/17 04:23:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/05/17 04:23:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-667954f3-06c4-458d-a251-ed09b30b53fa
21/05/17 04:23:43 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
21/05/17 04:23:43 INFO SparkEnv: Registering OutputCommitCoordinator
21/05/17 04:23:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/05/17 04:23:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://aec6a2f73954:4040
21/05/17 04:23:44 INFO Executor: Starting executor ID driver on host aec6a2f73954
21/05/17 04:23:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33925.
21/05/17 04:23:45 INFO NettyBlockTransferService: Server created on aec6a2f73954:33925
21/05/17 04:23:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/05/17 04:23:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aec6a2f73954, 33925, None)
21/05/17 04:23:45 INFO BlockManagerMasterEndpoint: Registering block manager aec6a2f73954:33925 with 934.4 MiB RAM, BlockManagerId(driver, aec6a2f73954, 33925, None)
21/05/17 04:23:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aec6a2f73954, 33925, None)
21/05/17 04:23:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aec6a2f73954, 33925, None)
21/05/17 04:23:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
21/05/17 04:23:45 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
21/05/17 04:23:47 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
21/05/17 04:23:47 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/17 04:23:47 INFO MetricsSystemImpl: s3a-file-system metrics system started
Traceback (most recent call last):
  File "/home/jovyan/spark.py", line 55, in <module>
data = spark.read.options(delimiter=";").csv(file, header=True).dropDuplicates()
  File "/usr/local/spark/python/pyspark/sql/readwriter.py", line 737, in csv
return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
File "/usr/local/spark/python/pyspark/sql/utils.py", line 111, in deco
return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError
: An error occurred while calling o77.csv.
: java.nio.file.AccessDeniedException: s3a://udac-forex-project/2/2021-05-17_01_06_56: getFileStatus on s3a://udac-forex-project/2/2021-05-17_01_06_56: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: RB4PMVR2PHFVACJY; S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=; Proxy: null), S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1903)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1882)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1882)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:149)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2016)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:2963)
	at org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:253)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$3(DataSource.scala:783)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: RB4PMVR2PHFVACJY; S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=; Proxy: null), S3 Extended Request ID: 89risH8I4Ed7A+E5gJxy4l9IssftUIUh4IjfwvMK0pii9EQcLEaS1DHaioj6n8jOVitLr1Qa6nU=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 27 more
21/05/17 04:23:56 INFO SparkContext: Invoking stop() from shutdown hook
21/05/17 04:23:56 INFO SparkUI: Stopped Spark web UI at http://aec6a2f73954:4040
21/05/17 04:23:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/05/17 04:23:57 INFO MemoryStore: MemoryStore cleared
21/05/17 04:23:57 INFO BlockManager: BlockManager stopped
21/05/17 04:23:57 INFO BlockManagerMaster: BlockManagerMaster stopped
21/05/17 04:23:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/05/17 04:23:57 INFO SparkContext: Successfully stopped SparkContext
21/05/17 04:23:57 INFO ShutdownHookManager: Shutdown hook called
21/05/17 04:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef6fe23-1fce-4e8d-914f-b9fc024fd0da
21/05/17 04:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6a7b826-3eff-48c2-ba43-5d679a5be4b7/pyspark-bd90686f-5575-4ae4-a3bd-91283b3a69a1
21/05/17 04:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6a7b826-3eff-48c2-ba43-5d679a5be4b7
21/05/17 04:23:57 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
21/05/17 04:23:57 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
21/05/17 04:23:57 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-17 02:23:50,802] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=etl, task_id=run_spark_job, execution_date=20210517T030000, start_date=20210517T042311, end_date=20210517T052350
[2021-05-17 02:23:50,848] {local_task_job.py:146} INFO - Task exited with return code 1
