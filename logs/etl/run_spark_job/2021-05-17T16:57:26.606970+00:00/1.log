[2021-05-17 13:58:09,603] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T16:57:26.606970+00:00 [queued]>
[2021-05-17 13:58:09,609] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T16:57:26.606970+00:00 [queued]>
[2021-05-17 13:58:09,609] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 13:58:09,609] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-17 13:58:09,609] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 13:58:09,614] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-17T16:57:26.606970+00:00
[2021-05-17 13:58:09,617] {standard_task_runner.py:52} INFO - Started process 36443 to run task
[2021-05-17 13:58:09,623] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-17T16:57:26.606970+00:00', '--job-id', '934', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp9vik64ly', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpx3u298_k']
[2021-05-17 13:58:09,625] {standard_task_runner.py:77} INFO - Job 934: Subtask run_spark_job
[2021-05-17 13:58:09,655] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-17T16:57:26.606970+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-17 13:58:09,679] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-17T16:57:26.606970+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-17T16:57:26.606970+00:00
[2021-05-17 13:58:09,682] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-17 13:58:13,044] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-17 13:58:13,051] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-17 13:58:13,051] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-17 13:58:13,061] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-17 13:58:15,389] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-17 13:58:16,013] {docker.py:276} INFO - 21/05/17 16:58:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-17 13:58:18,592] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-17 13:58:18,606] {docker.py:276} INFO - 21/05/17 16:58:18 INFO SparkContext: Running Spark version 3.1.1
[2021-05-17 13:58:18,670] {docker.py:276} INFO - 21/05/17 16:58:18 INFO ResourceUtils: ==============================================================
[2021-05-17 13:58:18,671] {docker.py:276} INFO - 21/05/17 16:58:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-17 13:58:18,671] {docker.py:276} INFO - 21/05/17 16:58:18 INFO ResourceUtils: ==============================================================
[2021-05-17 13:58:18,672] {docker.py:276} INFO - 21/05/17 16:58:18 INFO SparkContext: Submitted application: spark.py
[2021-05-17 13:58:18,718] {docker.py:276} INFO - 21/05/17 16:58:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-17 13:58:18,733] {docker.py:276} INFO - 21/05/17 16:58:18 INFO ResourceProfile: Limiting resource is cpu
[2021-05-17 13:58:18,734] {docker.py:276} INFO - 21/05/17 16:58:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-17 13:58:18,803] {docker.py:276} INFO - 21/05/17 16:58:18 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-17 13:58:18,803] {docker.py:276} INFO - 21/05/17 16:58:18 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-17 13:58:18,804] {docker.py:276} INFO - 21/05/17 16:58:18 INFO SecurityManager: Changing view acls groups to: 
21/05/17 16:58:18 INFO SecurityManager: Changing modify acls groups to: 
21/05/17 16:58:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-17 13:58:19,154] {docker.py:276} INFO - 21/05/17 16:58:19 INFO Utils: Successfully started service 'sparkDriver' on port 34735.
[2021-05-17 13:58:19,208] {docker.py:276} INFO - 21/05/17 16:58:19 INFO SparkEnv: Registering MapOutputTracker
[2021-05-17 13:58:19,260] {docker.py:276} INFO - 21/05/17 16:58:19 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-17 13:58:19,297] {docker.py:276} INFO - 21/05/17 16:58:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-17 13:58:19,298] {docker.py:276} INFO - 21/05/17 16:58:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-17 13:58:19,306] {docker.py:276} INFO - 21/05/17 16:58:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-17 13:58:19,325] {docker.py:276} INFO - 21/05/17 16:58:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-31321966-abfc-414e-8b0f-cd5ab4f8da2e
[2021-05-17 13:58:19,354] {docker.py:276} INFO - 21/05/17 16:58:19 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-17 13:58:19,378] {docker.py:276} INFO - 21/05/17 16:58:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-17 13:58:19,664] {docker.py:276} INFO - 21/05/17 16:58:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-17 13:58:19,752] {docker.py:276} INFO - 21/05/17 16:58:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bb4563f4559d:4040
[2021-05-17 13:58:20,035] {docker.py:276} INFO - 21/05/17 16:58:20 INFO Executor: Starting executor ID driver on host bb4563f4559d
[2021-05-17 13:58:20,075] {docker.py:276} INFO - 21/05/17 16:58:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33263.
21/05/17 16:58:20 INFO NettyBlockTransferService: Server created on bb4563f4559d:33263
[2021-05-17 13:58:20,078] {docker.py:276} INFO - 21/05/17 16:58:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-17 13:58:20,090] {docker.py:276} INFO - 21/05/17 16:58:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bb4563f4559d, 33263, None)
[2021-05-17 13:58:20,101] {docker.py:276} INFO - 21/05/17 16:58:20 INFO BlockManagerMasterEndpoint: Registering block manager bb4563f4559d:33263 with 934.4 MiB RAM, BlockManagerId(driver, bb4563f4559d, 33263, None)
[2021-05-17 13:58:20,104] {docker.py:276} INFO - 21/05/17 16:58:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bb4563f4559d, 33263, None)
[2021-05-17 13:58:20,106] {docker.py:276} INFO - 21/05/17 16:58:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bb4563f4559d, 33263, None)
[2021-05-17 13:58:20,915] {docker.py:276} INFO - 21/05/17 16:58:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-17 13:58:20,916] {docker.py:276} INFO - 21/05/17 16:58:20 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-17 13:58:22,015] {docker.py:276} INFO - 21/05/17 16:58:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-17 13:58:22,074] {docker.py:276} INFO - 21/05/17 16:58:22 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-17 13:58:22,075] {docker.py:276} INFO - 21/05/17 16:58:22 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-17 13:58:27,862] {docker.py:276} INFO - 21/05/17 16:58:27 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 90 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621215193_to_1621216662.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621216662_to_1621218462.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621218462_to_1621220262.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621220262_to_1621222062.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621222062_to_1621223862.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621223862_to_1621225662.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621225662_to_1621227462.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621227462_to_1621229262.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621229262_to_1621231062.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621231062_to_1621232862.csv.
[2021-05-17 13:58:28,417] {docker.py:276} INFO - 21/05/17 16:58:28 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:28,444] {docker.py:276} INFO - 21/05/17 16:58:28 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 90 output partitions
[2021-05-17 13:58:28,445] {docker.py:276} INFO - 21/05/17 16:58:28 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 13:58:28,447] {docker.py:276} INFO - 21/05/17 16:58:28 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 13:58:28,450] {docker.py:276} INFO - 21/05/17 16:58:28 INFO DAGScheduler: Missing parents: List()
[2021-05-17 13:58:28,458] {docker.py:276} INFO - 21/05/17 16:58:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 13:58:28,568] {docker.py:276} INFO - 21/05/17 16:58:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-17 13:58:28,649] {docker.py:276} INFO - 21/05/17 16:58:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-17 13:58:28,653] {docker.py:276} INFO - 21/05/17 16:58:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on bb4563f4559d:33263 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-17 13:58:28,661] {docker.py:276} INFO - 21/05/17 16:58:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-17 13:58:28,685] {docker.py:276} INFO - 21/05/17 16:58:28 INFO DAGScheduler: Submitting 90 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-17 13:58:28,696] {docker.py:276} INFO - 21/05/17 16:58:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 90 tasks resource profile 0
[2021-05-17 13:58:28,807] {docker.py:276} INFO - 21/05/17 16:58:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (bb4563f4559d, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:28,811] {docker.py:276} INFO - 21/05/17 16:58:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (bb4563f4559d, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:28,814] {docker.py:276} INFO - 21/05/17 16:58:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (bb4563f4559d, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:28,815] {docker.py:276} INFO - 21/05/17 16:58:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (bb4563f4559d, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:28,841] {docker.py:276} INFO - 21/05/17 16:58:28 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-17 13:58:28,841] {docker.py:276} INFO - 21/05/17 16:58:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-17 13:58:28,842] {docker.py:276} INFO - 21/05/17 16:58:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-17 13:58:28,842] {docker.py:276} INFO - 21/05/17 16:58:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-17 13:58:29,308] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-17 13:58:29,327] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (bb4563f4559d, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,327] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-17 13:58:29,328] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 556 ms on bb4563f4559d (executor driver) (1/90)
[2021-05-17 13:58:29,505] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-17 13:58:29,508] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (bb4563f4559d, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,510] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 197 ms on bb4563f4559d (executor driver) (2/90)
[2021-05-17 13:58:29,510] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-17 13:58:29,704] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1886 bytes result sent to driver
[2021-05-17 13:58:29,707] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (bb4563f4559d, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,709] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-17 13:58:29,710] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 203 ms on bb4563f4559d (executor driver) (3/90)
[2021-05-17 13:58:29,815] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-17 13:58:29,816] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-17 13:58:29,817] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-17 13:58:29,819] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (bb4563f4559d, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,820] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-17 13:58:29,821] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (bb4563f4559d, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,822] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-17 13:58:29,823] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1010 ms on bb4563f4559d (executor driver) (4/90)
[2021-05-17 13:58:29,832] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (bb4563f4559d, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,833] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-17 13:58:29,834] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1024 ms on bb4563f4559d (executor driver) (5/90)
[2021-05-17 13:58:29,836] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1025 ms on bb4563f4559d (executor driver) (6/90)
[2021-05-17 13:58:29,891] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-17 13:58:29,894] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (bb4563f4559d, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:29,895] {docker.py:276} INFO - 21/05/17 16:58:29 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-17 13:58:29,896] {docker.py:276} INFO - 21/05/17 16:58:29 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 190 ms on bb4563f4559d (executor driver) (7/90)
[2021-05-17 13:58:30,004] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-17 13:58:30,007] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (bb4563f4559d, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,009] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 189 ms on bb4563f4559d (executor driver) (8/90)
[2021-05-17 13:58:30,009] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-17 13:58:30,011] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-17 13:58:30,013] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (bb4563f4559d, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,015] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-17 13:58:30,016] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 196 ms on bb4563f4559d (executor driver) (9/90)
[2021-05-17 13:58:30,018] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-17 13:58:30,020] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (bb4563f4559d, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,021] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-17 13:58:30,022] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 191 ms on bb4563f4559d (executor driver) (10/90)
[2021-05-17 13:58:30,078] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-17 13:58:30,080] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (bb4563f4559d, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,081] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 189 ms on bb4563f4559d (executor driver) (11/90)
21/05/17 16:58:30 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-17 13:58:30,204] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-17 13:58:30,205] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-17 13:58:30,209] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (bb4563f4559d, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,212] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (bb4563f4559d, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,213] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 207 ms on bb4563f4559d (executor driver) (12/90)
[2021-05-17 13:58:30,215] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-17 13:58:30,215] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-17 13:58:30,216] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-17 13:58:30,217] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 202 ms on bb4563f4559d (executor driver) (13/90)
[2021-05-17 13:58:30,219] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (bb4563f4559d, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,220] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-17 13:58:30,221] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 202 ms on bb4563f4559d (executor driver) (14/90)
[2021-05-17 13:58:30,253] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-17 13:58:30,256] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (bb4563f4559d, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,258] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-17 13:58:30,259] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 179 ms on bb4563f4559d (executor driver) (15/90)
[2021-05-17 13:58:30,407] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
[2021-05-17 13:58:30,409] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1843 bytes result sent to driver
[2021-05-17 13:58:30,411] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-17 13:58:30,412] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (bb4563f4559d, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,413] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 202 ms on bb4563f4559d (executor driver) (16/90)
[2021-05-17 13:58:30,415] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (bb4563f4559d, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,415] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-17 13:58:30,416] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-17 13:58:30,418] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (bb4563f4559d, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,419] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 213 ms on bb4563f4559d (executor driver) (17/90)
[2021-05-17 13:58:30,420] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 202 ms on bb4563f4559d (executor driver) (18/90)
[2021-05-17 13:58:30,422] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-17 13:58:30,435] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1886 bytes result sent to driver
[2021-05-17 13:58:30,437] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (bb4563f4559d, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,438] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 184 ms on bb4563f4559d (executor driver) (19/90)
[2021-05-17 13:58:30,439] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-17 13:58:30,612] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1886 bytes result sent to driver
[2021-05-17 13:58:30,615] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1886 bytes result sent to driver
[2021-05-17 13:58:30,618] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (bb4563f4559d, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,619] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 209 ms on bb4563f4559d (executor driver) (20/90)
[2021-05-17 13:58:30,621] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-17 13:58:30,623] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (bb4563f4559d, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,625] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 208 ms on bb4563f4559d (executor driver) (21/90)
[2021-05-17 13:58:30,626] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-17 13:58:30,630] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-17 13:58:30,631] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1886 bytes result sent to driver
21/05/17 16:58:30 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (bb4563f4559d, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,633] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 196 ms on bb4563f4559d (executor driver) (22/90)
[2021-05-17 13:58:30,636] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (bb4563f4559d, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,637] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 223 ms on bb4563f4559d (executor driver) (23/90)
[2021-05-17 13:58:30,637] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-17 13:58:30,645] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-17 13:58:30,818] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
[2021-05-17 13:58:30,820] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-17 13:58:30,820] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-17 13:58:30,822] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (bb4563f4559d, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,824] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 188 ms on bb4563f4559d (executor driver) (24/90)
[2021-05-17 13:58:30,825] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 202 ms on bb4563f4559d (executor driver) (25/90)
[2021-05-17 13:58:30,826] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-17 13:58:30,828] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-17 13:58:30,830] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (bb4563f4559d, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,832] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (bb4563f4559d, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,833] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-17 13:58:30,833] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-17 13:58:30,835] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (bb4563f4559d, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:30,836] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 218 ms on bb4563f4559d (executor driver) (26/90)
[2021-05-17 13:58:30,836] {docker.py:276} INFO - 21/05/17 16:58:30 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 205 ms on bb4563f4559d (executor driver) (27/90)
[2021-05-17 13:58:30,843] {docker.py:276} INFO - 21/05/17 16:58:30 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-17 13:58:31,010] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-17 13:58:31,012] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
21/05/17 16:58:31 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (bb4563f4559d, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,014] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-17 13:58:31,016] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (bb4563f4559d, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,017] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 196 ms on bb4563f4559d (executor driver) (28/90)
[2021-05-17 13:58:31,018] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-17 13:58:31,025] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-17 13:58:31,026] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 187 ms on bb4563f4559d (executor driver) (29/90)
[2021-05-17 13:58:31,027] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-17 13:58:31,029] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (bb4563f4559d, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,031] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 202 ms on bb4563f4559d (executor driver) (30/90)
[2021-05-17 13:58:31,032] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-17 13:58:31,033] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (bb4563f4559d, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,034] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 199 ms on bb4563f4559d (executor driver) (31/90)
[2021-05-17 13:58:31,035] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-17 13:58:31,200] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-17 13:58:31,202] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (bb4563f4559d, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,203] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 192 ms on bb4563f4559d (executor driver) (32/90)
[2021-05-17 13:58:31,204] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-17 13:58:31,206] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-17 13:58:31,208] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (bb4563f4559d, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,209] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 195 ms on bb4563f4559d (executor driver) (33/90)
[2021-05-17 13:58:31,219] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-17 13:58:31,220] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1886 bytes result sent to driver
[2021-05-17 13:58:31,221] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (bb4563f4559d, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,222] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1886 bytes result sent to driver
[2021-05-17 13:58:31,224] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (bb4563f4559d, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,225] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 196 ms on bb4563f4559d (executor driver) (34/90)
[2021-05-17 13:58:31,226] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 193 ms on bb4563f4559d (executor driver) (35/90)
[2021-05-17 13:58:31,226] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-17 13:58:31,233] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-17 13:58:31,402] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-17 13:58:31,403] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
[2021-05-17 13:58:31,405] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (bb4563f4559d, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,408] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 199 ms on bb4563f4559d (executor driver) (36/90)
[2021-05-17 13:58:31,410] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-17 13:58:31,411] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (bb4563f4559d, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,412] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 188 ms on bb4563f4559d (executor driver) (37/90)
[2021-05-17 13:58:31,412] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-17 13:58:31,424] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-17 13:58:31,424] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1886 bytes result sent to driver
[2021-05-17 13:58:31,426] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (bb4563f4559d, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,427] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 205 ms on bb4563f4559d (executor driver) (38/90)
[2021-05-17 13:58:31,427] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-17 13:58:31,432] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (bb4563f4559d, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,433] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-17 13:58:31,433] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 231 ms on bb4563f4559d (executor driver) (39/90)
[2021-05-17 13:58:31,593] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1843 bytes result sent to driver
[2021-05-17 13:58:31,595] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (bb4563f4559d, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,596] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 193 ms on bb4563f4559d (executor driver) (40/90)
[2021-05-17 13:58:31,600] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
21/05/17 16:58:31 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-17 13:58:31,603] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-17 13:58:31,604] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (bb4563f4559d, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,605] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
21/05/17 16:58:31 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (bb4563f4559d, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,606] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 182 ms on bb4563f4559d (executor driver) (41/90)
[2021-05-17 13:58:31,607] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-17 13:58:31,608] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 198 ms on bb4563f4559d (executor driver) (42/90)
[2021-05-17 13:58:31,613] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-17 13:58:31,614] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (bb4563f4559d, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,615] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 187 ms on bb4563f4559d (executor driver) (43/90)
21/05/17 16:58:31 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-17 13:58:31,792] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-17 13:58:31,793] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-17 13:58:31,795] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (bb4563f4559d, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,796] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 191 ms on bb4563f4559d (executor driver) (44/90)
[2021-05-17 13:58:31,798] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-17 13:58:31,799] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-17 13:58:31,800] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (bb4563f4559d, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,801] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 207 ms on bb4563f4559d (executor driver) (45/90)
[2021-05-17 13:58:31,802] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-17 13:58:31,804] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (bb4563f4559d, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,809] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-17 13:58:31,809] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 205 ms on bb4563f4559d (executor driver) (46/90)
[2021-05-17 13:58:31,857] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-17 13:58:31,859] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (bb4563f4559d, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:31,861] {docker.py:276} INFO - 21/05/17 16:58:31 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-17 13:58:31,862] {docker.py:276} INFO - 21/05/17 16:58:31 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 247 ms on bb4563f4559d (executor driver) (47/90)
[2021-05-17 13:58:31,980] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-17 13:58:31,981] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-17 13:58:31,987] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (bb4563f4559d, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 16:58:32 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (bb4563f4559d, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 16:58:32 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
21/05/17 16:58:32 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 190 ms on bb4563f4559d (executor driver) (48/90)
21/05/17 16:58:32 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 182 ms on bb4563f4559d (executor driver) (49/90)
21/05/17 16:58:32 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-17 13:58:32,001] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1886 bytes result sent to driver
[2021-05-17 13:58:32,002] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (bb4563f4559d, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,003] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 205 ms on bb4563f4559d (executor driver) (50/90)
[2021-05-17 13:58:32,003] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-17 13:58:32,040] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1886 bytes result sent to driver
[2021-05-17 13:58:32,042] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (bb4563f4559d, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,043] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-17 13:58:32,044] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 185 ms on bb4563f4559d (executor driver) (51/90)
[2021-05-17 13:58:32,173] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1886 bytes result sent to driver
[2021-05-17 13:58:32,176] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1886 bytes result sent to driver
[2021-05-17 13:58:32,177] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (bb4563f4559d, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,178] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1843 bytes result sent to driver
21/05/17 16:58:32 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 195 ms on bb4563f4559d (executor driver) (52/90)
[2021-05-17 13:58:32,179] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
21/05/17 16:58:32 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (bb4563f4559d, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,180] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 200 ms on bb4563f4559d (executor driver) (53/90)
[2021-05-17 13:58:32,181] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-17 13:58:32,184] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (bb4563f4559d, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,185] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 183 ms on bb4563f4559d (executor driver) (54/90)
[2021-05-17 13:58:32,185] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-17 13:58:32,230] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1843 bytes result sent to driver
[2021-05-17 13:58:32,231] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (bb4563f4559d, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,232] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-17 13:58:32,233] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 191 ms on bb4563f4559d (executor driver) (55/90)
[2021-05-17 13:58:32,357] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
[2021-05-17 13:58:32,359] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (bb4563f4559d, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,361] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-17 13:58:32,362] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-17 13:58:32,363] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (bb4563f4559d, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,364] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-17 13:58:32,365] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 181 ms on bb4563f4559d (executor driver) (56/90)
[2021-05-17 13:58:32,366] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 186 ms on bb4563f4559d (executor driver) (57/90)
[2021-05-17 13:58:32,368] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
[2021-05-17 13:58:32,369] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (bb4563f4559d, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,370] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 193 ms on bb4563f4559d (executor driver) (58/90)
[2021-05-17 13:58:32,372] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-17 13:58:32,412] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-17 13:58:32,413] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (bb4563f4559d, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,414] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 183 ms on bb4563f4559d (executor driver) (59/90)
[2021-05-17 13:58:32,414] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-17 13:58:32,545] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-17 13:58:32,546] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (bb4563f4559d, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 16:58:32 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-17 13:58:32,547] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
21/05/17 16:58:32 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (bb4563f4559d, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,549] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-17 13:58:32,551] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-17 13:58:32,552] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 182 ms on bb4563f4559d (executor driver) (60/90)
[2021-05-17 13:58:32,554] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (bb4563f4559d, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,555] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 196 ms on bb4563f4559d (executor driver) (61/90)
[2021-05-17 13:58:32,555] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-17 13:58:32,556] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 193 ms on bb4563f4559d (executor driver) (62/90)
[2021-05-17 13:58:32,593] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-17 13:58:32,594] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (bb4563f4559d, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,596] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-17 13:58:32,596] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 183 ms on bb4563f4559d (executor driver) (63/90)
[2021-05-17 13:58:32,749] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-17 13:58:32,749] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (bb4563f4559d, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,750] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
21/05/17 16:58:32 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
[2021-05-17 13:58:32,750] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 203 ms on bb4563f4559d (executor driver) (64/90)
[2021-05-17 13:58:32,758] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (bb4563f4559d, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,759] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-17 13:58:32,774] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 220 ms on bb4563f4559d (executor driver) (65/90)
21/05/17 16:58:32 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-17 13:58:32,775] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (bb4563f4559d, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,779] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1886 bytes result sent to driver
[2021-05-17 13:58:32,779] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-17 13:58:32,780] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (bb4563f4559d, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,780] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 186 ms on bb4563f4559d (executor driver) (66/90)
[2021-05-17 13:58:32,781] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 234 ms on bb4563f4559d (executor driver) (67/90)
[2021-05-17 13:58:32,783] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-17 13:58:32,954] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1886 bytes result sent to driver
[2021-05-17 13:58:32,957] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-17 13:58:32,959] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-17 13:58:32,959] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (bb4563f4559d, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,961] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 220 ms on bb4563f4559d (executor driver) (68/90)
21/05/17 16:58:32 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-17 13:58:32,962] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 212 ms on bb4563f4559d (executor driver) (69/90)
[2021-05-17 13:58:32,963] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (bb4563f4559d, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,965] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
21/05/17 16:58:32 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (bb4563f4559d, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,966] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 188 ms on bb4563f4559d (executor driver) (70/90)
[2021-05-17 13:58:32,970] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-17 13:58:32,971] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-17 13:58:32,972] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (bb4563f4559d, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:32,973] {docker.py:276} INFO - 21/05/17 16:58:32 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 197 ms on bb4563f4559d (executor driver) (71/90)
[2021-05-17 13:58:32,975] {docker.py:276} INFO - 21/05/17 16:58:32 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-17 13:58:33,145] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-17 13:58:33,146] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-17 13:58:33,148] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (bb4563f4559d, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,149] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-17 13:58:33,151] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-17 13:58:33,152] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (bb4563f4559d, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,154] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-17 13:58:33,155] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (bb4563f4559d, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,156] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 192 ms on bb4563f4559d (executor driver) (72/90)
[2021-05-17 13:58:33,157] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-17 13:58:33,158] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 195 ms on bb4563f4559d (executor driver) (73/90)
[2021-05-17 13:58:33,160] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 202 ms on bb4563f4559d (executor driver) (74/90)
[2021-05-17 13:58:33,162] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1843 bytes result sent to driver
[2021-05-17 13:58:33,163] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (bb4563f4559d, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,164] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 194 ms on bb4563f4559d (executor driver) (75/90)
[2021-05-17 13:58:33,165] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-17 13:58:33,334] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1843 bytes result sent to driver
[2021-05-17 13:58:33,336] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-17 13:58:33,338] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (bb4563f4559d, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,339] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-17 13:58:33,341] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-17 13:58:33,342] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (bb4563f4559d, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,344] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 196 ms on bb4563f4559d (executor driver) (76/90)
21/05/17 16:58:33 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-17 13:58:33,344] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 182 ms on bb4563f4559d (executor driver) (77/90)
[2021-05-17 13:58:33,347] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (bb4563f4559d, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,347] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-17 13:58:33,348] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 194 ms on bb4563f4559d (executor driver) (78/90)
[2021-05-17 13:58:33,366] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-17 13:58:33,368] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (bb4563f4559d, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,369] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 218 ms on bb4563f4559d (executor driver) (79/90)
[2021-05-17 13:58:33,369] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-17 13:58:33,517] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-17 13:58:33,518] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (bb4563f4559d, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,519] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 184 ms on bb4563f4559d (executor driver) (80/90)
[2021-05-17 13:58:33,522] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
21/05/17 16:58:33 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-17 13:58:33,523] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (bb4563f4559d, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,523] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 182 ms on bb4563f4559d (executor driver) (81/90)
[2021-05-17 13:58:33,524] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
[2021-05-17 13:58:33,525] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-17 13:58:33,525] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 179 ms on bb4563f4559d (executor driver) (82/90)
[2021-05-17 13:58:33,526] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (bb4563f4559d, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,540] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-17 13:58:33,552] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1886 bytes result sent to driver
[2021-05-17 13:58:33,572] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (bb4563f4559d, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,573] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 187 ms on bb4563f4559d (executor driver) (83/90)
[2021-05-17 13:58:33,573] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-17 13:58:33,713] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1886 bytes result sent to driver
[2021-05-17 13:58:33,714] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (bb4563f4559d, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,716] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1886 bytes result sent to driver
[2021-05-17 13:58:33,717] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 199 ms on bb4563f4559d (executor driver) (84/90)
[2021-05-17 13:58:33,717] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-17 13:58:33,718] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-17 13:58:33,719] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (bb4563f4559d, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,721] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 199 ms on bb4563f4559d (executor driver) (85/90)
[2021-05-17 13:58:33,724] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (bb4563f4559d, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:33,725] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 199 ms on bb4563f4559d (executor driver) (86/90)
[2021-05-17 13:58:33,726] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
21/05/17 16:58:33 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-17 13:58:33,731] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
[2021-05-17 13:58:33,738] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 185 ms on bb4563f4559d (executor driver) (87/90)
[2021-05-17 13:58:33,899] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1843 bytes result sent to driver
[2021-05-17 13:58:33,900] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 177 ms on bb4563f4559d (executor driver) (88/90)
[2021-05-17 13:58:33,902] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1843 bytes result sent to driver
[2021-05-17 13:58:33,903] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 189 ms on bb4563f4559d (executor driver) (89/90)
[2021-05-17 13:58:33,908] {docker.py:276} INFO - 21/05/17 16:58:33 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1843 bytes result sent to driver
[2021-05-17 13:58:33,909] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 190 ms on bb4563f4559d (executor driver) (90/90)
[2021-05-17 13:58:33,913] {docker.py:276} INFO - 21/05/17 16:58:33 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 5.432 s
[2021-05-17 13:58:33,914] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-17 13:58:33,919] {docker.py:276} INFO - 21/05/17 16:58:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-17 13:58:33,920] {docker.py:276} INFO - 21/05/17 16:58:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-17 13:58:33,924] {docker.py:276} INFO - 21/05/17 16:58:33 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 5.512840 s
[2021-05-17 13:58:33,968] {docker.py:276} INFO - 21/05/17 16:58:33 INFO InMemoryFileIndex: It took 6127 ms to list leaf files for 90 paths.
[2021-05-17 13:58:34,091] {docker.py:276} INFO - 21/05/17 16:58:34 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 90 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621215193_to_1621216662.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621216662_to_1621218462.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621218462_to_1621220262.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621220262_to_1621222062.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621222062_to_1621223862.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621223862_to_1621225662.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621225662_to_1621227462.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621227462_to_1621229262.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621229262_to_1621231062.csv, s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621231062_to_1621232862.csv.
[2021-05-17 13:58:34,143] {docker.py:276} INFO - 21/05/17 16:58:34 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:34,145] {docker.py:276} INFO - 21/05/17 16:58:34 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 90 output partitions
21/05/17 16:58:34 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 13:58:34,145] {docker.py:276} INFO - 21/05/17 16:58:34 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 13:58:34,146] {docker.py:276} INFO - 21/05/17 16:58:34 INFO DAGScheduler: Missing parents: List()
[2021-05-17 13:58:34,147] {docker.py:276} INFO - 21/05/17 16:58:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 13:58:34,165] {docker.py:276} INFO - 21/05/17 16:58:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-17 13:58:34,180] {docker.py:276} INFO - 21/05/17 16:58:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-17 13:58:34,181] {docker.py:276} INFO - 21/05/17 16:58:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on bb4563f4559d:33263 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-17 13:58:34,182] {docker.py:276} INFO - 21/05/17 16:58:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-17 13:58:34,186] {docker.py:276} INFO - 21/05/17 16:58:34 INFO DAGScheduler: Submitting 90 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-17 13:58:34,187] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 90 tasks resource profile 0
[2021-05-17 13:58:34,192] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 90) (bb4563f4559d, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,193] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 91) (bb4563f4559d, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,193] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 92) (bb4563f4559d, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,195] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 93) (bb4563f4559d, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,195] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 1.0 in stage 1.0 (TID 91)
21/05/17 16:58:34 INFO Executor: Running task 2.0 in stage 1.0 (TID 92)
[2021-05-17 13:58:34,196] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 90)
[2021-05-17 13:58:34,197] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 3.0 in stage 1.0 (TID 93)
[2021-05-17 13:58:34,370] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 1.0 in stage 1.0 (TID 91). 1843 bytes result sent to driver
[2021-05-17 13:58:34,371] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 94) (bb4563f4559d, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,372] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 3.0 in stage 1.0 (TID 93). 1843 bytes result sent to driver
[2021-05-17 13:58:34,372] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 91) in 179 ms on bb4563f4559d (executor driver) (1/90)
[2021-05-17 13:58:34,372] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 4.0 in stage 1.0 (TID 94)
[2021-05-17 13:58:34,373] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 95) (bb4563f4559d, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,374] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 93) in 180 ms on bb4563f4559d (executor driver) (2/90)
[2021-05-17 13:58:34,375] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 2.0 in stage 1.0 (TID 92). 1843 bytes result sent to driver
[2021-05-17 13:58:34,376] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 5.0 in stage 1.0 (TID 95)
[2021-05-17 13:58:34,377] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 96) (bb4563f4559d, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,379] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 92) in 185 ms on bb4563f4559d (executor driver) (3/90)
[2021-05-17 13:58:34,380] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 6.0 in stage 1.0 (TID 96)
[2021-05-17 13:58:34,381] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 90). 1843 bytes result sent to driver
[2021-05-17 13:58:34,383] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 97) (bb4563f4559d, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,383] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 7.0 in stage 1.0 (TID 97)
[2021-05-17 13:58:34,384] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 90) in 192 ms on bb4563f4559d (executor driver) (4/90)
[2021-05-17 13:58:34,553] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 5.0 in stage 1.0 (TID 95). 1843 bytes result sent to driver
[2021-05-17 13:58:34,554] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 98) (bb4563f4559d, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,555] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 4.0 in stage 1.0 (TID 94). 1843 bytes result sent to driver
21/05/17 16:58:34 INFO Executor: Finished task 6.0 in stage 1.0 (TID 96). 1843 bytes result sent to driver
[2021-05-17 13:58:34,556] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 8.0 in stage 1.0 (TID 98)
[2021-05-17 13:58:34,557] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 99) (bb4563f4559d, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,559] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 9.0 in stage 1.0 (TID 99)
[2021-05-17 13:58:34,561] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 7.0 in stage 1.0 (TID 97). 1843 bytes result sent to driver
[2021-05-17 13:58:34,562] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 100) (bb4563f4559d, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,563] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 94) in 193 ms on bb4563f4559d (executor driver) (5/90)
[2021-05-17 13:58:34,563] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 96) in 187 ms on bb4563f4559d (executor driver) (6/90)
[2021-05-17 13:58:34,565] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 95) in 192 ms on bb4563f4559d (executor driver) (7/90)
[2021-05-17 13:58:34,567] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 101) (bb4563f4559d, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,568] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 97) in 186 ms on bb4563f4559d (executor driver) (8/90)
[2021-05-17 13:58:34,568] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 11.0 in stage 1.0 (TID 101)
[2021-05-17 13:58:34,569] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 10.0 in stage 1.0 (TID 100)
[2021-05-17 13:58:34,732] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 8.0 in stage 1.0 (TID 98). 1843 bytes result sent to driver
[2021-05-17 13:58:34,733] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 102) (bb4563f4559d, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,735] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 12.0 in stage 1.0 (TID 102)
[2021-05-17 13:58:34,736] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 98) in 182 ms on bb4563f4559d (executor driver) (9/90)
[2021-05-17 13:58:34,738] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 9.0 in stage 1.0 (TID 99). 1843 bytes result sent to driver
[2021-05-17 13:58:34,739] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 103) (bb4563f4559d, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,740] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 99) in 183 ms on bb4563f4559d (executor driver) (10/90)
[2021-05-17 13:58:34,741] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 13.0 in stage 1.0 (TID 103)
[2021-05-17 13:58:34,743] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 11.0 in stage 1.0 (TID 101). 1843 bytes result sent to driver
[2021-05-17 13:58:34,744] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 104) (bb4563f4559d, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,745] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 101) in 178 ms on bb4563f4559d (executor driver) (11/90)
[2021-05-17 13:58:34,749] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 10.0 in stage 1.0 (TID 100). 1886 bytes result sent to driver
[2021-05-17 13:58:34,749] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 14.0 in stage 1.0 (TID 104)
[2021-05-17 13:58:34,750] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 105) (bb4563f4559d, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,751] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 15.0 in stage 1.0 (TID 105)
[2021-05-17 13:58:34,752] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 100) in 193 ms on bb4563f4559d (executor driver) (12/90)
[2021-05-17 13:58:34,911] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 13.0 in stage 1.0 (TID 103). 1843 bytes result sent to driver
[2021-05-17 13:58:34,914] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 106) (bb4563f4559d, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,915] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 12.0 in stage 1.0 (TID 102). 1843 bytes result sent to driver
[2021-05-17 13:58:34,916] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 16.0 in stage 1.0 (TID 106)
[2021-05-17 13:58:34,916] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 107) (bb4563f4559d, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,918] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 102) in 186 ms on bb4563f4559d (executor driver) (13/90)
[2021-05-17 13:58:34,918] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 103) in 179 ms on bb4563f4559d (executor driver) (14/90)
[2021-05-17 13:58:34,919] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 17.0 in stage 1.0 (TID 107)
[2021-05-17 13:58:34,938] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 14.0 in stage 1.0 (TID 104). 1886 bytes result sent to driver
[2021-05-17 13:58:34,939] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 108) (bb4563f4559d, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,940] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 104) in 196 ms on bb4563f4559d (executor driver) (15/90)
[2021-05-17 13:58:34,941] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 18.0 in stage 1.0 (TID 108)
[2021-05-17 13:58:34,943] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Finished task 15.0 in stage 1.0 (TID 105). 1886 bytes result sent to driver
[2021-05-17 13:58:34,944] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 109) (bb4563f4559d, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:34,946] {docker.py:276} INFO - 21/05/17 16:58:34 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 105) in 196 ms on bb4563f4559d (executor driver) (16/90)
[2021-05-17 13:58:34,946] {docker.py:276} INFO - 21/05/17 16:58:34 INFO Executor: Running task 19.0 in stage 1.0 (TID 109)
[2021-05-17 13:58:35,116] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 16.0 in stage 1.0 (TID 106). 1886 bytes result sent to driver
[2021-05-17 13:58:35,117] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 19.0 in stage 1.0 (TID 109). 1843 bytes result sent to driver
[2021-05-17 13:58:35,120] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 110) (bb4563f4559d, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,122] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 20.0 in stage 1.0 (TID 110)
[2021-05-17 13:58:35,122] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 18.0 in stage 1.0 (TID 108). 1843 bytes result sent to driver
[2021-05-17 13:58:35,123] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 111) (bb4563f4559d, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,125] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 106) in 212 ms on bb4563f4559d (executor driver) (17/90)
[2021-05-17 13:58:35,125] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 109) in 181 ms on bb4563f4559d (executor driver) (18/90)
[2021-05-17 13:58:35,126] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 17.0 in stage 1.0 (TID 107). 1843 bytes result sent to driver
[2021-05-17 13:58:35,128] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 112) (bb4563f4559d, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,129] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 108) in 191 ms on bb4563f4559d (executor driver) (19/90)
[2021-05-17 13:58:35,130] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 22.0 in stage 1.0 (TID 112)
21/05/17 16:58:35 INFO Executor: Running task 21.0 in stage 1.0 (TID 111)
[2021-05-17 13:58:35,131] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 113) (bb4563f4559d, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,132] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 107) in 217 ms on bb4563f4559d (executor driver) (20/90)
[2021-05-17 13:58:35,133] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 23.0 in stage 1.0 (TID 113)
[2021-05-17 13:58:35,303] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 23.0 in stage 1.0 (TID 113). 1843 bytes result sent to driver
[2021-05-17 13:58:35,303] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 20.0 in stage 1.0 (TID 110). 1843 bytes result sent to driver
21/05/17 16:58:35 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 114) (bb4563f4559d, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,304] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 113) in 173 ms on bb4563f4559d (executor driver) (21/90)
[2021-05-17 13:58:35,305] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 24.0 in stage 1.0 (TID 114)
[2021-05-17 13:58:35,306] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 115) (bb4563f4559d, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,307] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 110) in 189 ms on bb4563f4559d (executor driver) (22/90)
[2021-05-17 13:58:35,308] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 22.0 in stage 1.0 (TID 112). 1886 bytes result sent to driver
[2021-05-17 13:58:35,309] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 25.0 in stage 1.0 (TID 115)
[2021-05-17 13:58:35,311] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 116) (bb4563f4559d, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,312] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 112) in 184 ms on bb4563f4559d (executor driver) (23/90)
[2021-05-17 13:58:35,313] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 26.0 in stage 1.0 (TID 116)
[2021-05-17 13:58:35,314] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 21.0 in stage 1.0 (TID 111). 1843 bytes result sent to driver
[2021-05-17 13:58:35,315] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 117) (bb4563f4559d, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,316] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 111) in 195 ms on bb4563f4559d (executor driver) (24/90)
[2021-05-17 13:58:35,317] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 27.0 in stage 1.0 (TID 117)
[2021-05-17 13:58:35,491] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 25.0 in stage 1.0 (TID 115). 1843 bytes result sent to driver
[2021-05-17 13:58:35,492] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 118) (bb4563f4559d, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,493] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 115) in 188 ms on bb4563f4559d (executor driver) (25/90)
[2021-05-17 13:58:35,494] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 28.0 in stage 1.0 (TID 118)
[2021-05-17 13:58:35,497] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 27.0 in stage 1.0 (TID 117). 1843 bytes result sent to driver
[2021-05-17 13:58:35,499] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 119) (bb4563f4559d, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,499] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 117) in 185 ms on bb4563f4559d (executor driver) (26/90)
[2021-05-17 13:58:35,500] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 29.0 in stage 1.0 (TID 119)
[2021-05-17 13:58:35,507] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 24.0 in stage 1.0 (TID 114). 1843 bytes result sent to driver
[2021-05-17 13:58:35,507] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 120) (bb4563f4559d, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,509] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 114) in 206 ms on bb4563f4559d (executor driver) (27/90)
[2021-05-17 13:58:35,509] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 30.0 in stage 1.0 (TID 120)
[2021-05-17 13:58:35,512] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 26.0 in stage 1.0 (TID 116). 1843 bytes result sent to driver
[2021-05-17 13:58:35,513] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 121) (bb4563f4559d, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,515] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 31.0 in stage 1.0 (TID 121)
[2021-05-17 13:58:35,515] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 116) in 205 ms on bb4563f4559d (executor driver) (28/90)
[2021-05-17 13:58:35,673] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 28.0 in stage 1.0 (TID 118). 1843 bytes result sent to driver
[2021-05-17 13:58:35,674] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 122) (bb4563f4559d, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,675] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 118) in 184 ms on bb4563f4559d (executor driver) (29/90)
[2021-05-17 13:58:35,677] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 32.0 in stage 1.0 (TID 122)
[2021-05-17 13:58:35,691] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 31.0 in stage 1.0 (TID 121). 1886 bytes result sent to driver
[2021-05-17 13:58:35,693] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 123) (bb4563f4559d, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,694] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 33.0 in stage 1.0 (TID 123)
21/05/17 16:58:35 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 121) in 181 ms on bb4563f4559d (executor driver) (30/90)
[2021-05-17 13:58:35,695] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 30.0 in stage 1.0 (TID 120). 1886 bytes result sent to driver
[2021-05-17 13:58:35,699] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 124) (bb4563f4559d, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,699] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 34.0 in stage 1.0 (TID 124)
21/05/17 16:58:35 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 120) in 191 ms on bb4563f4559d (executor driver) (31/90)
[2021-05-17 13:58:35,710] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 29.0 in stage 1.0 (TID 119). 1886 bytes result sent to driver
[2021-05-17 13:58:35,712] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 125) (bb4563f4559d, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,714] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 119) in 215 ms on bb4563f4559d (executor driver) (32/90)
21/05/17 16:58:35 INFO Executor: Running task 35.0 in stage 1.0 (TID 125)
[2021-05-17 13:58:35,789] {docker.py:276} INFO - 21/05/17 16:58:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on bb4563f4559d:33263 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-17 13:58:35,875] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 33.0 in stage 1.0 (TID 123). 1843 bytes result sent to driver
[2021-05-17 13:58:35,877] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 126) (bb4563f4559d, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,880] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 123) in 187 ms on bb4563f4559d (executor driver) (33/90)
[2021-05-17 13:58:35,881] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 36.0 in stage 1.0 (TID 126)
[2021-05-17 13:58:35,882] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 32.0 in stage 1.0 (TID 122). 1886 bytes result sent to driver
[2021-05-17 13:58:35,885] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 34.0 in stage 1.0 (TID 124). 1843 bytes result sent to driver
[2021-05-17 13:58:35,887] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 127) (bb4563f4559d, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,889] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 122) in 215 ms on bb4563f4559d (executor driver) (34/90)
[2021-05-17 13:58:35,891] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 128) (bb4563f4559d, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,892] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 37.0 in stage 1.0 (TID 127)
[2021-05-17 13:58:35,892] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 124) in 196 ms on bb4563f4559d (executor driver) (35/90)
[2021-05-17 13:58:35,893] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Finished task 35.0 in stage 1.0 (TID 125). 1843 bytes result sent to driver
[2021-05-17 13:58:35,894] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 129) (bb4563f4559d, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:35,894] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 38.0 in stage 1.0 (TID 128)
[2021-05-17 13:58:35,895] {docker.py:276} INFO - 21/05/17 16:58:35 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 125) in 183 ms on bb4563f4559d (executor driver) (36/90)
[2021-05-17 13:58:35,895] {docker.py:276} INFO - 21/05/17 16:58:35 INFO Executor: Running task 39.0 in stage 1.0 (TID 129)
[2021-05-17 13:58:36,067] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 36.0 in stage 1.0 (TID 126). 1843 bytes result sent to driver
[2021-05-17 13:58:36,069] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 130) (bb4563f4559d, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,071] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 40.0 in stage 1.0 (TID 130)
[2021-05-17 13:58:36,072] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 126) in 196 ms on bb4563f4559d (executor driver) (37/90)
[2021-05-17 13:58:36,074] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 39.0 in stage 1.0 (TID 129). 1843 bytes result sent to driver
21/05/17 16:58:36 INFO Executor: Finished task 38.0 in stage 1.0 (TID 128). 1843 bytes result sent to driver
[2021-05-17 13:58:36,075] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 37.0 in stage 1.0 (TID 127). 1886 bytes result sent to driver
21/05/17 16:58:36 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 131) (bb4563f4559d, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,076] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 41.0 in stage 1.0 (TID 131)
[2021-05-17 13:58:36,077] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 128) in 186 ms on bb4563f4559d (executor driver) (38/90)
[2021-05-17 13:58:36,078] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 129) in 185 ms on bb4563f4559d (executor driver) (39/90)
[2021-05-17 13:58:36,082] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 132) (bb4563f4559d, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,084] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 42.0 in stage 1.0 (TID 132)
[2021-05-17 13:58:36,085] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 133) (bb4563f4559d, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,086] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 127) in 202 ms on bb4563f4559d (executor driver) (40/90)
[2021-05-17 13:58:36,088] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 43.0 in stage 1.0 (TID 133)
[2021-05-17 13:58:36,257] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 40.0 in stage 1.0 (TID 130). 1843 bytes result sent to driver
[2021-05-17 13:58:36,259] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 41.0 in stage 1.0 (TID 131). 1843 bytes result sent to driver
[2021-05-17 13:58:36,260] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 134) (bb4563f4559d, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,261] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 130) in 192 ms on bb4563f4559d (executor driver) (41/90)
[2021-05-17 13:58:36,263] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 42.0 in stage 1.0 (TID 132). 1843 bytes result sent to driver
[2021-05-17 13:58:36,264] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 44.0 in stage 1.0 (TID 134)
[2021-05-17 13:58:36,265] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 135) (bb4563f4559d, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,266] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 136) (bb4563f4559d, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,267] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 46.0 in stage 1.0 (TID 136)
[2021-05-17 13:58:36,268] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 132) in 185 ms on bb4563f4559d (executor driver) (42/90)
[2021-05-17 13:58:36,269] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 131) in 193 ms on bb4563f4559d (executor driver) (43/90)
[2021-05-17 13:58:36,270] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 45.0 in stage 1.0 (TID 135)
[2021-05-17 13:58:36,318] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 43.0 in stage 1.0 (TID 133). 1843 bytes result sent to driver
[2021-05-17 13:58:36,319] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 137) (bb4563f4559d, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,320] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 133) in 233 ms on bb4563f4559d (executor driver) (44/90)
[2021-05-17 13:58:36,321] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 47.0 in stage 1.0 (TID 137)
[2021-05-17 13:58:36,445] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 45.0 in stage 1.0 (TID 135). 1843 bytes result sent to driver
[2021-05-17 13:58:36,446] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 44.0 in stage 1.0 (TID 134). 1843 bytes result sent to driver
[2021-05-17 13:58:36,448] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 138) (bb4563f4559d, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,450] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 135) in 185 ms on bb4563f4559d (executor driver) (45/90)
21/05/17 16:58:36 INFO Executor: Running task 48.0 in stage 1.0 (TID 138)
[2021-05-17 13:58:36,451] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 139) (bb4563f4559d, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,460] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 49.0 in stage 1.0 (TID 139)
[2021-05-17 13:58:36,460] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 134) in 193 ms on bb4563f4559d (executor driver) (46/90)
[2021-05-17 13:58:36,461] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 46.0 in stage 1.0 (TID 136). 1886 bytes result sent to driver
[2021-05-17 13:58:36,465] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 140) (bb4563f4559d, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,465] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 50.0 in stage 1.0 (TID 140)
[2021-05-17 13:58:36,466] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 136) in 198 ms on bb4563f4559d (executor driver) (47/90)
[2021-05-17 13:58:36,496] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 47.0 in stage 1.0 (TID 137). 1886 bytes result sent to driver
[2021-05-17 13:58:36,496] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 141) (bb4563f4559d, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,497] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 137) in 182 ms on bb4563f4559d (executor driver) (48/90)
[2021-05-17 13:58:36,498] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 51.0 in stage 1.0 (TID 141)
[2021-05-17 13:58:36,631] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 50.0 in stage 1.0 (TID 140). 1843 bytes result sent to driver
[2021-05-17 13:58:36,633] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 142) (bb4563f4559d, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,634] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 140) in 171 ms on bb4563f4559d (executor driver) (49/90)
[2021-05-17 13:58:36,635] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 49.0 in stage 1.0 (TID 139). 1843 bytes result sent to driver
[2021-05-17 13:58:36,636] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 48.0 in stage 1.0 (TID 138). 1886 bytes result sent to driver
[2021-05-17 13:58:36,637] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 52.0 in stage 1.0 (TID 142)
[2021-05-17 13:58:36,638] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 143) (bb4563f4559d, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,639] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 53.0 in stage 1.0 (TID 143)
[2021-05-17 13:58:36,640] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 144) (bb4563f4559d, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,641] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 139) in 190 ms on bb4563f4559d (executor driver) (50/90)
[2021-05-17 13:58:36,642] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 138) in 195 ms on bb4563f4559d (executor driver) (51/90)
[2021-05-17 13:58:36,642] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 54.0 in stage 1.0 (TID 144)
[2021-05-17 13:58:36,680] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 51.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-17 13:58:36,681] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 145) (bb4563f4559d, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,683] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 55.0 in stage 1.0 (TID 145)
[2021-05-17 13:58:36,683] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 141) in 186 ms on bb4563f4559d (executor driver) (52/90)
[2021-05-17 13:58:36,818] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 53.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-17 13:58:36,819] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 54.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-17 13:58:36,821] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 146) (bb4563f4559d, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,822] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 56.0 in stage 1.0 (TID 146)
[2021-05-17 13:58:36,823] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 147) (bb4563f4559d, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,825] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 57.0 in stage 1.0 (TID 147)
21/05/17 16:58:36 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 143) in 188 ms on bb4563f4559d (executor driver) (53/90)
[2021-05-17 13:58:36,826] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 144) in 186 ms on bb4563f4559d (executor driver) (54/90)
[2021-05-17 13:58:36,829] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 52.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-17 13:58:36,830] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 148) (bb4563f4559d, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,831] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 142) in 199 ms on bb4563f4559d (executor driver) (55/90)
[2021-05-17 13:58:36,833] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 58.0 in stage 1.0 (TID 148)
[2021-05-17 13:58:36,856] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Finished task 55.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
[2021-05-17 13:58:36,858] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 149) (bb4563f4559d, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:36,859] {docker.py:276} INFO - 21/05/17 16:58:36 INFO Executor: Running task 59.0 in stage 1.0 (TID 149)
[2021-05-17 13:58:36,860] {docker.py:276} INFO - 21/05/17 16:58:36 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 145) in 178 ms on bb4563f4559d (executor driver) (56/90)
[2021-05-17 13:58:36,999] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 56.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-17 13:58:37,001] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 150) (bb4563f4559d, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,002] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 146) in 183 ms on bb4563f4559d (executor driver) (57/90)
[2021-05-17 13:58:37,003] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 60.0 in stage 1.0 (TID 150)
[2021-05-17 13:58:37,005] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 57.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-17 13:58:37,007] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 151) (bb4563f4559d, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,008] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 61.0 in stage 1.0 (TID 151)
[2021-05-17 13:58:37,009] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 147) in 187 ms on bb4563f4559d (executor driver) (58/90)
[2021-05-17 13:58:37,010] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 58.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-17 13:58:37,013] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 152) (bb4563f4559d, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,013] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 148) in 183 ms on bb4563f4559d (executor driver) (59/90)
[2021-05-17 13:58:37,014] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 62.0 in stage 1.0 (TID 152)
[2021-05-17 13:58:37,032] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 59.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
[2021-05-17 13:58:37,034] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 153) (bb4563f4559d, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,034] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 149) in 178 ms on bb4563f4559d (executor driver) (60/90)
[2021-05-17 13:58:37,036] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 63.0 in stage 1.0 (TID 153)
[2021-05-17 13:58:37,188] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 62.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-17 13:58:37,189] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 61.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-17 13:58:37,191] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 154) (bb4563f4559d, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,192] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 60.0 in stage 1.0 (TID 150). 1886 bytes result sent to driver
[2021-05-17 13:58:37,193] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 152) in 181 ms on bb4563f4559d (executor driver) (61/90)
[2021-05-17 13:58:37,194] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 64.0 in stage 1.0 (TID 154)
[2021-05-17 13:58:37,195] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 155) (bb4563f4559d, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,197] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 65.0 in stage 1.0 (TID 155)
[2021-05-17 13:58:37,198] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 156) (bb4563f4559d, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,199] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 151) in 193 ms on bb4563f4559d (executor driver) (62/90)
[2021-05-17 13:58:37,200] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 66.0 in stage 1.0 (TID 156)
[2021-05-17 13:58:37,201] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 150) in 199 ms on bb4563f4559d (executor driver) (63/90)
[2021-05-17 13:58:37,212] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 63.0 in stage 1.0 (TID 153). 1886 bytes result sent to driver
[2021-05-17 13:58:37,216] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 157) (bb4563f4559d, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,217] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 153) in 181 ms on bb4563f4559d (executor driver) (64/90)
21/05/17 16:58:37 INFO Executor: Running task 67.0 in stage 1.0 (TID 157)
[2021-05-17 13:58:37,385] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 64.0 in stage 1.0 (TID 154). 1886 bytes result sent to driver
[2021-05-17 13:58:37,386] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 67.0 in stage 1.0 (TID 157). 1843 bytes result sent to driver
[2021-05-17 13:58:37,387] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 66.0 in stage 1.0 (TID 156). 1886 bytes result sent to driver
[2021-05-17 13:58:37,389] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 158) (bb4563f4559d, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,390] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 65.0 in stage 1.0 (TID 155). 1886 bytes result sent to driver
21/05/17 16:58:37 INFO Executor: Running task 68.0 in stage 1.0 (TID 158)
[2021-05-17 13:58:37,391] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 159) (bb4563f4559d, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,393] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 69.0 in stage 1.0 (TID 159)
[2021-05-17 13:58:37,395] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 160) (bb4563f4559d, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,396] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 154) in 207 ms on bb4563f4559d (executor driver) (65/90)
[2021-05-17 13:58:37,396] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 156) in 199 ms on bb4563f4559d (executor driver) (66/90)
[2021-05-17 13:58:37,397] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 157) in 184 ms on bb4563f4559d (executor driver) (67/90)
[2021-05-17 13:58:37,399] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 70.0 in stage 1.0 (TID 160)
[2021-05-17 13:58:37,399] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 155) in 205 ms on bb4563f4559d (executor driver) (68/90)
[2021-05-17 13:58:37,400] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 161) (bb4563f4559d, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,401] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 71.0 in stage 1.0 (TID 161)
[2021-05-17 13:58:37,573] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 68.0 in stage 1.0 (TID 158). 1843 bytes result sent to driver
[2021-05-17 13:58:37,575] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 70.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
21/05/17 16:58:37 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 162) (bb4563f4559d, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,577] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 72.0 in stage 1.0 (TID 162)
[2021-05-17 13:58:37,578] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 163) (bb4563f4559d, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,579] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 160) in 184 ms on bb4563f4559d (executor driver) (69/90)
[2021-05-17 13:58:37,580] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 158) in 191 ms on bb4563f4559d (executor driver) (70/90)
[2021-05-17 13:58:37,582] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 69.0 in stage 1.0 (TID 159). 1843 bytes result sent to driver
[2021-05-17 13:58:37,583] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 73.0 in stage 1.0 (TID 163)
[2021-05-17 13:58:37,584] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 164) (bb4563f4559d, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,584] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 159) in 193 ms on bb4563f4559d (executor driver) (71/90)
[2021-05-17 13:58:37,585] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 74.0 in stage 1.0 (TID 164)
[2021-05-17 13:58:37,591] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 71.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-17 13:58:37,592] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 165) (bb4563f4559d, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,593] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 161) in 194 ms on bb4563f4559d (executor driver) (72/90)
[2021-05-17 13:58:37,594] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 75.0 in stage 1.0 (TID 165)
[2021-05-17 13:58:37,760] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 73.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-17 13:58:37,761] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 74.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-17 13:58:37,761] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 72.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-17 13:58:37,762] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 166) (bb4563f4559d, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,764] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 76.0 in stage 1.0 (TID 166)
[2021-05-17 13:58:37,768] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 167) (bb4563f4559d, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 16:58:37 INFO Executor: Running task 77.0 in stage 1.0 (TID 167)
[2021-05-17 13:58:37,769] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 168) (bb4563f4559d, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,770] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 163) in 194 ms on bb4563f4559d (executor driver) (73/90)
[2021-05-17 13:58:37,771] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 162) in 197 ms on bb4563f4559d (executor driver) (74/90)
[2021-05-17 13:58:37,771] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 78.0 in stage 1.0 (TID 168)
[2021-05-17 13:58:37,772] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 164) in 190 ms on bb4563f4559d (executor driver) (75/90)
[2021-05-17 13:58:37,774] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 75.0 in stage 1.0 (TID 165). 1843 bytes result sent to driver
[2021-05-17 13:58:37,775] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 169) (bb4563f4559d, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,776] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 165) in 184 ms on bb4563f4559d (executor driver) (76/90)
[2021-05-17 13:58:37,777] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 79.0 in stage 1.0 (TID 169)
[2021-05-17 13:58:37,941] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 78.0 in stage 1.0 (TID 168). 1843 bytes result sent to driver
[2021-05-17 13:58:37,943] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 170) (bb4563f4559d, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,945] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 168) in 175 ms on bb4563f4559d (executor driver) (77/90)
[2021-05-17 13:58:37,946] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 80.0 in stage 1.0 (TID 170)
[2021-05-17 13:58:37,950] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 76.0 in stage 1.0 (TID 166). 1843 bytes result sent to driver
[2021-05-17 13:58:37,951] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 171) (bb4563f4559d, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,952] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 166) in 191 ms on bb4563f4559d (executor driver) (78/90)
[2021-05-17 13:58:37,953] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 81.0 in stage 1.0 (TID 171)
[2021-05-17 13:58:37,954] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 77.0 in stage 1.0 (TID 167). 1843 bytes result sent to driver
[2021-05-17 13:58:37,954] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Finished task 79.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
[2021-05-17 13:58:37,955] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 172) (bb4563f4559d, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,962] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 173) (bb4563f4559d, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:37,963] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 167) in 198 ms on bb4563f4559d (executor driver) (79/90)
[2021-05-17 13:58:37,964] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 83.0 in stage 1.0 (TID 173)
[2021-05-17 13:58:37,965] {docker.py:276} INFO - 21/05/17 16:58:37 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 169) in 189 ms on bb4563f4559d (executor driver) (80/90)
[2021-05-17 13:58:37,966] {docker.py:276} INFO - 21/05/17 16:58:37 INFO Executor: Running task 82.0 in stage 1.0 (TID 172)
[2021-05-17 13:58:38,136] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 80.0 in stage 1.0 (TID 170). 1886 bytes result sent to driver
[2021-05-17 13:58:38,137] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 81.0 in stage 1.0 (TID 171). 1886 bytes result sent to driver
[2021-05-17 13:58:38,138] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 82.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
[2021-05-17 13:58:38,139] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 174) (bb4563f4559d, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:38,140] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Running task 84.0 in stage 1.0 (TID 174)
[2021-05-17 13:58:38,141] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 170) in 198 ms on bb4563f4559d (executor driver) (81/90)
[2021-05-17 13:58:38,142] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 83.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-17 13:58:38,143] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 175) (bb4563f4559d, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:38,144] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 172) in 189 ms on bb4563f4559d (executor driver) (82/90)
[2021-05-17 13:58:38,145] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 176) (bb4563f4559d, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:38,146] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Running task 86.0 in stage 1.0 (TID 176)
[2021-05-17 13:58:38,147] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 171) in 196 ms on bb4563f4559d (executor driver) (83/90)
[2021-05-17 13:58:38,152] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 173) in 186 ms on bb4563f4559d (executor driver) (84/90)
[2021-05-17 13:58:38,153] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 177) (bb4563f4559d, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:38,153] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Running task 87.0 in stage 1.0 (TID 177)
[2021-05-17 13:58:38,153] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Running task 85.0 in stage 1.0 (TID 175)
[2021-05-17 13:58:38,323] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 84.0 in stage 1.0 (TID 174). 1843 bytes result sent to driver
[2021-05-17 13:58:38,324] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 86.0 in stage 1.0 (TID 176). 1843 bytes result sent to driver
[2021-05-17 13:58:38,326] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 178) (bb4563f4559d, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:38,328] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 85.0 in stage 1.0 (TID 175). 1843 bytes result sent to driver
[2021-05-17 13:58:38,328] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Running task 88.0 in stage 1.0 (TID 178)
[2021-05-17 13:58:38,330] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 179) (bb4563f4559d, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:38,330] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 174) in 192 ms on bb4563f4559d (executor driver) (85/90)
[2021-05-17 13:58:38,331] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 176) in 187 ms on bb4563f4559d (executor driver) (86/90)
[2021-05-17 13:58:38,332] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 175) in 191 ms on bb4563f4559d (executor driver) (87/90)
[2021-05-17 13:58:38,333] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Running task 89.0 in stage 1.0 (TID 179)
[2021-05-17 13:58:38,334] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 87.0 in stage 1.0 (TID 177). 1843 bytes result sent to driver
[2021-05-17 13:58:38,335] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 177) in 186 ms on bb4563f4559d (executor driver) (88/90)
[2021-05-17 13:58:38,506] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 88.0 in stage 1.0 (TID 178). 1843 bytes result sent to driver
[2021-05-17 13:58:38,507] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 178) in 182 ms on bb4563f4559d (executor driver) (89/90)
[2021-05-17 13:58:38,513] {docker.py:276} INFO - 21/05/17 16:58:38 INFO Executor: Finished task 89.0 in stage 1.0 (TID 179). 1843 bytes result sent to driver
[2021-05-17 13:58:38,515] {docker.py:276} INFO - 21/05/17 16:58:38 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 179) in 187 ms on bb4563f4559d (executor driver) (90/90)
21/05/17 16:58:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-17 13:58:38,516] {docker.py:276} INFO - 21/05/17 16:58:38 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 4.367 s
[2021-05-17 13:58:38,516] {docker.py:276} INFO - 21/05/17 16:58:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 16:58:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-17 13:58:38,517] {docker.py:276} INFO - 21/05/17 16:58:38 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 4.379464 s
[2021-05-17 13:58:38,531] {docker.py:276} INFO - 21/05/17 16:58:38 INFO InMemoryFileIndex: It took 4447 ms to list leaf files for 90 paths.
[2021-05-17 13:58:38,839] {docker.py:276} INFO - 21/05/17 16:58:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on bb4563f4559d:33263 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-17 13:58:41,213] {docker.py:276} INFO - 21/05/17 16:58:41 INFO FileSourceStrategy: Pushed Filters:
[2021-05-17 13:58:41,219] {docker.py:276} INFO - 21/05/17 16:58:41 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-17 13:58:41,223] {docker.py:276} INFO - 21/05/17 16:58:41 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-17 13:58:41,745] {docker.py:276} INFO - 21/05/17 16:58:41 INFO CodeGenerator: Code generated in 294.624 ms
[2021-05-17 13:58:41,760] {docker.py:276} INFO - 21/05/17 16:58:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-17 13:58:41,778] {docker.py:276} INFO - 21/05/17 16:58:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-17 13:58:41,779] {docker.py:276} INFO - 21/05/17 16:58:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on bb4563f4559d:33263 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 13:58:41,781] {docker.py:276} INFO - 21/05/17 16:58:41 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:41,822] {docker.py:276} INFO - 21/05/17 16:58:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 96699482 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 13:58:41,926] {docker.py:276} INFO - 21/05/17 16:58:41 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:41,928] {docker.py:276} INFO - 21/05/17 16:58:41 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-17 13:58:41,928] {docker.py:276} INFO - 21/05/17 16:58:41 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 13:58:41,929] {docker.py:276} INFO - 21/05/17 16:58:41 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 13:58:41,929] {docker.py:276} INFO - 21/05/17 16:58:41 INFO DAGScheduler: Missing parents: List()
[2021-05-17 13:58:41,929] {docker.py:276} INFO - 21/05/17 16:58:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 13:58:41,957] {docker.py:276} INFO - 21/05/17 16:58:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-17 13:58:41,969] {docker.py:276} INFO - 21/05/17 16:58:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-17 13:58:41,970] {docker.py:276} INFO - 21/05/17 16:58:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on bb4563f4559d:33263 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-17 13:58:41,971] {docker.py:276} INFO - 21/05/17 16:58:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-17 13:58:41,972] {docker.py:276} INFO - 21/05/17 16:58:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/17 16:58:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-17 13:58:41,976] {docker.py:276} INFO - 21/05/17 16:58:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 180) (bb4563f4559d, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:41,978] {docker.py:276} INFO - 21/05/17 16:58:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 180)
[2021-05-17 13:58:42,104] {docker.py:276} INFO - 21/05/17 16:58:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621259862_to_1621261662.csv, range: 0-104315, partition values: [empty row]
[2021-05-17 13:58:42,131] {docker.py:276} INFO - 21/05/17 16:58:42 INFO CodeGenerator: Code generated in 19.312 ms
[2021-05-17 13:58:42,538] {docker.py:276} INFO - 21/05/17 16:58:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 180). 1564 bytes result sent to driver
[2021-05-17 13:58:42,540] {docker.py:276} INFO - 21/05/17 16:58:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 180) in 567 ms on bb4563f4559d (executor driver) (1/1)
21/05/17 16:58:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-17 13:58:42,541] {docker.py:276} INFO - 21/05/17 16:58:42 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.609 s
[2021-05-17 13:58:42,542] {docker.py:276} INFO - 21/05/17 16:58:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-17 13:58:42,543] {docker.py:276} INFO - 21/05/17 16:58:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-17 13:58:42,544] {docker.py:276} INFO - 21/05/17 16:58:42 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.618525 s
[2021-05-17 13:58:42,575] {docker.py:276} INFO - 21/05/17 16:58:42 INFO CodeGenerator: Code generated in 14.9857 ms
[2021-05-17 13:58:42,649] {docker.py:276} INFO - 21/05/17 16:58:42 INFO FileSourceStrategy: Pushed Filters: 
21/05/17 16:58:42 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-17 13:58:42,650] {docker.py:276} INFO - 21/05/17 16:58:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-17 13:58:42,656] {docker.py:276} INFO - 21/05/17 16:58:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-17 13:58:42,694] {docker.py:276} INFO - 21/05/17 16:58:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-17 13:58:42,695] {docker.py:276} INFO - 21/05/17 16:58:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on bb4563f4559d:33263 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 13:58:42,696] {docker.py:276} INFO - 21/05/17 16:58:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on bb4563f4559d:33263 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-17 13:58:42,698] {docker.py:276} INFO - 21/05/17 16:58:42 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:42,703] {docker.py:276} INFO - 21/05/17 16:58:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 96699482 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 13:58:43,381] {docker.py:276} INFO - 21/05/17 16:58:43 INFO FileSourceStrategy: Pushed Filters:
[2021-05-17 13:58:43,382] {docker.py:276} INFO - 21/05/17 16:58:43 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-17 13:58:43,382] {docker.py:276} INFO - 21/05/17 16:58:43 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-17 13:58:44,016] {docker.py:276} INFO - 21/05/17 16:58:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:44,020] {docker.py:276} INFO - 21/05/17 16:58:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:44,020] {docker.py:276} INFO - 21/05/17 16:58:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658436558453948365756556_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658436558453948365756556_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658436558453948365756556_0000}; taskId=attempt_202105171658436558453948365756556_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26118572}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:44,021] {docker.py:276} INFO - 21/05/17 16:58:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:44,056] {docker.py:276} INFO - 21/05/17 16:58:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-17 13:58:44,147] {docker.py:276} INFO - 21/05/17 16:58:44 INFO CodeGenerator: Code generated in 58.7991 ms
[2021-05-17 13:58:44,149] {docker.py:276} INFO - 21/05/17 16:58:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-17 13:58:44,195] {docker.py:276} INFO - 21/05/17 16:58:44 INFO CodeGenerator: Code generated in 38.6547 ms
[2021-05-17 13:58:44,199] {docker.py:276} INFO - 21/05/17 16:58:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-17 13:58:44,222] {docker.py:276} INFO - 21/05/17 16:58:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-17 13:58:44,223] {docker.py:276} INFO - 21/05/17 16:58:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on bb4563f4559d:33263 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 13:58:44,225] {docker.py:276} INFO - 21/05/17 16:58:44 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:44,233] {docker.py:276} INFO - 21/05/17 16:58:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 96699482 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 13:58:44,312] {docker.py:276} INFO - 21/05/17 16:58:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on bb4563f4559d:33263 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 13:58:44,373] {docker.py:276} INFO - 21/05/17 16:58:44 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 13:58:44,377] {docker.py:276} INFO - 21/05/17 16:58:44 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-17 13:58:44,380] {docker.py:276} INFO - 21/05/17 16:58:44 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/17 16:58:44 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
21/05/17 16:58:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-17 13:58:44,382] {docker.py:276} INFO - 21/05/17 16:58:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-17 13:58:44,384] {docker.py:276} INFO - 21/05/17 16:58:44 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 13:58:44,400] {docker.py:276} INFO - 21/05/17 16:58:44 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-17 13:58:44,411] {docker.py:276} INFO - 21/05/17 16:58:44 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-17 13:58:44,412] {docker.py:276} INFO - 21/05/17 16:58:44 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on bb4563f4559d:33263 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-17 13:58:44,413] {docker.py:276} INFO - 21/05/17 16:58:44 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-17 13:58:44,415] {docker.py:276} INFO - 21/05/17 16:58:44 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21/05/17 16:58:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0
[2021-05-17 13:58:44,418] {docker.py:276} INFO - 21/05/17 16:58:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 181) (bb4563f4559d, executor driver, partition 0, PROCESS_LOCAL, 7314 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:44,419] {docker.py:276} INFO - 21/05/17 16:58:44 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 182) (bb4563f4559d, executor driver, partition 1, PROCESS_LOCAL, 7314 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:44,420] {docker.py:276} INFO - 21/05/17 16:58:44 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 183) (bb4563f4559d, executor driver, partition 2, PROCESS_LOCAL, 7314 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:44,421] {docker.py:276} INFO - 21/05/17 16:58:44 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 184) (bb4563f4559d, executor driver, partition 3, PROCESS_LOCAL, 7094 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:44,422] {docker.py:276} INFO - 21/05/17 16:58:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 181)
[2021-05-17 13:58:44,423] {docker.py:276} INFO - 21/05/17 16:58:44 INFO Executor: Running task 2.0 in stage 3.0 (TID 183)
21/05/17 16:58:44 INFO Executor: Running task 1.0 in stage 3.0 (TID 182)
[2021-05-17 13:58:44,424] {docker.py:276} INFO - 21/05/17 16:58:44 INFO Executor: Running task 3.0 in stage 3.0 (TID 184)
[2021-05-17 13:58:44,553] {docker.py:276} INFO - 21/05/17 16:58:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on bb4563f4559d:33263 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 13:58:44,575] {docker.py:276} INFO - 21/05/17 16:58:44 INFO CodeGenerator: Code generated in 75.0352 ms
[2021-05-17 13:58:44,624] {docker.py:276} INFO - 21/05/17 16:58:44 INFO CodeGenerator: Code generated in 14.3432 ms
[2021-05-17 13:58:44,657] {docker.py:276} INFO - 21/05/17 16:58:44 INFO CodeGenerator: Code generated in 22.9561 ms
[2021-05-17 13:58:44,691] {docker.py:276} INFO - 21/05/17 16:58:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621243662_to_1621245462.csv, range: 0-104226, partition values: [empty row]
[2021-05-17 13:58:44,693] {docker.py:276} INFO - 21/05/17 16:58:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621259862_to_1621261662.csv, range: 0-104315, partition values: [empty row]
[2021-05-17 13:58:44,695] {docker.py:276} INFO - 21/05/17 16:58:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621234662_to_1621236462.csv, range: 0-103897, partition values: [empty row]
[2021-05-17 13:58:44,695] {docker.py:276} INFO - 21/05/17 16:58:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621232862_to_1621234662.csv, range: 0-104150, partition values: [empty row]
[2021-05-17 13:58:45,367] {docker.py:276} INFO - 21/05/17 16:58:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621229262_to_1621231062.csv, range: 0-103879, partition values: [empty row]
[2021-05-17 13:58:45,736] {docker.py:276} INFO - 21/05/17 16:58:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621249062_to_1621250862.csv, range: 0-103875, partition values: [empty row]
[2021-05-17 13:58:46,021] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621241862_to_1621243662.csv, range: 0-104225, partition values: [empty row]
[2021-05-17 13:58:46,026] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621227462_to_1621229262.csv, range: 0-104304, partition values: [empty row]
[2021-05-17 13:58:46,092] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621259862_to_1621261662.csv, range: 0-103844, partition values: [empty row]
[2021-05-17 13:58:46,143] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621225662_to_1621227462.csv, range: 0-104144, partition values: [empty row]
[2021-05-17 13:58:46,409] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621247262_to_1621249062.csv, range: 0-104304, partition values: [empty row]
[2021-05-17 13:58:46,437] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621236462_to_1621238262.csv, range: 0-103837, partition values: [empty row]
[2021-05-17 13:58:46,512] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621261662_to_1621263462.csv, range: 0-104225, partition values: [empty row]
[2021-05-17 13:58:46,626] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621216662_to_1621218462.csv, range: 0-104138, partition values: [empty row]
[2021-05-17 13:58:46,789] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621220262_to_1621222062.csv, range: 0-103833, partition values: [empty row]
[2021-05-17 13:58:46,863] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621238262_to_1621240062.csv, range: 0-104223, partition values: [empty row]
[2021-05-17 13:58:46,907] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621250862_to_1621252662.csv, range: 0-104300, partition values: [empty row]
[2021-05-17 13:58:46,977] {docker.py:276} INFO - 21/05/17 16:58:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621218462_to_1621220262.csv, range: 0-104136, partition values: [empty row]
[2021-05-17 13:58:47,135] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621243662_to_1621245462.csv, range: 0-103801, partition values: [empty row]
[2021-05-17 13:58:47,218] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621220262_to_1621222062.csv, range: 0-104220, partition values: [empty row]
[2021-05-17 13:58:47,276] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621240062_to_1621241862.csv, range: 0-104299, partition values: [empty row]
[2021-05-17 13:58:47,339] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621258062_to_1621259862.csv, range: 0-104132, partition values: [empty row]
[2021-05-17 13:58:47,481] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621240062_to_1621241862.csv, range: 0-103800, partition values: [empty row]
[2021-05-17 13:58:47,586] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621256262_to_1621258062.csv, range: 0-104211, partition values: [empty row]
[2021-05-17 13:58:47,662] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621263462_to_1621265262.csv, range: 0-104288, partition values: [empty row]
[2021-05-17 13:58:47,686] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621222062_to_1621223862.csv, range: 0-104130, partition values: [empty row]
[2021-05-17 13:58:47,832] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621256262_to_1621258062.csv, range: 0-103799, partition values: [empty row]
[2021-05-17 13:58:47,936] {docker.py:276} INFO - 21/05/17 16:58:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621261662_to_1621263462.csv, range: 0-104210, partition values: [empty row]
[2021-05-17 13:58:48,038] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621234662_to_1621236462.csv, range: 0-104282, partition values: [empty row]
[2021-05-17 13:58:48,041] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621218462_to_1621220262.csv, range: 0-104127, partition values: [empty row]
[2021-05-17 13:58:48,254] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621258062_to_1621259862.csv, range: 0-103756, partition values: [empty row]
[2021-05-17 13:58:48,300] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621236462_to_1621238262.csv, range: 0-104208, partition values: [empty row]
[2021-05-17 13:58:48,407] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621232862_to_1621234662.csv, range: 0-104281, partition values: [empty row]
[2021-05-17 13:58:48,407] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621252662_to_1621254462.csv, range: 0-104115, partition values: [empty row]
[2021-05-17 13:58:48,604] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621238262_to_1621240062.csv, range: 0-103752, partition values: [empty row]
[2021-05-17 13:58:48,655] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621263462_to_1621265262.csv, range: 0-104204, partition values: [empty row]
[2021-05-17 13:58:48,769] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621256262_to_1621258062.csv, range: 0-104095, partition values: [empty row]
[2021-05-17 13:58:48,799] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621240062_to_1621241862.csv, range: 0-104278, partition values: [empty row]
[2021-05-17 13:58:48,949] {docker.py:276} INFO - 21/05/17 16:58:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621245462_to_1621247262.csv, range: 0-103752, partition values: [empty row]
[2021-05-17 13:58:48,994] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621222062_to_1621223862.csv, range: 0-104201, partition values: [empty row]
[2021-05-17 13:58:49,120] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621267062_to_1621268862.csv, range: 0-104091, partition values: [empty row]
[2021-05-17 13:58:49,197] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621234662_to_1621236462.csv, range: 0-104271, partition values: [empty row]
[2021-05-17 13:58:49,291] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621263462_to_1621265262.csv, range: 0-103750, partition values: [empty row]
[2021-05-17 13:58:49,360] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621243662_to_1621245462.csv, range: 0-104197, partition values: [empty row]
[2021-05-17 13:58:49,460] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621227462_to_1621229262.csv, range: 0-104071, partition values: [empty row]
[2021-05-17 13:58:49,549] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621252662_to_1621254462.csv, range: 0-104271, partition values: [empty row]
[2021-05-17 13:58:49,646] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621252662_to_1621254462.csv, range: 0-103744, partition values: [empty row]
[2021-05-17 13:58:49,724] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621245462_to_1621247262.csv, range: 0-104196, partition values: [empty row]
[2021-05-17 13:58:49,819] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621229262_to_1621231062.csv, range: 0-104067, partition values: [empty row]
[2021-05-17 13:58:49,895] {docker.py:276} INFO - 21/05/17 16:58:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621245462_to_1621247262.csv, range: 0-104264, partition values: [empty row]
[2021-05-17 13:58:49,993] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621265262_to_1621267062.csv, range: 0-103720, partition values: [empty row]
[2021-05-17 13:58:50,097] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621236462_to_1621238262.csv, range: 0-104195, partition values: [empty row]
[2021-05-17 13:58:50,168] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621223862_to_1621225662.csv, range: 0-104059, partition values: [empty row]
[2021-05-17 13:58:50,250] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621258062_to_1621259862.csv, range: 0-104262, partition values: [empty row]
[2021-05-17 13:58:50,372] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621232862_to_1621234662.csv, range: 0-103688, partition values: [empty row]
[2021-05-17 13:58:50,448] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621231062_to_1621232862.csv, range: 0-104194, partition values: [empty row]
[2021-05-17 13:58:50,603] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621249062_to_1621250862.csv, range: 0-104249, partition values: [empty row]
[2021-05-17 13:58:50,714] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621218462_to_1621220262.csv, range: 0-103675, partition values: [empty row]
[2021-05-17 13:58:50,794] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621254462_to_1621256262.csv, range: 0-104177, partition values: [empty row]
[2021-05-17 13:58:50,988] {docker.py:276} INFO - 21/05/17 16:58:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621259862_to_1621261662.csv, range: 0-104246, partition values: [empty row]
[2021-05-17 13:58:51,056] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621267062_to_1621268862.csv, range: 0-103612, partition values: [empty row]
[2021-05-17 13:58:51,141] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621265262_to_1621267062.csv, range: 0-104174, partition values: [empty row]
[2021-05-17 13:58:51,365] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621216662_to_1621218462.csv, range: 0-104241, partition values: [empty row]
[2021-05-17 13:58:51,399] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621215193_to_1621216662.csv, range: 0-85079, partition values: [empty row]
[2021-05-17 13:58:51,481] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621265262_to_1621267062.csv, range: 0-104168, partition values: [empty row]
[2021-05-17 13:58:51,536] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621267062_to_1621268862.csv, range: 0-104045, partition values: [empty row]
[2021-05-17 13:58:51,744] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621215193_to_1621216662.csv, range: 0-85023, partition values: [empty row]
[2021-05-17 13:58:51,830] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621247262_to_1621249062.csv, range: 0-104164, partition values: [empty row]
[2021-05-17 13:58:51,869] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621223862_to_1621225662.csv, range: 0-104239, partition values: [empty row]
[2021-05-17 13:58:51,893] {docker.py:276} INFO - 21/05/17 16:58:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621225662_to_1621227462.csv, range: 0-104023, partition values: [empty row]
[2021-05-17 13:58:52,082] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621215193_to_1621216662.csv, range: 0-84895, partition values: [empty row]
[2021-05-17 13:58:52,173] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621222062_to_1621223862.csv, range: 0-104163, partition values: [empty row]
[2021-05-17 13:58:52,218] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621227462_to_1621229262.csv, range: 0-104236, partition values: [empty row]
[2021-05-17 13:58:52,241] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621223862_to_1621225662.csv, range: 0-103999, partition values: [empty row]
[2021-05-17 13:58:52,534] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621231062_to_1621232862.csv, range: 0-104163, partition values: [empty row]
[2021-05-17 13:58:52,564] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621249062_to_1621250862.csv, range: 0-104233, partition values: [empty row]
[2021-05-17 13:58:52,582] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621250862_to_1621252662.csv, range: 0-103977, partition values: [empty row]
[2021-05-17 13:58:52,780] {docker.py:276} INFO - 21/05/17 16:58:52 INFO Executor: Finished task 3.0 in stage 3.0 (TID 184). 2722 bytes result sent to driver
[2021-05-17 13:58:52,784] {docker.py:276} INFO - 21/05/17 16:58:52 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 184) in 8373 ms on bb4563f4559d (executor driver) (1/4)
[2021-05-17 13:58:52,875] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621225662_to_1621227462.csv, range: 0-104162, partition values: [empty row]
[2021-05-17 13:58:52,928] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_13_57_42/from_1621220262_to_1621222062.csv, range: 0-104232, partition values: [empty row]
[2021-05-17 13:58:52,929] {docker.py:276} INFO - 21/05/17 16:58:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621247262_to_1621249062.csv, range: 0-103953, partition values: [empty row]
[2021-05-17 13:58:53,230] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621250862_to_1621252662.csv, range: 0-104159, partition values: [empty row]
[2021-05-17 13:58:53,278] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621261662_to_1621263462.csv, range: 0-103944, partition values: [empty row]
[2021-05-17 13:58:53,300] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621229262_to_1621231062.csv, range: 0-104232, partition values: [empty row]
[2021-05-17 13:58:53,584] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621241862_to_1621243662.csv, range: 0-104152, partition values: [empty row]
[2021-05-17 13:58:53,624] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621231062_to_1621232862.csv, range: 0-103938, partition values: [empty row]
[2021-05-17 13:58:53,671] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621254462_to_1621256262.csv, range: 0-104228, partition values: [empty row]
[2021-05-17 13:58:53,973] {docker.py:276} INFO - 21/05/17 16:58:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621254462_to_1621256262.csv, range: 0-103921, partition values: [empty row]
[2021-05-17 13:58:54,038] {docker.py:276} INFO - 21/05/17 16:58:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_13_57_42/from_1621238262_to_1621240062.csv, range: 0-104227, partition values: [empty row]
[2021-05-17 13:58:54,150] {docker.py:276} INFO - 21/05/17 16:58:54 INFO Executor: Finished task 1.0 in stage 3.0 (TID 182). 2679 bytes result sent to driver
[2021-05-17 13:58:54,150] {docker.py:276} INFO - 21/05/17 16:58:54 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 182) in 9743 ms on bb4563f4559d (executor driver) (2/4)
[2021-05-17 13:58:54,313] {docker.py:276} INFO - 21/05/17 16:58:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621241862_to_1621243662.csv, range: 0-103904, partition values: [empty row]
[2021-05-17 13:58:54,615] {docker.py:276} INFO - 21/05/17 16:58:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 181). 2679 bytes result sent to driver
[2021-05-17 13:58:54,616] {docker.py:276} INFO - 21/05/17 16:58:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 181) in 10211 ms on bb4563f4559d (executor driver) (3/4)
[2021-05-17 13:58:54,655] {docker.py:276} INFO - 21/05/17 16:58:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_13_57_42/from_1621216662_to_1621218462.csv, range: 0-103902, partition values: [empty row]
[2021-05-17 13:58:55,145] {docker.py:276} INFO - 21/05/17 16:58:55 INFO Executor: Finished task 2.0 in stage 3.0 (TID 183). 2679 bytes result sent to driver
[2021-05-17 13:58:55,146] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 183) in 10739 ms on bb4563f4559d (executor driver) (4/4)
[2021-05-17 13:58:55,147] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-17 13:58:55,147] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 10.768 s
[2021-05-17 13:58:55,148] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: looking for newly runnable stages
[2021-05-17 13:58:55,149] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: running: Set()
[2021-05-17 13:58:55,150] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-17 13:58:55,150] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: failed: Set()
[2021-05-17 13:58:55,155] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 13:58:55,199] {docker.py:276} INFO - 21/05/17 16:58:55 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-17 13:58:55,203] {docker.py:276} INFO - 21/05/17 16:58:55 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-17 13:58:55,204] {docker.py:276} INFO - 21/05/17 16:58:55 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on bb4563f4559d:33263 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-17 13:58:55,205] {docker.py:276} INFO - 21/05/17 16:58:55 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-17 13:58:55,206] {docker.py:276} INFO - 21/05/17 16:58:55 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-17 13:58:55,207] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-17 13:58:55,216] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 185) (bb4563f4559d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:55,217] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 186) (bb4563f4559d, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:55,217] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 187) (bb4563f4559d, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:55,218] {docker.py:276} INFO - 21/05/17 16:58:55 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 188) (bb4563f4559d, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:55,218] {docker.py:276} INFO - 21/05/17 16:58:55 INFO Executor: Running task 2.0 in stage 4.0 (TID 187)
[2021-05-17 13:58:55,218] {docker.py:276} INFO - 21/05/17 16:58:55 INFO Executor: Running task 0.0 in stage 4.0 (TID 185)
[2021-05-17 13:58:55,219] {docker.py:276} INFO - 21/05/17 16:58:55 INFO Executor: Running task 3.0 in stage 4.0 (TID 188)
[2021-05-17 13:58:55,219] {docker.py:276} INFO - 21/05/17 16:58:55 INFO Executor: Running task 1.0 in stage 4.0 (TID 186)
[2021-05-17 13:58:55,324] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:55,325] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:55,326] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:55,327] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-17 13:58:55,327] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:55,328] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-17 13:58:55,328] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-17 13:58:55,329] {docker.py:276} INFO - 21/05/17 16:58:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-17 13:58:55,349] {docker.py:276} INFO - 21/05/17 16:58:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:55,349] {docker.py:276} INFO - 21/05/17 16:58:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:55,351] {docker.py:276} INFO - 21/05/17 16:58:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:55,352] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,352] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,353] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445226150890730051960_0004_m_000000_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445226150890730051960_0004_m_000000_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445226150890730051960_0004}; taskId=attempt_202105171658445226150890730051960_0004_m_000000_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@787f24e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,353] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,353] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844900948512236071468_0004_m_000002_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844900948512236071468_0004_m_000002_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844900948512236071468_0004}; taskId=attempt_20210517165844900948512236071468_0004_m_000002_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48f1de37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:58:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:55,356] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441294100265239403704_0004_m_000001_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441294100265239403704_0004_m_000001_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441294100265239403704_0004}; taskId=attempt_202105171658441294100265239403704_0004_m_000001_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@646fc582}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,357] {docker.py:276} INFO - 21/05/17 16:58:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:55,358] {docker.py:276} INFO - 21/05/17 16:58:55 INFO StagingCommitter: Starting: Task committer attempt_20210517165844900948512236071468_0004_m_000002_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844900948512236071468_0004_m_000002_187
[2021-05-17 13:58:55,358] {docker.py:276} INFO - 21/05/17 16:58:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:58:55 INFO StagingCommitter: Starting: Task committer attempt_202105171658445226150890730051960_0004_m_000000_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445226150890730051960_0004_m_000000_185
[2021-05-17 13:58:55,360] {docker.py:276} INFO - 21/05/17 16:58:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:58:55 INFO StagingCommitter: Starting: Task committer attempt_202105171658441294100265239403704_0004_m_000001_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441294100265239403704_0004_m_000001_186
[2021-05-17 13:58:55,361] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,361] {docker.py:276} INFO - 21/05/17 16:58:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441286237063623923957_0004_m_000003_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441286237063623923957_0004_m_000003_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441286237063623923957_0004}; taskId=attempt_202105171658441286237063623923957_0004_m_000003_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c1ddc0f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:55,362] {docker.py:276} INFO - 21/05/17 16:58:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:55,362] {docker.py:276} INFO - 21/05/17 16:58:55 INFO StagingCommitter: Starting: Task committer attempt_202105171658441286237063623923957_0004_m_000003_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441286237063623923957_0004_m_000003_188
[2021-05-17 13:58:55,399] {docker.py:276} INFO - 21/05/17 16:58:55 INFO StagingCommitter: Task committer attempt_202105171658441294100265239403704_0004_m_000001_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441294100265239403704_0004_m_000001_186 : duration 0:00.040s
[2021-05-17 13:58:55,400] {docker.py:276} INFO - 21/05/17 16:58:55 INFO StagingCommitter: Task committer attempt_202105171658445226150890730051960_0004_m_000000_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445226150890730051960_0004_m_000000_185 : duration 0:00.042s
[2021-05-17 13:58:55,409] {docker.py:276} INFO - 21/05/17 16:58:55 INFO StagingCommitter: Task committer attempt_20210517165844900948512236071468_0004_m_000002_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844900948512236071468_0004_m_000002_187 : duration 0:00.053s
[2021-05-17 13:58:55,411] {docker.py:276} INFO - 21/05/17 16:58:55 INFO StagingCommitter: Task committer attempt_202105171658441286237063623923957_0004_m_000003_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441286237063623923957_0004_m_000003_188 : duration 0:00.050s
[2021-05-17 13:58:57,377] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_20210517165844900948512236071468_0004_m_000002_187: needsTaskCommit() Task attempt_20210517165844900948512236071468_0004_m_000002_187
[2021-05-17 13:58:57,377] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_20210517165844900948512236071468_0004_m_000002_187: needsTaskCommit() Task attempt_20210517165844900948512236071468_0004_m_000002_187: duration 0:00.001s
[2021-05-17 13:58:57,378] {docker.py:276} INFO - 21/05/17 16:58:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844900948512236071468_0004_m_000002_187
[2021-05-17 13:58:57,384] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Finished task 2.0 in stage 4.0 (TID 187). 4630 bytes result sent to driver
[2021-05-17 13:58:57,386] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 189) (bb4563f4559d, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:57,388] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Running task 4.0 in stage 4.0 (TID 189)
21/05/17 16:58:57 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 187) in 2173 ms on bb4563f4559d (executor driver) (1/200)
[2021-05-17 13:58:57,392] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658441286237063623923957_0004_m_000003_188: needsTaskCommit() Task attempt_202105171658441286237063623923957_0004_m_000003_188
[2021-05-17 13:58:57,392] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658441294100265239403704_0004_m_000001_186: needsTaskCommit() Task attempt_202105171658441294100265239403704_0004_m_000001_186
[2021-05-17 13:58:57,393] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_202105171658441286237063623923957_0004_m_000003_188: needsTaskCommit() Task attempt_202105171658441286237063623923957_0004_m_000003_188: duration 0:00.001s
[2021-05-17 13:58:57,394] {docker.py:276} INFO - 21/05/17 16:58:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441286237063623923957_0004_m_000003_188
[2021-05-17 13:58:57,394] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_202105171658441294100265239403704_0004_m_000001_186: needsTaskCommit() Task attempt_202105171658441294100265239403704_0004_m_000001_186: duration 0:00.001s
[2021-05-17 13:58:57,395] {docker.py:276} INFO - 21/05/17 16:58:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441294100265239403704_0004_m_000001_186
[2021-05-17 13:58:57,395] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Finished task 3.0 in stage 4.0 (TID 188). 4587 bytes result sent to driver
[2021-05-17 13:58:57,396] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Finished task 1.0 in stage 4.0 (TID 186). 4587 bytes result sent to driver
[2021-05-17 13:58:57,396] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 190) (bb4563f4559d, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:57,397] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 191) (bb4563f4559d, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 16:58:57 INFO Executor: Running task 5.0 in stage 4.0 (TID 190)
[2021-05-17 13:58:57,398] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 188) in 2184 ms on bb4563f4559d (executor driver) (2/200)
[2021-05-17 13:58:57,399] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Running task 6.0 in stage 4.0 (TID 191)
[2021-05-17 13:58:57,399] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 186) in 2186 ms on bb4563f4559d (executor driver) (3/200)
[2021-05-17 13:58:57,415] {docker.py:276} INFO - 21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:58:57,419] {docker.py:276} INFO - 21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:57,420] {docker.py:276} INFO - 21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:58:57,420] {docker.py:276} INFO - 21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Getting 4 (27.6 KiB) non-empty blocks including 4 (27.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:58:57,422] {docker.py:276} INFO - 21/05/17 16:58:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:57,423] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:57,424] {docker.py:276} INFO - 21/05/17 16:58:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:58:57,424] {docker.py:276} INFO - 21/05/17 16:58:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:58:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:58:57,425] {docker.py:276} INFO - 21/05/17 16:58:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:57,425] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447591608677401277583_0004_m_000006_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447591608677401277583_0004_m_000006_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447591608677401277583_0004}; taskId=attempt_202105171658447591608677401277583_0004_m_000006_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2430f07b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:57,425] {docker.py:276} INFO - 21/05/17 16:58:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:57,425] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658447591608677401277583_0004_m_000006_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447591608677401277583_0004_m_000006_191
[2021-05-17 13:58:57,427] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:57,427] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445282931670560459976_0004_m_000005_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445282931670560459976_0004_m_000005_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445282931670560459976_0004}; taskId=attempt_202105171658445282931670560459976_0004_m_000005_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fb1426c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:57,427] {docker.py:276} INFO - 21/05/17 16:58:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:57,428] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658445282931670560459976_0004_m_000005_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445282931670560459976_0004_m_000005_190
[2021-05-17 13:58:57,432] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844206115923975811448_0004_m_000004_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844206115923975811448_0004_m_000004_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844206115923975811448_0004}; taskId=attempt_20210517165844206115923975811448_0004_m_000004_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b07fb30}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:57,433] {docker.py:276} INFO - 21/05/17 16:58:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:57,434] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_20210517165844206115923975811448_0004_m_000004_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844206115923975811448_0004_m_000004_189
[2021-05-17 13:58:57,445] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_202105171658447591608677401277583_0004_m_000006_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447591608677401277583_0004_m_000006_191 : duration 0:00.018s
[2021-05-17 13:58:57,447] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_202105171658445282931670560459976_0004_m_000005_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445282931670560459976_0004_m_000005_190 : duration 0:00.015s
[2021-05-17 13:58:57,447] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_20210517165844206115923975811448_0004_m_000004_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844206115923975811448_0004_m_000004_189 : duration 0:00.014s
[2021-05-17 13:58:57,540] {docker.py:276} INFO - 21/05/17 16:58:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on bb4563f4559d:33263 in memory (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-17 13:58:57,887] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658445226150890730051960_0004_m_000000_185: needsTaskCommit() Task attempt_202105171658445226150890730051960_0004_m_000000_185
[2021-05-17 13:58:57,888] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_202105171658445226150890730051960_0004_m_000000_185: needsTaskCommit() Task attempt_202105171658445226150890730051960_0004_m_000000_185: duration 0:00.001s
21/05/17 16:58:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445226150890730051960_0004_m_000000_185
[2021-05-17 13:58:57,889] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 185). 4587 bytes result sent to driver
[2021-05-17 13:58:57,890] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 192) (bb4563f4559d, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:57,892] {docker.py:276} INFO - 21/05/17 16:58:57 INFO Executor: Running task 7.0 in stage 4.0 (TID 192)
[2021-05-17 13:58:57,892] {docker.py:276} INFO - 21/05/17 16:58:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 185) in 2682 ms on bb4563f4559d (executor driver) (4/200)
[2021-05-17 13:58:57,902] {docker.py:276} INFO - 21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:57,902] {docker.py:276} INFO - 21/05/17 16:58:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:58:57,905] {docker.py:276} INFO - 21/05/17 16:58:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:57,906] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:57,906] {docker.py:276} INFO - 21/05/17 16:58:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441994660785281710882_0004_m_000007_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441994660785281710882_0004_m_000007_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441994660785281710882_0004}; taskId=attempt_202105171658441994660785281710882_0004_m_000007_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23cc17cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:58:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:58:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658441994660785281710882_0004_m_000007_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441994660785281710882_0004_m_000007_192
[2021-05-17 13:58:57,911] {docker.py:276} INFO - 21/05/17 16:58:57 INFO StagingCommitter: Task committer attempt_202105171658441994660785281710882_0004_m_000007_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441994660785281710882_0004_m_000007_192 : duration 0:00.005s
[2021-05-17 13:58:59,832] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Starting: Task committer attempt_20210517165844206115923975811448_0004_m_000004_189: needsTaskCommit() Task attempt_20210517165844206115923975811448_0004_m_000004_189
[2021-05-17 13:58:59,832] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Task committer attempt_20210517165844206115923975811448_0004_m_000004_189: needsTaskCommit() Task attempt_20210517165844206115923975811448_0004_m_000004_189: duration 0:00.002s
21/05/17 16:58:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844206115923975811448_0004_m_000004_189
[2021-05-17 13:58:59,834] {docker.py:276} INFO - 21/05/17 16:58:59 INFO Executor: Finished task 4.0 in stage 4.0 (TID 189). 4587 bytes result sent to driver
[2021-05-17 13:58:59,836] {docker.py:276} INFO - 21/05/17 16:58:59 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 193) (bb4563f4559d, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:59,836] {docker.py:276} INFO - 21/05/17 16:58:59 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 189) in 2453 ms on bb4563f4559d (executor driver) (5/200)
[2021-05-17 13:58:59,837] {docker.py:276} INFO - 21/05/17 16:58:59 INFO Executor: Running task 8.0 in stage 4.0 (TID 193)
[2021-05-17 13:58:59,846] {docker.py:276} INFO - 21/05/17 16:58:59 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:58:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:58:59,854] {docker.py:276} INFO - 21/05/17 16:58:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:58:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:58:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:59,855] {docker.py:276} INFO - 21/05/17 16:58:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658449120807804912797402_0004_m_000008_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658449120807804912797402_0004_m_000008_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658449120807804912797402_0004}; taskId=attempt_202105171658449120807804912797402_0004_m_000008_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7bef18e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:58:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:58:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658449120807804912797402_0004_m_000008_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658449120807804912797402_0004_m_000008_193
[2021-05-17 13:58:59,859] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Task committer attempt_202105171658449120807804912797402_0004_m_000008_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658449120807804912797402_0004_m_000008_193 : duration 0:00.005s
[2021-05-17 13:58:59,878] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658447591608677401277583_0004_m_000006_191: needsTaskCommit() Task attempt_202105171658447591608677401277583_0004_m_000006_191
[2021-05-17 13:58:59,879] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Task committer attempt_202105171658447591608677401277583_0004_m_000006_191: needsTaskCommit() Task attempt_202105171658447591608677401277583_0004_m_000006_191: duration 0:00.001s
21/05/17 16:58:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447591608677401277583_0004_m_000006_191
[2021-05-17 13:58:59,880] {docker.py:276} INFO - 21/05/17 16:58:59 INFO Executor: Finished task 6.0 in stage 4.0 (TID 191). 4587 bytes result sent to driver
[2021-05-17 13:58:59,882] {docker.py:276} INFO - 21/05/17 16:58:59 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 194) (bb4563f4559d, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:58:59,883] {docker.py:276} INFO - 21/05/17 16:58:59 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 191) in 2488 ms on bb4563f4559d (executor driver) (6/200)
[2021-05-17 13:58:59,883] {docker.py:276} INFO - 21/05/17 16:58:59 INFO Executor: Running task 9.0 in stage 4.0 (TID 194)
[2021-05-17 13:58:59,901] {docker.py:276} INFO - 21/05/17 16:58:59 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:58:59,902] {docker.py:276} INFO - 21/05/17 16:58:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:58:59,904] {docker.py:276} INFO - 21/05/17 16:58:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:58:59,904] {docker.py:276} INFO - 21/05/17 16:58:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:58:59,905] {docker.py:276} INFO - 21/05/17 16:58:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:58:59,905] {docker.py:276} INFO - 21/05/17 16:58:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844931683974796667251_0004_m_000009_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844931683974796667251_0004_m_000009_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844931683974796667251_0004}; taskId=attempt_20210517165844931683974796667251_0004_m_000009_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5245b9a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:58:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:58:59,905] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Starting: Task committer attempt_20210517165844931683974796667251_0004_m_000009_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844931683974796667251_0004_m_000009_194
[2021-05-17 13:58:59,909] {docker.py:276} INFO - 21/05/17 16:58:59 INFO StagingCommitter: Task committer attempt_20210517165844931683974796667251_0004_m_000009_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844931683974796667251_0004_m_000009_194 : duration 0:00.004s
[2021-05-17 13:58:59,999] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658445282931670560459976_0004_m_000005_190: needsTaskCommit() Task attempt_202105171658445282931670560459976_0004_m_000005_190
[2021-05-17 13:59:00,000] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Task committer attempt_202105171658445282931670560459976_0004_m_000005_190: needsTaskCommit() Task attempt_202105171658445282931670560459976_0004_m_000005_190: duration 0:00.001s
[2021-05-17 13:59:00,000] {docker.py:276} INFO - 21/05/17 16:59:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445282931670560459976_0004_m_000005_190
[2021-05-17 13:59:00,002] {docker.py:276} INFO - 21/05/17 16:59:00 INFO Executor: Finished task 5.0 in stage 4.0 (TID 190). 4587 bytes result sent to driver
[2021-05-17 13:59:00,004] {docker.py:276} INFO - 21/05/17 16:59:00 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 195) (bb4563f4559d, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:00,005] {docker.py:276} INFO - 21/05/17 16:59:00 INFO Executor: Running task 10.0 in stage 4.0 (TID 195)
[2021-05-17 13:59:00,006] {docker.py:276} INFO - 21/05/17 16:59:00 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 190) in 2613 ms on bb4563f4559d (executor driver) (7/200)
[2021-05-17 13:59:00,016] {docker.py:276} INFO - 21/05/17 16:59:00 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:00,019] {docker.py:276} INFO - 21/05/17 16:59:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:00,020] {docker.py:276} INFO - 21/05/17 16:59:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:00,020] {docker.py:276} INFO - 21/05/17 16:59:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445559111285206462410_0004_m_000010_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445559111285206462410_0004_m_000010_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445559111285206462410_0004}; taskId=attempt_202105171658445559111285206462410_0004_m_000010_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f2f5ec9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:00,021] {docker.py:276} INFO - 21/05/17 16:59:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:00,021] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658445559111285206462410_0004_m_000010_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445559111285206462410_0004_m_000010_195
[2021-05-17 13:59:00,024] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Task committer attempt_202105171658445559111285206462410_0004_m_000010_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445559111285206462410_0004_m_000010_195 : duration 0:00.004s
[2021-05-17 13:59:00,437] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658441994660785281710882_0004_m_000007_192: needsTaskCommit() Task attempt_202105171658441994660785281710882_0004_m_000007_192
[2021-05-17 13:59:00,438] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Task committer attempt_202105171658441994660785281710882_0004_m_000007_192: needsTaskCommit() Task attempt_202105171658441994660785281710882_0004_m_000007_192: duration 0:00.003s
21/05/17 16:59:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441994660785281710882_0004_m_000007_192
[2021-05-17 13:59:00,441] {docker.py:276} INFO - 21/05/17 16:59:00 INFO Executor: Finished task 7.0 in stage 4.0 (TID 192). 4587 bytes result sent to driver
[2021-05-17 13:59:00,442] {docker.py:276} INFO - 21/05/17 16:59:00 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 196) (bb4563f4559d, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:00,443] {docker.py:276} INFO - 21/05/17 16:59:00 INFO Executor: Running task 11.0 in stage 4.0 (TID 196)
[2021-05-17 13:59:00,443] {docker.py:276} INFO - 21/05/17 16:59:00 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 192) in 2556 ms on bb4563f4559d (executor driver) (8/200)
[2021-05-17 13:59:00,460] {docker.py:276} INFO - 21/05/17 16:59:00 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:00,462] {docker.py:276} INFO - 21/05/17 16:59:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:00,463] {docker.py:276} INFO - 21/05/17 16:59:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441737367371739984145_0004_m_000011_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441737367371739984145_0004_m_000011_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441737367371739984145_0004}; taskId=attempt_202105171658441737367371739984145_0004_m_000011_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77263619}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:00,463] {docker.py:276} INFO - 21/05/17 16:59:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:00,463] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658441737367371739984145_0004_m_000011_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441737367371739984145_0004_m_000011_196
[2021-05-17 13:59:00,468] {docker.py:276} INFO - 21/05/17 16:59:00 INFO StagingCommitter: Task committer attempt_202105171658441737367371739984145_0004_m_000011_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441737367371739984145_0004_m_000011_196 : duration 0:00.005s
[2021-05-17 13:59:02,396] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_20210517165844931683974796667251_0004_m_000009_194: needsTaskCommit() Task attempt_20210517165844931683974796667251_0004_m_000009_194
[2021-05-17 13:59:02,396] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_20210517165844931683974796667251_0004_m_000009_194: needsTaskCommit() Task attempt_20210517165844931683974796667251_0004_m_000009_194: duration 0:00.001s
21/05/17 16:59:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844931683974796667251_0004_m_000009_194
[2021-05-17 13:59:02,397] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Finished task 9.0 in stage 4.0 (TID 194). 4587 bytes result sent to driver
[2021-05-17 13:59:02,398] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 197) (bb4563f4559d, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:02,399] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Running task 12.0 in stage 4.0 (TID 197)
[2021-05-17 13:59:02,400] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 194) in 2521 ms on bb4563f4559d (executor driver) (9/200)
[2021-05-17 13:59:02,410] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:02,410] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:02,412] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658449120807804912797402_0004_m_000008_193: needsTaskCommit() Task attempt_202105171658449120807804912797402_0004_m_000008_193
[2021-05-17 13:59:02,412] {docker.py:276} INFO - 21/05/17 16:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658449120807804912797402_0004_m_000008_193: needsTaskCommit() Task attempt_202105171658449120807804912797402_0004_m_000008_193: duration 0:00.001s
[2021-05-17 13:59:02,413] {docker.py:276} INFO - 21/05/17 16:59:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658449120807804912797402_0004_m_000008_193
[2021-05-17 13:59:02,413] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,414] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447301200792495160322_0004_m_000012_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447301200792495160322_0004_m_000012_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447301200792495160322_0004}; taskId=attempt_202105171658447301200792495160322_0004_m_000012_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d54b47b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,414] {docker.py:276} INFO - 21/05/17 16:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:02,414] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658447301200792495160322_0004_m_000012_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447301200792495160322_0004_m_000012_197
[2021-05-17 13:59:02,415] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Finished task 8.0 in stage 4.0 (TID 193). 4587 bytes result sent to driver
[2021-05-17 13:59:02,416] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 198) (bb4563f4559d, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:02,417] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 193) in 2585 ms on bb4563f4559d (executor driver) (10/200)
[2021-05-17 13:59:02,420] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658447301200792495160322_0004_m_000012_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447301200792495160322_0004_m_000012_197 : duration 0:00.006s
[2021-05-17 13:59:02,420] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Running task 13.0 in stage 4.0 (TID 198)
[2021-05-17 13:59:02,428] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:02,429] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:02,433] {docker.py:276} INFO - 21/05/17 16:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:02,433] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,434] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448264505497487113332_0004_m_000013_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448264505497487113332_0004_m_000013_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448264505497487113332_0004}; taskId=attempt_202105171658448264505497487113332_0004_m_000013_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3db9c1e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:02,434] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658448264505497487113332_0004_m_000013_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448264505497487113332_0004_m_000013_198
[2021-05-17 13:59:02,438] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658448264505497487113332_0004_m_000013_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448264505497487113332_0004_m_000013_198 : duration 0:00.004s
[2021-05-17 13:59:02,499] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658445559111285206462410_0004_m_000010_195: needsTaskCommit() Task attempt_202105171658445559111285206462410_0004_m_000010_195
[2021-05-17 13:59:02,499] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658445559111285206462410_0004_m_000010_195: needsTaskCommit() Task attempt_202105171658445559111285206462410_0004_m_000010_195: duration 0:00.001s
[2021-05-17 13:59:02,500] {docker.py:276} INFO - 21/05/17 16:59:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445559111285206462410_0004_m_000010_195
[2021-05-17 13:59:02,501] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Finished task 10.0 in stage 4.0 (TID 195). 4544 bytes result sent to driver
[2021-05-17 13:59:02,502] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 199) (bb4563f4559d, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:02,504] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 195) in 2502 ms on bb4563f4559d (executor driver) (11/200)
[2021-05-17 13:59:02,504] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Running task 14.0 in stage 4.0 (TID 199)
[2021-05-17 13:59:02,519] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:02,522] {docker.py:276} INFO - 21/05/17 16:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,523] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443145557387550743123_0004_m_000014_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443145557387550743123_0004_m_000014_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443145557387550743123_0004}; taskId=attempt_202105171658443145557387550743123_0004_m_000014_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c6ba7cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,523] {docker.py:276} INFO - 21/05/17 16:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658443145557387550743123_0004_m_000014_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443145557387550743123_0004_m_000014_199
[2021-05-17 13:59:02,528] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658443145557387550743123_0004_m_000014_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443145557387550743123_0004_m_000014_199 : duration 0:00.006s
[2021-05-17 13:59:02,886] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658441737367371739984145_0004_m_000011_196: needsTaskCommit() Task attempt_202105171658441737367371739984145_0004_m_000011_196
[2021-05-17 13:59:02,887] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658441737367371739984145_0004_m_000011_196: needsTaskCommit() Task attempt_202105171658441737367371739984145_0004_m_000011_196: duration 0:00.005s
21/05/17 16:59:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441737367371739984145_0004_m_000011_196
[2021-05-17 13:59:02,888] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Finished task 11.0 in stage 4.0 (TID 196). 4544 bytes result sent to driver
[2021-05-17 13:59:02,891] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 200) (bb4563f4559d, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:02,893] {docker.py:276} INFO - 21/05/17 16:59:02 INFO Executor: Running task 15.0 in stage 4.0 (TID 200)
[2021-05-17 13:59:02,894] {docker.py:276} INFO - 21/05/17 16:59:02 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 196) in 2455 ms on bb4563f4559d (executor driver) (12/200)
[2021-05-17 13:59:02,910] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Getting 4 (26.0 KiB) non-empty blocks including 4 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:02,910] {docker.py:276} INFO - 21/05/17 16:59:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:02,913] {docker.py:276} INFO - 21/05/17 16:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:02,913] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,913] {docker.py:276} INFO - 21/05/17 16:59:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448708097175676084210_0004_m_000015_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448708097175676084210_0004_m_000015_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448708097175676084210_0004}; taskId=attempt_202105171658448708097175676084210_0004_m_000015_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@463ed065}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:02,914] {docker.py:276} INFO - 21/05/17 16:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658448708097175676084210_0004_m_000015_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448708097175676084210_0004_m_000015_200
[2021-05-17 13:59:02,918] {docker.py:276} INFO - 21/05/17 16:59:02 INFO StagingCommitter: Task committer attempt_202105171658448708097175676084210_0004_m_000015_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448708097175676084210_0004_m_000015_200 : duration 0:00.005s
[2021-05-17 13:59:04,985] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658443145557387550743123_0004_m_000014_199: needsTaskCommit() Task attempt_202105171658443145557387550743123_0004_m_000014_199
[2021-05-17 13:59:04,986] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658443145557387550743123_0004_m_000014_199: needsTaskCommit() Task attempt_202105171658443145557387550743123_0004_m_000014_199: duration 0:00.002s
21/05/17 16:59:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443145557387550743123_0004_m_000014_199
[2021-05-17 13:59:04,987] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Finished task 14.0 in stage 4.0 (TID 199). 4587 bytes result sent to driver
[2021-05-17 13:59:04,988] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 201) (bb4563f4559d, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:04,990] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 199) in 2491 ms on bb4563f4559d (executor driver) (13/200)
[2021-05-17 13:59:04,991] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Running task 16.0 in stage 4.0 (TID 201)
[2021-05-17 13:59:04,992] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658448264505497487113332_0004_m_000013_198: needsTaskCommit() Task attempt_202105171658448264505497487113332_0004_m_000013_198
[2021-05-17 13:59:04,993] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658448264505497487113332_0004_m_000013_198: needsTaskCommit() Task attempt_202105171658448264505497487113332_0004_m_000013_198: duration 0:00.001s
[2021-05-17 13:59:04,993] {docker.py:276} INFO - 21/05/17 16:59:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448264505497487113332_0004_m_000013_198
[2021-05-17 13:59:04,994] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Finished task 13.0 in stage 4.0 (TID 198). 4587 bytes result sent to driver
[2021-05-17 13:59:04,995] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658447301200792495160322_0004_m_000012_197: needsTaskCommit() Task attempt_202105171658447301200792495160322_0004_m_000012_197
[2021-05-17 13:59:04,996] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658447301200792495160322_0004_m_000012_197: needsTaskCommit() Task attempt_202105171658447301200792495160322_0004_m_000012_197: duration 0:00.000s
[2021-05-17 13:59:04,996] {docker.py:276} INFO - 21/05/17 16:59:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447301200792495160322_0004_m_000012_197
[2021-05-17 13:59:04,997] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 202) (bb4563f4559d, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:04,998] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Finished task 12.0 in stage 4.0 (TID 197). 4587 bytes result sent to driver
[2021-05-17 13:59:04,998] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 198) in 2585 ms on bb4563f4559d (executor driver) (14/200)
[2021-05-17 13:59:04,999] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Running task 17.0 in stage 4.0 (TID 202)
[2021-05-17 13:59:05,000] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 203) (bb4563f4559d, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:05,001] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 197) in 2605 ms on bb4563f4559d (executor driver) (15/200)
[2021-05-17 13:59:05,001] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Running task 18.0 in stage 4.0 (TID 203)
[2021-05-17 13:59:05,011] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:05,012] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:05,014] {docker.py:276} INFO - 21/05/17 16:59:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:05,014] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,015] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447122914980708031321_0004_m_000016_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447122914980708031321_0004_m_000016_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447122914980708031321_0004}; taskId=attempt_202105171658447122914980708031321_0004_m_000016_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c8dc7f5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,015] {docker.py:276} INFO - 21/05/17 16:59:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:05,016] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658447122914980708031321_0004_m_000016_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447122914980708031321_0004_m_000016_201
[2021-05-17 13:59:05,021] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Getting 4 (27.3 KiB) non-empty blocks including 4 (27.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:05,024] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658447122914980708031321_0004_m_000016_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447122914980708031321_0004_m_000016_201 : duration 0:00.008s
[2021-05-17 13:59:05,025] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 13:59:05,030] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Getting 4 (27.2 KiB) non-empty blocks including 4 (27.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:05,030] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-17 13:59:05,032] {docker.py:276} INFO - 21/05/17 16:59:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:05,032] {docker.py:276} INFO - 21/05/17 16:59:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:05,033] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,034] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443520327833983327216_0004_m_000017_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443520327833983327216_0004_m_000017_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443520327833983327216_0004}; taskId=attempt_202105171658443520327833983327216_0004_m_000017_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e2269d9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,034] {docker.py:276} INFO - 21/05/17 16:59:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:05,035] {docker.py:276} INFO - 21/05/17 16:59:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:05,036] {docker.py:276} INFO - 21/05/17 16:59:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:05,037] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658443520327833983327216_0004_m_000017_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443520327833983327216_0004_m_000017_202
[2021-05-17 13:59:05,039] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,039] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441998924785008783056_0004_m_000018_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441998924785008783056_0004_m_000018_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441998924785008783056_0004}; taskId=attempt_202105171658441998924785008783056_0004_m_000018_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56a23dd3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,039] {docker.py:276} INFO - 21/05/17 16:59:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:05,040] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658441998924785008783056_0004_m_000018_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441998924785008783056_0004_m_000018_203
[2021-05-17 13:59:05,044] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658443520327833983327216_0004_m_000017_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443520327833983327216_0004_m_000017_202 : duration 0:00.009s
[2021-05-17 13:59:05,047] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658441998924785008783056_0004_m_000018_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441998924785008783056_0004_m_000018_203 : duration 0:00.007s
[2021-05-17 13:59:05,357] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658448708097175676084210_0004_m_000015_200: needsTaskCommit() Task attempt_202105171658448708097175676084210_0004_m_000015_200
[2021-05-17 13:59:05,359] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658448708097175676084210_0004_m_000015_200: needsTaskCommit() Task attempt_202105171658448708097175676084210_0004_m_000015_200: duration 0:00.002s
21/05/17 16:59:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448708097175676084210_0004_m_000015_200
[2021-05-17 13:59:05,361] {docker.py:276} INFO - 21/05/17 16:59:05 INFO Executor: Finished task 15.0 in stage 4.0 (TID 200). 4587 bytes result sent to driver
[2021-05-17 13:59:05,366] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 204) (bb4563f4559d, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 16:59:05 INFO Executor: Running task 19.0 in stage 4.0 (TID 204)
[2021-05-17 13:59:05,367] {docker.py:276} INFO - 21/05/17 16:59:05 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 200) in 2476 ms on bb4563f4559d (executor driver) (16/200)
[2021-05-17 13:59:05,375] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:05,376] {docker.py:276} INFO - 21/05/17 16:59:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:05,379] {docker.py:276} INFO - 21/05/17 16:59:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:05,380] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,381] {docker.py:276} INFO - 21/05/17 16:59:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441188267421908519377_0004_m_000019_204, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441188267421908519377_0004_m_000019_204}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441188267421908519377_0004}; taskId=attempt_202105171658441188267421908519377_0004_m_000019_204, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e454782}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:05,381] {docker.py:276} INFO - 21/05/17 16:59:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:05,382] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658441188267421908519377_0004_m_000019_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441188267421908519377_0004_m_000019_204
[2021-05-17 13:59:05,384] {docker.py:276} INFO - 21/05/17 16:59:05 INFO StagingCommitter: Task committer attempt_202105171658441188267421908519377_0004_m_000019_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441188267421908519377_0004_m_000019_204 : duration 0:00.004s
[2021-05-17 13:59:07,410] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658441188267421908519377_0004_m_000019_204: needsTaskCommit() Task attempt_202105171658441188267421908519377_0004_m_000019_204
[2021-05-17 13:59:07,411] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658441188267421908519377_0004_m_000019_204: needsTaskCommit() Task attempt_202105171658441188267421908519377_0004_m_000019_204: duration 0:00.001s
[2021-05-17 13:59:07,411] {docker.py:276} INFO - 21/05/17 16:59:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441188267421908519377_0004_m_000019_204
[2021-05-17 13:59:07,413] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Finished task 19.0 in stage 4.0 (TID 204). 4544 bytes result sent to driver
[2021-05-17 13:59:07,416] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 205) (bb4563f4559d, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:07,416] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Running task 20.0 in stage 4.0 (TID 205)
[2021-05-17 13:59:07,417] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 204) in 2057 ms on bb4563f4559d (executor driver) (17/200)
[2021-05-17 13:59:07,427] {docker.py:276} INFO - 21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:07,429] {docker.py:276} INFO - 21/05/17 16:59:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:07,429] {docker.py:276} INFO - 21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444971778483752891410_0004_m_000020_205, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444971778483752891410_0004_m_000020_205}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444971778483752891410_0004}; taskId=attempt_202105171658444971778483752891410_0004_m_000020_205, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4040987e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:07,430] {docker.py:276} INFO - 21/05/17 16:59:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:07,430] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658444971778483752891410_0004_m_000020_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444971778483752891410_0004_m_000020_205
[2021-05-17 13:59:07,435] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658444971778483752891410_0004_m_000020_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444971778483752891410_0004_m_000020_205 : duration 0:00.006s
[2021-05-17 13:59:07,511] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658447122914980708031321_0004_m_000016_201: needsTaskCommit() Task attempt_202105171658447122914980708031321_0004_m_000016_201
[2021-05-17 13:59:07,511] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658447122914980708031321_0004_m_000016_201: needsTaskCommit() Task attempt_202105171658447122914980708031321_0004_m_000016_201: duration 0:00.001s
21/05/17 16:59:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447122914980708031321_0004_m_000016_201
[2021-05-17 13:59:07,513] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Finished task 16.0 in stage 4.0 (TID 201). 4544 bytes result sent to driver
[2021-05-17 13:59:07,514] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 206) (bb4563f4559d, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:07,515] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 201) in 2530 ms on bb4563f4559d (executor driver) (18/200)
[2021-05-17 13:59:07,516] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Running task 21.0 in stage 4.0 (TID 206)
[2021-05-17 13:59:07,520] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658443520327833983327216_0004_m_000017_202: needsTaskCommit() Task attempt_202105171658443520327833983327216_0004_m_000017_202
[2021-05-17 13:59:07,520] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658443520327833983327216_0004_m_000017_202: needsTaskCommit() Task attempt_202105171658443520327833983327216_0004_m_000017_202: duration 0:00.001s
21/05/17 16:59:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443520327833983327216_0004_m_000017_202
[2021-05-17 13:59:07,521] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Finished task 17.0 in stage 4.0 (TID 202). 4544 bytes result sent to driver
[2021-05-17 13:59:07,522] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 207) (bb4563f4559d, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:07,523] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 202) in 2529 ms on bb4563f4559d (executor driver) (19/200)
[2021-05-17 13:59:07,524] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Running task 22.0 in stage 4.0 (TID 207)
[2021-05-17 13:59:07,540] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658441998924785008783056_0004_m_000018_203: needsTaskCommit() Task attempt_202105171658441998924785008783056_0004_m_000018_203
[2021-05-17 13:59:07,541] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658441998924785008783056_0004_m_000018_203: needsTaskCommit() Task attempt_202105171658441998924785008783056_0004_m_000018_203: duration 0:00.001s
[2021-05-17 13:59:07,542] {docker.py:276} INFO - 21/05/17 16:59:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441998924785008783056_0004_m_000018_203
[2021-05-17 13:59:07,549] {docker.py:276} INFO - 21/05/17 16:59:07 INFO Executor: Finished task 18.0 in stage 4.0 (TID 203). 4587 bytes result sent to driver
[2021-05-17 13:59:07,552] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 208) (bb4563f4559d, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 16:59:07 INFO Executor: Running task 23.0 in stage 4.0 (TID 208)
[2021-05-17 13:59:07,553] {docker.py:276} INFO - 21/05/17 16:59:07 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 203) in 2552 ms on bb4563f4559d (executor driver) (20/200)
[2021-05-17 13:59:07,555] {docker.py:276} INFO - 21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:07,556] {docker.py:276} INFO - 21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:07,558] {docker.py:276} INFO - 21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:07,562] {docker.py:276} INFO - 21/05/17 16:59:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:07,562] {docker.py:276} INFO - 21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-17 13:59:07,564] {docker.py:276} INFO - 21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442244590534050353238_0004_m_000022_207, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442244590534050353238_0004_m_000022_207}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442244590534050353238_0004}; taskId=attempt_202105171658442244590534050353238_0004_m_000022_207, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fe23bf6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658442244590534050353238_0004_m_000022_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442244590534050353238_0004_m_000022_207
[2021-05-17 13:59:07,568] {docker.py:276} INFO - 21/05/17 16:59:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:07,569] {docker.py:276} INFO - 21/05/17 16:59:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:07,573] {docker.py:276} INFO - 21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658442244590534050353238_0004_m_000022_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442244590534050353238_0004_m_000022_207 : duration 0:00.011s
[2021-05-17 13:59:07,575] {docker.py:276} INFO - 21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444378830727594181861_0004_m_000021_206, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444378830727594181861_0004_m_000021_206}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444378830727594181861_0004}; taskId=attempt_202105171658444378830727594181861_0004_m_000021_206, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6dc1c731}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658444378830727594181861_0004_m_000021_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444378830727594181861_0004_m_000021_206
[2021-05-17 13:59:07,584] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658444378830727594181861_0004_m_000021_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444378830727594181861_0004_m_000021_206 : duration 0:00.012s
[2021-05-17 13:59:07,606] {docker.py:276} INFO - 21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:07,609] {docker.py:276} INFO - 21/05/17 16:59:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:07,609] {docker.py:276} INFO - 21/05/17 16:59:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445275696099376123857_0004_m_000023_208, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445275696099376123857_0004_m_000023_208}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445275696099376123857_0004}; taskId=attempt_202105171658445275696099376123857_0004_m_000023_208, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@391352af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:07,610] {docker.py:276} INFO - 21/05/17 16:59:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658445275696099376123857_0004_m_000023_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445275696099376123857_0004_m_000023_208
[2021-05-17 13:59:07,615] {docker.py:276} INFO - 21/05/17 16:59:07 INFO StagingCommitter: Task committer attempt_202105171658445275696099376123857_0004_m_000023_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445275696099376123857_0004_m_000023_208 : duration 0:00.005s
[2021-05-17 13:59:09,542] {docker.py:276} INFO - 21/05/17 16:59:09 INFO StagingCommitter: Starting: Task committer attempt_202105171658444971778483752891410_0004_m_000020_205: needsTaskCommit() Task attempt_202105171658444971778483752891410_0004_m_000020_205
21/05/17 16:59:09 INFO StagingCommitter: Task committer attempt_202105171658444971778483752891410_0004_m_000020_205: needsTaskCommit() Task attempt_202105171658444971778483752891410_0004_m_000020_205: duration 0:00.002s
[2021-05-17 13:59:09,543] {docker.py:276} INFO - 21/05/17 16:59:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444971778483752891410_0004_m_000020_205
[2021-05-17 13:59:09,546] {docker.py:276} INFO - 21/05/17 16:59:09 INFO Executor: Finished task 20.0 in stage 4.0 (TID 205). 4587 bytes result sent to driver
[2021-05-17 13:59:09,548] {docker.py:276} INFO - 21/05/17 16:59:09 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 209) (bb4563f4559d, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:09,549] {docker.py:276} INFO - 21/05/17 16:59:09 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 205) in 2137 ms on bb4563f4559d (executor driver) (21/200)
[2021-05-17 13:59:09,550] {docker.py:276} INFO - 21/05/17 16:59:09 INFO Executor: Running task 24.0 in stage 4.0 (TID 209)
[2021-05-17 13:59:09,559] {docker.py:276} INFO - 21/05/17 16:59:09 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:09,560] {docker.py:276} INFO - 21/05/17 16:59:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:09,562] {docker.py:276} INFO - 21/05/17 16:59:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:09,562] {docker.py:276} INFO - 21/05/17 16:59:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444593329573115934784_0004_m_000024_209, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444593329573115934784_0004_m_000024_209}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444593329573115934784_0004}; taskId=attempt_202105171658444593329573115934784_0004_m_000024_209, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5437b616}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:09,562] {docker.py:276} INFO - 21/05/17 16:59:09 INFO StagingCommitter: Starting: Task committer attempt_202105171658444593329573115934784_0004_m_000024_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444593329573115934784_0004_m_000024_209
[2021-05-17 13:59:09,567] {docker.py:276} INFO - 21/05/17 16:59:09 INFO StagingCommitter: Task committer attempt_202105171658444593329573115934784_0004_m_000024_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444593329573115934784_0004_m_000024_209 : duration 0:00.005s
[2021-05-17 13:59:10,089] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658445275696099376123857_0004_m_000023_208: needsTaskCommit() Task attempt_202105171658445275696099376123857_0004_m_000023_208
21/05/17 16:59:10 INFO StagingCommitter: Task committer attempt_202105171658445275696099376123857_0004_m_000023_208: needsTaskCommit() Task attempt_202105171658445275696099376123857_0004_m_000023_208: duration 0:00.005s
21/05/17 16:59:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445275696099376123857_0004_m_000023_208
[2021-05-17 13:59:10,091] {docker.py:276} INFO - 21/05/17 16:59:10 INFO Executor: Finished task 23.0 in stage 4.0 (TID 208). 4544 bytes result sent to driver
[2021-05-17 13:59:10,092] {docker.py:276} INFO - 21/05/17 16:59:10 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 210) (bb4563f4559d, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:10,093] {docker.py:276} INFO - 21/05/17 16:59:10 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 208) in 2553 ms on bb4563f4559d (executor driver) (22/200)
[2021-05-17 13:59:10,094] {docker.py:276} INFO - 21/05/17 16:59:10 INFO Executor: Running task 25.0 in stage 4.0 (TID 210)
[2021-05-17 13:59:10,101] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658444378830727594181861_0004_m_000021_206: needsTaskCommit() Task attempt_202105171658444378830727594181861_0004_m_000021_206
[2021-05-17 13:59:10,101] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Task committer attempt_202105171658444378830727594181861_0004_m_000021_206: needsTaskCommit() Task attempt_202105171658444378830727594181861_0004_m_000021_206: duration 0:00.000s
21/05/17 16:59:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444378830727594181861_0004_m_000021_206
[2021-05-17 13:59:10,102] {docker.py:276} INFO - 21/05/17 16:59:10 INFO Executor: Finished task 21.0 in stage 4.0 (TID 206). 4587 bytes result sent to driver
[2021-05-17 13:59:10,103] {docker.py:276} INFO - 21/05/17 16:59:10 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 211) (bb4563f4559d, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:10,104] {docker.py:276} INFO - 21/05/17 16:59:10 INFO Executor: Running task 26.0 in stage 4.0 (TID 211)
[2021-05-17 13:59:10,104] {docker.py:276} INFO - 21/05/17 16:59:10 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 206) in 2593 ms on bb4563f4559d (executor driver) (23/200)
[2021-05-17 13:59:10,108] {docker.py:276} INFO - 21/05/17 16:59:10 INFO ShuffleBlockFetcherIterator: Getting 4 (27.2 KiB) non-empty blocks including 4 (27.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:10,110] {docker.py:276} INFO - 21/05/17 16:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:10,110] {docker.py:276} INFO - 21/05/17 16:59:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442431356645888694369_0004_m_000025_210, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442431356645888694369_0004_m_000025_210}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442431356645888694369_0004}; taskId=attempt_202105171658442431356645888694369_0004_m_000025_210, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7702237d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658442431356645888694369_0004_m_000025_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442431356645888694369_0004_m_000025_210
[2021-05-17 13:59:10,114] {docker.py:276} INFO - 21/05/17 16:59:10 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:10 INFO StagingCommitter: Task committer attempt_202105171658442431356645888694369_0004_m_000025_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442431356645888694369_0004_m_000025_210 : duration 0:00.004s
[2021-05-17 13:59:10,114] {docker.py:276} INFO - 21/05/17 16:59:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:10,116] {docker.py:276} INFO - 21/05/17 16:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:10,117] {docker.py:276} INFO - 21/05/17 16:59:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:10,118] {docker.py:276} INFO - 21/05/17 16:59:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445686461858290434404_0004_m_000026_211, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445686461858290434404_0004_m_000026_211}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445686461858290434404_0004}; taskId=attempt_202105171658445686461858290434404_0004_m_000026_211, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32904a4d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:10,118] {docker.py:276} INFO - 21/05/17 16:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:10,118] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658445686461858290434404_0004_m_000026_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445686461858290434404_0004_m_000026_211
[2021-05-17 13:59:10,119] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658442244590534050353238_0004_m_000022_207: needsTaskCommit() Task attempt_202105171658442244590534050353238_0004_m_000022_207
[2021-05-17 13:59:10,120] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Task committer attempt_202105171658442244590534050353238_0004_m_000022_207: needsTaskCommit() Task attempt_202105171658442244590534050353238_0004_m_000022_207: duration 0:00.001s
21/05/17 16:59:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442244590534050353238_0004_m_000022_207
[2021-05-17 13:59:10,122] {docker.py:276} INFO - 21/05/17 16:59:10 INFO Executor: Finished task 22.0 in stage 4.0 (TID 207). 4587 bytes result sent to driver
[2021-05-17 13:59:10,123] {docker.py:276} INFO - 21/05/17 16:59:10 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 212) (bb4563f4559d, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:10,123] {docker.py:276} INFO - 21/05/17 16:59:10 INFO Executor: Running task 27.0 in stage 4.0 (TID 212)
[2021-05-17 13:59:10,124] {docker.py:276} INFO - 21/05/17 16:59:10 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 207) in 2605 ms on bb4563f4559d (executor driver) (24/200)
[2021-05-17 13:59:10,129] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Task committer attempt_202105171658445686461858290434404_0004_m_000026_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445686461858290434404_0004_m_000026_211 : duration 0:00.010s
[2021-05-17 13:59:10,141] {docker.py:276} INFO - 21/05/17 16:59:10 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:10,143] {docker.py:276} INFO - 21/05/17 16:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:10,144] {docker.py:276} INFO - 21/05/17 16:59:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441886043694753435131_0004_m_000027_212, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441886043694753435131_0004_m_000027_212}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441886043694753435131_0004}; taskId=attempt_202105171658441886043694753435131_0004_m_000027_212, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c4e0401}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658441886043694753435131_0004_m_000027_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441886043694753435131_0004_m_000027_212
[2021-05-17 13:59:10,148] {docker.py:276} INFO - 21/05/17 16:59:10 INFO StagingCommitter: Task committer attempt_202105171658441886043694753435131_0004_m_000027_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441886043694753435131_0004_m_000027_212 : duration 0:00.005s
[2021-05-17 13:59:12,197] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658444593329573115934784_0004_m_000024_209: needsTaskCommit() Task attempt_202105171658444593329573115934784_0004_m_000024_209
[2021-05-17 13:59:12,199] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658444593329573115934784_0004_m_000024_209: needsTaskCommit() Task attempt_202105171658444593329573115934784_0004_m_000024_209: duration 0:00.002s
21/05/17 16:59:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444593329573115934784_0004_m_000024_209
[2021-05-17 13:59:12,200] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Finished task 24.0 in stage 4.0 (TID 209). 4544 bytes result sent to driver
[2021-05-17 13:59:12,202] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 213) (bb4563f4559d, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:12,203] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Running task 28.0 in stage 4.0 (TID 213)
[2021-05-17 13:59:12,204] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 209) in 2624 ms on bb4563f4559d (executor driver) (25/200)
[2021-05-17 13:59:12,219] {docker.py:276} INFO - 21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:12,222] {docker.py:276} INFO - 21/05/17 16:59:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:12,223] {docker.py:276} INFO - 21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447434205220102549415_0004_m_000028_213, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447434205220102549415_0004_m_000028_213}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447434205220102549415_0004}; taskId=attempt_202105171658447434205220102549415_0004_m_000028_213, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c6c9e81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:12,223] {docker.py:276} INFO - 21/05/17 16:59:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:12,223] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658447434205220102549415_0004_m_000028_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447434205220102549415_0004_m_000028_213
[2021-05-17 13:59:12,226] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658447434205220102549415_0004_m_000028_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447434205220102549415_0004_m_000028_213 : duration 0:00.004s
[2021-05-17 13:59:12,581] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658441886043694753435131_0004_m_000027_212: needsTaskCommit() Task attempt_202105171658441886043694753435131_0004_m_000027_212
[2021-05-17 13:59:12,582] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658441886043694753435131_0004_m_000027_212: needsTaskCommit() Task attempt_202105171658441886043694753435131_0004_m_000027_212: duration 0:00.002s
[2021-05-17 13:59:12,582] {docker.py:276} INFO - 21/05/17 16:59:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441886043694753435131_0004_m_000027_212
[2021-05-17 13:59:12,583] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Finished task 27.0 in stage 4.0 (TID 212). 4544 bytes result sent to driver
[2021-05-17 13:59:12,585] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 214) (bb4563f4559d, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:12,585] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 212) in 2431 ms on bb4563f4559d (executor driver) (26/200)
[2021-05-17 13:59:12,586] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Running task 29.0 in stage 4.0 (TID 214)
[2021-05-17 13:59:12,619] {docker.py:276} INFO - 21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:12,621] {docker.py:276} INFO - 21/05/17 16:59:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:12,622] {docker.py:276} INFO - 21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444708924616740912273_0004_m_000029_214, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444708924616740912273_0004_m_000029_214}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444708924616740912273_0004}; taskId=attempt_202105171658444708924616740912273_0004_m_000029_214, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@450709a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658444708924616740912273_0004_m_000029_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444708924616740912273_0004_m_000029_214
[2021-05-17 13:59:12,625] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658444708924616740912273_0004_m_000029_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444708924616740912273_0004_m_000029_214 : duration 0:00.003s
[2021-05-17 13:59:12,672] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658445686461858290434404_0004_m_000026_211: needsTaskCommit() Task attempt_202105171658445686461858290434404_0004_m_000026_211
[2021-05-17 13:59:12,673] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658445686461858290434404_0004_m_000026_211: needsTaskCommit() Task attempt_202105171658445686461858290434404_0004_m_000026_211: duration 0:00.001s
[2021-05-17 13:59:12,673] {docker.py:276} INFO - 21/05/17 16:59:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445686461858290434404_0004_m_000026_211
[2021-05-17 13:59:12,674] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Finished task 26.0 in stage 4.0 (TID 211). 4587 bytes result sent to driver
[2021-05-17 13:59:12,676] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 215) (bb4563f4559d, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:12,678] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 211) in 2543 ms on bb4563f4559d (executor driver) (27/200)
[2021-05-17 13:59:12,678] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Running task 30.0 in stage 4.0 (TID 215)
[2021-05-17 13:59:12,688] {docker.py:276} INFO - 21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Getting 4 (25.3 KiB) non-empty blocks including 4 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:12,689] {docker.py:276} INFO - 21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:12,692] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658442431356645888694369_0004_m_000025_210: needsTaskCommit() Task attempt_202105171658442431356645888694369_0004_m_000025_210
[2021-05-17 13:59:12,692] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658442431356645888694369_0004_m_000025_210: needsTaskCommit() Task attempt_202105171658442431356645888694369_0004_m_000025_210: duration 0:00.000s
21/05/17 16:59:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442431356645888694369_0004_m_000025_210
[2021-05-17 13:59:12,692] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Finished task 25.0 in stage 4.0 (TID 210). 4587 bytes result sent to driver
[2021-05-17 13:59:12,693] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 216) (bb4563f4559d, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:12,694] {docker.py:276} INFO - 21/05/17 16:59:12 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 210) in 2569 ms on bb4563f4559d (executor driver) (28/200)
[2021-05-17 13:59:12,695] {docker.py:276} INFO - 21/05/17 16:59:12 INFO Executor: Running task 31.0 in stage 4.0 (TID 216)
[2021-05-17 13:59:12,700] {docker.py:276} INFO - 21/05/17 16:59:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:12,700] {docker.py:276} INFO - 21/05/17 16:59:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:12,701] {docker.py:276} INFO - 21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:12,701] {docker.py:276} INFO - 21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442571139432857261704_0004_m_000030_215, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442571139432857261704_0004_m_000030_215}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442571139432857261704_0004}; taskId=attempt_202105171658442571139432857261704_0004_m_000030_215, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5afba6dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:12,702] {docker.py:276} INFO - 21/05/17 16:59:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:12,702] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658442571139432857261704_0004_m_000030_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442571139432857261704_0004_m_000030_215
[2021-05-17 13:59:12,704] {docker.py:276} INFO - 21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:12,705] {docker.py:276} INFO - 21/05/17 16:59:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:12,706] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658442571139432857261704_0004_m_000030_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442571139432857261704_0004_m_000030_215 : duration 0:00.003s
[2021-05-17 13:59:12,706] {docker.py:276} INFO - 21/05/17 16:59:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:12,707] {docker.py:276} INFO - 21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442354060234307035537_0004_m_000031_216, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442354060234307035537_0004_m_000031_216}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442354060234307035537_0004}; taskId=attempt_202105171658442354060234307035537_0004_m_000031_216, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42453870}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:12,707] {docker.py:276} INFO - 21/05/17 16:59:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658442354060234307035537_0004_m_000031_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442354060234307035537_0004_m_000031_216
[2021-05-17 13:59:12,714] {docker.py:276} INFO - 21/05/17 16:59:12 INFO StagingCommitter: Task committer attempt_202105171658442354060234307035537_0004_m_000031_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442354060234307035537_0004_m_000031_216 : duration 0:00.007s
[2021-05-17 13:59:14,728] {docker.py:276} INFO - 21/05/17 16:59:14 INFO StagingCommitter: Starting: Task committer attempt_202105171658447434205220102549415_0004_m_000028_213: needsTaskCommit() Task attempt_202105171658447434205220102549415_0004_m_000028_213
[2021-05-17 13:59:14,728] {docker.py:276} INFO - 21/05/17 16:59:14 INFO StagingCommitter: Task committer attempt_202105171658447434205220102549415_0004_m_000028_213: needsTaskCommit() Task attempt_202105171658447434205220102549415_0004_m_000028_213: duration 0:00.001s
21/05/17 16:59:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447434205220102549415_0004_m_000028_213
[2021-05-17 13:59:14,729] {docker.py:276} INFO - 21/05/17 16:59:14 INFO Executor: Finished task 28.0 in stage 4.0 (TID 213). 4587 bytes result sent to driver
[2021-05-17 13:59:14,731] {docker.py:276} INFO - 21/05/17 16:59:14 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 217) (bb4563f4559d, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:14,732] {docker.py:276} INFO - 21/05/17 16:59:14 INFO Executor: Running task 32.0 in stage 4.0 (TID 217)
[2021-05-17 13:59:14,732] {docker.py:276} INFO - 21/05/17 16:59:14 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 213) in 2533 ms on bb4563f4559d (executor driver) (29/200)
[2021-05-17 13:59:14,742] {docker.py:276} INFO - 21/05/17 16:59:14 INFO ShuffleBlockFetcherIterator: Getting 4 (27.3 KiB) non-empty blocks including 4 (27.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:14,742] {docker.py:276} INFO - 21/05/17 16:59:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:14,744] {docker.py:276} INFO - 21/05/17 16:59:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:14,745] {docker.py:276} INFO - 21/05/17 16:59:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:14,745] {docker.py:276} INFO - 21/05/17 16:59:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:14,746] {docker.py:276} INFO - 21/05/17 16:59:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441598812857380479814_0004_m_000032_217, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441598812857380479814_0004_m_000032_217}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441598812857380479814_0004}; taskId=attempt_202105171658441598812857380479814_0004_m_000032_217, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2fd0ba60}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:14,746] {docker.py:276} INFO - 21/05/17 16:59:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:14,747] {docker.py:276} INFO - 21/05/17 16:59:14 INFO StagingCommitter: Starting: Task committer attempt_202105171658441598812857380479814_0004_m_000032_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441598812857380479814_0004_m_000032_217
[2021-05-17 13:59:14,750] {docker.py:276} INFO - 21/05/17 16:59:14 INFO StagingCommitter: Task committer attempt_202105171658441598812857380479814_0004_m_000032_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441598812857380479814_0004_m_000032_217 : duration 0:00.004s
[2021-05-17 13:59:15,044] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658444708924616740912273_0004_m_000029_214: needsTaskCommit() Task attempt_202105171658444708924616740912273_0004_m_000029_214
[2021-05-17 13:59:15,044] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Task committer attempt_202105171658444708924616740912273_0004_m_000029_214: needsTaskCommit() Task attempt_202105171658444708924616740912273_0004_m_000029_214: duration 0:00.001s
21/05/17 16:59:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444708924616740912273_0004_m_000029_214
[2021-05-17 13:59:15,045] {docker.py:276} INFO - 21/05/17 16:59:15 INFO Executor: Finished task 29.0 in stage 4.0 (TID 214). 4587 bytes result sent to driver
[2021-05-17 13:59:15,046] {docker.py:276} INFO - 21/05/17 16:59:15 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 218) (bb4563f4559d, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:15,047] {docker.py:276} INFO - 21/05/17 16:59:15 INFO Executor: Running task 33.0 in stage 4.0 (TID 218)
[2021-05-17 13:59:15,047] {docker.py:276} INFO - 21/05/17 16:59:15 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 214) in 2465 ms on bb4563f4559d (executor driver) (30/200)
[2021-05-17 13:59:15,054] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658442571139432857261704_0004_m_000030_215: needsTaskCommit() Task attempt_202105171658442571139432857261704_0004_m_000030_215
[2021-05-17 13:59:15,054] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Task committer attempt_202105171658442571139432857261704_0004_m_000030_215: needsTaskCommit() Task attempt_202105171658442571139432857261704_0004_m_000030_215: duration 0:00.000s
21/05/17 16:59:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442571139432857261704_0004_m_000030_215
[2021-05-17 13:59:15,056] {docker.py:276} INFO - 21/05/17 16:59:15 INFO Executor: Finished task 30.0 in stage 4.0 (TID 215). 4544 bytes result sent to driver
[2021-05-17 13:59:15,058] {docker.py:276} INFO - 21/05/17 16:59:15 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 219) (bb4563f4559d, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:15,058] {docker.py:276} INFO - 21/05/17 16:59:15 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 215) in 2385 ms on bb4563f4559d (executor driver) (31/200)
[2021-05-17 13:59:15,059] {docker.py:276} INFO - 21/05/17 16:59:15 INFO Executor: Running task 34.0 in stage 4.0 (TID 219)
[2021-05-17 13:59:15,061] {docker.py:276} INFO - 21/05/17 16:59:15 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:15,064] {docker.py:276} INFO - 21/05/17 16:59:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:15,065] {docker.py:276} INFO - 21/05/17 16:59:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443194596841075567711_0004_m_000033_218, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443194596841075567711_0004_m_000033_218}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443194596841075567711_0004}; taskId=attempt_202105171658443194596841075567711_0004_m_000033_218, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76b8e75c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658443194596841075567711_0004_m_000033_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443194596841075567711_0004_m_000033_218
[2021-05-17 13:59:15,069] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Task committer attempt_202105171658443194596841075567711_0004_m_000033_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443194596841075567711_0004_m_000033_218 : duration 0:00.004s
[2021-05-17 13:59:15,072] {docker.py:276} INFO - 21/05/17 16:59:15 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:15,073] {docker.py:276} INFO - 21/05/17 16:59:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:15,075] {docker.py:276} INFO - 21/05/17 16:59:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:15,075] {docker.py:276} INFO - 21/05/17 16:59:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448250525140114258320_0004_m_000034_219, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448250525140114258320_0004_m_000034_219}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448250525140114258320_0004}; taskId=attempt_202105171658448250525140114258320_0004_m_000034_219, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7836a632}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658448250525140114258320_0004_m_000034_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448250525140114258320_0004_m_000034_219
[2021-05-17 13:59:15,078] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Task committer attempt_202105171658448250525140114258320_0004_m_000034_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448250525140114258320_0004_m_000034_219 : duration 0:00.003s
[2021-05-17 13:59:15,223] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658442354060234307035537_0004_m_000031_216: needsTaskCommit() Task attempt_202105171658442354060234307035537_0004_m_000031_216
[2021-05-17 13:59:15,224] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Task committer attempt_202105171658442354060234307035537_0004_m_000031_216: needsTaskCommit() Task attempt_202105171658442354060234307035537_0004_m_000031_216: duration 0:00.001s
21/05/17 16:59:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442354060234307035537_0004_m_000031_216
[2021-05-17 13:59:15,225] {docker.py:276} INFO - 21/05/17 16:59:15 INFO Executor: Finished task 31.0 in stage 4.0 (TID 216). 4544 bytes result sent to driver
[2021-05-17 13:59:15,226] {docker.py:276} INFO - 21/05/17 16:59:15 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 220) (bb4563f4559d, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:15,227] {docker.py:276} INFO - 21/05/17 16:59:15 INFO Executor: Running task 35.0 in stage 4.0 (TID 220)
[2021-05-17 13:59:15,227] {docker.py:276} INFO - 21/05/17 16:59:15 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 216) in 2537 ms on bb4563f4559d (executor driver) (32/200)
[2021-05-17 13:59:15,236] {docker.py:276} INFO - 21/05/17 16:59:15 INFO ShuffleBlockFetcherIterator: Getting 4 (28.6 KiB) non-empty blocks including 4 (28.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:15,237] {docker.py:276} INFO - 21/05/17 16:59:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:15,239] {docker.py:276} INFO - 21/05/17 16:59:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:15,240] {docker.py:276} INFO - 21/05/17 16:59:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:15,240] {docker.py:276} INFO - 21/05/17 16:59:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443696618397157356294_0004_m_000035_220, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443696618397157356294_0004_m_000035_220}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443696618397157356294_0004}; taskId=attempt_202105171658443696618397157356294_0004_m_000035_220, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7496a917}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:15,241] {docker.py:276} INFO - 21/05/17 16:59:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:15,242] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658443696618397157356294_0004_m_000035_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443696618397157356294_0004_m_000035_220
[2021-05-17 13:59:15,247] {docker.py:276} INFO - 21/05/17 16:59:15 INFO StagingCommitter: Task committer attempt_202105171658443696618397157356294_0004_m_000035_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443696618397157356294_0004_m_000035_220 : duration 0:00.006s
[2021-05-17 13:59:17,574] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Starting: Task committer attempt_202105171658448250525140114258320_0004_m_000034_219: needsTaskCommit() Task attempt_202105171658448250525140114258320_0004_m_000034_219
[2021-05-17 13:59:17,574] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Task committer attempt_202105171658448250525140114258320_0004_m_000034_219: needsTaskCommit() Task attempt_202105171658448250525140114258320_0004_m_000034_219: duration 0:00.003s
21/05/17 16:59:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448250525140114258320_0004_m_000034_219
[2021-05-17 13:59:17,576] {docker.py:276} INFO - 21/05/17 16:59:17 INFO Executor: Finished task 34.0 in stage 4.0 (TID 219). 4544 bytes result sent to driver
[2021-05-17 13:59:17,576] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Starting: Task committer attempt_202105171658443194596841075567711_0004_m_000033_218: needsTaskCommit() Task attempt_202105171658443194596841075567711_0004_m_000033_218
[2021-05-17 13:59:17,577] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Task committer attempt_202105171658443194596841075567711_0004_m_000033_218: needsTaskCommit() Task attempt_202105171658443194596841075567711_0004_m_000033_218: duration 0:00.001s
21/05/17 16:59:17 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 221) (bb4563f4559d, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 16:59:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443194596841075567711_0004_m_000033_218
[2021-05-17 13:59:17,578] {docker.py:276} INFO - 21/05/17 16:59:17 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 219) in 2524 ms on bb4563f4559d (executor driver) (33/200)
[2021-05-17 13:59:17,578] {docker.py:276} INFO - 21/05/17 16:59:17 INFO Executor: Finished task 33.0 in stage 4.0 (TID 218). 4544 bytes result sent to driver
[2021-05-17 13:59:17,579] {docker.py:276} INFO - 21/05/17 16:59:17 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 222) (bb4563f4559d, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:17,583] {docker.py:276} INFO - 21/05/17 16:59:17 INFO Executor: Running task 37.0 in stage 4.0 (TID 222)
21/05/17 16:59:17 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 218) in 2537 ms on bb4563f4559d (executor driver) (34/200)
21/05/17 16:59:17 INFO Executor: Running task 36.0 in stage 4.0 (TID 221)
[2021-05-17 13:59:17,591] {docker.py:276} INFO - 21/05/17 16:59:17 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:17,591] {docker.py:276} INFO - 21/05/17 16:59:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:17,592] {docker.py:276} INFO - 21/05/17 16:59:17 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:17,593] {docker.py:276} INFO - 21/05/17 16:59:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:17,593] {docker.py:276} INFO - 21/05/17 16:59:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:17,594] {docker.py:276} INFO - 21/05/17 16:59:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447906878975777556840_0004_m_000036_221, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447906878975777556840_0004_m_000036_221}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447906878975777556840_0004}; taskId=attempt_202105171658447906878975777556840_0004_m_000036_221, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7deca826}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:17,594] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Starting: Task committer attempt_202105171658447906878975777556840_0004_m_000036_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447906878975777556840_0004_m_000036_221
[2021-05-17 13:59:17,595] {docker.py:276} INFO - 21/05/17 16:59:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:17,596] {docker.py:276} INFO - 21/05/17 16:59:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:17,596] {docker.py:276} INFO - 21/05/17 16:59:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442082771117797253439_0004_m_000037_222, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442082771117797253439_0004_m_000037_222}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442082771117797253439_0004}; taskId=attempt_202105171658442082771117797253439_0004_m_000037_222, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74d1450e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:17,596] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Starting: Task committer attempt_202105171658442082771117797253439_0004_m_000037_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442082771117797253439_0004_m_000037_222
[2021-05-17 13:59:17,596] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Task committer attempt_202105171658447906878975777556840_0004_m_000036_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447906878975777556840_0004_m_000036_221 : duration 0:00.003s
[2021-05-17 13:59:17,600] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Task committer attempt_202105171658442082771117797253439_0004_m_000037_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442082771117797253439_0004_m_000037_222 : duration 0:00.005s
[2021-05-17 13:59:17,892] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Starting: Task committer attempt_202105171658443696618397157356294_0004_m_000035_220: needsTaskCommit() Task attempt_202105171658443696618397157356294_0004_m_000035_220
[2021-05-17 13:59:17,893] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Task committer attempt_202105171658443696618397157356294_0004_m_000035_220: needsTaskCommit() Task attempt_202105171658443696618397157356294_0004_m_000035_220: duration 0:00.000s
[2021-05-17 13:59:17,893] {docker.py:276} INFO - 21/05/17 16:59:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443696618397157356294_0004_m_000035_220
[2021-05-17 13:59:17,894] {docker.py:276} INFO - 21/05/17 16:59:17 INFO Executor: Finished task 35.0 in stage 4.0 (TID 220). 4587 bytes result sent to driver
[2021-05-17 13:59:17,896] {docker.py:276} INFO - 21/05/17 16:59:17 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 223) (bb4563f4559d, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:17,896] {docker.py:276} INFO - 21/05/17 16:59:17 INFO Executor: Running task 38.0 in stage 4.0 (TID 223)
[2021-05-17 13:59:17,897] {docker.py:276} INFO - 21/05/17 16:59:17 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 220) in 2675 ms on bb4563f4559d (executor driver) (35/200)
[2021-05-17 13:59:17,905] {docker.py:276} INFO - 21/05/17 16:59:17 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:17,907] {docker.py:276} INFO - 21/05/17 16:59:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:17,908] {docker.py:276} INFO - 21/05/17 16:59:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:17,908] {docker.py:276} INFO - 21/05/17 16:59:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:17,908] {docker.py:276} INFO - 21/05/17 16:59:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844618232487680892032_0004_m_000038_223, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844618232487680892032_0004_m_000038_223}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844618232487680892032_0004}; taskId=attempt_20210517165844618232487680892032_0004_m_000038_223, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43cfb98e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:17,909] {docker.py:276} INFO - 21/05/17 16:59:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:17,909] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Starting: Task committer attempt_20210517165844618232487680892032_0004_m_000038_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844618232487680892032_0004_m_000038_223
[2021-05-17 13:59:17,912] {docker.py:276} INFO - 21/05/17 16:59:17 INFO StagingCommitter: Task committer attempt_20210517165844618232487680892032_0004_m_000038_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844618232487680892032_0004_m_000038_223 : duration 0:00.003s
[2021-05-17 13:59:18,298] {docker.py:276} INFO - 21/05/17 16:59:18 INFO StagingCommitter: Starting: Task committer attempt_202105171658441598812857380479814_0004_m_000032_217: needsTaskCommit() Task attempt_202105171658441598812857380479814_0004_m_000032_217
[2021-05-17 13:59:18,299] {docker.py:276} INFO - 21/05/17 16:59:18 INFO StagingCommitter: Task committer attempt_202105171658441598812857380479814_0004_m_000032_217: needsTaskCommit() Task attempt_202105171658441598812857380479814_0004_m_000032_217: duration 0:00.001s
21/05/17 16:59:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441598812857380479814_0004_m_000032_217
[2021-05-17 13:59:18,301] {docker.py:276} INFO - 21/05/17 16:59:18 INFO Executor: Finished task 32.0 in stage 4.0 (TID 217). 4587 bytes result sent to driver
[2021-05-17 13:59:18,303] {docker.py:276} INFO - 21/05/17 16:59:18 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 224) (bb4563f4559d, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:18,304] {docker.py:276} INFO - 21/05/17 16:59:18 INFO Executor: Running task 39.0 in stage 4.0 (TID 224)
[2021-05-17 13:59:18,305] {docker.py:276} INFO - 21/05/17 16:59:18 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 217) in 3579 ms on bb4563f4559d (executor driver) (36/200)
[2021-05-17 13:59:18,315] {docker.py:276} INFO - 21/05/17 16:59:18 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:18,317] {docker.py:276} INFO - 21/05/17 16:59:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:18,317] {docker.py:276} INFO - 21/05/17 16:59:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:18,317] {docker.py:276} INFO - 21/05/17 16:59:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:18,318] {docker.py:276} INFO - 21/05/17 16:59:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441977655116258858694_0004_m_000039_224, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441977655116258858694_0004_m_000039_224}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441977655116258858694_0004}; taskId=attempt_202105171658441977655116258858694_0004_m_000039_224, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7584ebc7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:18,318] {docker.py:276} INFO - 21/05/17 16:59:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:18,318] {docker.py:276} INFO - 21/05/17 16:59:18 INFO StagingCommitter: Starting: Task committer attempt_202105171658441977655116258858694_0004_m_000039_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441977655116258858694_0004_m_000039_224
[2021-05-17 13:59:18,321] {docker.py:276} INFO - 21/05/17 16:59:18 INFO StagingCommitter: Task committer attempt_202105171658441977655116258858694_0004_m_000039_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441977655116258858694_0004_m_000039_224 : duration 0:00.003s
[2021-05-17 13:59:20,048] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658442082771117797253439_0004_m_000037_222: needsTaskCommit() Task attempt_202105171658442082771117797253439_0004_m_000037_222
[2021-05-17 13:59:20,049] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658442082771117797253439_0004_m_000037_222: needsTaskCommit() Task attempt_202105171658442082771117797253439_0004_m_000037_222: duration 0:00.001s
21/05/17 16:59:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442082771117797253439_0004_m_000037_222
[2021-05-17 13:59:20,051] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Finished task 37.0 in stage 4.0 (TID 222). 4587 bytes result sent to driver
[2021-05-17 13:59:20,054] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 225) (bb4563f4559d, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:20,055] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 222) in 2478 ms on bb4563f4559d (executor driver) (37/200)
[2021-05-17 13:59:20,056] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Running task 40.0 in stage 4.0 (TID 225)
[2021-05-17 13:59:20,066] {docker.py:276} INFO - 21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:20,068] {docker.py:276} INFO - 21/05/17 16:59:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:20,069] {docker.py:276} INFO - 21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:20,070] {docker.py:276} INFO - 21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441012635503363717788_0004_m_000040_225, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441012635503363717788_0004_m_000040_225}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441012635503363717788_0004}; taskId=attempt_202105171658441012635503363717788_0004_m_000040_225, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e247355}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:20,070] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658441012635503363717788_0004_m_000040_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441012635503363717788_0004_m_000040_225
[2021-05-17 13:59:20,074] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658441012635503363717788_0004_m_000040_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441012635503363717788_0004_m_000040_225 : duration 0:00.005s
[2021-05-17 13:59:20,104] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658447906878975777556840_0004_m_000036_221: needsTaskCommit() Task attempt_202105171658447906878975777556840_0004_m_000036_221
21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658447906878975777556840_0004_m_000036_221: needsTaskCommit() Task attempt_202105171658447906878975777556840_0004_m_000036_221: duration 0:00.000s
21/05/17 16:59:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447906878975777556840_0004_m_000036_221
[2021-05-17 13:59:20,106] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Finished task 36.0 in stage 4.0 (TID 221). 4587 bytes result sent to driver
[2021-05-17 13:59:20,107] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 226) (bb4563f4559d, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:20,108] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Running task 41.0 in stage 4.0 (TID 226)
21/05/17 16:59:20 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 221) in 2534 ms on bb4563f4559d (executor driver) (38/200)
[2021-05-17 13:59:20,117] {docker.py:276} INFO - 21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:20,119] {docker.py:276} INFO - 21/05/17 16:59:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442916040460024494898_0004_m_000041_226, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442916040460024494898_0004_m_000041_226}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442916040460024494898_0004}; taskId=attempt_202105171658442916040460024494898_0004_m_000041_226, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77d59026}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:20,119] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658442916040460024494898_0004_m_000041_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442916040460024494898_0004_m_000041_226
[2021-05-17 13:59:20,122] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658442916040460024494898_0004_m_000041_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442916040460024494898_0004_m_000041_226 : duration 0:00.003s
[2021-05-17 13:59:20,226] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_20210517165844618232487680892032_0004_m_000038_223: needsTaskCommit() Task attempt_20210517165844618232487680892032_0004_m_000038_223
[2021-05-17 13:59:20,227] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_20210517165844618232487680892032_0004_m_000038_223: needsTaskCommit() Task attempt_20210517165844618232487680892032_0004_m_000038_223: duration 0:00.000s
21/05/17 16:59:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844618232487680892032_0004_m_000038_223
[2021-05-17 13:59:20,231] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Finished task 38.0 in stage 4.0 (TID 223). 4544 bytes result sent to driver
[2021-05-17 13:59:20,233] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 227) (bb4563f4559d, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:20,234] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 223) in 2340 ms on bb4563f4559d (executor driver) (39/200)
[2021-05-17 13:59:20,235] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Running task 42.0 in stage 4.0 (TID 227)
[2021-05-17 13:59:20,245] {docker.py:276} INFO - 21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Getting 4 (25.2 KiB) non-empty blocks including 4 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:20,246] {docker.py:276} INFO - 21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:20,248] {docker.py:276} INFO - 21/05/17 16:59:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:20,249] {docker.py:276} INFO - 21/05/17 16:59:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:20,249] {docker.py:276} INFO - 21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:20,250] {docker.py:276} INFO - 21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445086370411960124062_0004_m_000042_227, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445086370411960124062_0004_m_000042_227}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445086370411960124062_0004}; taskId=attempt_202105171658445086370411960124062_0004_m_000042_227, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d2fdffc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:20,251] {docker.py:276} INFO - 21/05/17 16:59:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658445086370411960124062_0004_m_000042_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445086370411960124062_0004_m_000042_227
[2021-05-17 13:59:20,255] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658445086370411960124062_0004_m_000042_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445086370411960124062_0004_m_000042_227 : duration 0:00.005s
[2021-05-17 13:59:20,271] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658441977655116258858694_0004_m_000039_224: needsTaskCommit() Task attempt_202105171658441977655116258858694_0004_m_000039_224
[2021-05-17 13:59:20,272] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658441977655116258858694_0004_m_000039_224: needsTaskCommit() Task attempt_202105171658441977655116258858694_0004_m_000039_224: duration 0:00.001s
[2021-05-17 13:59:20,273] {docker.py:276} INFO - 21/05/17 16:59:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441977655116258858694_0004_m_000039_224
[2021-05-17 13:59:20,273] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Finished task 39.0 in stage 4.0 (TID 224). 4544 bytes result sent to driver
[2021-05-17 13:59:20,274] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 228) (bb4563f4559d, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:20,275] {docker.py:276} INFO - 21/05/17 16:59:20 INFO Executor: Running task 43.0 in stage 4.0 (TID 228)
[2021-05-17 13:59:20,276] {docker.py:276} INFO - 21/05/17 16:59:20 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 224) in 1976 ms on bb4563f4559d (executor driver) (40/200)
[2021-05-17 13:59:20,285] {docker.py:276} INFO - 21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:20,286] {docker.py:276} INFO - 21/05/17 16:59:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:20,288] {docker.py:276} INFO - 21/05/17 16:59:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:20,289] {docker.py:276} INFO - 21/05/17 16:59:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:20,290] {docker.py:276} INFO - 21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:20,290] {docker.py:276} INFO - 21/05/17 16:59:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444868023466017759647_0004_m_000043_228, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444868023466017759647_0004_m_000043_228}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444868023466017759647_0004}; taskId=attempt_202105171658444868023466017759647_0004_m_000043_228, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d3f6609}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:20,291] {docker.py:276} INFO - 21/05/17 16:59:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:20,291] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658444868023466017759647_0004_m_000043_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444868023466017759647_0004_m_000043_228
[2021-05-17 13:59:20,294] {docker.py:276} INFO - 21/05/17 16:59:20 INFO StagingCommitter: Task committer attempt_202105171658444868023466017759647_0004_m_000043_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444868023466017759647_0004_m_000043_228 : duration 0:00.004s
[2021-05-17 13:59:22,570] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658441012635503363717788_0004_m_000040_225: needsTaskCommit() Task attempt_202105171658441012635503363717788_0004_m_000040_225
[2021-05-17 13:59:22,572] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658441012635503363717788_0004_m_000040_225: needsTaskCommit() Task attempt_202105171658441012635503363717788_0004_m_000040_225: duration 0:00.001s
21/05/17 16:59:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441012635503363717788_0004_m_000040_225
[2021-05-17 13:59:22,574] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Finished task 40.0 in stage 4.0 (TID 225). 4544 bytes result sent to driver
[2021-05-17 13:59:22,576] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 229) (bb4563f4559d, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:22,578] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Running task 44.0 in stage 4.0 (TID 229)
21/05/17 16:59:22 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 225) in 2527 ms on bb4563f4559d (executor driver) (41/200)
[2021-05-17 13:59:22,587] {docker.py:276} INFO - 21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Getting 4 (27.6 KiB) non-empty blocks including 4 (27.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:22,589] {docker.py:276} INFO - 21/05/17 16:59:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442308085293153758276_0004_m_000044_229, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442308085293153758276_0004_m_000044_229}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442308085293153758276_0004}; taskId=attempt_202105171658442308085293153758276_0004_m_000044_229, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2723422b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658442308085293153758276_0004_m_000044_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442308085293153758276_0004_m_000044_229
[2021-05-17 13:59:22,591] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658442308085293153758276_0004_m_000044_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442308085293153758276_0004_m_000044_229 : duration 0:00.003s
[2021-05-17 13:59:22,630] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658445086370411960124062_0004_m_000042_227: needsTaskCommit() Task attempt_202105171658445086370411960124062_0004_m_000042_227
[2021-05-17 13:59:22,631] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658445086370411960124062_0004_m_000042_227: needsTaskCommit() Task attempt_202105171658445086370411960124062_0004_m_000042_227: duration 0:00.000s
21/05/17 16:59:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445086370411960124062_0004_m_000042_227
[2021-05-17 13:59:22,632] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Finished task 42.0 in stage 4.0 (TID 227). 4544 bytes result sent to driver
[2021-05-17 13:59:22,633] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 230) (bb4563f4559d, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:22,634] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 227) in 2405 ms on bb4563f4559d (executor driver) (42/200)
[2021-05-17 13:59:22,635] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Running task 45.0 in stage 4.0 (TID 230)
[2021-05-17 13:59:22,644] {docker.py:276} INFO - 21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Getting 4 (25.1 KiB) non-empty blocks including 4 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:22,646] {docker.py:276} INFO - 21/05/17 16:59:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:22,647] {docker.py:276} INFO - 21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446198040721201993609_0004_m_000045_230, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446198040721201993609_0004_m_000045_230}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446198040721201993609_0004}; taskId=attempt_202105171658446198040721201993609_0004_m_000045_230, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34a041a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:22,647] {docker.py:276} INFO - 21/05/17 16:59:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:22,647] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658446198040721201993609_0004_m_000045_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446198040721201993609_0004_m_000045_230
[2021-05-17 13:59:22,649] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658442916040460024494898_0004_m_000041_226: needsTaskCommit() Task attempt_202105171658442916040460024494898_0004_m_000041_226
[2021-05-17 13:59:22,649] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658442916040460024494898_0004_m_000041_226: needsTaskCommit() Task attempt_202105171658442916040460024494898_0004_m_000041_226: duration 0:00.000s
21/05/17 16:59:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442916040460024494898_0004_m_000041_226
[2021-05-17 13:59:22,650] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658446198040721201993609_0004_m_000045_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446198040721201993609_0004_m_000045_230 : duration 0:00.002s
[2021-05-17 13:59:22,650] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Finished task 41.0 in stage 4.0 (TID 226). 4544 bytes result sent to driver
[2021-05-17 13:59:22,655] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 231) (bb4563f4559d, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:22,655] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Running task 46.0 in stage 4.0 (TID 231)
[2021-05-17 13:59:22,656] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 226) in 2548 ms on bb4563f4559d (executor driver) (43/200)
[2021-05-17 13:59:22,668] {docker.py:276} INFO - 21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Getting 4 (26.5 KiB) non-empty blocks including 4 (26.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:22,671] {docker.py:276} INFO - 21/05/17 16:59:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:22,671] {docker.py:276} INFO - 21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448177550689463032587_0004_m_000046_231, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448177550689463032587_0004_m_000046_231}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448177550689463032587_0004}; taskId=attempt_202105171658448177550689463032587_0004_m_000046_231, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70b3f5f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:22,671] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658448177550689463032587_0004_m_000046_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448177550689463032587_0004_m_000046_231
[2021-05-17 13:59:22,674] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658448177550689463032587_0004_m_000046_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448177550689463032587_0004_m_000046_231 : duration 0:00.003s
[2021-05-17 13:59:22,729] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658444868023466017759647_0004_m_000043_228: needsTaskCommit() Task attempt_202105171658444868023466017759647_0004_m_000043_228
[2021-05-17 13:59:22,730] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658444868023466017759647_0004_m_000043_228: needsTaskCommit() Task attempt_202105171658444868023466017759647_0004_m_000043_228: duration 0:00.000s
21/05/17 16:59:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444868023466017759647_0004_m_000043_228
[2021-05-17 13:59:22,732] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Finished task 43.0 in stage 4.0 (TID 228). 4587 bytes result sent to driver
[2021-05-17 13:59:22,733] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 232) (bb4563f4559d, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:22,734] {docker.py:276} INFO - 21/05/17 16:59:22 INFO Executor: Running task 47.0 in stage 4.0 (TID 232)
[2021-05-17 13:59:22,735] {docker.py:276} INFO - 21/05/17 16:59:22 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 228) in 2464 ms on bb4563f4559d (executor driver) (44/200)
[2021-05-17 13:59:22,748] {docker.py:276} INFO - 21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Getting 4 (25.3 KiB) non-empty blocks including 4 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:22,748] {docker.py:276} INFO - 21/05/17 16:59:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:22,750] {docker.py:276} INFO - 21/05/17 16:59:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:22,751] {docker.py:276} INFO - 21/05/17 16:59:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:22,751] {docker.py:276} INFO - 21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442282072038481000605_0004_m_000047_232, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442282072038481000605_0004_m_000047_232}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442282072038481000605_0004}; taskId=attempt_202105171658442282072038481000605_0004_m_000047_232, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@248d0df9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:22,752] {docker.py:276} INFO - 21/05/17 16:59:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:22,752] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Starting: Task committer attempt_202105171658442282072038481000605_0004_m_000047_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442282072038481000605_0004_m_000047_232
[2021-05-17 13:59:22,754] {docker.py:276} INFO - 21/05/17 16:59:22 INFO StagingCommitter: Task committer attempt_202105171658442282072038481000605_0004_m_000047_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442282072038481000605_0004_m_000047_232 : duration 0:00.003s
[2021-05-17 13:59:25,076] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658448177550689463032587_0004_m_000046_231: needsTaskCommit() Task attempt_202105171658448177550689463032587_0004_m_000046_231
[2021-05-17 13:59:25,077] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658448177550689463032587_0004_m_000046_231: needsTaskCommit() Task attempt_202105171658448177550689463032587_0004_m_000046_231: duration 0:00.002s
[2021-05-17 13:59:25,078] {docker.py:276} INFO - 21/05/17 16:59:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448177550689463032587_0004_m_000046_231
[2021-05-17 13:59:25,079] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Finished task 46.0 in stage 4.0 (TID 231). 4587 bytes result sent to driver
[2021-05-17 13:59:25,081] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 233) (bb4563f4559d, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:25,082] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 231) in 2433 ms on bb4563f4559d (executor driver) (45/200)
[2021-05-17 13:59:25,083] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Running task 48.0 in stage 4.0 (TID 233)
[2021-05-17 13:59:25,084] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658446198040721201993609_0004_m_000045_230: needsTaskCommit() Task attempt_202105171658446198040721201993609_0004_m_000045_230
[2021-05-17 13:59:25,085] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658446198040721201993609_0004_m_000045_230: needsTaskCommit() Task attempt_202105171658446198040721201993609_0004_m_000045_230: duration 0:00.000s
21/05/17 16:59:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446198040721201993609_0004_m_000045_230
[2021-05-17 13:59:25,085] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Finished task 45.0 in stage 4.0 (TID 230). 4587 bytes result sent to driver
[2021-05-17 13:59:25,087] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 234) (bb4563f4559d, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:25,088] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Running task 49.0 in stage 4.0 (TID 234)
[2021-05-17 13:59:25,089] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 230) in 2459 ms on bb4563f4559d (executor driver) (46/200)
[2021-05-17 13:59:25,095] {docker.py:276} INFO - 21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:25,097] {docker.py:276} INFO - 21/05/17 16:59:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:25,098] {docker.py:276} INFO - 21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446524890418287798827_0004_m_000048_233, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446524890418287798827_0004_m_000048_233}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446524890418287798827_0004}; taskId=attempt_202105171658446524890418287798827_0004_m_000048_233, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bcf5ef8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658446524890418287798827_0004_m_000048_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446524890418287798827_0004_m_000048_233 
21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:25,100] {docker.py:276} INFO - 21/05/17 16:59:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:25,101] {docker.py:276} INFO - 21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447594355955429708788_0004_m_000049_234, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447594355955429708788_0004_m_000049_234}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447594355955429708788_0004}; taskId=attempt_202105171658447594355955429708788_0004_m_000049_234, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f1ba5b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:25,102] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658447594355955429708788_0004_m_000049_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447594355955429708788_0004_m_000049_234
[2021-05-17 13:59:25,102] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658446524890418287798827_0004_m_000048_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446524890418287798827_0004_m_000048_233 : duration 0:00.005s
[2021-05-17 13:59:25,106] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658447594355955429708788_0004_m_000049_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447594355955429708788_0004_m_000049_234 : duration 0:00.005s
[2021-05-17 13:59:25,156] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658442282072038481000605_0004_m_000047_232: needsTaskCommit() Task attempt_202105171658442282072038481000605_0004_m_000047_232
[2021-05-17 13:59:25,156] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658442282072038481000605_0004_m_000047_232: needsTaskCommit() Task attempt_202105171658442282072038481000605_0004_m_000047_232: duration 0:00.000s
[2021-05-17 13:59:25,156] {docker.py:276} INFO - 21/05/17 16:59:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442282072038481000605_0004_m_000047_232
[2021-05-17 13:59:25,158] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Finished task 47.0 in stage 4.0 (TID 232). 4544 bytes result sent to driver
[2021-05-17 13:59:25,159] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 235) (bb4563f4559d, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:25,160] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 232) in 2430 ms on bb4563f4559d (executor driver) (47/200)
[2021-05-17 13:59:25,160] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Running task 50.0 in stage 4.0 (TID 235)
[2021-05-17 13:59:25,169] {docker.py:276} INFO - 21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Getting 4 (28.0 KiB) non-empty blocks including 4 (28.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:25,171] {docker.py:276} INFO - 21/05/17 16:59:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445333503466071300395_0004_m_000050_235, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445333503466071300395_0004_m_000050_235}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445333503466071300395_0004}; taskId=attempt_202105171658445333503466071300395_0004_m_000050_235, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63229c3d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:25,172] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658445333503466071300395_0004_m_000050_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445333503466071300395_0004_m_000050_235
[2021-05-17 13:59:25,174] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658445333503466071300395_0004_m_000050_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445333503466071300395_0004_m_000050_235 : duration 0:00.003s
[2021-05-17 13:59:25,181] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658442308085293153758276_0004_m_000044_229: needsTaskCommit() Task attempt_202105171658442308085293153758276_0004_m_000044_229
[2021-05-17 13:59:25,181] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658442308085293153758276_0004_m_000044_229: needsTaskCommit() Task attempt_202105171658442308085293153758276_0004_m_000044_229: duration 0:00.000s
[2021-05-17 13:59:25,182] {docker.py:276} INFO - 21/05/17 16:59:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442308085293153758276_0004_m_000044_229
[2021-05-17 13:59:25,183] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Finished task 44.0 in stage 4.0 (TID 229). 4587 bytes result sent to driver
[2021-05-17 13:59:25,184] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 236) (bb4563f4559d, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:25,185] {docker.py:276} INFO - 21/05/17 16:59:25 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 229) in 2614 ms on bb4563f4559d (executor driver) (48/200)
[2021-05-17 13:59:25,185] {docker.py:276} INFO - 21/05/17 16:59:25 INFO Executor: Running task 51.0 in stage 4.0 (TID 236)
[2021-05-17 13:59:25,196] {docker.py:276} INFO - 21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:25,198] {docker.py:276} INFO - 21/05/17 16:59:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:25,198] {docker.py:276} INFO - 21/05/17 16:59:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442856086749217851348_0004_m_000051_236, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442856086749217851348_0004_m_000051_236}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442856086749217851348_0004}; taskId=attempt_202105171658442856086749217851348_0004_m_000051_236, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d001664}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:25,198] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658442856086749217851348_0004_m_000051_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442856086749217851348_0004_m_000051_236
[2021-05-17 13:59:25,201] {docker.py:276} INFO - 21/05/17 16:59:25 INFO StagingCommitter: Task committer attempt_202105171658442856086749217851348_0004_m_000051_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442856086749217851348_0004_m_000051_236 : duration 0:00.003s
[2021-05-17 13:59:27,054] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Starting: Task committer attempt_202105171658447594355955429708788_0004_m_000049_234: needsTaskCommit() Task attempt_202105171658447594355955429708788_0004_m_000049_234
[2021-05-17 13:59:27,055] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Task committer attempt_202105171658447594355955429708788_0004_m_000049_234: needsTaskCommit() Task attempt_202105171658447594355955429708788_0004_m_000049_234: duration 0:00.001s
21/05/17 16:59:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447594355955429708788_0004_m_000049_234
[2021-05-17 13:59:27,057] {docker.py:276} INFO - 21/05/17 16:59:27 INFO Executor: Finished task 49.0 in stage 4.0 (TID 234). 4544 bytes result sent to driver
[2021-05-17 13:59:27,059] {docker.py:276} INFO - 21/05/17 16:59:27 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 237) (bb4563f4559d, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:27,060] {docker.py:276} INFO - 21/05/17 16:59:27 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 234) in 1974 ms on bb4563f4559d (executor driver) (49/200)
[2021-05-17 13:59:27,061] {docker.py:276} INFO - 21/05/17 16:59:27 INFO Executor: Running task 52.0 in stage 4.0 (TID 237)
[2021-05-17 13:59:27,072] {docker.py:276} INFO - 21/05/17 16:59:27 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:27,074] {docker.py:276} INFO - 21/05/17 16:59:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:27,074] {docker.py:276} INFO - 21/05/17 16:59:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444354091323351152439_0004_m_000052_237, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444354091323351152439_0004_m_000052_237}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444354091323351152439_0004}; taskId=attempt_202105171658444354091323351152439_0004_m_000052_237, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d5e8e32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:27,075] {docker.py:276} INFO - 21/05/17 16:59:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:27,075] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Starting: Task committer attempt_202105171658444354091323351152439_0004_m_000052_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444354091323351152439_0004_m_000052_237
[2021-05-17 13:59:27,078] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Task committer attempt_202105171658444354091323351152439_0004_m_000052_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444354091323351152439_0004_m_000052_237 : duration 0:00.003s
[2021-05-17 13:59:27,251] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Starting: Task committer attempt_202105171658445333503466071300395_0004_m_000050_235: needsTaskCommit() Task attempt_202105171658445333503466071300395_0004_m_000050_235
[2021-05-17 13:59:27,251] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Task committer attempt_202105171658445333503466071300395_0004_m_000050_235: needsTaskCommit() Task attempt_202105171658445333503466071300395_0004_m_000050_235: duration 0:00.000s
21/05/17 16:59:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445333503466071300395_0004_m_000050_235
[2021-05-17 13:59:27,252] {docker.py:276} INFO - 21/05/17 16:59:27 INFO Executor: Finished task 50.0 in stage 4.0 (TID 235). 4544 bytes result sent to driver
[2021-05-17 13:59:27,254] {docker.py:276} INFO - 21/05/17 16:59:27 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 238) (bb4563f4559d, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:27,255] {docker.py:276} INFO - 21/05/17 16:59:27 INFO Executor: Running task 53.0 in stage 4.0 (TID 238)
[2021-05-17 13:59:27,255] {docker.py:276} INFO - 21/05/17 16:59:27 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 235) in 2098 ms on bb4563f4559d (executor driver) (50/200)
[2021-05-17 13:59:27,264] {docker.py:276} INFO - 21/05/17 16:59:27 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:27,266] {docker.py:276} INFO - 21/05/17 16:59:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:27,267] {docker.py:276} INFO - 21/05/17 16:59:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443657658597292377676_0004_m_000053_238, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443657658597292377676_0004_m_000053_238}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443657658597292377676_0004}; taskId=attempt_202105171658443657658597292377676_0004_m_000053_238, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@387ed939}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:27 INFO StagingCommitter: Starting: Task committer attempt_202105171658443657658597292377676_0004_m_000053_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443657658597292377676_0004_m_000053_238
[2021-05-17 13:59:27,270] {docker.py:276} INFO - 21/05/17 16:59:27 INFO StagingCommitter: Task committer attempt_202105171658443657658597292377676_0004_m_000053_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443657658597292377676_0004_m_000053_238 : duration 0:00.004s
[2021-05-17 13:59:28,033] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658446524890418287798827_0004_m_000048_233: needsTaskCommit() Task attempt_202105171658446524890418287798827_0004_m_000048_233
[2021-05-17 13:59:28,034] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Task committer attempt_202105171658446524890418287798827_0004_m_000048_233: needsTaskCommit() Task attempt_202105171658446524890418287798827_0004_m_000048_233: duration 0:00.001s
[2021-05-17 13:59:28,035] {docker.py:276} INFO - 21/05/17 16:59:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446524890418287798827_0004_m_000048_233
[2021-05-17 13:59:28,036] {docker.py:276} INFO - 21/05/17 16:59:28 INFO Executor: Finished task 48.0 in stage 4.0 (TID 233). 4544 bytes result sent to driver
[2021-05-17 13:59:28,037] {docker.py:276} INFO - 21/05/17 16:59:28 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 239) (bb4563f4559d, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:28,037] {docker.py:276} INFO - 21/05/17 16:59:28 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 233) in 2961 ms on bb4563f4559d (executor driver) (51/200)
[2021-05-17 13:59:28,038] {docker.py:276} INFO - 21/05/17 16:59:28 INFO Executor: Running task 54.0 in stage 4.0 (TID 239)
[2021-05-17 13:59:28,046] {docker.py:276} INFO - 21/05/17 16:59:28 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:28,047] {docker.py:276} INFO - 21/05/17 16:59:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:28,049] {docker.py:276} INFO - 21/05/17 16:59:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:28,050] {docker.py:276} INFO - 21/05/17 16:59:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442195392134312008273_0004_m_000054_239, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442195392134312008273_0004_m_000054_239}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442195392134312008273_0004}; taskId=attempt_202105171658442195392134312008273_0004_m_000054_239, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15462148}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:28,050] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658442195392134312008273_0004_m_000054_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442195392134312008273_0004_m_000054_239
[2021-05-17 13:59:28,053] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Task committer attempt_202105171658442195392134312008273_0004_m_000054_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442195392134312008273_0004_m_000054_239 : duration 0:00.004s
[2021-05-17 13:59:28,215] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658442856086749217851348_0004_m_000051_236: needsTaskCommit() Task attempt_202105171658442856086749217851348_0004_m_000051_236
[2021-05-17 13:59:28,216] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Task committer attempt_202105171658442856086749217851348_0004_m_000051_236: needsTaskCommit() Task attempt_202105171658442856086749217851348_0004_m_000051_236: duration 0:00.000s
21/05/17 16:59:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442856086749217851348_0004_m_000051_236
[2021-05-17 13:59:28,218] {docker.py:276} INFO - 21/05/17 16:59:28 INFO Executor: Finished task 51.0 in stage 4.0 (TID 236). 4544 bytes result sent to driver
[2021-05-17 13:59:28,220] {docker.py:276} INFO - 21/05/17 16:59:28 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 240) (bb4563f4559d, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:28,221] {docker.py:276} INFO - 21/05/17 16:59:28 INFO Executor: Running task 55.0 in stage 4.0 (TID 240)
[2021-05-17 13:59:28,222] {docker.py:276} INFO - 21/05/17 16:59:28 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 236) in 3040 ms on bb4563f4559d (executor driver) (52/200)
[2021-05-17 13:59:28,241] {docker.py:276} INFO - 21/05/17 16:59:28 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:28,241] {docker.py:276} INFO - 21/05/17 16:59:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:28,243] {docker.py:276} INFO - 21/05/17 16:59:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:28,244] {docker.py:276} INFO - 21/05/17 16:59:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:28,244] {docker.py:276} INFO - 21/05/17 16:59:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442250804479021581864_0004_m_000055_240, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442250804479021581864_0004_m_000055_240}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442250804479021581864_0004}; taskId=attempt_202105171658442250804479021581864_0004_m_000055_240, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d4b79ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:28,244] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658442250804479021581864_0004_m_000055_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442250804479021581864_0004_m_000055_240
[2021-05-17 13:59:28,247] {docker.py:276} INFO - 21/05/17 16:59:28 INFO StagingCommitter: Task committer attempt_202105171658442250804479021581864_0004_m_000055_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442250804479021581864_0004_m_000055_240 : duration 0:00.003s
[2021-05-17 13:59:29,437] {docker.py:276} INFO - 21/05/17 16:59:29 INFO StagingCommitter: Starting: Task committer attempt_202105171658443657658597292377676_0004_m_000053_238: needsTaskCommit() Task attempt_202105171658443657658597292377676_0004_m_000053_238
[2021-05-17 13:59:29,439] {docker.py:276} INFO - 21/05/17 16:59:29 INFO StagingCommitter: Task committer attempt_202105171658443657658597292377676_0004_m_000053_238: needsTaskCommit() Task attempt_202105171658443657658597292377676_0004_m_000053_238: duration 0:00.000s
[2021-05-17 13:59:29,439] {docker.py:276} INFO - 21/05/17 16:59:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443657658597292377676_0004_m_000053_238
[2021-05-17 13:59:29,441] {docker.py:276} INFO - 21/05/17 16:59:29 INFO Executor: Finished task 53.0 in stage 4.0 (TID 238). 4587 bytes result sent to driver
[2021-05-17 13:59:29,443] {docker.py:276} INFO - 21/05/17 16:59:29 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 241) (bb4563f4559d, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:29,444] {docker.py:276} INFO - 21/05/17 16:59:29 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 238) in 2193 ms on bb4563f4559d (executor driver) (53/200)
[2021-05-17 13:59:29,445] {docker.py:276} INFO - 21/05/17 16:59:29 INFO Executor: Running task 56.0 in stage 4.0 (TID 241)
[2021-05-17 13:59:29,455] {docker.py:276} INFO - 21/05/17 16:59:29 INFO ShuffleBlockFetcherIterator: Getting 4 (26.5 KiB) non-empty blocks including 4 (26.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:29,455] {docker.py:276} INFO - 21/05/17 16:59:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:29,457] {docker.py:276} INFO - 21/05/17 16:59:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:29,458] {docker.py:276} INFO - 21/05/17 16:59:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:29,458] {docker.py:276} INFO - 21/05/17 16:59:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446757752547688602768_0004_m_000056_241, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446757752547688602768_0004_m_000056_241}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446757752547688602768_0004}; taskId=attempt_202105171658446757752547688602768_0004_m_000056_241, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4344c9e0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:29 INFO StagingCommitter: Starting: Task committer attempt_202105171658446757752547688602768_0004_m_000056_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446757752547688602768_0004_m_000056_241
[2021-05-17 13:59:29,461] {docker.py:276} INFO - 21/05/17 16:59:29 INFO StagingCommitter: Task committer attempt_202105171658446757752547688602768_0004_m_000056_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446757752547688602768_0004_m_000056_241 : duration 0:00.003s
[2021-05-17 13:59:30,010] {docker.py:276} INFO - 21/05/17 16:59:30 INFO StagingCommitter: Starting: Task committer attempt_202105171658444354091323351152439_0004_m_000052_237: needsTaskCommit() Task attempt_202105171658444354091323351152439_0004_m_000052_237
[2021-05-17 13:59:30,011] {docker.py:276} INFO - 21/05/17 16:59:30 INFO StagingCommitter: Task committer attempt_202105171658444354091323351152439_0004_m_000052_237: needsTaskCommit() Task attempt_202105171658444354091323351152439_0004_m_000052_237: duration 0:00.001s
21/05/17 16:59:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444354091323351152439_0004_m_000052_237
[2021-05-17 13:59:30,013] {docker.py:276} INFO - 21/05/17 16:59:30 INFO Executor: Finished task 52.0 in stage 4.0 (TID 237). 4587 bytes result sent to driver
[2021-05-17 13:59:30,016] {docker.py:276} INFO - 21/05/17 16:59:30 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 242) (bb4563f4559d, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:30,017] {docker.py:276} INFO - 21/05/17 16:59:30 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 237) in 2961 ms on bb4563f4559d (executor driver) (54/200)
[2021-05-17 13:59:30,017] {docker.py:276} INFO - 21/05/17 16:59:30 INFO Executor: Running task 57.0 in stage 4.0 (TID 242)
[2021-05-17 13:59:30,027] {docker.py:276} INFO - 21/05/17 16:59:30 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:30,028] {docker.py:276} INFO - 21/05/17 16:59:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:30,029] {docker.py:276} INFO - 21/05/17 16:59:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441019287317461132023_0004_m_000057_242, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441019287317461132023_0004_m_000057_242}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441019287317461132023_0004}; taskId=attempt_202105171658441019287317461132023_0004_m_000057_242, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2fc075aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:30 INFO StagingCommitter: Starting: Task committer attempt_202105171658441019287317461132023_0004_m_000057_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441019287317461132023_0004_m_000057_242
[2021-05-17 13:59:30,031] {docker.py:276} INFO - 21/05/17 16:59:30 INFO StagingCommitter: Task committer attempt_202105171658441019287317461132023_0004_m_000057_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441019287317461132023_0004_m_000057_242 : duration 0:00.003s
[2021-05-17 13:59:30,783] {docker.py:276} INFO - 21/05/17 16:59:30 INFO StagingCommitter: Starting: Task committer attempt_202105171658442195392134312008273_0004_m_000054_239: needsTaskCommit() Task attempt_202105171658442195392134312008273_0004_m_000054_239
[2021-05-17 13:59:30,783] {docker.py:276} INFO - 21/05/17 16:59:30 INFO StagingCommitter: Task committer attempt_202105171658442195392134312008273_0004_m_000054_239: needsTaskCommit() Task attempt_202105171658442195392134312008273_0004_m_000054_239: duration 0:00.000s
21/05/17 16:59:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442195392134312008273_0004_m_000054_239
[2021-05-17 13:59:30,784] {docker.py:276} INFO - 21/05/17 16:59:30 INFO Executor: Finished task 54.0 in stage 4.0 (TID 239). 4587 bytes result sent to driver
[2021-05-17 13:59:30,785] {docker.py:276} INFO - 21/05/17 16:59:30 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 243) (bb4563f4559d, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:30,786] {docker.py:276} INFO - 21/05/17 16:59:30 INFO Executor: Running task 58.0 in stage 4.0 (TID 243)
21/05/17 16:59:30 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 239) in 2753 ms on bb4563f4559d (executor driver) (55/200)
[2021-05-17 13:59:30,795] {docker.py:276} INFO - 21/05/17 16:59:30 INFO ShuffleBlockFetcherIterator: Getting 4 (26.1 KiB) non-empty blocks including 4 (26.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:30,797] {docker.py:276} INFO - 21/05/17 16:59:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:30,798] {docker.py:276} INFO - 21/05/17 16:59:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844730299686334338239_0004_m_000058_243, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844730299686334338239_0004_m_000058_243}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844730299686334338239_0004}; taskId=attempt_20210517165844730299686334338239_0004_m_000058_243, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@773e0854}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:30 INFO StagingCommitter: Starting: Task committer attempt_20210517165844730299686334338239_0004_m_000058_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844730299686334338239_0004_m_000058_243
[2021-05-17 13:59:30,800] {docker.py:276} INFO - 21/05/17 16:59:30 INFO StagingCommitter: Task committer attempt_20210517165844730299686334338239_0004_m_000058_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844730299686334338239_0004_m_000058_243 : duration 0:00.003s
[2021-05-17 13:59:31,145] {docker.py:276} INFO - 21/05/17 16:59:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658442250804479021581864_0004_m_000055_240: needsTaskCommit() Task attempt_202105171658442250804479021581864_0004_m_000055_240
[2021-05-17 13:59:31,146] {docker.py:276} INFO - 21/05/17 16:59:31 INFO StagingCommitter: Task committer attempt_202105171658442250804479021581864_0004_m_000055_240: needsTaskCommit() Task attempt_202105171658442250804479021581864_0004_m_000055_240: duration 0:00.001s
[2021-05-17 13:59:31,148] {docker.py:276} INFO - 21/05/17 16:59:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442250804479021581864_0004_m_000055_240
[2021-05-17 13:59:31,151] {docker.py:276} INFO - 21/05/17 16:59:31 INFO Executor: Finished task 55.0 in stage 4.0 (TID 240). 4587 bytes result sent to driver
[2021-05-17 13:59:31,152] {docker.py:276} INFO - 21/05/17 16:59:31 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 244) (bb4563f4559d, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:31,153] {docker.py:276} INFO - 21/05/17 16:59:31 INFO Executor: Running task 59.0 in stage 4.0 (TID 244)
21/05/17 16:59:31 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 240) in 2937 ms on bb4563f4559d (executor driver) (56/200)
[2021-05-17 13:59:31,163] {docker.py:276} INFO - 21/05/17 16:59:31 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:31,166] {docker.py:276} INFO - 21/05/17 16:59:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448474331024192304500_0004_m_000059_244, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448474331024192304500_0004_m_000059_244}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448474331024192304500_0004}; taskId=attempt_202105171658448474331024192304500_0004_m_000059_244, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7588b751}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658448474331024192304500_0004_m_000059_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448474331024192304500_0004_m_000059_244
[2021-05-17 13:59:31,169] {docker.py:276} INFO - 21/05/17 16:59:31 INFO StagingCommitter: Task committer attempt_202105171658448474331024192304500_0004_m_000059_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448474331024192304500_0004_m_000059_244 : duration 0:00.003s
[2021-05-17 13:59:31,324] {docker.py:276} INFO - 21/05/17 16:59:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658446757752547688602768_0004_m_000056_241: needsTaskCommit() Task attempt_202105171658446757752547688602768_0004_m_000056_241
[2021-05-17 13:59:31,327] {docker.py:276} INFO - 21/05/17 16:59:31 INFO StagingCommitter: Task committer attempt_202105171658446757752547688602768_0004_m_000056_241: needsTaskCommit() Task attempt_202105171658446757752547688602768_0004_m_000056_241: duration 0:00.003s
21/05/17 16:59:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446757752547688602768_0004_m_000056_241
[2021-05-17 13:59:31,330] {docker.py:276} INFO - 21/05/17 16:59:31 INFO Executor: Finished task 56.0 in stage 4.0 (TID 241). 4544 bytes result sent to driver
[2021-05-17 13:59:31,332] {docker.py:276} INFO - 21/05/17 16:59:31 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 245) (bb4563f4559d, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:31,333] {docker.py:276} INFO - 21/05/17 16:59:31 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 241) in 1893 ms on bb4563f4559d (executor driver) (57/200)
[2021-05-17 13:59:31,334] {docker.py:276} INFO - 21/05/17 16:59:31 INFO Executor: Running task 60.0 in stage 4.0 (TID 245)
[2021-05-17 13:59:31,344] {docker.py:276} INFO - 21/05/17 16:59:31 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:31,346] {docker.py:276} INFO - 21/05/17 16:59:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:31,347] {docker.py:276} INFO - 21/05/17 16:59:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443407736980474690167_0004_m_000060_245, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443407736980474690167_0004_m_000060_245}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443407736980474690167_0004}; taskId=attempt_202105171658443407736980474690167_0004_m_000060_245, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@537acd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658443407736980474690167_0004_m_000060_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443407736980474690167_0004_m_000060_245
[2021-05-17 13:59:31,349] {docker.py:276} INFO - 21/05/17 16:59:31 INFO StagingCommitter: Task committer attempt_202105171658443407736980474690167_0004_m_000060_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443407736980474690167_0004_m_000060_245 : duration 0:00.003s
[2021-05-17 13:59:33,272] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658441019287317461132023_0004_m_000057_242: needsTaskCommit() Task attempt_202105171658441019287317461132023_0004_m_000057_242
[2021-05-17 13:59:33,272] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Task committer attempt_202105171658441019287317461132023_0004_m_000057_242: needsTaskCommit() Task attempt_202105171658441019287317461132023_0004_m_000057_242: duration 0:00.001s
21/05/17 16:59:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441019287317461132023_0004_m_000057_242
[2021-05-17 13:59:33,274] {docker.py:276} INFO - 21/05/17 16:59:33 INFO Executor: Finished task 57.0 in stage 4.0 (TID 242). 4544 bytes result sent to driver
[2021-05-17 13:59:33,275] {docker.py:276} INFO - 21/05/17 16:59:33 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 246) (bb4563f4559d, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:33,276] {docker.py:276} INFO - 21/05/17 16:59:33 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 242) in 3266 ms on bb4563f4559d (executor driver) (58/200)
[2021-05-17 13:59:33,277] {docker.py:276} INFO - 21/05/17 16:59:33 INFO Executor: Running task 61.0 in stage 4.0 (TID 246)
[2021-05-17 13:59:33,289] {docker.py:276} INFO - 21/05/17 16:59:33 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:33,292] {docker.py:276} INFO - 21/05/17 16:59:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:33,293] {docker.py:276} INFO - 21/05/17 16:59:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447300989954923057098_0004_m_000061_246, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447300989954923057098_0004_m_000061_246}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447300989954923057098_0004}; taskId=attempt_202105171658447300989954923057098_0004_m_000061_246, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15bd41a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:33,293] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658447300989954923057098_0004_m_000061_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447300989954923057098_0004_m_000061_246
[2021-05-17 13:59:33,297] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Task committer attempt_202105171658447300989954923057098_0004_m_000061_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447300989954923057098_0004_m_000061_246 : duration 0:00.004s
[2021-05-17 13:59:33,423] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658443407736980474690167_0004_m_000060_245: needsTaskCommit() Task attempt_202105171658443407736980474690167_0004_m_000060_245
[2021-05-17 13:59:33,425] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Task committer attempt_202105171658443407736980474690167_0004_m_000060_245: needsTaskCommit() Task attempt_202105171658443407736980474690167_0004_m_000060_245: duration 0:00.002s
21/05/17 16:59:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443407736980474690167_0004_m_000060_245
[2021-05-17 13:59:33,428] {docker.py:276} INFO - 21/05/17 16:59:33 INFO Executor: Finished task 60.0 in stage 4.0 (TID 245). 4544 bytes result sent to driver
[2021-05-17 13:59:33,430] {docker.py:276} INFO - 21/05/17 16:59:33 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 247) (bb4563f4559d, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:33,430] {docker.py:276} INFO - 21/05/17 16:59:33 INFO Executor: Running task 62.0 in stage 4.0 (TID 247)
[2021-05-17 13:59:33,431] {docker.py:276} INFO - 21/05/17 16:59:33 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 245) in 2102 ms on bb4563f4559d (executor driver) (59/200)
[2021-05-17 13:59:33,436] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Starting: Task committer attempt_20210517165844730299686334338239_0004_m_000058_243: needsTaskCommit() Task attempt_20210517165844730299686334338239_0004_m_000058_243
[2021-05-17 13:59:33,437] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Task committer attempt_20210517165844730299686334338239_0004_m_000058_243: needsTaskCommit() Task attempt_20210517165844730299686334338239_0004_m_000058_243: duration 0:00.000s
[2021-05-17 13:59:33,437] {docker.py:276} INFO - 21/05/17 16:59:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844730299686334338239_0004_m_000058_243
[2021-05-17 13:59:33,438] {docker.py:276} INFO - 21/05/17 16:59:33 INFO Executor: Finished task 58.0 in stage 4.0 (TID 243). 4544 bytes result sent to driver
[2021-05-17 13:59:33,440] {docker.py:276} INFO - 21/05/17 16:59:33 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 248) (bb4563f4559d, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:33,441] {docker.py:276} INFO - 21/05/17 16:59:33 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 243) in 2658 ms on bb4563f4559d (executor driver) (60/200)
[2021-05-17 13:59:33,441] {docker.py:276} INFO - 21/05/17 16:59:33 INFO Executor: Running task 63.0 in stage 4.0 (TID 248)
[2021-05-17 13:59:33,444] {docker.py:276} INFO - 21/05/17 16:59:33 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:33,444] {docker.py:276} INFO - 21/05/17 16:59:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:33,447] {docker.py:276} INFO - 21/05/17 16:59:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:33,448] {docker.py:276} INFO - 21/05/17 16:59:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:33,448] {docker.py:276} INFO - 21/05/17 16:59:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442400115660671783740_0004_m_000062_247, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442400115660671783740_0004_m_000062_247}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442400115660671783740_0004}; taskId=attempt_202105171658442400115660671783740_0004_m_000062_247, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21c4957a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:33,449] {docker.py:276} INFO - 21/05/17 16:59:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:33,449] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658442400115660671783740_0004_m_000062_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442400115660671783740_0004_m_000062_247
[2021-05-17 13:59:33,452] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Task committer attempt_202105171658442400115660671783740_0004_m_000062_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442400115660671783740_0004_m_000062_247 : duration 0:00.004s
[2021-05-17 13:59:33,454] {docker.py:276} INFO - 21/05/17 16:59:33 INFO ShuffleBlockFetcherIterator: Getting 4 (29.0 KiB) non-empty blocks including 4 (29.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:33,454] {docker.py:276} INFO - 21/05/17 16:59:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:33,458] {docker.py:276} INFO - 21/05/17 16:59:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:33,458] {docker.py:276} INFO - 21/05/17 16:59:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:33,458] {docker.py:276} INFO - 21/05/17 16:59:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443422729373106363207_0004_m_000063_248, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443422729373106363207_0004_m_000063_248}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443422729373106363207_0004}; taskId=attempt_202105171658443422729373106363207_0004_m_000063_248, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37e4804b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:33,459] {docker.py:276} INFO - 21/05/17 16:59:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:33,459] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658443422729373106363207_0004_m_000063_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443422729373106363207_0004_m_000063_248
[2021-05-17 13:59:33,463] {docker.py:276} INFO - 21/05/17 16:59:33 INFO StagingCommitter: Task committer attempt_202105171658443422729373106363207_0004_m_000063_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443422729373106363207_0004_m_000063_248 : duration 0:00.005s
[2021-05-17 13:59:34,403] {docker.py:276} INFO - 21/05/17 16:59:34 INFO StagingCommitter: Starting: Task committer attempt_202105171658448474331024192304500_0004_m_000059_244: needsTaskCommit() Task attempt_202105171658448474331024192304500_0004_m_000059_244
[2021-05-17 13:59:34,404] {docker.py:276} INFO - 21/05/17 16:59:34 INFO StagingCommitter: Task committer attempt_202105171658448474331024192304500_0004_m_000059_244: needsTaskCommit() Task attempt_202105171658448474331024192304500_0004_m_000059_244: duration 0:00.000s
21/05/17 16:59:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448474331024192304500_0004_m_000059_244
[2021-05-17 13:59:34,406] {docker.py:276} INFO - 21/05/17 16:59:34 INFO Executor: Finished task 59.0 in stage 4.0 (TID 244). 4544 bytes result sent to driver
[2021-05-17 13:59:34,421] {docker.py:276} INFO - 21/05/17 16:59:34 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 249) (bb4563f4559d, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 16:59:34 INFO Executor: Running task 64.0 in stage 4.0 (TID 249)
21/05/17 16:59:34 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 244) in 3272 ms on bb4563f4559d (executor driver) (61/200)
[2021-05-17 13:59:34,431] {docker.py:276} INFO - 21/05/17 16:59:34 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:34,431] {docker.py:276} INFO - 21/05/17 16:59:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:34,434] {docker.py:276} INFO - 21/05/17 16:59:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:34,434] {docker.py:276} INFO - 21/05/17 16:59:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:34,435] {docker.py:276} INFO - 21/05/17 16:59:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443933286319943406031_0004_m_000064_249, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443933286319943406031_0004_m_000064_249}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443933286319943406031_0004}; taskId=attempt_202105171658443933286319943406031_0004_m_000064_249, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@128f6507}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:34,436] {docker.py:276} INFO - 21/05/17 16:59:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:34,436] {docker.py:276} INFO - 21/05/17 16:59:34 INFO StagingCommitter: Starting: Task committer attempt_202105171658443933286319943406031_0004_m_000064_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443933286319943406031_0004_m_000064_249
[2021-05-17 13:59:34,440] {docker.py:276} INFO - 21/05/17 16:59:34 INFO StagingCommitter: Task committer attempt_202105171658443933286319943406031_0004_m_000064_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443933286319943406031_0004_m_000064_249 : duration 0:00.005s
[2021-05-17 13:59:35,764] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Starting: Task committer attempt_202105171658447300989954923057098_0004_m_000061_246: needsTaskCommit() Task attempt_202105171658447300989954923057098_0004_m_000061_246
[2021-05-17 13:59:35,765] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Task committer attempt_202105171658447300989954923057098_0004_m_000061_246: needsTaskCommit() Task attempt_202105171658447300989954923057098_0004_m_000061_246: duration 0:00.000s
[2021-05-17 13:59:35,765] {docker.py:276} INFO - 21/05/17 16:59:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447300989954923057098_0004_m_000061_246
[2021-05-17 13:59:35,766] {docker.py:276} INFO - 21/05/17 16:59:35 INFO Executor: Finished task 61.0 in stage 4.0 (TID 246). 4587 bytes result sent to driver
[2021-05-17 13:59:35,768] {docker.py:276} INFO - 21/05/17 16:59:35 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 250) (bb4563f4559d, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:35,768] {docker.py:276} INFO - 21/05/17 16:59:35 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 246) in 2496 ms on bb4563f4559d (executor driver) (62/200)
[2021-05-17 13:59:35,769] {docker.py:276} INFO - 21/05/17 16:59:35 INFO Executor: Running task 65.0 in stage 4.0 (TID 250)
[2021-05-17 13:59:35,777] {docker.py:276} INFO - 21/05/17 16:59:35 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:35,778] {docker.py:276} INFO - 21/05/17 16:59:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:35,780] {docker.py:276} INFO - 21/05/17 16:59:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:35,780] {docker.py:276} INFO - 21/05/17 16:59:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:35,781] {docker.py:276} INFO - 21/05/17 16:59:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:35,781] {docker.py:276} INFO - 21/05/17 16:59:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441565751981695770671_0004_m_000065_250, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441565751981695770671_0004_m_000065_250}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441565751981695770671_0004}; taskId=attempt_202105171658441565751981695770671_0004_m_000065_250, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@395e35de}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:35,782] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Starting: Task committer attempt_202105171658441565751981695770671_0004_m_000065_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441565751981695770671_0004_m_000065_250
[2021-05-17 13:59:35,783] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Starting: Task committer attempt_202105171658442400115660671783740_0004_m_000062_247: needsTaskCommit() Task attempt_202105171658442400115660671783740_0004_m_000062_247
[2021-05-17 13:59:35,783] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Task committer attempt_202105171658442400115660671783740_0004_m_000062_247: needsTaskCommit() Task attempt_202105171658442400115660671783740_0004_m_000062_247: duration 0:00.001s
21/05/17 16:59:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442400115660671783740_0004_m_000062_247
[2021-05-17 13:59:35,784] {docker.py:276} INFO - 21/05/17 16:59:35 INFO Executor: Finished task 62.0 in stage 4.0 (TID 247). 4587 bytes result sent to driver
[2021-05-17 13:59:35,785] {docker.py:276} INFO - 21/05/17 16:59:35 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 251) (bb4563f4559d, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:35,787] {docker.py:276} INFO - 21/05/17 16:59:35 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 247) in 2361 ms on bb4563f4559d (executor driver) (63/200)
[2021-05-17 13:59:35,789] {docker.py:276} INFO - 21/05/17 16:59:35 INFO Executor: Running task 66.0 in stage 4.0 (TID 251)
[2021-05-17 13:59:35,792] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Task committer attempt_202105171658441565751981695770671_0004_m_000065_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441565751981695770671_0004_m_000065_250 : duration 0:00.009s
[2021-05-17 13:59:35,798] {docker.py:276} INFO - 21/05/17 16:59:35 INFO ShuffleBlockFetcherIterator: Getting 4 (26.6 KiB) non-empty blocks including 4 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:35,800] {docker.py:276} INFO - 21/05/17 16:59:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:35,800] {docker.py:276} INFO - 21/05/17 16:59:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442179058589138882778_0004_m_000066_251, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442179058589138882778_0004_m_000066_251}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442179058589138882778_0004}; taskId=attempt_202105171658442179058589138882778_0004_m_000066_251, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76e3978a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:35 INFO StagingCommitter: Starting: Task committer attempt_202105171658442179058589138882778_0004_m_000066_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442179058589138882778_0004_m_000066_251
[2021-05-17 13:59:35,803] {docker.py:276} INFO - 21/05/17 16:59:35 INFO StagingCommitter: Task committer attempt_202105171658442179058589138882778_0004_m_000066_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442179058589138882778_0004_m_000066_251 : duration 0:00.003s
[2021-05-17 13:59:36,097] {docker.py:276} INFO - 21/05/17 16:59:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658443422729373106363207_0004_m_000063_248: needsTaskCommit() Task attempt_202105171658443422729373106363207_0004_m_000063_248
[2021-05-17 13:59:36,098] {docker.py:276} INFO - 21/05/17 16:59:36 INFO StagingCommitter: Task committer attempt_202105171658443422729373106363207_0004_m_000063_248: needsTaskCommit() Task attempt_202105171658443422729373106363207_0004_m_000063_248: duration 0:00.001s
21/05/17 16:59:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443422729373106363207_0004_m_000063_248
[2021-05-17 13:59:36,099] {docker.py:276} INFO - 21/05/17 16:59:36 INFO Executor: Finished task 63.0 in stage 4.0 (TID 248). 4587 bytes result sent to driver
[2021-05-17 13:59:36,101] {docker.py:276} INFO - 21/05/17 16:59:36 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 252) (bb4563f4559d, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:36,102] {docker.py:276} INFO - 21/05/17 16:59:36 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 248) in 2666 ms on bb4563f4559d (executor driver) (64/200)
[2021-05-17 13:59:36,103] {docker.py:276} INFO - 21/05/17 16:59:36 INFO Executor: Running task 67.0 in stage 4.0 (TID 252)
[2021-05-17 13:59:36,113] {docker.py:276} INFO - 21/05/17 16:59:36 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:36,115] {docker.py:276} INFO - 21/05/17 16:59:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:36,115] {docker.py:276} INFO - 21/05/17 16:59:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443174428803084831758_0004_m_000067_252, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443174428803084831758_0004_m_000067_252}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443174428803084831758_0004}; taskId=attempt_202105171658443174428803084831758_0004_m_000067_252, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e75556c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658443174428803084831758_0004_m_000067_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443174428803084831758_0004_m_000067_252
[2021-05-17 13:59:36,118] {docker.py:276} INFO - 21/05/17 16:59:36 INFO StagingCommitter: Task committer attempt_202105171658443174428803084831758_0004_m_000067_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443174428803084831758_0004_m_000067_252 : duration 0:00.003s
[2021-05-17 13:59:36,901] {docker.py:276} INFO - 21/05/17 16:59:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658443933286319943406031_0004_m_000064_249: needsTaskCommit() Task attempt_202105171658443933286319943406031_0004_m_000064_249
[2021-05-17 13:59:36,902] {docker.py:276} INFO - 21/05/17 16:59:36 INFO StagingCommitter: Task committer attempt_202105171658443933286319943406031_0004_m_000064_249: needsTaskCommit() Task attempt_202105171658443933286319943406031_0004_m_000064_249: duration 0:00.000s
21/05/17 16:59:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443933286319943406031_0004_m_000064_249
[2021-05-17 13:59:36,903] {docker.py:276} INFO - 21/05/17 16:59:36 INFO Executor: Finished task 64.0 in stage 4.0 (TID 249). 4544 bytes result sent to driver
[2021-05-17 13:59:36,904] {docker.py:276} INFO - 21/05/17 16:59:36 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 253) (bb4563f4559d, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:36,905] {docker.py:276} INFO - 21/05/17 16:59:36 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 249) in 2489 ms on bb4563f4559d (executor driver) (65/200)
[2021-05-17 13:59:36,906] {docker.py:276} INFO - 21/05/17 16:59:36 INFO Executor: Running task 68.0 in stage 4.0 (TID 253)
[2021-05-17 13:59:36,915] {docker.py:276} INFO - 21/05/17 16:59:36 INFO ShuffleBlockFetcherIterator: Getting 4 (26.0 KiB) non-empty blocks including 4 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:36,916] {docker.py:276} INFO - 21/05/17 16:59:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:36,917] {docker.py:276} INFO - 21/05/17 16:59:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447331660404200308115_0004_m_000068_253, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447331660404200308115_0004_m_000068_253}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447331660404200308115_0004}; taskId=attempt_202105171658447331660404200308115_0004_m_000068_253, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2257c748}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658447331660404200308115_0004_m_000068_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447331660404200308115_0004_m_000068_253
[2021-05-17 13:59:36,920] {docker.py:276} INFO - 21/05/17 16:59:36 INFO StagingCommitter: Task committer attempt_202105171658447331660404200308115_0004_m_000068_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447331660404200308115_0004_m_000068_253 : duration 0:00.003s
[2021-05-17 13:59:38,287] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658441565751981695770671_0004_m_000065_250: needsTaskCommit() Task attempt_202105171658441565751981695770671_0004_m_000065_250
[2021-05-17 13:59:38,289] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Task committer attempt_202105171658441565751981695770671_0004_m_000065_250: needsTaskCommit() Task attempt_202105171658441565751981695770671_0004_m_000065_250: duration 0:00.001s
21/05/17 16:59:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441565751981695770671_0004_m_000065_250
[2021-05-17 13:59:38,290] {docker.py:276} INFO - 21/05/17 16:59:38 INFO Executor: Finished task 65.0 in stage 4.0 (TID 250). 4544 bytes result sent to driver
[2021-05-17 13:59:38,293] {docker.py:276} INFO - 21/05/17 16:59:38 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 254) (bb4563f4559d, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:38,294] {docker.py:276} INFO - 21/05/17 16:59:38 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 250) in 2529 ms on bb4563f4559d (executor driver) (66/200)
[2021-05-17 13:59:38,295] {docker.py:276} INFO - 21/05/17 16:59:38 INFO Executor: Running task 69.0 in stage 4.0 (TID 254)
[2021-05-17 13:59:38,306] {docker.py:276} INFO - 21/05/17 16:59:38 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:38,308] {docker.py:276} INFO - 21/05/17 16:59:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446090965884579551100_0004_m_000069_254, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446090965884579551100_0004_m_000069_254}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446090965884579551100_0004}; taskId=attempt_202105171658446090965884579551100_0004_m_000069_254, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13cd5790}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:38,308] {docker.py:276} INFO - 21/05/17 16:59:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658446090965884579551100_0004_m_000069_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446090965884579551100_0004_m_000069_254
[2021-05-17 13:59:38,310] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Task committer attempt_202105171658446090965884579551100_0004_m_000069_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446090965884579551100_0004_m_000069_254 : duration 0:00.002s
[2021-05-17 13:59:38,411] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658442179058589138882778_0004_m_000066_251: needsTaskCommit() Task attempt_202105171658442179058589138882778_0004_m_000066_251
[2021-05-17 13:59:38,412] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Task committer attempt_202105171658442179058589138882778_0004_m_000066_251: needsTaskCommit() Task attempt_202105171658442179058589138882778_0004_m_000066_251: duration 0:00.001s
21/05/17 16:59:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442179058589138882778_0004_m_000066_251
[2021-05-17 13:59:38,413] {docker.py:276} INFO - 21/05/17 16:59:38 INFO Executor: Finished task 66.0 in stage 4.0 (TID 251). 4544 bytes result sent to driver
[2021-05-17 13:59:38,415] {docker.py:276} INFO - 21/05/17 16:59:38 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 255) (bb4563f4559d, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:38,416] {docker.py:276} INFO - 21/05/17 16:59:38 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 251) in 2634 ms on bb4563f4559d (executor driver) (67/200)
[2021-05-17 13:59:38,417] {docker.py:276} INFO - 21/05/17 16:59:38 INFO Executor: Running task 70.0 in stage 4.0 (TID 255)
[2021-05-17 13:59:38,427] {docker.py:276} INFO - 21/05/17 16:59:38 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:38,430] {docker.py:276} INFO - 21/05/17 16:59:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442426882989129276768_0004_m_000070_255, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442426882989129276768_0004_m_000070_255}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442426882989129276768_0004}; taskId=attempt_202105171658442426882989129276768_0004_m_000070_255, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bc9aa96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658442426882989129276768_0004_m_000070_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442426882989129276768_0004_m_000070_255
[2021-05-17 13:59:38,431] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Task committer attempt_202105171658442426882989129276768_0004_m_000070_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442426882989129276768_0004_m_000070_255 : duration 0:00.002s
[2021-05-17 13:59:38,690] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658443174428803084831758_0004_m_000067_252: needsTaskCommit() Task attempt_202105171658443174428803084831758_0004_m_000067_252
[2021-05-17 13:59:38,691] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Task committer attempt_202105171658443174428803084831758_0004_m_000067_252: needsTaskCommit() Task attempt_202105171658443174428803084831758_0004_m_000067_252: duration 0:00.001s
21/05/17 16:59:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443174428803084831758_0004_m_000067_252
[2021-05-17 13:59:38,692] {docker.py:276} INFO - 21/05/17 16:59:38 INFO Executor: Finished task 67.0 in stage 4.0 (TID 252). 4544 bytes result sent to driver
[2021-05-17 13:59:38,693] {docker.py:276} INFO - 21/05/17 16:59:38 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 256) (bb4563f4559d, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:38,695] {docker.py:276} INFO - 21/05/17 16:59:38 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 252) in 2598 ms on bb4563f4559d (executor driver) (68/200)
[2021-05-17 13:59:38,696] {docker.py:276} INFO - 21/05/17 16:59:38 INFO Executor: Running task 71.0 in stage 4.0 (TID 256)
[2021-05-17 13:59:38,705] {docker.py:276} INFO - 21/05/17 16:59:38 INFO ShuffleBlockFetcherIterator: Getting 4 (27.2 KiB) non-empty blocks including 4 (27.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:38,708] {docker.py:276} INFO - 21/05/17 16:59:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444763784428663435232_0004_m_000071_256, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444763784428663435232_0004_m_000071_256}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444763784428663435232_0004}; taskId=attempt_202105171658444763784428663435232_0004_m_000071_256, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2823c6fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:38,708] {docker.py:276} INFO - 21/05/17 16:59:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658444763784428663435232_0004_m_000071_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444763784428663435232_0004_m_000071_256
[2021-05-17 13:59:38,711] {docker.py:276} INFO - 21/05/17 16:59:38 INFO StagingCommitter: Task committer attempt_202105171658444763784428663435232_0004_m_000071_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444763784428663435232_0004_m_000071_256 : duration 0:00.004s
[2021-05-17 13:59:39,273] {docker.py:276} INFO - 21/05/17 16:59:39 INFO StagingCommitter: Starting: Task committer attempt_202105171658447331660404200308115_0004_m_000068_253: needsTaskCommit() Task attempt_202105171658447331660404200308115_0004_m_000068_253
[2021-05-17 13:59:39,274] {docker.py:276} INFO - 21/05/17 16:59:39 INFO StagingCommitter: Task committer attempt_202105171658447331660404200308115_0004_m_000068_253: needsTaskCommit() Task attempt_202105171658447331660404200308115_0004_m_000068_253: duration 0:00.001s
21/05/17 16:59:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447331660404200308115_0004_m_000068_253
[2021-05-17 13:59:39,276] {docker.py:276} INFO - 21/05/17 16:59:39 INFO Executor: Finished task 68.0 in stage 4.0 (TID 253). 4544 bytes result sent to driver
[2021-05-17 13:59:39,277] {docker.py:276} INFO - 21/05/17 16:59:39 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 257) (bb4563f4559d, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:39,278] {docker.py:276} INFO - 21/05/17 16:59:39 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 253) in 2377 ms on bb4563f4559d (executor driver) (69/200)
[2021-05-17 13:59:39,279] {docker.py:276} INFO - 21/05/17 16:59:39 INFO Executor: Running task 72.0 in stage 4.0 (TID 257)
[2021-05-17 13:59:39,297] {docker.py:276} INFO - 21/05/17 16:59:39 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:39,300] {docker.py:276} INFO - 21/05/17 16:59:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:39,300] {docker.py:276} INFO - 21/05/17 16:59:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442828227880626122251_0004_m_000072_257, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442828227880626122251_0004_m_000072_257}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442828227880626122251_0004}; taskId=attempt_202105171658442828227880626122251_0004_m_000072_257, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5414d085}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:39 INFO StagingCommitter: Starting: Task committer attempt_202105171658442828227880626122251_0004_m_000072_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442828227880626122251_0004_m_000072_257
[2021-05-17 13:59:39,303] {docker.py:276} INFO - 21/05/17 16:59:39 INFO StagingCommitter: Task committer attempt_202105171658442828227880626122251_0004_m_000072_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442828227880626122251_0004_m_000072_257 : duration 0:00.003s
[2021-05-17 13:59:40,792] {docker.py:276} INFO - 21/05/17 16:59:40 INFO StagingCommitter: Starting: Task committer attempt_202105171658446090965884579551100_0004_m_000069_254: needsTaskCommit() Task attempt_202105171658446090965884579551100_0004_m_000069_254
[2021-05-17 13:59:40,793] {docker.py:276} INFO - 21/05/17 16:59:40 INFO StagingCommitter: Task committer attempt_202105171658446090965884579551100_0004_m_000069_254: needsTaskCommit() Task attempt_202105171658446090965884579551100_0004_m_000069_254: duration 0:00.001s
21/05/17 16:59:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446090965884579551100_0004_m_000069_254
[2021-05-17 13:59:40,795] {docker.py:276} INFO - 21/05/17 16:59:40 INFO Executor: Finished task 69.0 in stage 4.0 (TID 254). 4587 bytes result sent to driver
[2021-05-17 13:59:40,796] {docker.py:276} INFO - 21/05/17 16:59:40 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 258) (bb4563f4559d, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:40,797] {docker.py:276} INFO - 21/05/17 16:59:40 INFO Executor: Running task 73.0 in stage 4.0 (TID 258)
[2021-05-17 13:59:40,798] {docker.py:276} INFO - 21/05/17 16:59:40 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 254) in 2509 ms on bb4563f4559d (executor driver) (70/200)
[2021-05-17 13:59:40,807] {docker.py:276} INFO - 21/05/17 16:59:40 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:40,808] {docker.py:276} INFO - 21/05/17 16:59:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:40,812] {docker.py:276} INFO - 21/05/17 16:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:40,812] {docker.py:276} INFO - 21/05/17 16:59:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:40,813] {docker.py:276} INFO - 21/05/17 16:59:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:40,813] {docker.py:276} INFO - 21/05/17 16:59:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446712254754136326162_0004_m_000073_258, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446712254754136326162_0004_m_000073_258}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446712254754136326162_0004}; taskId=attempt_202105171658446712254754136326162_0004_m_000073_258, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5100320c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:40,814] {docker.py:276} INFO - 21/05/17 16:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:40,815] {docker.py:276} INFO - 21/05/17 16:59:40 INFO StagingCommitter: Starting: Task committer attempt_202105171658446712254754136326162_0004_m_000073_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446712254754136326162_0004_m_000073_258
[2021-05-17 13:59:40,819] {docker.py:276} INFO - 21/05/17 16:59:40 INFO StagingCommitter: Task committer attempt_202105171658446712254754136326162_0004_m_000073_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446712254754136326162_0004_m_000073_258 : duration 0:00.004s
[2021-05-17 13:59:40,997] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658442828227880626122251_0004_m_000072_257: needsTaskCommit() Task attempt_202105171658442828227880626122251_0004_m_000072_257
[2021-05-17 13:59:40,999] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Task committer attempt_202105171658442828227880626122251_0004_m_000072_257: needsTaskCommit() Task attempt_202105171658442828227880626122251_0004_m_000072_257: duration 0:00.000s
[2021-05-17 13:59:40,999] {docker.py:276} INFO - 21/05/17 16:59:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442828227880626122251_0004_m_000072_257
[2021-05-17 13:59:41,001] {docker.py:276} INFO - 21/05/17 16:59:41 INFO Executor: Finished task 72.0 in stage 4.0 (TID 257). 4587 bytes result sent to driver
[2021-05-17 13:59:41,002] {docker.py:276} INFO - 21/05/17 16:59:41 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 259) (bb4563f4559d, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:41,003] {docker.py:276} INFO - 21/05/17 16:59:41 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 257) in 1728 ms on bb4563f4559d (executor driver) (71/200)
[2021-05-17 13:59:41,005] {docker.py:276} INFO - 21/05/17 16:59:41 INFO Executor: Running task 74.0 in stage 4.0 (TID 259)
[2021-05-17 13:59:41,016] {docker.py:276} INFO - 21/05/17 16:59:41 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:41,018] {docker.py:276} INFO - 21/05/17 16:59:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445370558605555752209_0004_m_000074_259, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445370558605555752209_0004_m_000074_259}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445370558605555752209_0004}; taskId=attempt_202105171658445370558605555752209_0004_m_000074_259, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@595aeb3a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658445370558605555752209_0004_m_000074_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445370558605555752209_0004_m_000074_259
[2021-05-17 13:59:41,021] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Task committer attempt_202105171658445370558605555752209_0004_m_000074_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445370558605555752209_0004_m_000074_259 : duration 0:00.003s
[2021-05-17 13:59:41,169] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658444763784428663435232_0004_m_000071_256: needsTaskCommit() Task attempt_202105171658444763784428663435232_0004_m_000071_256
[2021-05-17 13:59:41,170] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Task committer attempt_202105171658444763784428663435232_0004_m_000071_256: needsTaskCommit() Task attempt_202105171658444763784428663435232_0004_m_000071_256: duration 0:00.001s
[2021-05-17 13:59:41,170] {docker.py:276} INFO - 21/05/17 16:59:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444763784428663435232_0004_m_000071_256
[2021-05-17 13:59:41,172] {docker.py:276} INFO - 21/05/17 16:59:41 INFO Executor: Finished task 71.0 in stage 4.0 (TID 256). 4587 bytes result sent to driver
[2021-05-17 13:59:41,173] {docker.py:276} INFO - 21/05/17 16:59:41 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 260) (bb4563f4559d, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:41,174] {docker.py:276} INFO - 21/05/17 16:59:41 INFO Executor: Running task 75.0 in stage 4.0 (TID 260)
[2021-05-17 13:59:41,175] {docker.py:276} INFO - 21/05/17 16:59:41 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 256) in 2485 ms on bb4563f4559d (executor driver) (72/200)
[2021-05-17 13:59:41,184] {docker.py:276} INFO - 21/05/17 16:59:41 INFO ShuffleBlockFetcherIterator: Getting 4 (29.1 KiB) non-empty blocks including 4 (29.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:41,186] {docker.py:276} INFO - 21/05/17 16:59:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:41,187] {docker.py:276} INFO - 21/05/17 16:59:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:41,188] {docker.py:276} INFO - 21/05/17 16:59:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443894008332959346418_0004_m_000075_260, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443894008332959346418_0004_m_000075_260}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443894008332959346418_0004}; taskId=attempt_202105171658443894008332959346418_0004_m_000075_260, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f4d58ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:41,188] {docker.py:276} INFO - 21/05/17 16:59:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:41,188] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658443894008332959346418_0004_m_000075_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443894008332959346418_0004_m_000075_260
[2021-05-17 13:59:41,191] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Task committer attempt_202105171658443894008332959346418_0004_m_000075_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443894008332959346418_0004_m_000075_260 : duration 0:00.003s
[2021-05-17 13:59:41,946] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658442426882989129276768_0004_m_000070_255: needsTaskCommit() Task attempt_202105171658442426882989129276768_0004_m_000070_255
[2021-05-17 13:59:41,947] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Task committer attempt_202105171658442426882989129276768_0004_m_000070_255: needsTaskCommit() Task attempt_202105171658442426882989129276768_0004_m_000070_255: duration 0:00.001s
21/05/17 16:59:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442426882989129276768_0004_m_000070_255
[2021-05-17 13:59:41,949] {docker.py:276} INFO - 21/05/17 16:59:41 INFO Executor: Finished task 70.0 in stage 4.0 (TID 255). 4587 bytes result sent to driver
[2021-05-17 13:59:41,950] {docker.py:276} INFO - 21/05/17 16:59:41 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 261) (bb4563f4559d, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:41,952] {docker.py:276} INFO - 21/05/17 16:59:41 INFO Executor: Running task 76.0 in stage 4.0 (TID 261)
[2021-05-17 13:59:41,952] {docker.py:276} INFO - 21/05/17 16:59:41 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 255) in 3506 ms on bb4563f4559d (executor driver) (73/200)
[2021-05-17 13:59:41,961] {docker.py:276} INFO - 21/05/17 16:59:41 INFO ShuffleBlockFetcherIterator: Getting 4 (30.5 KiB) non-empty blocks including 4 (30.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:41,964] {docker.py:276} INFO - 21/05/17 16:59:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:41,964] {docker.py:276} INFO - 21/05/17 16:59:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447709010903431033361_0004_m_000076_261, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447709010903431033361_0004_m_000076_261}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447709010903431033361_0004}; taskId=attempt_202105171658447709010903431033361_0004_m_000076_261, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bb2cfb0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658447709010903431033361_0004_m_000076_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447709010903431033361_0004_m_000076_261
[2021-05-17 13:59:41,968] {docker.py:276} INFO - 21/05/17 16:59:41 INFO StagingCommitter: Task committer attempt_202105171658447709010903431033361_0004_m_000076_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447709010903431033361_0004_m_000076_261 : duration 0:00.004s
[2021-05-17 13:59:43,277] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658446712254754136326162_0004_m_000073_258: needsTaskCommit() Task attempt_202105171658446712254754136326162_0004_m_000073_258
21/05/17 16:59:43 INFO StagingCommitter: Task committer attempt_202105171658446712254754136326162_0004_m_000073_258: needsTaskCommit() Task attempt_202105171658446712254754136326162_0004_m_000073_258: duration 0:00.001s
21/05/17 16:59:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446712254754136326162_0004_m_000073_258
[2021-05-17 13:59:43,280] {docker.py:276} INFO - 21/05/17 16:59:43 INFO Executor: Finished task 73.0 in stage 4.0 (TID 258). 4544 bytes result sent to driver
[2021-05-17 13:59:43,281] {docker.py:276} INFO - 21/05/17 16:59:43 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 262) (bb4563f4559d, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:43,282] {docker.py:276} INFO - 21/05/17 16:59:43 INFO Executor: Running task 77.0 in stage 4.0 (TID 262)
[2021-05-17 13:59:43,283] {docker.py:276} INFO - 21/05/17 16:59:43 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 258) in 2454 ms on bb4563f4559d (executor driver) (74/200)
[2021-05-17 13:59:43,293] {docker.py:276} INFO - 21/05/17 16:59:43 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:43,294] {docker.py:276} INFO - 21/05/17 16:59:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:43,295] {docker.py:276} INFO - 21/05/17 16:59:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441960490384837142331_0004_m_000077_262, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441960490384837142331_0004_m_000077_262}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441960490384837142331_0004}; taskId=attempt_202105171658441960490384837142331_0004_m_000077_262, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@290326b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:43,295] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658441960490384837142331_0004_m_000077_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441960490384837142331_0004_m_000077_262
[2021-05-17 13:59:43,298] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Task committer attempt_202105171658441960490384837142331_0004_m_000077_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441960490384837142331_0004_m_000077_262 : duration 0:00.003s
[2021-05-17 13:59:43,507] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658445370558605555752209_0004_m_000074_259: needsTaskCommit() Task attempt_202105171658445370558605555752209_0004_m_000074_259
[2021-05-17 13:59:43,508] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Task committer attempt_202105171658445370558605555752209_0004_m_000074_259: needsTaskCommit() Task attempt_202105171658445370558605555752209_0004_m_000074_259: duration 0:00.001s
21/05/17 16:59:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445370558605555752209_0004_m_000074_259
[2021-05-17 13:59:43,509] {docker.py:276} INFO - 21/05/17 16:59:43 INFO Executor: Finished task 74.0 in stage 4.0 (TID 259). 4544 bytes result sent to driver
[2021-05-17 13:59:43,510] {docker.py:276} INFO - 21/05/17 16:59:43 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 263) (bb4563f4559d, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:43,512] {docker.py:276} INFO - 21/05/17 16:59:43 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 259) in 2477 ms on bb4563f4559d (executor driver) (75/200)
[2021-05-17 13:59:43,513] {docker.py:276} INFO - 21/05/17 16:59:43 INFO Executor: Running task 78.0 in stage 4.0 (TID 263)
[2021-05-17 13:59:43,522] {docker.py:276} INFO - 21/05/17 16:59:43 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:43,524] {docker.py:276} INFO - 21/05/17 16:59:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:43,524] {docker.py:276} INFO - 21/05/17 16:59:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445426655696859752357_0004_m_000078_263, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445426655696859752357_0004_m_000078_263}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445426655696859752357_0004}; taskId=attempt_202105171658445426655696859752357_0004_m_000078_263, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1622d914}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658445426655696859752357_0004_m_000078_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445426655696859752357_0004_m_000078_263
[2021-05-17 13:59:43,528] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Task committer attempt_202105171658445426655696859752357_0004_m_000078_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445426655696859752357_0004_m_000078_263 : duration 0:00.003s
[2021-05-17 13:59:43,682] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658443894008332959346418_0004_m_000075_260: needsTaskCommit() Task attempt_202105171658443894008332959346418_0004_m_000075_260
[2021-05-17 13:59:43,684] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Task committer attempt_202105171658443894008332959346418_0004_m_000075_260: needsTaskCommit() Task attempt_202105171658443894008332959346418_0004_m_000075_260: duration 0:00.001s
21/05/17 16:59:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443894008332959346418_0004_m_000075_260
[2021-05-17 13:59:43,687] {docker.py:276} INFO - 21/05/17 16:59:43 INFO Executor: Finished task 75.0 in stage 4.0 (TID 260). 4544 bytes result sent to driver
[2021-05-17 13:59:43,688] {docker.py:276} INFO - 21/05/17 16:59:43 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 264) (bb4563f4559d, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:43,688] {docker.py:276} INFO - 21/05/17 16:59:43 INFO Executor: Running task 79.0 in stage 4.0 (TID 264)
[2021-05-17 13:59:43,689] {docker.py:276} INFO - 21/05/17 16:59:43 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 260) in 2481 ms on bb4563f4559d (executor driver) (76/200)
[2021-05-17 13:59:43,697] {docker.py:276} INFO - 21/05/17 16:59:43 INFO ShuffleBlockFetcherIterator: Getting 4 (26.0 KiB) non-empty blocks including 4 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:43,699] {docker.py:276} INFO - 21/05/17 16:59:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:43,700] {docker.py:276} INFO - 21/05/17 16:59:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445515455975763741037_0004_m_000079_264, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445515455975763741037_0004_m_000079_264}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445515455975763741037_0004}; taskId=attempt_202105171658445515455975763741037_0004_m_000079_264, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@515002c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658445515455975763741037_0004_m_000079_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445515455975763741037_0004_m_000079_264
[2021-05-17 13:59:43,704] {docker.py:276} INFO - 21/05/17 16:59:43 INFO StagingCommitter: Task committer attempt_202105171658445515455975763741037_0004_m_000079_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445515455975763741037_0004_m_000079_264 : duration 0:00.004s
[2021-05-17 13:59:44,455] {docker.py:276} INFO - 21/05/17 16:59:44 INFO StagingCommitter: Starting: Task committer attempt_202105171658447709010903431033361_0004_m_000076_261: needsTaskCommit() Task attempt_202105171658447709010903431033361_0004_m_000076_261
[2021-05-17 13:59:44,457] {docker.py:276} INFO - 21/05/17 16:59:44 INFO StagingCommitter: Task committer attempt_202105171658447709010903431033361_0004_m_000076_261: needsTaskCommit() Task attempt_202105171658447709010903431033361_0004_m_000076_261: duration 0:00.002s
[2021-05-17 13:59:44,457] {docker.py:276} INFO - 21/05/17 16:59:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447709010903431033361_0004_m_000076_261
[2021-05-17 13:59:44,459] {docker.py:276} INFO - 21/05/17 16:59:44 INFO Executor: Finished task 76.0 in stage 4.0 (TID 261). 4544 bytes result sent to driver
[2021-05-17 13:59:44,461] {docker.py:276} INFO - 21/05/17 16:59:44 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 265) (bb4563f4559d, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:44,461] {docker.py:276} INFO - 21/05/17 16:59:44 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 261) in 2514 ms on bb4563f4559d (executor driver) (77/200)
[2021-05-17 13:59:44,462] {docker.py:276} INFO - 21/05/17 16:59:44 INFO Executor: Running task 80.0 in stage 4.0 (TID 265)
[2021-05-17 13:59:44,471] {docker.py:276} INFO - 21/05/17 16:59:44 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:44,471] {docker.py:276} INFO - 21/05/17 16:59:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:44,473] {docker.py:276} INFO - 21/05/17 16:59:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441804490183514572292_0004_m_000080_265, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441804490183514572292_0004_m_000080_265}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441804490183514572292_0004}; taskId=attempt_202105171658441804490183514572292_0004_m_000080_265, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6cf10994}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:44,473] {docker.py:276} INFO - 21/05/17 16:59:44 INFO StagingCommitter: Starting: Task committer attempt_202105171658441804490183514572292_0004_m_000080_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441804490183514572292_0004_m_000080_265
[2021-05-17 13:59:44,476] {docker.py:276} INFO - 21/05/17 16:59:44 INFO StagingCommitter: Task committer attempt_202105171658441804490183514572292_0004_m_000080_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441804490183514572292_0004_m_000080_265 : duration 0:00.004s
[2021-05-17 13:59:45,316] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Starting: Task committer attempt_202105171658441960490384837142331_0004_m_000077_262: needsTaskCommit() Task attempt_202105171658441960490384837142331_0004_m_000077_262
[2021-05-17 13:59:45,317] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Task committer attempt_202105171658441960490384837142331_0004_m_000077_262: needsTaskCommit() Task attempt_202105171658441960490384837142331_0004_m_000077_262: duration 0:00.001s
[2021-05-17 13:59:45,317] {docker.py:276} INFO - 21/05/17 16:59:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441960490384837142331_0004_m_000077_262
[2021-05-17 13:59:45,319] {docker.py:276} INFO - 21/05/17 16:59:45 INFO Executor: Finished task 77.0 in stage 4.0 (TID 262). 4544 bytes result sent to driver
[2021-05-17 13:59:45,320] {docker.py:276} INFO - 21/05/17 16:59:45 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 266) (bb4563f4559d, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:45,321] {docker.py:276} INFO - 21/05/17 16:59:45 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 262) in 2043 ms on bb4563f4559d (executor driver) (78/200)
[2021-05-17 13:59:45,322] {docker.py:276} INFO - 21/05/17 16:59:45 INFO Executor: Running task 81.0 in stage 4.0 (TID 266)
[2021-05-17 13:59:45,331] {docker.py:276} INFO - 21/05/17 16:59:45 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:45,334] {docker.py:276} INFO - 21/05/17 16:59:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448309996012443061055_0004_m_000081_266, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448309996012443061055_0004_m_000081_266}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448309996012443061055_0004}; taskId=attempt_202105171658448309996012443061055_0004_m_000081_266, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f2c595a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:45,334] {docker.py:276} INFO - 21/05/17 16:59:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:45,334] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Starting: Task committer attempt_202105171658448309996012443061055_0004_m_000081_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448309996012443061055_0004_m_000081_266
[2021-05-17 13:59:45,338] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Task committer attempt_202105171658448309996012443061055_0004_m_000081_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448309996012443061055_0004_m_000081_266 : duration 0:00.003s
[2021-05-17 13:59:45,501] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Starting: Task committer attempt_202105171658445426655696859752357_0004_m_000078_263: needsTaskCommit() Task attempt_202105171658445426655696859752357_0004_m_000078_263
[2021-05-17 13:59:45,502] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Task committer attempt_202105171658445426655696859752357_0004_m_000078_263: needsTaskCommit() Task attempt_202105171658445426655696859752357_0004_m_000078_263: duration 0:00.001s
[2021-05-17 13:59:45,502] {docker.py:276} INFO - 21/05/17 16:59:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445426655696859752357_0004_m_000078_263
[2021-05-17 13:59:45,503] {docker.py:276} INFO - 21/05/17 16:59:45 INFO Executor: Finished task 78.0 in stage 4.0 (TID 263). 4544 bytes result sent to driver
[2021-05-17 13:59:45,504] {docker.py:276} INFO - 21/05/17 16:59:45 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 267) (bb4563f4559d, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:45,504] {docker.py:276} INFO - 21/05/17 16:59:45 INFO Executor: Running task 82.0 in stage 4.0 (TID 267)
[2021-05-17 13:59:45,505] {docker.py:276} INFO - 21/05/17 16:59:45 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 263) in 1997 ms on bb4563f4559d (executor driver) (79/200)
[2021-05-17 13:59:45,521] {docker.py:276} INFO - 21/05/17 16:59:45 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:45,523] {docker.py:276} INFO - 21/05/17 16:59:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:45,523] {docker.py:276} INFO - 21/05/17 16:59:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:45,523] {docker.py:276} INFO - 21/05/17 16:59:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441523416222520534582_0004_m_000082_267, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441523416222520534582_0004_m_000082_267}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441523416222520534582_0004}; taskId=attempt_202105171658441523416222520534582_0004_m_000082_267, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cbe30c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:45,524] {docker.py:276} INFO - 21/05/17 16:59:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:45,524] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Starting: Task committer attempt_202105171658441523416222520534582_0004_m_000082_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441523416222520534582_0004_m_000082_267
[2021-05-17 13:59:45,526] {docker.py:276} INFO - 21/05/17 16:59:45 INFO StagingCommitter: Task committer attempt_202105171658441523416222520534582_0004_m_000082_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441523416222520534582_0004_m_000082_267 : duration 0:00.003s
[2021-05-17 13:59:46,132] {docker.py:276} INFO - 21/05/17 16:59:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658445515455975763741037_0004_m_000079_264: needsTaskCommit() Task attempt_202105171658445515455975763741037_0004_m_000079_264
21/05/17 16:59:46 INFO StagingCommitter: Task committer attempt_202105171658445515455975763741037_0004_m_000079_264: needsTaskCommit() Task attempt_202105171658445515455975763741037_0004_m_000079_264: duration 0:00.000s
21/05/17 16:59:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445515455975763741037_0004_m_000079_264
[2021-05-17 13:59:46,133] {docker.py:276} INFO - 21/05/17 16:59:46 INFO Executor: Finished task 79.0 in stage 4.0 (TID 264). 4587 bytes result sent to driver
[2021-05-17 13:59:46,134] {docker.py:276} INFO - 21/05/17 16:59:46 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 268) (bb4563f4559d, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:46,135] {docker.py:276} INFO - 21/05/17 16:59:46 INFO Executor: Running task 83.0 in stage 4.0 (TID 268)
[2021-05-17 13:59:46,136] {docker.py:276} INFO - 21/05/17 16:59:46 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 264) in 2452 ms on bb4563f4559d (executor driver) (80/200)
[2021-05-17 13:59:46,145] {docker.py:276} INFO - 21/05/17 16:59:46 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:46,148] {docker.py:276} INFO - 21/05/17 16:59:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:46,149] {docker.py:276} INFO - 21/05/17 16:59:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448154914559113055442_0004_m_000083_268, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448154914559113055442_0004_m_000083_268}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448154914559113055442_0004}; taskId=attempt_202105171658448154914559113055442_0004_m_000083_268, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a6fc074}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:46,150] {docker.py:276} INFO - 21/05/17 16:59:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658448154914559113055442_0004_m_000083_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448154914559113055442_0004_m_000083_268
[2021-05-17 13:59:46,153] {docker.py:276} INFO - 21/05/17 16:59:46 INFO StagingCommitter: Task committer attempt_202105171658448154914559113055442_0004_m_000083_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448154914559113055442_0004_m_000083_268 : duration 0:00.005s
[2021-05-17 13:59:46,952] {docker.py:276} INFO - 21/05/17 16:59:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658441804490183514572292_0004_m_000080_265: needsTaskCommit() Task attempt_202105171658441804490183514572292_0004_m_000080_265
[2021-05-17 13:59:46,953] {docker.py:276} INFO - 21/05/17 16:59:46 INFO StagingCommitter: Task committer attempt_202105171658441804490183514572292_0004_m_000080_265: needsTaskCommit() Task attempt_202105171658441804490183514572292_0004_m_000080_265: duration 0:00.001s
21/05/17 16:59:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441804490183514572292_0004_m_000080_265
[2021-05-17 13:59:46,954] {docker.py:276} INFO - 21/05/17 16:59:46 INFO Executor: Finished task 80.0 in stage 4.0 (TID 265). 4587 bytes result sent to driver
[2021-05-17 13:59:46,956] {docker.py:276} INFO - 21/05/17 16:59:46 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 269) (bb4563f4559d, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:46,956] {docker.py:276} INFO - 21/05/17 16:59:46 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 265) in 2500 ms on bb4563f4559d (executor driver) (81/200)
21/05/17 16:59:46 INFO Executor: Running task 84.0 in stage 4.0 (TID 269)
[2021-05-17 13:59:46,967] {docker.py:276} INFO - 21/05/17 16:59:46 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:46,969] {docker.py:276} INFO - 21/05/17 16:59:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844654430110558093964_0004_m_000084_269, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844654430110558093964_0004_m_000084_269}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844654430110558093964_0004}; taskId=attempt_20210517165844654430110558093964_0004_m_000084_269, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6de0c8f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:46,969] {docker.py:276} INFO - 21/05/17 16:59:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:46 INFO StagingCommitter: Starting: Task committer attempt_20210517165844654430110558093964_0004_m_000084_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844654430110558093964_0004_m_000084_269
[2021-05-17 13:59:46,972] {docker.py:276} INFO - 21/05/17 16:59:46 INFO StagingCommitter: Task committer attempt_20210517165844654430110558093964_0004_m_000084_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844654430110558093964_0004_m_000084_269 : duration 0:00.004s
[2021-05-17 13:59:47,813] {docker.py:276} INFO - 21/05/17 16:59:47 INFO StagingCommitter: Starting: Task committer attempt_202105171658441523416222520534582_0004_m_000082_267: needsTaskCommit() Task attempt_202105171658441523416222520534582_0004_m_000082_267
[2021-05-17 13:59:47,814] {docker.py:276} INFO - 21/05/17 16:59:47 INFO StagingCommitter: Task committer attempt_202105171658441523416222520534582_0004_m_000082_267: needsTaskCommit() Task attempt_202105171658441523416222520534582_0004_m_000082_267: duration 0:00.001s
[2021-05-17 13:59:47,815] {docker.py:276} INFO - 21/05/17 16:59:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441523416222520534582_0004_m_000082_267
[2021-05-17 13:59:47,817] {docker.py:276} INFO - 21/05/17 16:59:47 INFO Executor: Finished task 82.0 in stage 4.0 (TID 267). 4630 bytes result sent to driver
[2021-05-17 13:59:47,820] {docker.py:276} INFO - 21/05/17 16:59:47 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 270) (bb4563f4559d, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:47,821] {docker.py:276} INFO - 21/05/17 16:59:47 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 267) in 2319 ms on bb4563f4559d (executor driver) (82/200)
21/05/17 16:59:47 INFO Executor: Running task 85.0 in stage 4.0 (TID 270)
[2021-05-17 13:59:47,836] {docker.py:276} INFO - 21/05/17 16:59:47 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:47,836] {docker.py:276} INFO - 21/05/17 16:59:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 13:59:47,839] {docker.py:276} INFO - 21/05/17 16:59:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:47,840] {docker.py:276} INFO - 21/05/17 16:59:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:47,842] {docker.py:276} INFO - 21/05/17 16:59:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:47,842] {docker.py:276} INFO - 21/05/17 16:59:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442222881034473870576_0004_m_000085_270, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442222881034473870576_0004_m_000085_270}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442222881034473870576_0004}; taskId=attempt_202105171658442222881034473870576_0004_m_000085_270, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dfb3c83}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:47,843] {docker.py:276} INFO - 21/05/17 16:59:47 INFO StagingCommitter: Starting: Task committer attempt_202105171658442222881034473870576_0004_m_000085_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442222881034473870576_0004_m_000085_270
[2021-05-17 13:59:47,846] {docker.py:276} INFO - 21/05/17 16:59:47 INFO StagingCommitter: Task committer attempt_202105171658442222881034473870576_0004_m_000085_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442222881034473870576_0004_m_000085_270 : duration 0:00.005s
[2021-05-17 13:59:48,339] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658448309996012443061055_0004_m_000081_266: needsTaskCommit() Task attempt_202105171658448309996012443061055_0004_m_000081_266
[2021-05-17 13:59:48,340] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Task committer attempt_202105171658448309996012443061055_0004_m_000081_266: needsTaskCommit() Task attempt_202105171658448309996012443061055_0004_m_000081_266: duration 0:00.001s
21/05/17 16:59:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448309996012443061055_0004_m_000081_266
[2021-05-17 13:59:48,341] {docker.py:276} INFO - 21/05/17 16:59:48 INFO Executor: Finished task 81.0 in stage 4.0 (TID 266). 4587 bytes result sent to driver
[2021-05-17 13:59:48,343] {docker.py:276} INFO - 21/05/17 16:59:48 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 266) in 3026 ms on bb4563f4559d (executor driver) (83/200)
[2021-05-17 13:59:48,344] {docker.py:276} INFO - 21/05/17 16:59:48 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 271) (bb4563f4559d, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:48,345] {docker.py:276} INFO - 21/05/17 16:59:48 INFO Executor: Running task 86.0 in stage 4.0 (TID 271)
[2021-05-17 13:59:48,354] {docker.py:276} INFO - 21/05/17 16:59:48 INFO ShuffleBlockFetcherIterator: Getting 4 (25.9 KiB) non-empty blocks including 4 (25.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:48,354] {docker.py:276} INFO - 21/05/17 16:59:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:48,356] {docker.py:276} INFO - 21/05/17 16:59:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:48,356] {docker.py:276} INFO - 21/05/17 16:59:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446578500525295672757_0004_m_000086_271, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446578500525295672757_0004_m_000086_271}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446578500525295672757_0004}; taskId=attempt_202105171658446578500525295672757_0004_m_000086_271, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d8ef111}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:48,356] {docker.py:276} INFO - 21/05/17 16:59:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:48,357] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658446578500525295672757_0004_m_000086_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446578500525295672757_0004_m_000086_271
[2021-05-17 13:59:48,359] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Task committer attempt_202105171658446578500525295672757_0004_m_000086_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446578500525295672757_0004_m_000086_271 : duration 0:00.003s
[2021-05-17 13:59:48,621] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658448154914559113055442_0004_m_000083_268: needsTaskCommit() Task attempt_202105171658448154914559113055442_0004_m_000083_268
[2021-05-17 13:59:48,622] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Task committer attempt_202105171658448154914559113055442_0004_m_000083_268: needsTaskCommit() Task attempt_202105171658448154914559113055442_0004_m_000083_268: duration 0:00.001s
21/05/17 16:59:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448154914559113055442_0004_m_000083_268
[2021-05-17 13:59:48,624] {docker.py:276} INFO - 21/05/17 16:59:48 INFO Executor: Finished task 83.0 in stage 4.0 (TID 268). 4544 bytes result sent to driver
[2021-05-17 13:59:48,625] {docker.py:276} INFO - 21/05/17 16:59:48 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 272) (bb4563f4559d, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:48,626] {docker.py:276} INFO - 21/05/17 16:59:48 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 268) in 2494 ms on bb4563f4559d (executor driver) (84/200)
21/05/17 16:59:48 INFO Executor: Running task 87.0 in stage 4.0 (TID 272)
[2021-05-17 13:59:48,637] {docker.py:276} INFO - 21/05/17 16:59:48 INFO ShuffleBlockFetcherIterator: Getting 4 (25.2 KiB) non-empty blocks including 4 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:48,639] {docker.py:276} INFO - 21/05/17 16:59:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445308526415024772748_0004_m_000087_272, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445308526415024772748_0004_m_000087_272}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445308526415024772748_0004}; taskId=attempt_202105171658445308526415024772748_0004_m_000087_272, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22269c57}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:48,640] {docker.py:276} INFO - 21/05/17 16:59:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:48,640] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658445308526415024772748_0004_m_000087_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445308526415024772748_0004_m_000087_272
[2021-05-17 13:59:48,643] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Task committer attempt_202105171658445308526415024772748_0004_m_000087_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445308526415024772748_0004_m_000087_272 : duration 0:00.003s
[2021-05-17 13:59:48,981] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Starting: Task committer attempt_20210517165844654430110558093964_0004_m_000084_269: needsTaskCommit() Task attempt_20210517165844654430110558093964_0004_m_000084_269
[2021-05-17 13:59:48,991] {docker.py:276} INFO - 21/05/17 16:59:48 INFO StagingCommitter: Task committer attempt_20210517165844654430110558093964_0004_m_000084_269: needsTaskCommit() Task attempt_20210517165844654430110558093964_0004_m_000084_269: duration 0:00.001s
21/05/17 16:59:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844654430110558093964_0004_m_000084_269
[2021-05-17 13:59:48,992] {docker.py:276} INFO - 21/05/17 16:59:48 INFO Executor: Finished task 84.0 in stage 4.0 (TID 269). 4544 bytes result sent to driver
21/05/17 16:59:48 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 273) (bb4563f4559d, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 16:59:48 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 269) in 2032 ms on bb4563f4559d (executor driver) (85/200)
[2021-05-17 13:59:48,993] {docker.py:276} INFO - 21/05/17 16:59:48 INFO Executor: Running task 88.0 in stage 4.0 (TID 273)
[2021-05-17 13:59:48,994] {docker.py:276} INFO - 21/05/17 16:59:49 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:48,996] {docker.py:276} INFO - 21/05/17 16:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:48,997] {docker.py:276} INFO - 21/05/17 16:59:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442633289335626849948_0004_m_000088_273, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442633289335626849948_0004_m_000088_273}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442633289335626849948_0004}; taskId=attempt_202105171658442633289335626849948_0004_m_000088_273, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c68ddd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:49 INFO StagingCommitter: Starting: Task committer attempt_202105171658442633289335626849948_0004_m_000088_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442633289335626849948_0004_m_000088_273
[2021-05-17 13:59:48,999] {docker.py:276} INFO - 21/05/17 16:59:49 INFO StagingCommitter: Task committer attempt_202105171658442633289335626849948_0004_m_000088_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442633289335626849948_0004_m_000088_273 : duration 0:00.003s
[2021-05-17 13:59:50,422] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Starting: Task committer attempt_202105171658442222881034473870576_0004_m_000085_270: needsTaskCommit() Task attempt_202105171658442222881034473870576_0004_m_000085_270
[2021-05-17 13:59:50,423] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Task committer attempt_202105171658442222881034473870576_0004_m_000085_270: needsTaskCommit() Task attempt_202105171658442222881034473870576_0004_m_000085_270: duration 0:00.001s
21/05/17 16:59:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442222881034473870576_0004_m_000085_270
[2021-05-17 13:59:50,425] {docker.py:276} INFO - 21/05/17 16:59:50 INFO Executor: Finished task 85.0 in stage 4.0 (TID 270). 4544 bytes result sent to driver
[2021-05-17 13:59:50,427] {docker.py:276} INFO - 21/05/17 16:59:50 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 274) (bb4563f4559d, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:50,428] {docker.py:276} INFO - 21/05/17 16:59:50 INFO Executor: Running task 89.0 in stage 4.0 (TID 274)
[2021-05-17 13:59:50,429] {docker.py:276} INFO - 21/05/17 16:59:50 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 270) in 2614 ms on bb4563f4559d (executor driver) (86/200)
[2021-05-17 13:59:50,438] {docker.py:276} INFO - 21/05/17 16:59:50 INFO ShuffleBlockFetcherIterator: Getting 4 (25.2 KiB) non-empty blocks including 4 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:50,438] {docker.py:276} INFO - 21/05/17 16:59:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:50,440] {docker.py:276} INFO - 21/05/17 16:59:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:50,440] {docker.py:276} INFO - 21/05/17 16:59:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:50,441] {docker.py:276} INFO - 21/05/17 16:59:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:50,441] {docker.py:276} INFO - 21/05/17 16:59:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444947135292143618536_0004_m_000089_274, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444947135292143618536_0004_m_000089_274}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444947135292143618536_0004}; taskId=attempt_202105171658444947135292143618536_0004_m_000089_274, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f3637}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:50,441] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Starting: Task committer attempt_202105171658444947135292143618536_0004_m_000089_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444947135292143618536_0004_m_000089_274
[2021-05-17 13:59:50,444] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Task committer attempt_202105171658444947135292143618536_0004_m_000089_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444947135292143618536_0004_m_000089_274 : duration 0:00.004s
[2021-05-17 13:59:50,915] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Starting: Task committer attempt_202105171658445308526415024772748_0004_m_000087_272: needsTaskCommit() Task attempt_202105171658445308526415024772748_0004_m_000087_272
[2021-05-17 13:59:50,916] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Task committer attempt_202105171658445308526415024772748_0004_m_000087_272: needsTaskCommit() Task attempt_202105171658445308526415024772748_0004_m_000087_272: duration 0:00.001s
[2021-05-17 13:59:50,917] {docker.py:276} INFO - 21/05/17 16:59:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445308526415024772748_0004_m_000087_272
[2021-05-17 13:59:50,918] {docker.py:276} INFO - 21/05/17 16:59:50 INFO Executor: Finished task 87.0 in stage 4.0 (TID 272). 4544 bytes result sent to driver
[2021-05-17 13:59:50,919] {docker.py:276} INFO - 21/05/17 16:59:50 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 275) (bb4563f4559d, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:50,920] {docker.py:276} INFO - 21/05/17 16:59:50 INFO Executor: Running task 90.0 in stage 4.0 (TID 275)
[2021-05-17 13:59:50,921] {docker.py:276} INFO - 21/05/17 16:59:50 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 272) in 2299 ms on bb4563f4559d (executor driver) (87/200)
[2021-05-17 13:59:50,929] {docker.py:276} INFO - 21/05/17 16:59:50 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:50,930] {docker.py:276} INFO - 21/05/17 16:59:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:50,932] {docker.py:276} INFO - 21/05/17 16:59:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:50,933] {docker.py:276} INFO - 21/05/17 16:59:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:50,933] {docker.py:276} INFO - 21/05/17 16:59:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447546648755309495515_0004_m_000090_275, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447546648755309495515_0004_m_000090_275}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447546648755309495515_0004}; taskId=attempt_202105171658447546648755309495515_0004_m_000090_275, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7921a561}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:50,934] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Starting: Task committer attempt_202105171658447546648755309495515_0004_m_000090_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447546648755309495515_0004_m_000090_275
[2021-05-17 13:59:50,937] {docker.py:276} INFO - 21/05/17 16:59:50 INFO StagingCommitter: Task committer attempt_202105171658447546648755309495515_0004_m_000090_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447546648755309495515_0004_m_000090_275 : duration 0:00.003s
[2021-05-17 13:59:51,346] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658442633289335626849948_0004_m_000088_273: needsTaskCommit() Task attempt_202105171658442633289335626849948_0004_m_000088_273
[2021-05-17 13:59:51,347] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Task committer attempt_202105171658442633289335626849948_0004_m_000088_273: needsTaskCommit() Task attempt_202105171658442633289335626849948_0004_m_000088_273: duration 0:00.001s
21/05/17 16:59:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442633289335626849948_0004_m_000088_273
[2021-05-17 13:59:51,349] {docker.py:276} INFO - 21/05/17 16:59:51 INFO Executor: Finished task 88.0 in stage 4.0 (TID 273). 4544 bytes result sent to driver
[2021-05-17 13:59:51,350] {docker.py:276} INFO - 21/05/17 16:59:51 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 276) (bb4563f4559d, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:51,351] {docker.py:276} INFO - 21/05/17 16:59:51 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 273) in 2369 ms on bb4563f4559d (executor driver) (88/200)
21/05/17 16:59:51 INFO Executor: Running task 91.0 in stage 4.0 (TID 276)
[2021-05-17 13:59:51,370] {docker.py:276} INFO - 21/05/17 16:59:51 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:51,372] {docker.py:276} INFO - 21/05/17 16:59:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448039712321725804654_0004_m_000091_276, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448039712321725804654_0004_m_000091_276}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448039712321725804654_0004}; taskId=attempt_202105171658448039712321725804654_0004_m_000091_276, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c6cb31a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:51,372] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658448039712321725804654_0004_m_000091_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448039712321725804654_0004_m_000091_276
[2021-05-17 13:59:51,377] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Task committer attempt_202105171658448039712321725804654_0004_m_000091_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448039712321725804654_0004_m_000091_276 : duration 0:00.004s
[2021-05-17 13:59:51,537] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658446578500525295672757_0004_m_000086_271: needsTaskCommit() Task attempt_202105171658446578500525295672757_0004_m_000086_271
[2021-05-17 13:59:51,538] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Task committer attempt_202105171658446578500525295672757_0004_m_000086_271: needsTaskCommit() Task attempt_202105171658446578500525295672757_0004_m_000086_271: duration 0:00.001s
21/05/17 16:59:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446578500525295672757_0004_m_000086_271
[2021-05-17 13:59:51,540] {docker.py:276} INFO - 21/05/17 16:59:51 INFO Executor: Finished task 86.0 in stage 4.0 (TID 271). 4587 bytes result sent to driver
[2021-05-17 13:59:51,541] {docker.py:276} INFO - 21/05/17 16:59:51 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 277) (bb4563f4559d, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:51,542] {docker.py:276} INFO - 21/05/17 16:59:51 INFO Executor: Running task 92.0 in stage 4.0 (TID 277)
[2021-05-17 13:59:51,543] {docker.py:276} INFO - 21/05/17 16:59:51 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 271) in 3203 ms on bb4563f4559d (executor driver) (89/200)
[2021-05-17 13:59:51,552] {docker.py:276} INFO - 21/05/17 16:59:51 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:51,552] {docker.py:276} INFO - 21/05/17 16:59:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:51,553] {docker.py:276} INFO - 21/05/17 16:59:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:51,554] {docker.py:276} INFO - 21/05/17 16:59:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:51,554] {docker.py:276} INFO - 21/05/17 16:59:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:51,554] {docker.py:276} INFO - 21/05/17 16:59:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448399158636811027651_0004_m_000092_277, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448399158636811027651_0004_m_000092_277}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448399158636811027651_0004}; taskId=attempt_202105171658448399158636811027651_0004_m_000092_277, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2120c3b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:51,555] {docker.py:276} INFO - 21/05/17 16:59:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:51,555] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658448399158636811027651_0004_m_000092_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448399158636811027651_0004_m_000092_277
[2021-05-17 13:59:51,557] {docker.py:276} INFO - 21/05/17 16:59:51 INFO StagingCommitter: Task committer attempt_202105171658448399158636811027651_0004_m_000092_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448399158636811027651_0004_m_000092_277 : duration 0:00.003s
[2021-05-17 13:59:53,393] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658447546648755309495515_0004_m_000090_275: needsTaskCommit() Task attempt_202105171658447546648755309495515_0004_m_000090_275
[2021-05-17 13:59:53,394] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658447546648755309495515_0004_m_000090_275: needsTaskCommit() Task attempt_202105171658447546648755309495515_0004_m_000090_275: duration 0:00.001s
21/05/17 16:59:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447546648755309495515_0004_m_000090_275
[2021-05-17 13:59:53,396] {docker.py:276} INFO - 21/05/17 16:59:53 INFO Executor: Finished task 90.0 in stage 4.0 (TID 275). 4587 bytes result sent to driver
[2021-05-17 13:59:53,397] {docker.py:276} INFO - 21/05/17 16:59:53 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 278) (bb4563f4559d, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:53,398] {docker.py:276} INFO - 21/05/17 16:59:53 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 275) in 2481 ms on bb4563f4559d (executor driver) (90/200)
21/05/17 16:59:53 INFO Executor: Running task 93.0 in stage 4.0 (TID 278)
[2021-05-17 13:59:53,408] {docker.py:276} INFO - 21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Getting 4 (26.5 KiB) non-empty blocks including 4 (26.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:53,410] {docker.py:276} INFO - 21/05/17 16:59:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:53,411] {docker.py:276} INFO - 21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:53,411] {docker.py:276} INFO - 21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441904064786285232924_0004_m_000093_278, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441904064786285232924_0004_m_000093_278}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441904064786285232924_0004}; taskId=attempt_202105171658441904064786285232924_0004_m_000093_278, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44b0f36f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:53,412] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658441904064786285232924_0004_m_000093_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441904064786285232924_0004_m_000093_278
[2021-05-17 13:59:53,414] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658441904064786285232924_0004_m_000093_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441904064786285232924_0004_m_000093_278 : duration 0:00.003s
[2021-05-17 13:59:53,506] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658444947135292143618536_0004_m_000089_274: needsTaskCommit() Task attempt_202105171658444947135292143618536_0004_m_000089_274
[2021-05-17 13:59:53,507] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658444947135292143618536_0004_m_000089_274: needsTaskCommit() Task attempt_202105171658444947135292143618536_0004_m_000089_274: duration 0:00.001s
21/05/17 16:59:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444947135292143618536_0004_m_000089_274
[2021-05-17 13:59:53,509] {docker.py:276} INFO - 21/05/17 16:59:53 INFO Executor: Finished task 89.0 in stage 4.0 (TID 274). 4587 bytes result sent to driver
[2021-05-17 13:59:53,510] {docker.py:276} INFO - 21/05/17 16:59:53 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 279) (bb4563f4559d, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:53,512] {docker.py:276} INFO - 21/05/17 16:59:53 INFO Executor: Running task 94.0 in stage 4.0 (TID 279)
[2021-05-17 13:59:53,512] {docker.py:276} INFO - 21/05/17 16:59:53 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 274) in 3088 ms on bb4563f4559d (executor driver) (91/200)
[2021-05-17 13:59:53,523] {docker.py:276} INFO - 21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:53,526] {docker.py:276} INFO - 21/05/17 16:59:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447438625228326523784_0004_m_000094_279, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447438625228326523784_0004_m_000094_279}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447438625228326523784_0004}; taskId=attempt_202105171658447438625228326523784_0004_m_000094_279, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@104da44a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658447438625228326523784_0004_m_000094_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447438625228326523784_0004_m_000094_279
[2021-05-17 13:59:53,529] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658447438625228326523784_0004_m_000094_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447438625228326523784_0004_m_000094_279 : duration 0:00.003s
[2021-05-17 13:59:53,845] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658448399158636811027651_0004_m_000092_277: needsTaskCommit() Task attempt_202105171658448399158636811027651_0004_m_000092_277
[2021-05-17 13:59:53,846] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658448039712321725804654_0004_m_000091_276: needsTaskCommit() Task attempt_202105171658448039712321725804654_0004_m_000091_276
[2021-05-17 13:59:53,847] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658448039712321725804654_0004_m_000091_276: needsTaskCommit() Task attempt_202105171658448039712321725804654_0004_m_000091_276: duration 0:00.002s
21/05/17 16:59:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448039712321725804654_0004_m_000091_276
[2021-05-17 13:59:53,848] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658448399158636811027651_0004_m_000092_277: needsTaskCommit() Task attempt_202105171658448399158636811027651_0004_m_000092_277: duration 0:00.002s
[2021-05-17 13:59:53,848] {docker.py:276} INFO - 21/05/17 16:59:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448399158636811027651_0004_m_000092_277
[2021-05-17 13:59:53,849] {docker.py:276} INFO - 21/05/17 16:59:53 INFO Executor: Finished task 91.0 in stage 4.0 (TID 276). 4587 bytes result sent to driver
[2021-05-17 13:59:53,850] {docker.py:276} INFO - 21/05/17 16:59:53 INFO Executor: Finished task 92.0 in stage 4.0 (TID 277). 4544 bytes result sent to driver
21/05/17 16:59:53 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 280) (bb4563f4559d, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:53,851] {docker.py:276} INFO - 21/05/17 16:59:53 INFO Executor: Running task 95.0 in stage 4.0 (TID 280)
21/05/17 16:59:53 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 281) (bb4563f4559d, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:53,853] {docker.py:276} INFO - 21/05/17 16:59:53 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 276) in 2505 ms on bb4563f4559d (executor driver) (92/200)
21/05/17 16:59:53 INFO Executor: Running task 96.0 in stage 4.0 (TID 281)
[2021-05-17 13:59:53,854] {docker.py:276} INFO - 21/05/17 16:59:53 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 277) in 2315 ms on bb4563f4559d (executor driver) (93/200)
[2021-05-17 13:59:53,865] {docker.py:276} INFO - 21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:53,866] {docker.py:276} INFO - 21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:53,867] {docker.py:276} INFO - 21/05/17 16:59:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:53,868] {docker.py:276} INFO - 21/05/17 16:59:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:53,868] {docker.py:276} INFO - 21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:53,868] {docker.py:276} INFO - 21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442306280331695304259_0004_m_000096_281, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442306280331695304259_0004_m_000096_281}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442306280331695304259_0004}; taskId=attempt_202105171658442306280331695304259_0004_m_000096_281, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a83a6f5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:53,869] {docker.py:276} INFO - 21/05/17 16:59:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658442306280331695304259_0004_m_000096_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442306280331695304259_0004_m_000096_281
[2021-05-17 13:59:53,872] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658442306280331695304259_0004_m_000096_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442306280331695304259_0004_m_000096_281 : duration 0:00.003s
[2021-05-17 13:59:53,872] {docker.py:276} INFO - 21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Getting 4 (25.2 KiB) non-empty blocks including 4 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:53,875] {docker.py:276} INFO - 21/05/17 16:59:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:53,875] {docker.py:276} INFO - 21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445823211885193546459_0004_m_000095_280, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445823211885193546459_0004_m_000095_280}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445823211885193546459_0004}; taskId=attempt_202105171658445823211885193546459_0004_m_000095_280, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74982f37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:53,876] {docker.py:276} INFO - 21/05/17 16:59:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:53,876] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658445823211885193546459_0004_m_000095_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445823211885193546459_0004_m_000095_280
[2021-05-17 13:59:53,879] {docker.py:276} INFO - 21/05/17 16:59:53 INFO StagingCommitter: Task committer attempt_202105171658445823211885193546459_0004_m_000095_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445823211885193546459_0004_m_000095_280 : duration 0:00.004s
[2021-05-17 13:59:55,921] {docker.py:276} INFO - 21/05/17 16:59:55 INFO StagingCommitter: Starting: Task committer attempt_202105171658441904064786285232924_0004_m_000093_278: needsTaskCommit() Task attempt_202105171658441904064786285232924_0004_m_000093_278
[2021-05-17 13:59:55,929] {docker.py:276} INFO - 21/05/17 16:59:55 INFO StagingCommitter: Task committer attempt_202105171658441904064786285232924_0004_m_000093_278: needsTaskCommit() Task attempt_202105171658441904064786285232924_0004_m_000093_278: duration 0:00.000s
21/05/17 16:59:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441904064786285232924_0004_m_000093_278
[2021-05-17 13:59:55,929] {docker.py:276} INFO - 21/05/17 16:59:55 INFO Executor: Finished task 93.0 in stage 4.0 (TID 278). 4544 bytes result sent to driver
[2021-05-17 13:59:55,930] {docker.py:276} INFO - 21/05/17 16:59:55 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 282) (bb4563f4559d, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:55,930] {docker.py:276} INFO - 21/05/17 16:59:55 INFO Executor: Running task 97.0 in stage 4.0 (TID 282)
[2021-05-17 13:59:55,931] {docker.py:276} INFO - 21/05/17 16:59:55 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 278) in 2534 ms on bb4563f4559d (executor driver) (94/200)
[2021-05-17 13:59:55,938] {docker.py:276} INFO - 21/05/17 16:59:55 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:55,940] {docker.py:276} INFO - 21/05/17 16:59:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:55,941] {docker.py:276} INFO - 21/05/17 16:59:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443002391429427220039_0004_m_000097_282, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443002391429427220039_0004_m_000097_282}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443002391429427220039_0004}; taskId=attempt_202105171658443002391429427220039_0004_m_000097_282, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25bb88d3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:55,941] {docker.py:276} INFO - 21/05/17 16:59:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:55 INFO StagingCommitter: Starting: Task committer attempt_202105171658443002391429427220039_0004_m_000097_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443002391429427220039_0004_m_000097_282
[2021-05-17 13:59:55,943] {docker.py:276} INFO - 21/05/17 16:59:55 INFO StagingCommitter: Task committer attempt_202105171658443002391429427220039_0004_m_000097_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443002391429427220039_0004_m_000097_282 : duration 0:00.003s
[2021-05-17 13:59:56,190] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658442306280331695304259_0004_m_000096_281: needsTaskCommit() Task attempt_202105171658442306280331695304259_0004_m_000096_281
[2021-05-17 13:59:56,191] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Task committer attempt_202105171658442306280331695304259_0004_m_000096_281: needsTaskCommit() Task attempt_202105171658442306280331695304259_0004_m_000096_281: duration 0:00.001s
21/05/17 16:59:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442306280331695304259_0004_m_000096_281
[2021-05-17 13:59:56,192] {docker.py:276} INFO - 21/05/17 16:59:56 INFO Executor: Finished task 96.0 in stage 4.0 (TID 281). 4544 bytes result sent to driver
[2021-05-17 13:59:56,193] {docker.py:276} INFO - 21/05/17 16:59:56 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 283) (bb4563f4559d, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:56,194] {docker.py:276} INFO - 21/05/17 16:59:56 INFO Executor: Running task 98.0 in stage 4.0 (TID 283)
[2021-05-17 13:59:56,195] {docker.py:276} INFO - 21/05/17 16:59:56 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 281) in 2347 ms on bb4563f4559d (executor driver) (95/200)
[2021-05-17 13:59:56,203] {docker.py:276} INFO - 21/05/17 16:59:56 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:56,205] {docker.py:276} INFO - 21/05/17 16:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448837980048010441227_0004_m_000098_283, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448837980048010441227_0004_m_000098_283}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448837980048010441227_0004}; taskId=attempt_202105171658448837980048010441227_0004_m_000098_283, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e37fb2d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:56,205] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658448837980048010441227_0004_m_000098_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448837980048010441227_0004_m_000098_283
[2021-05-17 13:59:56,208] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Task committer attempt_202105171658448837980048010441227_0004_m_000098_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448837980048010441227_0004_m_000098_283 : duration 0:00.004s
[2021-05-17 13:59:56,326] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658447438625228326523784_0004_m_000094_279: needsTaskCommit() Task attempt_202105171658447438625228326523784_0004_m_000094_279
[2021-05-17 13:59:56,327] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Task committer attempt_202105171658447438625228326523784_0004_m_000094_279: needsTaskCommit() Task attempt_202105171658447438625228326523784_0004_m_000094_279: duration 0:00.000s
21/05/17 16:59:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447438625228326523784_0004_m_000094_279
[2021-05-17 13:59:56,328] {docker.py:276} INFO - 21/05/17 16:59:56 INFO Executor: Finished task 94.0 in stage 4.0 (TID 279). 4544 bytes result sent to driver
[2021-05-17 13:59:56,329] {docker.py:276} INFO - 21/05/17 16:59:56 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 284) (bb4563f4559d, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:56,330] {docker.py:276} INFO - 21/05/17 16:59:56 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 279) in 2823 ms on bb4563f4559d (executor driver) (96/200)
21/05/17 16:59:56 INFO Executor: Running task 99.0 in stage 4.0 (TID 284)
[2021-05-17 13:59:56,337] {docker.py:276} INFO - 21/05/17 16:59:56 INFO ShuffleBlockFetcherIterator: Getting 4 (26.6 KiB) non-empty blocks including 4 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:56,340] {docker.py:276} INFO - 21/05/17 16:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445193458695552086868_0004_m_000099_284, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445193458695552086868_0004_m_000099_284}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445193458695552086868_0004}; taskId=attempt_202105171658445193458695552086868_0004_m_000099_284, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c7fda48}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:56,340] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658445193458695552086868_0004_m_000099_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445193458695552086868_0004_m_000099_284
[2021-05-17 13:59:56,343] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Task committer attempt_202105171658445193458695552086868_0004_m_000099_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445193458695552086868_0004_m_000099_284 : duration 0:00.004s
[2021-05-17 13:59:56,862] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658445823211885193546459_0004_m_000095_280: needsTaskCommit() Task attempt_202105171658445823211885193546459_0004_m_000095_280
21/05/17 16:59:56 INFO StagingCommitter: Task committer attempt_202105171658445823211885193546459_0004_m_000095_280: needsTaskCommit() Task attempt_202105171658445823211885193546459_0004_m_000095_280: duration 0:00.001s
[2021-05-17 13:59:56,863] {docker.py:276} INFO - 21/05/17 16:59:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445823211885193546459_0004_m_000095_280
[2021-05-17 13:59:56,863] {docker.py:276} INFO - 21/05/17 16:59:56 INFO Executor: Finished task 95.0 in stage 4.0 (TID 280). 4544 bytes result sent to driver
[2021-05-17 13:59:56,865] {docker.py:276} INFO - 21/05/17 16:59:56 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 285) (bb4563f4559d, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:56,866] {docker.py:276} INFO - 21/05/17 16:59:56 INFO Executor: Running task 100.0 in stage 4.0 (TID 285)
[2021-05-17 13:59:56,867] {docker.py:276} INFO - 21/05/17 16:59:56 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 280) in 3022 ms on bb4563f4559d (executor driver) (97/200)
[2021-05-17 13:59:56,880] {docker.py:276} INFO - 21/05/17 16:59:56 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:56,882] {docker.py:276} INFO - 21/05/17 16:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442729304672371200349_0004_m_000100_285, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442729304672371200349_0004_m_000100_285}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442729304672371200349_0004}; taskId=attempt_202105171658442729304672371200349_0004_m_000100_285, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@696062f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:56,882] {docker.py:276} INFO - 21/05/17 16:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658442729304672371200349_0004_m_000100_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442729304672371200349_0004_m_000100_285
[2021-05-17 13:59:56,885] {docker.py:276} INFO - 21/05/17 16:59:56 INFO StagingCommitter: Task committer attempt_202105171658442729304672371200349_0004_m_000100_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442729304672371200349_0004_m_000100_285 : duration 0:00.003s
[2021-05-17 13:59:58,092] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658443002391429427220039_0004_m_000097_282: needsTaskCommit() Task attempt_202105171658443002391429427220039_0004_m_000097_282
[2021-05-17 13:59:58,093] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Task committer attempt_202105171658443002391429427220039_0004_m_000097_282: needsTaskCommit() Task attempt_202105171658443002391429427220039_0004_m_000097_282: duration 0:00.000s
21/05/17 16:59:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443002391429427220039_0004_m_000097_282
[2021-05-17 13:59:58,094] {docker.py:276} INFO - 21/05/17 16:59:58 INFO Executor: Finished task 97.0 in stage 4.0 (TID 282). 4587 bytes result sent to driver
[2021-05-17 13:59:58,096] {docker.py:276} INFO - 21/05/17 16:59:58 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 286) (bb4563f4559d, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:58,097] {docker.py:276} INFO - 21/05/17 16:59:58 INFO Executor: Running task 101.0 in stage 4.0 (TID 286)
[2021-05-17 13:59:58,098] {docker.py:276} INFO - 21/05/17 16:59:58 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 282) in 2175 ms on bb4563f4559d (executor driver) (98/200)
[2021-05-17 13:59:58,108] {docker.py:276} INFO - 21/05/17 16:59:58 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:58,110] {docker.py:276} INFO - 21/05/17 16:59:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445579384266653465752_0004_m_000101_286, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445579384266653465752_0004_m_000101_286}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445579384266653465752_0004}; taskId=attempt_202105171658445579384266653465752_0004_m_000101_286, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@767bc0fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:58,110] {docker.py:276} INFO - 21/05/17 16:59:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658445579384266653465752_0004_m_000101_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445579384266653465752_0004_m_000101_286
[2021-05-17 13:59:58,113] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Task committer attempt_202105171658445579384266653465752_0004_m_000101_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445579384266653465752_0004_m_000101_286 : duration 0:00.003s
[2021-05-17 13:59:58,613] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658448837980048010441227_0004_m_000098_283: needsTaskCommit() Task attempt_202105171658448837980048010441227_0004_m_000098_283
[2021-05-17 13:59:58,613] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Task committer attempt_202105171658448837980048010441227_0004_m_000098_283: needsTaskCommit() Task attempt_202105171658448837980048010441227_0004_m_000098_283: duration 0:00.001s
21/05/17 16:59:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448837980048010441227_0004_m_000098_283
[2021-05-17 13:59:58,616] {docker.py:276} INFO - 21/05/17 16:59:58 INFO Executor: Finished task 98.0 in stage 4.0 (TID 283). 4587 bytes result sent to driver
[2021-05-17 13:59:58,617] {docker.py:276} INFO - 21/05/17 16:59:58 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 287) (bb4563f4559d, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:58,619] {docker.py:276} INFO - 21/05/17 16:59:58 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 283) in 2428 ms on bb4563f4559d (executor driver) (99/200)
[2021-05-17 13:59:58,619] {docker.py:276} INFO - 21/05/17 16:59:58 INFO Executor: Running task 102.0 in stage 4.0 (TID 287)
[2021-05-17 13:59:58,629] {docker.py:276} INFO - 21/05/17 16:59:58 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:58,631] {docker.py:276} INFO - 21/05/17 16:59:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441768383808331577304_0004_m_000102_287, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441768383808331577304_0004_m_000102_287}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441768383808331577304_0004}; taskId=attempt_202105171658441768383808331577304_0004_m_000102_287, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5284bd17}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:58,631] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658441768383808331577304_0004_m_000102_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441768383808331577304_0004_m_000102_287
[2021-05-17 13:59:58,634] {docker.py:276} INFO - 21/05/17 16:59:58 INFO StagingCommitter: Task committer attempt_202105171658441768383808331577304_0004_m_000102_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441768383808331577304_0004_m_000102_287 : duration 0:00.003s
[2021-05-17 13:59:59,016] {docker.py:276} INFO - 21/05/17 16:59:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658445193458695552086868_0004_m_000099_284: needsTaskCommit() Task attempt_202105171658445193458695552086868_0004_m_000099_284
21/05/17 16:59:59 INFO StagingCommitter: Task committer attempt_202105171658445193458695552086868_0004_m_000099_284: needsTaskCommit() Task attempt_202105171658445193458695552086868_0004_m_000099_284: duration 0:00.001s
[2021-05-17 13:59:59,016] {docker.py:276} INFO - 21/05/17 16:59:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445193458695552086868_0004_m_000099_284
[2021-05-17 13:59:59,018] {docker.py:276} INFO - 21/05/17 16:59:59 INFO Executor: Finished task 99.0 in stage 4.0 (TID 284). 4587 bytes result sent to driver
[2021-05-17 13:59:59,019] {docker.py:276} INFO - 21/05/17 16:59:59 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 288) (bb4563f4559d, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:59,020] {docker.py:276} INFO - 21/05/17 16:59:59 INFO Executor: Running task 103.0 in stage 4.0 (TID 288)
21/05/17 16:59:59 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 284) in 2694 ms on bb4563f4559d (executor driver) (100/200)
[2021-05-17 13:59:59,029] {docker.py:276} INFO - 21/05/17 16:59:59 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 16:59:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:59,031] {docker.py:276} INFO - 21/05/17 16:59:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 16:59:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 16:59:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844918564303921589356_0004_m_000103_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844918564303921589356_0004_m_000103_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844918564303921589356_0004}; taskId=attempt_20210517165844918564303921589356_0004_m_000103_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29e2171a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 16:59:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 16:59:59 INFO StagingCommitter: Starting: Task committer attempt_20210517165844918564303921589356_0004_m_000103_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844918564303921589356_0004_m_000103_288
[2021-05-17 13:59:59,034] {docker.py:276} INFO - 21/05/17 16:59:59 INFO StagingCommitter: Task committer attempt_20210517165844918564303921589356_0004_m_000103_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844918564303921589356_0004_m_000103_288 : duration 0:00.002s
[2021-05-17 13:59:59,371] {docker.py:276} INFO - 21/05/17 16:59:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658442729304672371200349_0004_m_000100_285: needsTaskCommit() Task attempt_202105171658442729304672371200349_0004_m_000100_285
[2021-05-17 13:59:59,372] {docker.py:276} INFO - 21/05/17 16:59:59 INFO StagingCommitter: Task committer attempt_202105171658442729304672371200349_0004_m_000100_285: needsTaskCommit() Task attempt_202105171658442729304672371200349_0004_m_000100_285: duration 0:00.000s
21/05/17 16:59:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442729304672371200349_0004_m_000100_285
[2021-05-17 13:59:59,374] {docker.py:276} INFO - 21/05/17 16:59:59 INFO Executor: Finished task 100.0 in stage 4.0 (TID 285). 4587 bytes result sent to driver
[2021-05-17 13:59:59,375] {docker.py:276} INFO - 21/05/17 16:59:59 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 289) (bb4563f4559d, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 13:59:59,378] {docker.py:276} INFO - 21/05/17 16:59:59 INFO Executor: Running task 104.0 in stage 4.0 (TID 289)
[2021-05-17 13:59:59,378] {docker.py:276} INFO - 21/05/17 16:59:59 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 285) in 2515 ms on bb4563f4559d (executor driver) (101/200)
[2021-05-17 13:59:59,385] {docker.py:276} INFO - 21/05/17 16:59:59 INFO ShuffleBlockFetcherIterator: Getting 4 (28.3 KiB) non-empty blocks including 4 (28.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 13:59:59,386] {docker.py:276} INFO - 21/05/17 16:59:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 13:59:59,387] {docker.py:276} INFO - 21/05/17 16:59:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 13:59:59,388] {docker.py:276} INFO - 21/05/17 16:59:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 13:59:59,389] {docker.py:276} INFO - 21/05/17 16:59:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 16:59:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447848820123101812493_0004_m_000104_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447848820123101812493_0004_m_000104_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447848820123101812493_0004}; taskId=attempt_202105171658447848820123101812493_0004_m_000104_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ec4a3fb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 13:59:59,389] {docker.py:276} INFO - 21/05/17 16:59:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 13:59:59,389] {docker.py:276} INFO - 21/05/17 16:59:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658447848820123101812493_0004_m_000104_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447848820123101812493_0004_m_000104_289
[2021-05-17 13:59:59,392] {docker.py:276} INFO - 21/05/17 16:59:59 INFO StagingCommitter: Task committer attempt_202105171658447848820123101812493_0004_m_000104_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447848820123101812493_0004_m_000104_289 : duration 0:00.002s
[2021-05-17 14:00:00,647] {docker.py:276} INFO - 21/05/17 17:00:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658445579384266653465752_0004_m_000101_286: needsTaskCommit() Task attempt_202105171658445579384266653465752_0004_m_000101_286
21/05/17 17:00:00 INFO StagingCommitter: Task committer attempt_202105171658445579384266653465752_0004_m_000101_286: needsTaskCommit() Task attempt_202105171658445579384266653465752_0004_m_000101_286: duration 0:00.001s
21/05/17 17:00:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445579384266653465752_0004_m_000101_286
[2021-05-17 14:00:00,650] {docker.py:276} INFO - 21/05/17 17:00:00 INFO Executor: Finished task 101.0 in stage 4.0 (TID 286). 4544 bytes result sent to driver
[2021-05-17 14:00:00,651] {docker.py:276} INFO - 21/05/17 17:00:00 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 290) (bb4563f4559d, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:00,652] {docker.py:276} INFO - 21/05/17 17:00:00 INFO Executor: Running task 105.0 in stage 4.0 (TID 290)
21/05/17 17:00:00 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 286) in 2560 ms on bb4563f4559d (executor driver) (102/200)
[2021-05-17 14:00:00,662] {docker.py:276} INFO - 21/05/17 17:00:00 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:00,664] {docker.py:276} INFO - 21/05/17 17:00:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:00,665] {docker.py:276} INFO - 21/05/17 17:00:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:00,665] {docker.py:276} INFO - 21/05/17 17:00:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446078103406776679169_0004_m_000105_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446078103406776679169_0004_m_000105_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446078103406776679169_0004}; taskId=attempt_202105171658446078103406776679169_0004_m_000105_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c1ec477}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658446078103406776679169_0004_m_000105_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446078103406776679169_0004_m_000105_290
[2021-05-17 14:00:00,669] {docker.py:276} INFO - 21/05/17 17:00:00 INFO StagingCommitter: Task committer attempt_202105171658446078103406776679169_0004_m_000105_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446078103406776679169_0004_m_000105_290 : duration 0:00.003s
[2021-05-17 14:00:01,163] {docker.py:276} INFO - 21/05/17 17:00:01 INFO StagingCommitter: Starting: Task committer attempt_202105171658441768383808331577304_0004_m_000102_287: needsTaskCommit() Task attempt_202105171658441768383808331577304_0004_m_000102_287
[2021-05-17 14:00:01,164] {docker.py:276} INFO - 21/05/17 17:00:01 INFO StagingCommitter: Task committer attempt_202105171658441768383808331577304_0004_m_000102_287: needsTaskCommit() Task attempt_202105171658441768383808331577304_0004_m_000102_287: duration 0:00.001s
[2021-05-17 14:00:01,164] {docker.py:276} INFO - 21/05/17 17:00:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441768383808331577304_0004_m_000102_287
[2021-05-17 14:00:01,166] {docker.py:276} INFO - 21/05/17 17:00:01 INFO Executor: Finished task 102.0 in stage 4.0 (TID 287). 4544 bytes result sent to driver
[2021-05-17 14:00:01,167] {docker.py:276} INFO - 21/05/17 17:00:01 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 291) (bb4563f4559d, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:01,168] {docker.py:276} INFO - 21/05/17 17:00:01 INFO Executor: Running task 106.0 in stage 4.0 (TID 291)
[2021-05-17 14:00:01,169] {docker.py:276} INFO - 21/05/17 17:00:01 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 287) in 2555 ms on bb4563f4559d (executor driver) (103/200)
[2021-05-17 14:00:01,177] {docker.py:276} INFO - 21/05/17 17:00:01 INFO ShuffleBlockFetcherIterator: Getting 4 (28.3 KiB) non-empty blocks including 4 (28.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:01,179] {docker.py:276} INFO - 21/05/17 17:00:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:01,180] {docker.py:276} INFO - 21/05/17 17:00:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445738875078678591844_0004_m_000106_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445738875078678591844_0004_m_000106_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445738875078678591844_0004}; taskId=attempt_202105171658445738875078678591844_0004_m_000106_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c2cdc46}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:01 INFO StagingCommitter: Starting: Task committer attempt_202105171658445738875078678591844_0004_m_000106_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445738875078678591844_0004_m_000106_291
[2021-05-17 14:00:01,183] {docker.py:276} INFO - 21/05/17 17:00:01 INFO StagingCommitter: Task committer attempt_202105171658445738875078678591844_0004_m_000106_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445738875078678591844_0004_m_000106_291 : duration 0:00.003s
[2021-05-17 14:00:01,654] {docker.py:276} INFO - 21/05/17 17:00:01 INFO StagingCommitter: Starting: Task committer attempt_20210517165844918564303921589356_0004_m_000103_288: needsTaskCommit() Task attempt_20210517165844918564303921589356_0004_m_000103_288
21/05/17 17:00:01 INFO StagingCommitter: Task committer attempt_20210517165844918564303921589356_0004_m_000103_288: needsTaskCommit() Task attempt_20210517165844918564303921589356_0004_m_000103_288: duration 0:00.001s
21/05/17 17:00:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844918564303921589356_0004_m_000103_288
[2021-05-17 14:00:01,655] {docker.py:276} INFO - 21/05/17 17:00:01 INFO Executor: Finished task 103.0 in stage 4.0 (TID 288). 4544 bytes result sent to driver
[2021-05-17 14:00:01,657] {docker.py:276} INFO - 21/05/17 17:00:01 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 292) (bb4563f4559d, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:01,658] {docker.py:276} INFO - 21/05/17 17:00:01 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 288) in 2642 ms on bb4563f4559d (executor driver) (104/200)
[2021-05-17 14:00:01,658] {docker.py:276} INFO - 21/05/17 17:00:01 INFO Executor: Running task 107.0 in stage 4.0 (TID 292)
[2021-05-17 14:00:01,667] {docker.py:276} INFO - 21/05/17 17:00:01 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:01,672] {docker.py:276} INFO - 21/05/17 17:00:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:01,673] {docker.py:276} INFO - 21/05/17 17:00:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441539984372061145909_0004_m_000107_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441539984372061145909_0004_m_000107_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441539984372061145909_0004}; taskId=attempt_202105171658441539984372061145909_0004_m_000107_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d7b235f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:01 INFO StagingCommitter: Starting: Task committer attempt_202105171658441539984372061145909_0004_m_000107_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441539984372061145909_0004_m_000107_292
[2021-05-17 14:00:01,680] {docker.py:276} INFO - 21/05/17 17:00:01 INFO StagingCommitter: Task committer attempt_202105171658441539984372061145909_0004_m_000107_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441539984372061145909_0004_m_000107_292 : duration 0:00.005s
[2021-05-17 14:00:02,082] {docker.py:276} INFO - 21/05/17 17:00:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658447848820123101812493_0004_m_000104_289: needsTaskCommit() Task attempt_202105171658447848820123101812493_0004_m_000104_289
[2021-05-17 14:00:02,083] {docker.py:276} INFO - 21/05/17 17:00:02 INFO StagingCommitter: Task committer attempt_202105171658447848820123101812493_0004_m_000104_289: needsTaskCommit() Task attempt_202105171658447848820123101812493_0004_m_000104_289: duration 0:00.001s
21/05/17 17:00:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447848820123101812493_0004_m_000104_289
[2021-05-17 14:00:02,083] {docker.py:276} INFO - 21/05/17 17:00:02 INFO Executor: Finished task 104.0 in stage 4.0 (TID 289). 4544 bytes result sent to driver
[2021-05-17 14:00:02,084] {docker.py:276} INFO - 21/05/17 17:00:02 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 293) (bb4563f4559d, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:02,085] {docker.py:276} INFO - 21/05/17 17:00:02 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 289) in 2714 ms on bb4563f4559d (executor driver) (105/200)
21/05/17 17:00:02 INFO Executor: Running task 108.0 in stage 4.0 (TID 293)
[2021-05-17 14:00:02,100] {docker.py:276} INFO - 21/05/17 17:00:02 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:02,103] {docker.py:276} INFO - 21/05/17 17:00:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:02,104] {docker.py:276} INFO - 21/05/17 17:00:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445072034010332989245_0004_m_000108_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445072034010332989245_0004_m_000108_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445072034010332989245_0004}; taskId=attempt_202105171658445072034010332989245_0004_m_000108_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@521640df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:02,104] {docker.py:276} INFO - 21/05/17 17:00:02 INFO StagingCommitter: Starting: Task committer attempt_202105171658445072034010332989245_0004_m_000108_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445072034010332989245_0004_m_000108_293
[2021-05-17 14:00:02,107] {docker.py:276} INFO - 21/05/17 17:00:02 INFO StagingCommitter: Task committer attempt_202105171658445072034010332989245_0004_m_000108_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445072034010332989245_0004_m_000108_293 : duration 0:00.004s
[2021-05-17 14:00:03,330] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Starting: Task committer attempt_202105171658446078103406776679169_0004_m_000105_290: needsTaskCommit() Task attempt_202105171658446078103406776679169_0004_m_000105_290
[2021-05-17 14:00:03,331] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Task committer attempt_202105171658446078103406776679169_0004_m_000105_290: needsTaskCommit() Task attempt_202105171658446078103406776679169_0004_m_000105_290: duration 0:00.001s
21/05/17 17:00:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446078103406776679169_0004_m_000105_290
[2021-05-17 14:00:03,332] {docker.py:276} INFO - 21/05/17 17:00:03 INFO Executor: Finished task 105.0 in stage 4.0 (TID 290). 4544 bytes result sent to driver
[2021-05-17 14:00:03,333] {docker.py:276} INFO - 21/05/17 17:00:03 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 294) (bb4563f4559d, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:03,335] {docker.py:276} INFO - 21/05/17 17:00:03 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 290) in 2687 ms on bb4563f4559d (executor driver) (106/200)
[2021-05-17 14:00:03,335] {docker.py:276} INFO - 21/05/17 17:00:03 INFO Executor: Running task 109.0 in stage 4.0 (TID 294)
[2021-05-17 14:00:03,345] {docker.py:276} INFO - 21/05/17 17:00:03 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:03,345] {docker.py:276} INFO - 21/05/17 17:00:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:03,348] {docker.py:276} INFO - 21/05/17 17:00:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:03,349] {docker.py:276} INFO - 21/05/17 17:00:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:03,349] {docker.py:276} INFO - 21/05/17 17:00:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:03,350] {docker.py:276} INFO - 21/05/17 17:00:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448375881150738181172_0004_m_000109_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448375881150738181172_0004_m_000109_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448375881150738181172_0004}; taskId=attempt_202105171658448375881150738181172_0004_m_000109_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@267a2e97}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:03,350] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Starting: Task committer attempt_202105171658448375881150738181172_0004_m_000109_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448375881150738181172_0004_m_000109_294
[2021-05-17 14:00:03,353] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Task committer attempt_202105171658448375881150738181172_0004_m_000109_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448375881150738181172_0004_m_000109_294 : duration 0:00.004s
[2021-05-17 14:00:03,836] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Starting: Task committer attempt_202105171658445738875078678591844_0004_m_000106_291: needsTaskCommit() Task attempt_202105171658445738875078678591844_0004_m_000106_291
[2021-05-17 14:00:03,836] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Task committer attempt_202105171658445738875078678591844_0004_m_000106_291: needsTaskCommit() Task attempt_202105171658445738875078678591844_0004_m_000106_291: duration 0:00.000s
21/05/17 17:00:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445738875078678591844_0004_m_000106_291
[2021-05-17 14:00:03,837] {docker.py:276} INFO - 21/05/17 17:00:03 INFO Executor: Finished task 106.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
[2021-05-17 14:00:03,838] {docker.py:276} INFO - 21/05/17 17:00:03 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 295) (bb4563f4559d, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:03,839] {docker.py:276} INFO - 21/05/17 17:00:03 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 291) in 2675 ms on bb4563f4559d (executor driver) (107/200)
[2021-05-17 14:00:03,840] {docker.py:276} INFO - 21/05/17 17:00:03 INFO Executor: Running task 110.0 in stage 4.0 (TID 295)
[2021-05-17 14:00:03,850] {docker.py:276} INFO - 21/05/17 17:00:03 INFO ShuffleBlockFetcherIterator: Getting 4 (27.6 KiB) non-empty blocks including 4 (27.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:03,851] {docker.py:276} INFO - 21/05/17 17:00:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 14:00:03,853] {docker.py:276} INFO - 21/05/17 17:00:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441308907418255634429_0004_m_000110_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441308907418255634429_0004_m_000110_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441308907418255634429_0004}; taskId=attempt_202105171658441308907418255634429_0004_m_000110_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22949f5e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:03,854] {docker.py:276} INFO - 21/05/17 17:00:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:03 INFO StagingCommitter: Starting: Task committer attempt_202105171658441308907418255634429_0004_m_000110_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441308907418255634429_0004_m_000110_295
[2021-05-17 14:00:03,857] {docker.py:276} INFO - 21/05/17 17:00:03 INFO StagingCommitter: Task committer attempt_202105171658441308907418255634429_0004_m_000110_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441308907418255634429_0004_m_000110_295 : duration 0:00.004s
[2021-05-17 14:00:04,331] {docker.py:276} INFO - 21/05/17 17:00:04 INFO StagingCommitter: Starting: Task committer attempt_202105171658441539984372061145909_0004_m_000107_292: needsTaskCommit() Task attempt_202105171658441539984372061145909_0004_m_000107_292
21/05/17 17:00:04 INFO StagingCommitter: Task committer attempt_202105171658441539984372061145909_0004_m_000107_292: needsTaskCommit() Task attempt_202105171658441539984372061145909_0004_m_000107_292: duration 0:00.001s
21/05/17 17:00:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441539984372061145909_0004_m_000107_292
[2021-05-17 14:00:04,332] {docker.py:276} INFO - 21/05/17 17:00:04 INFO Executor: Finished task 107.0 in stage 4.0 (TID 292). 4587 bytes result sent to driver
[2021-05-17 14:00:04,333] {docker.py:276} INFO - 21/05/17 17:00:04 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 296) (bb4563f4559d, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:04,335] {docker.py:276} INFO - 21/05/17 17:00:04 INFO Executor: Running task 111.0 in stage 4.0 (TID 296)
21/05/17 17:00:04 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 292) in 2682 ms on bb4563f4559d (executor driver) (108/200)
[2021-05-17 14:00:04,343] {docker.py:276} INFO - 21/05/17 17:00:04 INFO ShuffleBlockFetcherIterator: Getting 4 (27.2 KiB) non-empty blocks including 4 (27.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:04,344] {docker.py:276} INFO - 21/05/17 17:00:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:04,345] {docker.py:276} INFO - 21/05/17 17:00:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447154194851104260416_0004_m_000111_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447154194851104260416_0004_m_000111_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447154194851104260416_0004}; taskId=attempt_202105171658447154194851104260416_0004_m_000111_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44b82c13}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:04,345] {docker.py:276} INFO - 21/05/17 17:00:04 INFO StagingCommitter: Starting: Task committer attempt_202105171658447154194851104260416_0004_m_000111_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447154194851104260416_0004_m_000111_296
[2021-05-17 14:00:04,350] {docker.py:276} INFO - 21/05/17 17:00:04 INFO StagingCommitter: Task committer attempt_202105171658447154194851104260416_0004_m_000111_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447154194851104260416_0004_m_000111_296 : duration 0:00.004s
[2021-05-17 14:00:04,674] {docker.py:276} INFO - 21/05/17 17:00:04 INFO StagingCommitter: Starting: Task committer attempt_202105171658445072034010332989245_0004_m_000108_293: needsTaskCommit() Task attempt_202105171658445072034010332989245_0004_m_000108_293
[2021-05-17 14:00:04,675] {docker.py:276} INFO - 21/05/17 17:00:04 INFO StagingCommitter: Task committer attempt_202105171658445072034010332989245_0004_m_000108_293: needsTaskCommit() Task attempt_202105171658445072034010332989245_0004_m_000108_293: duration 0:00.001s
21/05/17 17:00:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445072034010332989245_0004_m_000108_293
[2021-05-17 14:00:04,676] {docker.py:276} INFO - 21/05/17 17:00:04 INFO Executor: Finished task 108.0 in stage 4.0 (TID 293). 4587 bytes result sent to driver
[2021-05-17 14:00:04,677] {docker.py:276} INFO - 21/05/17 17:00:04 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 297) (bb4563f4559d, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:04,678] {docker.py:276} INFO - 21/05/17 17:00:04 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 293) in 2597 ms on bb4563f4559d (executor driver) (109/200)
[2021-05-17 14:00:04,680] {docker.py:276} INFO - 21/05/17 17:00:04 INFO Executor: Running task 112.0 in stage 4.0 (TID 297)
[2021-05-17 14:00:04,689] {docker.py:276} INFO - 21/05/17 17:00:04 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:04,691] {docker.py:276} INFO - 21/05/17 17:00:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444323796159182450192_0004_m_000112_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444323796159182450192_0004_m_000112_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444323796159182450192_0004}; taskId=attempt_202105171658444323796159182450192_0004_m_000112_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d645ffc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:04 INFO StagingCommitter: Starting: Task committer attempt_202105171658444323796159182450192_0004_m_000112_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444323796159182450192_0004_m_000112_297
[2021-05-17 14:00:04,696] {docker.py:276} INFO - 21/05/17 17:00:04 INFO StagingCommitter: Task committer attempt_202105171658444323796159182450192_0004_m_000112_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444323796159182450192_0004_m_000112_297 : duration 0:00.004s
[2021-05-17 14:00:05,894] {docker.py:276} INFO - 21/05/17 17:00:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658448375881150738181172_0004_m_000109_294: needsTaskCommit() Task attempt_202105171658448375881150738181172_0004_m_000109_294
[2021-05-17 14:00:05,895] {docker.py:276} INFO - 21/05/17 17:00:05 INFO StagingCommitter: Task committer attempt_202105171658448375881150738181172_0004_m_000109_294: needsTaskCommit() Task attempt_202105171658448375881150738181172_0004_m_000109_294: duration 0:00.001s
[2021-05-17 14:00:05,896] {docker.py:276} INFO - 21/05/17 17:00:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448375881150738181172_0004_m_000109_294
[2021-05-17 14:00:05,897] {docker.py:276} INFO - 21/05/17 17:00:05 INFO Executor: Finished task 109.0 in stage 4.0 (TID 294). 4587 bytes result sent to driver
[2021-05-17 14:00:05,898] {docker.py:276} INFO - 21/05/17 17:00:05 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 298) (bb4563f4559d, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:05,899] {docker.py:276} INFO - 21/05/17 17:00:05 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 294) in 2568 ms on bb4563f4559d (executor driver) (110/200)
[2021-05-17 14:00:05,899] {docker.py:276} INFO - 21/05/17 17:00:05 INFO Executor: Running task 113.0 in stage 4.0 (TID 298)
[2021-05-17 14:00:05,909] {docker.py:276} INFO - 21/05/17 17:00:05 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:05,910] {docker.py:276} INFO - 21/05/17 17:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:05,911] {docker.py:276} INFO - 21/05/17 17:00:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:05,911] {docker.py:276} INFO - 21/05/17 17:00:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445770860136454683407_0004_m_000113_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445770860136454683407_0004_m_000113_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445770860136454683407_0004}; taskId=attempt_202105171658445770860136454683407_0004_m_000113_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56893246}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:05,912] {docker.py:276} INFO - 21/05/17 17:00:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:05,912] {docker.py:276} INFO - 21/05/17 17:00:05 INFO StagingCommitter: Starting: Task committer attempt_202105171658445770860136454683407_0004_m_000113_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445770860136454683407_0004_m_000113_298
[2021-05-17 14:00:05,915] {docker.py:276} INFO - 21/05/17 17:00:05 INFO StagingCommitter: Task committer attempt_202105171658445770860136454683407_0004_m_000113_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445770860136454683407_0004_m_000113_298 : duration 0:00.003s
[2021-05-17 14:00:06,462] {docker.py:276} INFO - 21/05/17 17:00:06 INFO StagingCommitter: Starting: Task committer attempt_202105171658441308907418255634429_0004_m_000110_295: needsTaskCommit() Task attempt_202105171658441308907418255634429_0004_m_000110_295
[2021-05-17 14:00:06,463] {docker.py:276} INFO - 21/05/17 17:00:06 INFO StagingCommitter: Task committer attempt_202105171658441308907418255634429_0004_m_000110_295: needsTaskCommit() Task attempt_202105171658441308907418255634429_0004_m_000110_295: duration 0:00.000s
21/05/17 17:00:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441308907418255634429_0004_m_000110_295
[2021-05-17 14:00:06,464] {docker.py:276} INFO - 21/05/17 17:00:06 INFO Executor: Finished task 110.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-17 14:00:06,465] {docker.py:276} INFO - 21/05/17 17:00:06 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 299) (bb4563f4559d, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:06,466] {docker.py:276} INFO - 21/05/17 17:00:06 INFO Executor: Running task 114.0 in stage 4.0 (TID 299)
[2021-05-17 14:00:06,467] {docker.py:276} INFO - 21/05/17 17:00:06 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 295) in 2632 ms on bb4563f4559d (executor driver) (111/200)
[2021-05-17 14:00:06,475] {docker.py:276} INFO - 21/05/17 17:00:06 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:06,477] {docker.py:276} INFO - 21/05/17 17:00:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:06,477] {docker.py:276} INFO - 21/05/17 17:00:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444275259878639519536_0004_m_000114_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444275259878639519536_0004_m_000114_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444275259878639519536_0004}; taskId=attempt_202105171658444275259878639519536_0004_m_000114_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6769a0d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:06 INFO StagingCommitter: Starting: Task committer attempt_202105171658444275259878639519536_0004_m_000114_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444275259878639519536_0004_m_000114_299
[2021-05-17 14:00:06,480] {docker.py:276} INFO - 21/05/17 17:00:06 INFO StagingCommitter: Task committer attempt_202105171658444275259878639519536_0004_m_000114_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444275259878639519536_0004_m_000114_299 : duration 0:00.002s
[2021-05-17 14:00:06,820] {docker.py:276} INFO - 21/05/17 17:00:06 INFO StagingCommitter: Starting: Task committer attempt_202105171658447154194851104260416_0004_m_000111_296: needsTaskCommit() Task attempt_202105171658447154194851104260416_0004_m_000111_296
[2021-05-17 14:00:06,821] {docker.py:276} INFO - 21/05/17 17:00:06 INFO StagingCommitter: Task committer attempt_202105171658447154194851104260416_0004_m_000111_296: needsTaskCommit() Task attempt_202105171658447154194851104260416_0004_m_000111_296: duration 0:00.001s
[2021-05-17 14:00:06,822] {docker.py:276} INFO - 21/05/17 17:00:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447154194851104260416_0004_m_000111_296
[2021-05-17 14:00:06,823] {docker.py:276} INFO - 21/05/17 17:00:06 INFO Executor: Finished task 111.0 in stage 4.0 (TID 296). 4544 bytes result sent to driver
[2021-05-17 14:00:06,825] {docker.py:276} INFO - 21/05/17 17:00:06 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 300) (bb4563f4559d, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:06,827] {docker.py:276} INFO - 21/05/17 17:00:06 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 296) in 2496 ms on bb4563f4559d (executor driver) (112/200)
[2021-05-17 14:00:06,828] {docker.py:276} INFO - 21/05/17 17:00:06 INFO Executor: Running task 115.0 in stage 4.0 (TID 300)
[2021-05-17 14:00:06,838] {docker.py:276} INFO - 21/05/17 17:00:06 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:06,840] {docker.py:276} INFO - 21/05/17 17:00:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:06,841] {docker.py:276} INFO - 21/05/17 17:00:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844502575516980790270_0004_m_000115_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844502575516980790270_0004_m_000115_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844502575516980790270_0004}; taskId=attempt_20210517165844502575516980790270_0004_m_000115_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@697c799d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:06 INFO StagingCommitter: Starting: Task committer attempt_20210517165844502575516980790270_0004_m_000115_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844502575516980790270_0004_m_000115_300
[2021-05-17 14:00:06,845] {docker.py:276} INFO - 21/05/17 17:00:06 INFO StagingCommitter: Task committer attempt_20210517165844502575516980790270_0004_m_000115_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844502575516980790270_0004_m_000115_300 : duration 0:00.005s
[2021-05-17 14:00:07,220] {docker.py:276} INFO - 21/05/17 17:00:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658444323796159182450192_0004_m_000112_297: needsTaskCommit() Task attempt_202105171658444323796159182450192_0004_m_000112_297
[2021-05-17 14:00:07,221] {docker.py:276} INFO - 21/05/17 17:00:07 INFO StagingCommitter: Task committer attempt_202105171658444323796159182450192_0004_m_000112_297: needsTaskCommit() Task attempt_202105171658444323796159182450192_0004_m_000112_297: duration 0:00.002s
21/05/17 17:00:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444323796159182450192_0004_m_000112_297
[2021-05-17 14:00:07,223] {docker.py:276} INFO - 21/05/17 17:00:07 INFO Executor: Finished task 112.0 in stage 4.0 (TID 297). 4544 bytes result sent to driver
[2021-05-17 14:00:07,225] {docker.py:276} INFO - 21/05/17 17:00:07 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 301) (bb4563f4559d, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:07,226] {docker.py:276} INFO - 21/05/17 17:00:07 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 297) in 2551 ms on bb4563f4559d (executor driver) (113/200)
[2021-05-17 14:00:07,227] {docker.py:276} INFO - 21/05/17 17:00:07 INFO Executor: Running task 116.0 in stage 4.0 (TID 301)
[2021-05-17 14:00:07,237] {docker.py:276} INFO - 21/05/17 17:00:07 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:07,239] {docker.py:276} INFO - 21/05/17 17:00:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446136059861688552678_0004_m_000116_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446136059861688552678_0004_m_000116_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446136059861688552678_0004}; taskId=attempt_202105171658446136059861688552678_0004_m_000116_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6dd8f809}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:07,239] {docker.py:276} INFO - 21/05/17 17:00:07 INFO StagingCommitter: Starting: Task committer attempt_202105171658446136059861688552678_0004_m_000116_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446136059861688552678_0004_m_000116_301
[2021-05-17 14:00:07,243] {docker.py:276} INFO - 21/05/17 17:00:07 INFO StagingCommitter: Task committer attempt_202105171658446136059861688552678_0004_m_000116_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446136059861688552678_0004_m_000116_301 : duration 0:00.004s
[2021-05-17 14:00:08,411] {docker.py:276} INFO - 21/05/17 17:00:08 INFO StagingCommitter: Starting: Task committer attempt_202105171658445770860136454683407_0004_m_000113_298: needsTaskCommit() Task attempt_202105171658445770860136454683407_0004_m_000113_298
[2021-05-17 14:00:08,412] {docker.py:276} INFO - 21/05/17 17:00:08 INFO StagingCommitter: Task committer attempt_202105171658445770860136454683407_0004_m_000113_298: needsTaskCommit() Task attempt_202105171658445770860136454683407_0004_m_000113_298: duration 0:00.000s
21/05/17 17:00:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445770860136454683407_0004_m_000113_298
[2021-05-17 14:00:08,413] {docker.py:276} INFO - 21/05/17 17:00:08 INFO Executor: Finished task 113.0 in stage 4.0 (TID 298). 4544 bytes result sent to driver
[2021-05-17 14:00:08,414] {docker.py:276} INFO - 21/05/17 17:00:08 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 302) (bb4563f4559d, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:08,416] {docker.py:276} INFO - 21/05/17 17:00:08 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 298) in 2521 ms on bb4563f4559d (executor driver) (114/200)
[2021-05-17 14:00:08,417] {docker.py:276} INFO - 21/05/17 17:00:08 INFO Executor: Running task 117.0 in stage 4.0 (TID 302)
[2021-05-17 14:00:08,425] {docker.py:276} INFO - 21/05/17 17:00:08 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:08,427] {docker.py:276} INFO - 21/05/17 17:00:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:08,427] {docker.py:276} INFO - 21/05/17 17:00:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441811296625397281923_0004_m_000117_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441811296625397281923_0004_m_000117_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441811296625397281923_0004}; taskId=attempt_202105171658441811296625397281923_0004_m_000117_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c349d4a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:08,428] {docker.py:276} INFO - 21/05/17 17:00:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:08 INFO StagingCommitter: Starting: Task committer attempt_202105171658441811296625397281923_0004_m_000117_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441811296625397281923_0004_m_000117_302
[2021-05-17 14:00:08,430] {docker.py:276} INFO - 21/05/17 17:00:08 INFO StagingCommitter: Task committer attempt_202105171658441811296625397281923_0004_m_000117_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441811296625397281923_0004_m_000117_302 : duration 0:00.003s
[2021-05-17 14:00:08,911] {docker.py:276} INFO - 21/05/17 17:00:08 INFO StagingCommitter: Starting: Task committer attempt_202105171658444275259878639519536_0004_m_000114_299: needsTaskCommit() Task attempt_202105171658444275259878639519536_0004_m_000114_299
[2021-05-17 14:00:08,912] {docker.py:276} INFO - 21/05/17 17:00:08 INFO StagingCommitter: Task committer attempt_202105171658444275259878639519536_0004_m_000114_299: needsTaskCommit() Task attempt_202105171658444275259878639519536_0004_m_000114_299: duration 0:00.001s
21/05/17 17:00:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444275259878639519536_0004_m_000114_299
[2021-05-17 14:00:08,913] {docker.py:276} INFO - 21/05/17 17:00:08 INFO Executor: Finished task 114.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-17 14:00:08,914] {docker.py:276} INFO - 21/05/17 17:00:08 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 303) (bb4563f4559d, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:08,915] {docker.py:276} INFO - 21/05/17 17:00:08 INFO Executor: Running task 118.0 in stage 4.0 (TID 303)
[2021-05-17 14:00:08,916] {docker.py:276} INFO - 21/05/17 17:00:08 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 299) in 2453 ms on bb4563f4559d (executor driver) (115/200)
[2021-05-17 14:00:08,925] {docker.py:276} INFO - 21/05/17 17:00:08 INFO ShuffleBlockFetcherIterator: Getting 4 (27.6 KiB) non-empty blocks including 4 (27.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:08,925] {docker.py:276} INFO - 21/05/17 17:00:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:08,927] {docker.py:276} INFO - 21/05/17 17:00:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:08,927] {docker.py:276} INFO - 21/05/17 17:00:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446130041014578847407_0004_m_000118_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446130041014578847407_0004_m_000118_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446130041014578847407_0004}; taskId=attempt_202105171658446130041014578847407_0004_m_000118_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61cbfa27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:08 INFO StagingCommitter: Starting: Task committer attempt_202105171658446130041014578847407_0004_m_000118_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446130041014578847407_0004_m_000118_303
[2021-05-17 14:00:08,930] {docker.py:276} INFO - 21/05/17 17:00:08 INFO StagingCommitter: Task committer attempt_202105171658446130041014578847407_0004_m_000118_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446130041014578847407_0004_m_000118_303 : duration 0:00.003s
[2021-05-17 14:00:09,168] {docker.py:276} INFO - 21/05/17 17:00:09 INFO StagingCommitter: Starting: Task committer attempt_202105171658446136059861688552678_0004_m_000116_301: needsTaskCommit() Task attempt_202105171658446136059861688552678_0004_m_000116_301
[2021-05-17 14:00:09,169] {docker.py:276} INFO - 21/05/17 17:00:09 INFO StagingCommitter: Task committer attempt_202105171658446136059861688552678_0004_m_000116_301: needsTaskCommit() Task attempt_202105171658446136059861688552678_0004_m_000116_301: duration 0:00.001s
[2021-05-17 14:00:09,170] {docker.py:276} INFO - 21/05/17 17:00:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446136059861688552678_0004_m_000116_301
[2021-05-17 14:00:09,171] {docker.py:276} INFO - 21/05/17 17:00:09 INFO Executor: Finished task 116.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-17 14:00:09,173] {docker.py:276} INFO - 21/05/17 17:00:09 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 304) (bb4563f4559d, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:09,174] {docker.py:276} INFO - 21/05/17 17:00:09 INFO Executor: Running task 119.0 in stage 4.0 (TID 304)
21/05/17 17:00:09 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 301) in 1952 ms on bb4563f4559d (executor driver) (116/200)
[2021-05-17 14:00:09,191] {docker.py:276} INFO - 21/05/17 17:00:09 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:09,194] {docker.py:276} INFO - 21/05/17 17:00:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:09,194] {docker.py:276} INFO - 21/05/17 17:00:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:09,194] {docker.py:276} INFO - 21/05/17 17:00:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443595260742486895023_0004_m_000119_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443595260742486895023_0004_m_000119_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443595260742486895023_0004}; taskId=attempt_202105171658443595260742486895023_0004_m_000119_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60c007b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:09,195] {docker.py:276} INFO - 21/05/17 17:00:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:09 INFO StagingCommitter: Starting: Task committer attempt_202105171658443595260742486895023_0004_m_000119_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443595260742486895023_0004_m_000119_304
[2021-05-17 14:00:09,197] {docker.py:276} INFO - 21/05/17 17:00:09 INFO StagingCommitter: Task committer attempt_202105171658443595260742486895023_0004_m_000119_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443595260742486895023_0004_m_000119_304 : duration 0:00.003s
[2021-05-17 14:00:09,410] {docker.py:276} INFO - 21/05/17 17:00:09 INFO StagingCommitter: Starting: Task committer attempt_20210517165844502575516980790270_0004_m_000115_300: needsTaskCommit() Task attempt_20210517165844502575516980790270_0004_m_000115_300
[2021-05-17 14:00:09,412] {docker.py:276} INFO - 21/05/17 17:00:09 INFO StagingCommitter: Task committer attempt_20210517165844502575516980790270_0004_m_000115_300: needsTaskCommit() Task attempt_20210517165844502575516980790270_0004_m_000115_300: duration 0:00.001s
21/05/17 17:00:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844502575516980790270_0004_m_000115_300
[2021-05-17 14:00:09,414] {docker.py:276} INFO - 21/05/17 17:00:09 INFO Executor: Finished task 115.0 in stage 4.0 (TID 300). 4587 bytes result sent to driver
[2021-05-17 14:00:09,415] {docker.py:276} INFO - 21/05/17 17:00:09 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 305) (bb4563f4559d, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:09,416] {docker.py:276} INFO - 21/05/17 17:00:09 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 300) in 2594 ms on bb4563f4559d (executor driver) (117/200)
[2021-05-17 14:00:09,416] {docker.py:276} INFO - 21/05/17 17:00:09 INFO Executor: Running task 120.0 in stage 4.0 (TID 305)
[2021-05-17 14:00:09,425] {docker.py:276} INFO - 21/05/17 17:00:09 INFO ShuffleBlockFetcherIterator: Getting 4 (26.5 KiB) non-empty blocks including 4 (26.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:09,427] {docker.py:276} INFO - 21/05/17 17:00:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442021907365506548773_0004_m_000120_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442021907365506548773_0004_m_000120_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442021907365506548773_0004}; taskId=attempt_202105171658442021907365506548773_0004_m_000120_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@611eec01}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:09 INFO StagingCommitter: Starting: Task committer attempt_202105171658442021907365506548773_0004_m_000120_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442021907365506548773_0004_m_000120_305
[2021-05-17 14:00:09,430] {docker.py:276} INFO - 21/05/17 17:00:09 INFO StagingCommitter: Task committer attempt_202105171658442021907365506548773_0004_m_000120_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442021907365506548773_0004_m_000120_305 : duration 0:00.003s
[2021-05-17 14:00:10,855] {docker.py:276} INFO - 21/05/17 17:00:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658441811296625397281923_0004_m_000117_302: needsTaskCommit() Task attempt_202105171658441811296625397281923_0004_m_000117_302
[2021-05-17 14:00:10,855] {docker.py:276} INFO - 21/05/17 17:00:10 INFO StagingCommitter: Task committer attempt_202105171658441811296625397281923_0004_m_000117_302: needsTaskCommit() Task attempt_202105171658441811296625397281923_0004_m_000117_302: duration 0:00.001s
[2021-05-17 14:00:10,856] {docker.py:276} INFO - 21/05/17 17:00:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441811296625397281923_0004_m_000117_302
[2021-05-17 14:00:10,857] {docker.py:276} INFO - 21/05/17 17:00:10 INFO Executor: Finished task 117.0 in stage 4.0 (TID 302). 4587 bytes result sent to driver
[2021-05-17 14:00:10,858] {docker.py:276} INFO - 21/05/17 17:00:10 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 306) (bb4563f4559d, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:10,859] {docker.py:276} INFO - 21/05/17 17:00:10 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 302) in 2447 ms on bb4563f4559d (executor driver) (118/200)
[2021-05-17 14:00:10,860] {docker.py:276} INFO - 21/05/17 17:00:10 INFO Executor: Running task 121.0 in stage 4.0 (TID 306)
[2021-05-17 14:00:10,867] {docker.py:276} INFO - 21/05/17 17:00:10 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:10,868] {docker.py:276} INFO - 21/05/17 17:00:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:10,869] {docker.py:276} INFO - 21/05/17 17:00:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441966521065227804476_0004_m_000121_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441966521065227804476_0004_m_000121_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441966521065227804476_0004}; taskId=attempt_202105171658441966521065227804476_0004_m_000121_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40af05ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:10,869] {docker.py:276} INFO - 21/05/17 17:00:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:10,869] {docker.py:276} INFO - 21/05/17 17:00:10 INFO StagingCommitter: Starting: Task committer attempt_202105171658441966521065227804476_0004_m_000121_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441966521065227804476_0004_m_000121_306
[2021-05-17 14:00:10,871] {docker.py:276} INFO - 21/05/17 17:00:10 INFO StagingCommitter: Task committer attempt_202105171658441966521065227804476_0004_m_000121_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441966521065227804476_0004_m_000121_306 : duration 0:00.003s
[2021-05-17 14:00:11,164] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Starting: Task committer attempt_202105171658443595260742486895023_0004_m_000119_304: needsTaskCommit() Task attempt_202105171658443595260742486895023_0004_m_000119_304
21/05/17 17:00:11 INFO StagingCommitter: Task committer attempt_202105171658443595260742486895023_0004_m_000119_304: needsTaskCommit() Task attempt_202105171658443595260742486895023_0004_m_000119_304: duration 0:00.001s
21/05/17 17:00:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443595260742486895023_0004_m_000119_304
[2021-05-17 14:00:11,165] {docker.py:276} INFO - 21/05/17 17:00:11 INFO Executor: Finished task 119.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-17 14:00:11,167] {docker.py:276} INFO - 21/05/17 17:00:11 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 307) (bb4563f4559d, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 17:00:11 INFO Executor: Running task 122.0 in stage 4.0 (TID 307)
21/05/17 17:00:11 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 304) in 1997 ms on bb4563f4559d (executor driver) (119/200)
[2021-05-17 14:00:11,175] {docker.py:276} INFO - 21/05/17 17:00:11 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:11,177] {docker.py:276} INFO - 21/05/17 17:00:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443234615583549133834_0004_m_000122_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443234615583549133834_0004_m_000122_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443234615583549133834_0004}; taskId=attempt_202105171658443234615583549133834_0004_m_000122_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4258948a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:11,177] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Starting: Task committer attempt_202105171658443234615583549133834_0004_m_000122_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443234615583549133834_0004_m_000122_307
[2021-05-17 14:00:11,180] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Task committer attempt_202105171658443234615583549133834_0004_m_000122_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443234615583549133834_0004_m_000122_307 : duration 0:00.003s
[2021-05-17 14:00:11,210] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Starting: Task committer attempt_202105171658446130041014578847407_0004_m_000118_303: needsTaskCommit() Task attempt_202105171658446130041014578847407_0004_m_000118_303
[2021-05-17 14:00:11,210] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Task committer attempt_202105171658446130041014578847407_0004_m_000118_303: needsTaskCommit() Task attempt_202105171658446130041014578847407_0004_m_000118_303: duration 0:00.000s
[2021-05-17 14:00:11,211] {docker.py:276} INFO - 21/05/17 17:00:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446130041014578847407_0004_m_000118_303
[2021-05-17 14:00:11,212] {docker.py:276} INFO - 21/05/17 17:00:11 INFO Executor: Finished task 118.0 in stage 4.0 (TID 303). 4587 bytes result sent to driver
[2021-05-17 14:00:11,213] {docker.py:276} INFO - 21/05/17 17:00:11 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 308) (bb4563f4559d, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:11,214] {docker.py:276} INFO - 21/05/17 17:00:11 INFO Executor: Running task 123.0 in stage 4.0 (TID 308)
[2021-05-17 14:00:11,214] {docker.py:276} INFO - 21/05/17 17:00:11 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 303) in 2304 ms on bb4563f4559d (executor driver) (120/200)
[2021-05-17 14:00:11,221] {docker.py:276} INFO - 21/05/17 17:00:11 INFO ShuffleBlockFetcherIterator: Getting 4 (27.9 KiB) non-empty blocks including 4 (27.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:11,223] {docker.py:276} INFO - 21/05/17 17:00:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445495980005558114325_0004_m_000123_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445495980005558114325_0004_m_000123_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445495980005558114325_0004}; taskId=attempt_202105171658445495980005558114325_0004_m_000123_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2005ca5c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:11 INFO StagingCommitter: Starting: Task committer attempt_202105171658445495980005558114325_0004_m_000123_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445495980005558114325_0004_m_000123_308
[2021-05-17 14:00:11,226] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Task committer attempt_202105171658445495980005558114325_0004_m_000123_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445495980005558114325_0004_m_000123_308 : duration 0:00.003s
[2021-05-17 14:00:11,999] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Starting: Task committer attempt_202105171658442021907365506548773_0004_m_000120_305: needsTaskCommit() Task attempt_202105171658442021907365506548773_0004_m_000120_305
[2021-05-17 14:00:12,000] {docker.py:276} INFO - 21/05/17 17:00:11 INFO StagingCommitter: Task committer attempt_202105171658442021907365506548773_0004_m_000120_305: needsTaskCommit() Task attempt_202105171658442021907365506548773_0004_m_000120_305: duration 0:00.001s
21/05/17 17:00:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442021907365506548773_0004_m_000120_305
[2021-05-17 14:00:12,002] {docker.py:276} INFO - 21/05/17 17:00:12 INFO Executor: Finished task 120.0 in stage 4.0 (TID 305). 4544 bytes result sent to driver
[2021-05-17 14:00:12,004] {docker.py:276} INFO - 21/05/17 17:00:12 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 309) (bb4563f4559d, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:12,005] {docker.py:276} INFO - 21/05/17 17:00:12 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 305) in 2557 ms on bb4563f4559d (executor driver) (121/200)
[2021-05-17 14:00:12,005] {docker.py:276} INFO - 21/05/17 17:00:12 INFO Executor: Running task 124.0 in stage 4.0 (TID 309)
[2021-05-17 14:00:12,015] {docker.py:276} INFO - 21/05/17 17:00:12 INFO ShuffleBlockFetcherIterator: Getting 4 (25.2 KiB) non-empty blocks including 4 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:12,017] {docker.py:276} INFO - 21/05/17 17:00:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442247315620991275082_0004_m_000124_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442247315620991275082_0004_m_000124_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442247315620991275082_0004}; taskId=attempt_202105171658442247315620991275082_0004_m_000124_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d5380ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:12,018] {docker.py:276} INFO - 21/05/17 17:00:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:12,018] {docker.py:276} INFO - 21/05/17 17:00:12 INFO StagingCommitter: Starting: Task committer attempt_202105171658442247315620991275082_0004_m_000124_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442247315620991275082_0004_m_000124_309
[2021-05-17 14:00:12,021] {docker.py:276} INFO - 21/05/17 17:00:12 INFO StagingCommitter: Task committer attempt_202105171658442247315620991275082_0004_m_000124_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442247315620991275082_0004_m_000124_309 : duration 0:00.003s
[2021-05-17 14:00:13,347] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Starting: Task committer attempt_202105171658441966521065227804476_0004_m_000121_306: needsTaskCommit() Task attempt_202105171658441966521065227804476_0004_m_000121_306
[2021-05-17 14:00:13,348] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Task committer attempt_202105171658441966521065227804476_0004_m_000121_306: needsTaskCommit() Task attempt_202105171658441966521065227804476_0004_m_000121_306: duration 0:00.002s
21/05/17 17:00:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441966521065227804476_0004_m_000121_306
[2021-05-17 14:00:13,350] {docker.py:276} INFO - 21/05/17 17:00:13 INFO Executor: Finished task 121.0 in stage 4.0 (TID 306). 4544 bytes result sent to driver
[2021-05-17 14:00:13,352] {docker.py:276} INFO - 21/05/17 17:00:13 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 310) (bb4563f4559d, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:13,353] {docker.py:276} INFO - 21/05/17 17:00:13 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 306) in 2462 ms on bb4563f4559d (executor driver) (122/200)
[2021-05-17 14:00:13,354] {docker.py:276} INFO - 21/05/17 17:00:13 INFO Executor: Running task 125.0 in stage 4.0 (TID 310)
[2021-05-17 14:00:13,364] {docker.py:276} INFO - 21/05/17 17:00:13 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:13,366] {docker.py:276} INFO - 21/05/17 17:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446507343346247806212_0004_m_000125_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446507343346247806212_0004_m_000125_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446507343346247806212_0004}; taskId=attempt_202105171658446507343346247806212_0004_m_000125_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7359fc4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:13 INFO StagingCommitter: Starting: Task committer attempt_202105171658446507343346247806212_0004_m_000125_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446507343346247806212_0004_m_000125_310
[2021-05-17 14:00:13,369] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Task committer attempt_202105171658446507343346247806212_0004_m_000125_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446507343346247806212_0004_m_000125_310 : duration 0:00.003s
[2021-05-17 14:00:13,557] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Starting: Task committer attempt_202105171658443234615583549133834_0004_m_000122_307: needsTaskCommit() Task attempt_202105171658443234615583549133834_0004_m_000122_307
[2021-05-17 14:00:13,558] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Task committer attempt_202105171658443234615583549133834_0004_m_000122_307: needsTaskCommit() Task attempt_202105171658443234615583549133834_0004_m_000122_307: duration 0:00.001s
21/05/17 17:00:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443234615583549133834_0004_m_000122_307
[2021-05-17 14:00:13,560] {docker.py:276} INFO - 21/05/17 17:00:13 INFO Executor: Finished task 122.0 in stage 4.0 (TID 307). 4544 bytes result sent to driver
[2021-05-17 14:00:13,561] {docker.py:276} INFO - 21/05/17 17:00:13 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 311) (bb4563f4559d, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:13,562] {docker.py:276} INFO - 21/05/17 17:00:13 INFO Executor: Running task 126.0 in stage 4.0 (TID 311)
[2021-05-17 14:00:13,563] {docker.py:276} INFO - 21/05/17 17:00:13 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 307) in 2363 ms on bb4563f4559d (executor driver) (123/200)
[2021-05-17 14:00:13,573] {docker.py:276} INFO - 21/05/17 17:00:13 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:13,573] {docker.py:276} INFO - 21/05/17 17:00:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:13,575] {docker.py:276} INFO - 21/05/17 17:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:13,576] {docker.py:276} INFO - 21/05/17 17:00:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:13,576] {docker.py:276} INFO - 21/05/17 17:00:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444580063176465873796_0004_m_000126_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444580063176465873796_0004_m_000126_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444580063176465873796_0004}; taskId=attempt_202105171658444580063176465873796_0004_m_000126_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@510eda3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:13,577] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Starting: Task committer attempt_202105171658444580063176465873796_0004_m_000126_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444580063176465873796_0004_m_000126_311
[2021-05-17 14:00:13,579] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Task committer attempt_202105171658444580063176465873796_0004_m_000126_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444580063176465873796_0004_m_000126_311 : duration 0:00.002s
[2021-05-17 14:00:13,879] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Starting: Task committer attempt_202105171658445495980005558114325_0004_m_000123_308: needsTaskCommit() Task attempt_202105171658445495980005558114325_0004_m_000123_308
[2021-05-17 14:00:13,880] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Task committer attempt_202105171658445495980005558114325_0004_m_000123_308: needsTaskCommit() Task attempt_202105171658445495980005558114325_0004_m_000123_308: duration 0:00.001s
21/05/17 17:00:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445495980005558114325_0004_m_000123_308
[2021-05-17 14:00:13,881] {docker.py:276} INFO - 21/05/17 17:00:13 INFO Executor: Finished task 123.0 in stage 4.0 (TID 308). 4544 bytes result sent to driver
[2021-05-17 14:00:13,883] {docker.py:276} INFO - 21/05/17 17:00:13 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 312) (bb4563f4559d, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:13,884] {docker.py:276} INFO - 21/05/17 17:00:13 INFO Executor: Running task 127.0 in stage 4.0 (TID 312)
21/05/17 17:00:13 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 308) in 2639 ms on bb4563f4559d (executor driver) (124/200)
[2021-05-17 14:00:13,894] {docker.py:276} INFO - 21/05/17 17:00:13 INFO ShuffleBlockFetcherIterator: Getting 4 (28.6 KiB) non-empty blocks including 4 (28.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:13,896] {docker.py:276} INFO - 21/05/17 17:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446477929411715811951_0004_m_000127_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446477929411715811951_0004_m_000127_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446477929411715811951_0004}; taskId=attempt_202105171658446477929411715811951_0004_m_000127_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7339a2b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:13 INFO StagingCommitter: Starting: Task committer attempt_202105171658446477929411715811951_0004_m_000127_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446477929411715811951_0004_m_000127_312
[2021-05-17 14:00:13,899] {docker.py:276} INFO - 21/05/17 17:00:13 INFO StagingCommitter: Task committer attempt_202105171658446477929411715811951_0004_m_000127_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446477929411715811951_0004_m_000127_312 : duration 0:00.003s
[2021-05-17 14:00:14,509] {docker.py:276} INFO - 21/05/17 17:00:14 INFO StagingCommitter: Starting: Task committer attempt_202105171658442247315620991275082_0004_m_000124_309: needsTaskCommit() Task attempt_202105171658442247315620991275082_0004_m_000124_309
[2021-05-17 14:00:14,517] {docker.py:276} INFO - 21/05/17 17:00:14 INFO StagingCommitter: Task committer attempt_202105171658442247315620991275082_0004_m_000124_309: needsTaskCommit() Task attempt_202105171658442247315620991275082_0004_m_000124_309: duration 0:00.007s
21/05/17 17:00:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442247315620991275082_0004_m_000124_309
21/05/17 17:00:14 INFO Executor: Finished task 124.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
[2021-05-17 14:00:14,518] {docker.py:276} INFO - 21/05/17 17:00:14 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 313) (bb4563f4559d, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:14,518] {docker.py:276} INFO - 21/05/17 17:00:14 INFO Executor: Running task 128.0 in stage 4.0 (TID 313)
[2021-05-17 14:00:14,519] {docker.py:276} INFO - 21/05/17 17:00:14 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 309) in 2519 ms on bb4563f4559d (executor driver) (125/200)
[2021-05-17 14:00:14,539] {docker.py:276} INFO - 21/05/17 17:00:14 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:14,539] {docker.py:276} INFO - 21/05/17 17:00:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:14,541] {docker.py:276} INFO - 21/05/17 17:00:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:14,542] {docker.py:276} INFO - 21/05/17 17:00:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:14,542] {docker.py:276} INFO - 21/05/17 17:00:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:14,543] {docker.py:276} INFO - 21/05/17 17:00:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444331183894497268614_0004_m_000128_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444331183894497268614_0004_m_000128_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444331183894497268614_0004}; taskId=attempt_202105171658444331183894497268614_0004_m_000128_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3862cf6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:14,543] {docker.py:276} INFO - 21/05/17 17:00:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:14,543] {docker.py:276} INFO - 21/05/17 17:00:14 INFO StagingCommitter: Starting: Task committer attempt_202105171658444331183894497268614_0004_m_000128_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444331183894497268614_0004_m_000128_313
[2021-05-17 14:00:14,546] {docker.py:276} INFO - 21/05/17 17:00:14 INFO StagingCommitter: Task committer attempt_202105171658444331183894497268614_0004_m_000128_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444331183894497268614_0004_m_000128_313 : duration 0:00.005s
[2021-05-17 14:00:15,967] {docker.py:276} INFO - 21/05/17 17:00:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658444580063176465873796_0004_m_000126_311: needsTaskCommit() Task attempt_202105171658444580063176465873796_0004_m_000126_311
[2021-05-17 14:00:15,968] {docker.py:276} INFO - 21/05/17 17:00:15 INFO StagingCommitter: Task committer attempt_202105171658444580063176465873796_0004_m_000126_311: needsTaskCommit() Task attempt_202105171658444580063176465873796_0004_m_000126_311: duration 0:00.000s
[2021-05-17 14:00:15,968] {docker.py:276} INFO - 21/05/17 17:00:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444580063176465873796_0004_m_000126_311
[2021-05-17 14:00:15,969] {docker.py:276} INFO - 21/05/17 17:00:15 INFO Executor: Finished task 126.0 in stage 4.0 (TID 311). 4587 bytes result sent to driver
[2021-05-17 14:00:15,970] {docker.py:276} INFO - 21/05/17 17:00:15 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 314) (bb4563f4559d, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:15,972] {docker.py:276} INFO - 21/05/17 17:00:15 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 311) in 2413 ms on bb4563f4559d (executor driver) (126/200)
21/05/17 17:00:15 INFO Executor: Running task 129.0 in stage 4.0 (TID 314)
[2021-05-17 14:00:15,980] {docker.py:276} INFO - 21/05/17 17:00:15 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:15,982] {docker.py:276} INFO - 21/05/17 17:00:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447450969137400893628_0004_m_000129_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447450969137400893628_0004_m_000129_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447450969137400893628_0004}; taskId=attempt_202105171658447450969137400893628_0004_m_000129_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22910adb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:15,983] {docker.py:276} INFO - 21/05/17 17:00:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658447450969137400893628_0004_m_000129_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447450969137400893628_0004_m_000129_314
[2021-05-17 14:00:15,987] {docker.py:276} INFO - 21/05/17 17:00:15 INFO StagingCommitter: Task committer attempt_202105171658447450969137400893628_0004_m_000129_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447450969137400893628_0004_m_000129_314 : duration 0:00.004s
[2021-05-17 14:00:15,990] {docker.py:276} INFO - 21/05/17 17:00:15 INFO StagingCommitter: Starting: Task committer attempt_202105171658446507343346247806212_0004_m_000125_310: needsTaskCommit() Task attempt_202105171658446507343346247806212_0004_m_000125_310
[2021-05-17 14:00:15,991] {docker.py:276} INFO - 21/05/17 17:00:15 INFO StagingCommitter: Task committer attempt_202105171658446507343346247806212_0004_m_000125_310: needsTaskCommit() Task attempt_202105171658446507343346247806212_0004_m_000125_310: duration 0:00.001s
21/05/17 17:00:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446507343346247806212_0004_m_000125_310
[2021-05-17 14:00:15,992] {docker.py:276} INFO - 21/05/17 17:00:15 INFO Executor: Finished task 125.0 in stage 4.0 (TID 310). 4587 bytes result sent to driver
[2021-05-17 14:00:15,994] {docker.py:276} INFO - 21/05/17 17:00:15 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 315) (bb4563f4559d, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:15,995] {docker.py:276} INFO - 21/05/17 17:00:15 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 310) in 2647 ms on bb4563f4559d (executor driver) (127/200)
[2021-05-17 14:00:15,995] {docker.py:276} INFO - 21/05/17 17:00:15 INFO Executor: Running task 130.0 in stage 4.0 (TID 315)
[2021-05-17 14:00:16,004] {docker.py:276} INFO - 21/05/17 17:00:16 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:16,005] {docker.py:276} INFO - 21/05/17 17:00:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:16,006] {docker.py:276} INFO - 21/05/17 17:00:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:16,007] {docker.py:276} INFO - 21/05/17 17:00:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:16,007] {docker.py:276} INFO - 21/05/17 17:00:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441629578534579852159_0004_m_000130_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441629578534579852159_0004_m_000130_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441629578534579852159_0004}; taskId=attempt_202105171658441629578534579852159_0004_m_000130_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c9ede7e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:16,008] {docker.py:276} INFO - 21/05/17 17:00:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:16,008] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Starting: Task committer attempt_202105171658441629578534579852159_0004_m_000130_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441629578534579852159_0004_m_000130_315
[2021-05-17 14:00:16,011] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Task committer attempt_202105171658441629578534579852159_0004_m_000130_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441629578534579852159_0004_m_000130_315 : duration 0:00.004s
[2021-05-17 14:00:16,496] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Starting: Task committer attempt_202105171658446477929411715811951_0004_m_000127_312: needsTaskCommit() Task attempt_202105171658446477929411715811951_0004_m_000127_312
[2021-05-17 14:00:16,497] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Task committer attempt_202105171658446477929411715811951_0004_m_000127_312: needsTaskCommit() Task attempt_202105171658446477929411715811951_0004_m_000127_312: duration 0:00.001s
21/05/17 17:00:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446477929411715811951_0004_m_000127_312
[2021-05-17 14:00:16,499] {docker.py:276} INFO - 21/05/17 17:00:16 INFO Executor: Finished task 127.0 in stage 4.0 (TID 312). 4587 bytes result sent to driver
[2021-05-17 14:00:16,500] {docker.py:276} INFO - 21/05/17 17:00:16 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 316) (bb4563f4559d, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:16,501] {docker.py:276} INFO - 21/05/17 17:00:16 INFO Executor: Running task 131.0 in stage 4.0 (TID 316)
21/05/17 17:00:16 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 312) in 2621 ms on bb4563f4559d (executor driver) (128/200)
[2021-05-17 14:00:16,509] {docker.py:276} INFO - 21/05/17 17:00:16 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:16,513] {docker.py:276} INFO - 21/05/17 17:00:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442002085802416316257_0004_m_000131_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442002085802416316257_0004_m_000131_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442002085802416316257_0004}; taskId=attempt_202105171658442002085802416316257_0004_m_000131_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24cf0f56}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:16 INFO StagingCommitter: Starting: Task committer attempt_202105171658442002085802416316257_0004_m_000131_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442002085802416316257_0004_m_000131_316
[2021-05-17 14:00:16,516] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Task committer attempt_202105171658442002085802416316257_0004_m_000131_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442002085802416316257_0004_m_000131_316 : duration 0:00.005s
[2021-05-17 14:00:16,988] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Starting: Task committer attempt_202105171658444331183894497268614_0004_m_000128_313: needsTaskCommit() Task attempt_202105171658444331183894497268614_0004_m_000128_313
[2021-05-17 14:00:16,989] {docker.py:276} INFO - 21/05/17 17:00:16 INFO StagingCommitter: Task committer attempt_202105171658444331183894497268614_0004_m_000128_313: needsTaskCommit() Task attempt_202105171658444331183894497268614_0004_m_000128_313: duration 0:00.001s
21/05/17 17:00:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444331183894497268614_0004_m_000128_313
[2021-05-17 14:00:16,990] {docker.py:276} INFO - 21/05/17 17:00:16 INFO Executor: Finished task 128.0 in stage 4.0 (TID 313). 4587 bytes result sent to driver
[2021-05-17 14:00:16,992] {docker.py:276} INFO - 21/05/17 17:00:16 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 317) (bb4563f4559d, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:16,992] {docker.py:276} INFO - 21/05/17 17:00:16 INFO Executor: Running task 132.0 in stage 4.0 (TID 317)
[2021-05-17 14:00:16,993] {docker.py:276} INFO - 21/05/17 17:00:16 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 313) in 2477 ms on bb4563f4559d (executor driver) (129/200)
[2021-05-17 14:00:17,003] {docker.py:276} INFO - 21/05/17 17:00:17 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:17,005] {docker.py:276} INFO - 21/05/17 17:00:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:17,006] {docker.py:276} INFO - 21/05/17 17:00:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448683801454261606407_0004_m_000132_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448683801454261606407_0004_m_000132_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448683801454261606407_0004}; taskId=attempt_202105171658448683801454261606407_0004_m_000132_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@653df014}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:17,006] {docker.py:276} INFO - 21/05/17 17:00:17 INFO StagingCommitter: Starting: Task committer attempt_202105171658448683801454261606407_0004_m_000132_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448683801454261606407_0004_m_000132_317
[2021-05-17 14:00:17,010] {docker.py:276} INFO - 21/05/17 17:00:17 INFO StagingCommitter: Task committer attempt_202105171658448683801454261606407_0004_m_000132_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448683801454261606407_0004_m_000132_317 : duration 0:00.004s
[2021-05-17 14:00:18,377] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Starting: Task committer attempt_202105171658441629578534579852159_0004_m_000130_315: needsTaskCommit() Task attempt_202105171658441629578534579852159_0004_m_000130_315
[2021-05-17 14:00:18,378] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Task committer attempt_202105171658441629578534579852159_0004_m_000130_315: needsTaskCommit() Task attempt_202105171658441629578534579852159_0004_m_000130_315: duration 0:00.001s
21/05/17 17:00:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441629578534579852159_0004_m_000130_315
[2021-05-17 14:00:18,379] {docker.py:276} INFO - 21/05/17 17:00:18 INFO Executor: Finished task 130.0 in stage 4.0 (TID 315). 4544 bytes result sent to driver
[2021-05-17 14:00:18,381] {docker.py:276} INFO - 21/05/17 17:00:18 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 318) (bb4563f4559d, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:18,382] {docker.py:276} INFO - 21/05/17 17:00:18 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 315) in 2391 ms on bb4563f4559d (executor driver) (130/200)
[2021-05-17 14:00:18,382] {docker.py:276} INFO - 21/05/17 17:00:18 INFO Executor: Running task 133.0 in stage 4.0 (TID 318)
[2021-05-17 14:00:18,392] {docker.py:276} INFO - 21/05/17 17:00:18 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:18,394] {docker.py:276} INFO - 21/05/17 17:00:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441941626959792862649_0004_m_000133_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441941626959792862649_0004_m_000133_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441941626959792862649_0004}; taskId=attempt_202105171658441941626959792862649_0004_m_000133_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5854c5fa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:18 INFO StagingCommitter: Starting: Task committer attempt_202105171658441941626959792862649_0004_m_000133_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441941626959792862649_0004_m_000133_318
[2021-05-17 14:00:18,398] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Task committer attempt_202105171658441941626959792862649_0004_m_000133_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441941626959792862649_0004_m_000133_318 : duration 0:00.004s
[2021-05-17 14:00:18,560] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Starting: Task committer attempt_202105171658447450969137400893628_0004_m_000129_314: needsTaskCommit() Task attempt_202105171658447450969137400893628_0004_m_000129_314
[2021-05-17 14:00:18,561] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Task committer attempt_202105171658447450969137400893628_0004_m_000129_314: needsTaskCommit() Task attempt_202105171658447450969137400893628_0004_m_000129_314: duration 0:00.001s
21/05/17 17:00:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447450969137400893628_0004_m_000129_314
[2021-05-17 14:00:18,562] {docker.py:276} INFO - 21/05/17 17:00:18 INFO Executor: Finished task 129.0 in stage 4.0 (TID 314). 4544 bytes result sent to driver
[2021-05-17 14:00:18,563] {docker.py:276} INFO - 21/05/17 17:00:18 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 319) (bb4563f4559d, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:18,566] {docker.py:276} INFO - 21/05/17 17:00:18 INFO Executor: Running task 134.0 in stage 4.0 (TID 319)
21/05/17 17:00:18 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 314) in 2599 ms on bb4563f4559d (executor driver) (131/200)
[2021-05-17 14:00:18,576] {docker.py:276} INFO - 21/05/17 17:00:18 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:18,578] {docker.py:276} INFO - 21/05/17 17:00:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444628019048047505816_0004_m_000134_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444628019048047505816_0004_m_000134_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444628019048047505816_0004}; taskId=attempt_202105171658444628019048047505816_0004_m_000134_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71344dcd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:18,578] {docker.py:276} INFO - 21/05/17 17:00:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:18,579] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Starting: Task committer attempt_202105171658444628019048047505816_0004_m_000134_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444628019048047505816_0004_m_000134_319
[2021-05-17 14:00:18,582] {docker.py:276} INFO - 21/05/17 17:00:18 INFO StagingCommitter: Task committer attempt_202105171658444628019048047505816_0004_m_000134_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444628019048047505816_0004_m_000134_319 : duration 0:00.003s
[2021-05-17 14:00:19,015] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Starting: Task committer attempt_202105171658442002085802416316257_0004_m_000131_316: needsTaskCommit() Task attempt_202105171658442002085802416316257_0004_m_000131_316
[2021-05-17 14:00:19,016] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Task committer attempt_202105171658442002085802416316257_0004_m_000131_316: needsTaskCommit() Task attempt_202105171658442002085802416316257_0004_m_000131_316: duration 0:00.002s
[2021-05-17 14:00:19,017] {docker.py:276} INFO - 21/05/17 17:00:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442002085802416316257_0004_m_000131_316
[2021-05-17 14:00:19,020] {docker.py:276} INFO - 21/05/17 17:00:19 INFO Executor: Finished task 131.0 in stage 4.0 (TID 316). 4544 bytes result sent to driver
[2021-05-17 14:00:19,021] {docker.py:276} INFO - 21/05/17 17:00:19 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 320) (bb4563f4559d, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:19,023] {docker.py:276} INFO - 21/05/17 17:00:19 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 316) in 2527 ms on bb4563f4559d (executor driver) (132/200)
[2021-05-17 14:00:19,023] {docker.py:276} INFO - 21/05/17 17:00:19 INFO Executor: Running task 135.0 in stage 4.0 (TID 320)
[2021-05-17 14:00:19,033] {docker.py:276} INFO - 21/05/17 17:00:19 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:19,035] {docker.py:276} INFO - 21/05/17 17:00:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658449128433480150423381_0004_m_000135_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658449128433480150423381_0004_m_000135_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658449128433480150423381_0004}; taskId=attempt_202105171658449128433480150423381_0004_m_000135_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@695d02e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:19,036] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Starting: Task committer attempt_202105171658449128433480150423381_0004_m_000135_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658449128433480150423381_0004_m_000135_320
[2021-05-17 14:00:19,038] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Task committer attempt_202105171658449128433480150423381_0004_m_000135_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658449128433480150423381_0004_m_000135_320 : duration 0:00.003s
[2021-05-17 14:00:19,613] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Starting: Task committer attempt_202105171658448683801454261606407_0004_m_000132_317: needsTaskCommit() Task attempt_202105171658448683801454261606407_0004_m_000132_317
[2021-05-17 14:00:19,613] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Task committer attempt_202105171658448683801454261606407_0004_m_000132_317: needsTaskCommit() Task attempt_202105171658448683801454261606407_0004_m_000132_317: duration 0:00.001s
21/05/17 17:00:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448683801454261606407_0004_m_000132_317
[2021-05-17 14:00:19,616] {docker.py:276} INFO - 21/05/17 17:00:19 INFO Executor: Finished task 132.0 in stage 4.0 (TID 317). 4544 bytes result sent to driver
[2021-05-17 14:00:19,618] {docker.py:276} INFO - 21/05/17 17:00:19 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 321) (bb4563f4559d, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:19,619] {docker.py:276} INFO - 21/05/17 17:00:19 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 317) in 2631 ms on bb4563f4559d (executor driver) (133/200)
21/05/17 17:00:19 INFO Executor: Running task 136.0 in stage 4.0 (TID 321)
[2021-05-17 14:00:19,636] {docker.py:276} INFO - 21/05/17 17:00:19 INFO ShuffleBlockFetcherIterator: Getting 4 (26.7 KiB) non-empty blocks including 4 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:19,637] {docker.py:276} INFO - 21/05/17 17:00:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:19,637] {docker.py:276} INFO - 21/05/17 17:00:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443863425743817730388_0004_m_000136_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443863425743817730388_0004_m_000136_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443863425743817730388_0004}; taskId=attempt_202105171658443863425743817730388_0004_m_000136_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f3afef9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:19,638] {docker.py:276} INFO - 21/05/17 17:00:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:19 INFO StagingCommitter: Starting: Task committer attempt_202105171658443863425743817730388_0004_m_000136_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443863425743817730388_0004_m_000136_321
[2021-05-17 14:00:19,641] {docker.py:276} INFO - 21/05/17 17:00:19 INFO StagingCommitter: Task committer attempt_202105171658443863425743817730388_0004_m_000136_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443863425743817730388_0004_m_000136_321 : duration 0:00.004s
[2021-05-17 14:00:20,815] {docker.py:276} INFO - 21/05/17 17:00:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658441941626959792862649_0004_m_000133_318: needsTaskCommit() Task attempt_202105171658441941626959792862649_0004_m_000133_318
[2021-05-17 14:00:20,816] {docker.py:276} INFO - 21/05/17 17:00:20 INFO StagingCommitter: Task committer attempt_202105171658441941626959792862649_0004_m_000133_318: needsTaskCommit() Task attempt_202105171658441941626959792862649_0004_m_000133_318: duration 0:00.001s
[2021-05-17 14:00:20,816] {docker.py:276} INFO - 21/05/17 17:00:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441941626959792862649_0004_m_000133_318
[2021-05-17 14:00:20,818] {docker.py:276} INFO - 21/05/17 17:00:20 INFO Executor: Finished task 133.0 in stage 4.0 (TID 318). 4544 bytes result sent to driver
[2021-05-17 14:00:20,820] {docker.py:276} INFO - 21/05/17 17:00:20 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 322) (bb4563f4559d, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:20,821] {docker.py:276} INFO - 21/05/17 17:00:20 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 318) in 2444 ms on bb4563f4559d (executor driver) (134/200)
21/05/17 17:00:20 INFO Executor: Running task 137.0 in stage 4.0 (TID 322)
[2021-05-17 14:00:20,839] {docker.py:276} INFO - 21/05/17 17:00:20 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:20,841] {docker.py:276} INFO - 21/05/17 17:00:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:20,841] {docker.py:276} INFO - 21/05/17 17:00:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442813048249792015015_0004_m_000137_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442813048249792015015_0004_m_000137_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442813048249792015015_0004}; taskId=attempt_202105171658442813048249792015015_0004_m_000137_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24ef769b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:20 INFO StagingCommitter: Starting: Task committer attempt_202105171658442813048249792015015_0004_m_000137_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442813048249792015015_0004_m_000137_322
[2021-05-17 14:00:20,844] {docker.py:276} INFO - 21/05/17 17:00:20 INFO StagingCommitter: Task committer attempt_202105171658442813048249792015015_0004_m_000137_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442813048249792015015_0004_m_000137_322 : duration 0:00.003s
[2021-05-17 14:00:21,085] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Starting: Task committer attempt_202105171658444628019048047505816_0004_m_000134_319: needsTaskCommit() Task attempt_202105171658444628019048047505816_0004_m_000134_319
[2021-05-17 14:00:21,086] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Task committer attempt_202105171658444628019048047505816_0004_m_000134_319: needsTaskCommit() Task attempt_202105171658444628019048047505816_0004_m_000134_319: duration 0:00.001s
21/05/17 17:00:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444628019048047505816_0004_m_000134_319
[2021-05-17 14:00:21,089] {docker.py:276} INFO - 21/05/17 17:00:21 INFO Executor: Finished task 134.0 in stage 4.0 (TID 319). 4587 bytes result sent to driver
21/05/17 17:00:21 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 323) (bb4563f4559d, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:21,089] {docker.py:276} INFO - 21/05/17 17:00:21 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 319) in 2529 ms on bb4563f4559d (executor driver) (135/200)
[2021-05-17 14:00:21,090] {docker.py:276} INFO - 21/05/17 17:00:21 INFO Executor: Running task 138.0 in stage 4.0 (TID 323)
[2021-05-17 14:00:21,101] {docker.py:276} INFO - 21/05/17 17:00:21 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:21,102] {docker.py:276} INFO - 21/05/17 17:00:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441432980122638569944_0004_m_000138_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441432980122638569944_0004_m_000138_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441432980122638569944_0004}; taskId=attempt_202105171658441432980122638569944_0004_m_000138_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53ce3ed3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:21,103] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Starting: Task committer attempt_202105171658441432980122638569944_0004_m_000138_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441432980122638569944_0004_m_000138_323
[2021-05-17 14:00:21,106] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Task committer attempt_202105171658441432980122638569944_0004_m_000138_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441432980122638569944_0004_m_000138_323 : duration 0:00.003s
[2021-05-17 14:00:21,411] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Starting: Task committer attempt_202105171658449128433480150423381_0004_m_000135_320: needsTaskCommit() Task attempt_202105171658449128433480150423381_0004_m_000135_320
[2021-05-17 14:00:21,413] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Task committer attempt_202105171658449128433480150423381_0004_m_000135_320: needsTaskCommit() Task attempt_202105171658449128433480150423381_0004_m_000135_320: duration 0:00.001s
21/05/17 17:00:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658449128433480150423381_0004_m_000135_320
[2021-05-17 14:00:21,415] {docker.py:276} INFO - 21/05/17 17:00:21 INFO Executor: Finished task 135.0 in stage 4.0 (TID 320). 4587 bytes result sent to driver
[2021-05-17 14:00:21,416] {docker.py:276} INFO - 21/05/17 17:00:21 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 324) (bb4563f4559d, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:21,417] {docker.py:276} INFO - 21/05/17 17:00:21 INFO Executor: Running task 139.0 in stage 4.0 (TID 324)
[2021-05-17 14:00:21,418] {docker.py:276} INFO - 21/05/17 17:00:21 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 320) in 2399 ms on bb4563f4559d (executor driver) (136/200)
[2021-05-17 14:00:21,427] {docker.py:276} INFO - 21/05/17 17:00:21 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:21,427] {docker.py:276} INFO - 21/05/17 17:00:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:21,429] {docker.py:276} INFO - 21/05/17 17:00:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:21,429] {docker.py:276} INFO - 21/05/17 17:00:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:21,430] {docker.py:276} INFO - 21/05/17 17:00:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447407047482463088919_0004_m_000139_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447407047482463088919_0004_m_000139_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447407047482463088919_0004}; taskId=attempt_202105171658447407047482463088919_0004_m_000139_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61800864}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:21,430] {docker.py:276} INFO - 21/05/17 17:00:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:21,430] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Starting: Task committer attempt_202105171658447407047482463088919_0004_m_000139_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447407047482463088919_0004_m_000139_324
[2021-05-17 14:00:21,433] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Task committer attempt_202105171658447407047482463088919_0004_m_000139_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447407047482463088919_0004_m_000139_324 : duration 0:00.003s
[2021-05-17 14:00:21,940] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Starting: Task committer attempt_202105171658443863425743817730388_0004_m_000136_321: needsTaskCommit() Task attempt_202105171658443863425743817730388_0004_m_000136_321
[2021-05-17 14:00:21,941] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Task committer attempt_202105171658443863425743817730388_0004_m_000136_321: needsTaskCommit() Task attempt_202105171658443863425743817730388_0004_m_000136_321: duration 0:00.001s
21/05/17 17:00:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443863425743817730388_0004_m_000136_321
[2021-05-17 14:00:21,944] {docker.py:276} INFO - 21/05/17 17:00:21 INFO Executor: Finished task 136.0 in stage 4.0 (TID 321). 4587 bytes result sent to driver
21/05/17 17:00:21 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 325) (bb4563f4559d, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:21,945] {docker.py:276} INFO - 21/05/17 17:00:21 INFO Executor: Running task 140.0 in stage 4.0 (TID 325)
[2021-05-17 14:00:21,946] {docker.py:276} INFO - 21/05/17 17:00:21 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 321) in 2330 ms on bb4563f4559d (executor driver) (137/200)
[2021-05-17 14:00:21,954] {docker.py:276} INFO - 21/05/17 17:00:21 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:21,956] {docker.py:276} INFO - 21/05/17 17:00:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447478671928617383536_0004_m_000140_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447478671928617383536_0004_m_000140_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447478671928617383536_0004}; taskId=attempt_202105171658447478671928617383536_0004_m_000140_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61f02414}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:21,956] {docker.py:276} INFO - 21/05/17 17:00:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:21,957] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Starting: Task committer attempt_202105171658447478671928617383536_0004_m_000140_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447478671928617383536_0004_m_000140_325
[2021-05-17 14:00:21,959] {docker.py:276} INFO - 21/05/17 17:00:21 INFO StagingCommitter: Task committer attempt_202105171658447478671928617383536_0004_m_000140_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447478671928617383536_0004_m_000140_325 : duration 0:00.003s
[2021-05-17 14:00:23,254] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Starting: Task committer attempt_202105171658442813048249792015015_0004_m_000137_322: needsTaskCommit() Task attempt_202105171658442813048249792015015_0004_m_000137_322
[2021-05-17 14:00:23,255] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Task committer attempt_202105171658442813048249792015015_0004_m_000137_322: needsTaskCommit() Task attempt_202105171658442813048249792015015_0004_m_000137_322: duration 0:00.001s
21/05/17 17:00:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442813048249792015015_0004_m_000137_322
[2021-05-17 14:00:23,257] {docker.py:276} INFO - 21/05/17 17:00:23 INFO Executor: Finished task 137.0 in stage 4.0 (TID 322). 4587 bytes result sent to driver
[2021-05-17 14:00:23,258] {docker.py:276} INFO - 21/05/17 17:00:23 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 326) (bb4563f4559d, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 17:00:23 INFO Executor: Running task 141.0 in stage 4.0 (TID 326)
[2021-05-17 14:00:23,259] {docker.py:276} INFO - 21/05/17 17:00:23 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 322) in 2442 ms on bb4563f4559d (executor driver) (138/200)
[2021-05-17 14:00:23,268] {docker.py:276} INFO - 21/05/17 17:00:23 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:23,270] {docker.py:276} INFO - 21/05/17 17:00:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441148843404275612293_0004_m_000141_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441148843404275612293_0004_m_000141_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441148843404275612293_0004}; taskId=attempt_202105171658441148843404275612293_0004_m_000141_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11e7009e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:23,270] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Starting: Task committer attempt_202105171658441148843404275612293_0004_m_000141_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441148843404275612293_0004_m_000141_326
[2021-05-17 14:00:23,273] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Task committer attempt_202105171658441148843404275612293_0004_m_000141_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441148843404275612293_0004_m_000141_326 : duration 0:00.004s
[2021-05-17 14:00:23,564] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Starting: Task committer attempt_202105171658441432980122638569944_0004_m_000138_323: needsTaskCommit() Task attempt_202105171658441432980122638569944_0004_m_000138_323
[2021-05-17 14:00:23,565] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Task committer attempt_202105171658441432980122638569944_0004_m_000138_323: needsTaskCommit() Task attempt_202105171658441432980122638569944_0004_m_000138_323: duration 0:00.001s
21/05/17 17:00:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441432980122638569944_0004_m_000138_323
[2021-05-17 14:00:23,566] {docker.py:276} INFO - 21/05/17 17:00:23 INFO Executor: Finished task 138.0 in stage 4.0 (TID 323). 4544 bytes result sent to driver
[2021-05-17 14:00:23,567] {docker.py:276} INFO - 21/05/17 17:00:23 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 327) (bb4563f4559d, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:23,568] {docker.py:276} INFO - 21/05/17 17:00:23 INFO Executor: Running task 142.0 in stage 4.0 (TID 327)
21/05/17 17:00:23 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 323) in 2483 ms on bb4563f4559d (executor driver) (139/200)
[2021-05-17 14:00:23,576] {docker.py:276} INFO - 21/05/17 17:00:23 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:23,578] {docker.py:276} INFO - 21/05/17 17:00:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443010909783119527875_0004_m_000142_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443010909783119527875_0004_m_000142_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443010909783119527875_0004}; taskId=attempt_202105171658443010909783119527875_0004_m_000142_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56d11a9b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:23,578] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Starting: Task committer attempt_202105171658443010909783119527875_0004_m_000142_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443010909783119527875_0004_m_000142_327
[2021-05-17 14:00:23,581] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Task committer attempt_202105171658443010909783119527875_0004_m_000142_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443010909783119527875_0004_m_000142_327 : duration 0:00.003s
[2021-05-17 14:00:23,878] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Starting: Task committer attempt_202105171658447407047482463088919_0004_m_000139_324: needsTaskCommit() Task attempt_202105171658447407047482463088919_0004_m_000139_324
[2021-05-17 14:00:23,878] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Task committer attempt_202105171658447407047482463088919_0004_m_000139_324: needsTaskCommit() Task attempt_202105171658447407047482463088919_0004_m_000139_324: duration 0:00.000s
[2021-05-17 14:00:23,879] {docker.py:276} INFO - 21/05/17 17:00:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447407047482463088919_0004_m_000139_324
[2021-05-17 14:00:23,880] {docker.py:276} INFO - 21/05/17 17:00:23 INFO Executor: Finished task 139.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-17 14:00:23,881] {docker.py:276} INFO - 21/05/17 17:00:23 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 328) (bb4563f4559d, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:23,882] {docker.py:276} INFO - 21/05/17 17:00:23 INFO Executor: Running task 143.0 in stage 4.0 (TID 328)
[2021-05-17 14:00:23,883] {docker.py:276} INFO - 21/05/17 17:00:23 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 324) in 2470 ms on bb4563f4559d (executor driver) (140/200)
[2021-05-17 14:00:23,892] {docker.py:276} INFO - 21/05/17 17:00:23 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:23,892] {docker.py:276} INFO - 21/05/17 17:00:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:23,894] {docker.py:276} INFO - 21/05/17 17:00:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:23,894] {docker.py:276} INFO - 21/05/17 17:00:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446988061347153790166_0004_m_000143_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446988061347153790166_0004_m_000143_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446988061347153790166_0004}; taskId=attempt_202105171658446988061347153790166_0004_m_000143_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@799405ab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:23,894] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Starting: Task committer attempt_202105171658446988061347153790166_0004_m_000143_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446988061347153790166_0004_m_000143_328
[2021-05-17 14:00:23,898] {docker.py:276} INFO - 21/05/17 17:00:23 INFO StagingCommitter: Task committer attempt_202105171658446988061347153790166_0004_m_000143_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446988061347153790166_0004_m_000143_328 : duration 0:00.005s
[2021-05-17 14:00:24,101] {docker.py:276} INFO - 21/05/17 17:00:24 INFO StagingCommitter: Starting: Task committer attempt_202105171658447478671928617383536_0004_m_000140_325: needsTaskCommit() Task attempt_202105171658447478671928617383536_0004_m_000140_325
[2021-05-17 14:00:24,102] {docker.py:276} INFO - 21/05/17 17:00:24 INFO StagingCommitter: Task committer attempt_202105171658447478671928617383536_0004_m_000140_325: needsTaskCommit() Task attempt_202105171658447478671928617383536_0004_m_000140_325: duration 0:00.001s
21/05/17 17:00:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447478671928617383536_0004_m_000140_325
[2021-05-17 14:00:24,103] {docker.py:276} INFO - 21/05/17 17:00:24 INFO Executor: Finished task 140.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
[2021-05-17 14:00:24,104] {docker.py:276} INFO - 21/05/17 17:00:24 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 329) (bb4563f4559d, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:24,105] {docker.py:276} INFO - 21/05/17 17:00:24 INFO Executor: Running task 144.0 in stage 4.0 (TID 329)
[2021-05-17 14:00:24,106] {docker.py:276} INFO - 21/05/17 17:00:24 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 325) in 2165 ms on bb4563f4559d (executor driver) (141/200)
[2021-05-17 14:00:24,115] {docker.py:276} INFO - 21/05/17 17:00:24 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:24,116] {docker.py:276} INFO - 21/05/17 17:00:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:24,117] {docker.py:276} INFO - 21/05/17 17:00:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:24,117] {docker.py:276} INFO - 21/05/17 17:00:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:24,118] {docker.py:276} INFO - 21/05/17 17:00:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446701028734043236272_0004_m_000144_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446701028734043236272_0004_m_000144_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446701028734043236272_0004}; taskId=attempt_202105171658446701028734043236272_0004_m_000144_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e992f55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:24,118] {docker.py:276} INFO - 21/05/17 17:00:24 INFO StagingCommitter: Starting: Task committer attempt_202105171658446701028734043236272_0004_m_000144_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446701028734043236272_0004_m_000144_329
[2021-05-17 14:00:24,120] {docker.py:276} INFO - 21/05/17 17:00:24 INFO StagingCommitter: Task committer attempt_202105171658446701028734043236272_0004_m_000144_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446701028734043236272_0004_m_000144_329 : duration 0:00.003s
[2021-05-17 14:00:25,685] {docker.py:276} INFO - 21/05/17 17:00:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658441148843404275612293_0004_m_000141_326: needsTaskCommit() Task attempt_202105171658441148843404275612293_0004_m_000141_326
[2021-05-17 14:00:25,686] {docker.py:276} INFO - 21/05/17 17:00:25 INFO StagingCommitter: Task committer attempt_202105171658441148843404275612293_0004_m_000141_326: needsTaskCommit() Task attempt_202105171658441148843404275612293_0004_m_000141_326: duration 0:00.002s
[2021-05-17 14:00:25,687] {docker.py:276} INFO - 21/05/17 17:00:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441148843404275612293_0004_m_000141_326
[2021-05-17 14:00:25,688] {docker.py:276} INFO - 21/05/17 17:00:25 INFO Executor: Finished task 141.0 in stage 4.0 (TID 326). 4544 bytes result sent to driver
[2021-05-17 14:00:25,691] {docker.py:276} INFO - 21/05/17 17:00:25 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 330) (bb4563f4559d, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:25,692] {docker.py:276} INFO - 21/05/17 17:00:25 INFO Executor: Running task 145.0 in stage 4.0 (TID 330)
21/05/17 17:00:25 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 326) in 2437 ms on bb4563f4559d (executor driver) (142/200)
[2021-05-17 14:00:25,702] {docker.py:276} INFO - 21/05/17 17:00:25 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:25,704] {docker.py:276} INFO - 21/05/17 17:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:25,705] {docker.py:276} INFO - 21/05/17 17:00:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446166795194710339912_0004_m_000145_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446166795194710339912_0004_m_000145_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446166795194710339912_0004}; taskId=attempt_202105171658446166795194710339912_0004_m_000145_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6803d239}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:25,705] {docker.py:276} INFO - 21/05/17 17:00:25 INFO StagingCommitter: Starting: Task committer attempt_202105171658446166795194710339912_0004_m_000145_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446166795194710339912_0004_m_000145_330
[2021-05-17 14:00:25,708] {docker.py:276} INFO - 21/05/17 17:00:25 INFO StagingCommitter: Task committer attempt_202105171658446166795194710339912_0004_m_000145_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446166795194710339912_0004_m_000145_330 : duration 0:00.004s
[2021-05-17 14:00:26,061] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Starting: Task committer attempt_202105171658443010909783119527875_0004_m_000142_327: needsTaskCommit() Task attempt_202105171658443010909783119527875_0004_m_000142_327
[2021-05-17 14:00:26,062] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Task committer attempt_202105171658443010909783119527875_0004_m_000142_327: needsTaskCommit() Task attempt_202105171658443010909783119527875_0004_m_000142_327: duration 0:00.001s
21/05/17 17:00:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443010909783119527875_0004_m_000142_327
[2021-05-17 14:00:26,063] {docker.py:276} INFO - 21/05/17 17:00:26 INFO Executor: Finished task 142.0 in stage 4.0 (TID 327). 4544 bytes result sent to driver
[2021-05-17 14:00:26,065] {docker.py:276} INFO - 21/05/17 17:00:26 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 331) (bb4563f4559d, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:26,066] {docker.py:276} INFO - 21/05/17 17:00:26 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 327) in 2502 ms on bb4563f4559d (executor driver) (143/200)
[2021-05-17 14:00:26,068] {docker.py:276} INFO - 21/05/17 17:00:26 INFO Executor: Running task 146.0 in stage 4.0 (TID 331)
[2021-05-17 14:00:26,078] {docker.py:276} INFO - 21/05/17 17:00:26 INFO ShuffleBlockFetcherIterator: Getting 4 (27.2 KiB) non-empty blocks including 4 (27.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:26,080] {docker.py:276} INFO - 21/05/17 17:00:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:26,080] {docker.py:276} INFO - 21/05/17 17:00:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446191607610525058746_0004_m_000146_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446191607610525058746_0004_m_000146_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446191607610525058746_0004}; taskId=attempt_202105171658446191607610525058746_0004_m_000146_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d226d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:26,080] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Starting: Task committer attempt_202105171658446191607610525058746_0004_m_000146_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446191607610525058746_0004_m_000146_331
[2021-05-17 14:00:26,084] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Task committer attempt_202105171658446191607610525058746_0004_m_000146_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446191607610525058746_0004_m_000146_331 : duration 0:00.004s
[2021-05-17 14:00:26,400] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Starting: Task committer attempt_202105171658446988061347153790166_0004_m_000143_328: needsTaskCommit() Task attempt_202105171658446988061347153790166_0004_m_000143_328
[2021-05-17 14:00:26,401] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Task committer attempt_202105171658446988061347153790166_0004_m_000143_328: needsTaskCommit() Task attempt_202105171658446988061347153790166_0004_m_000143_328: duration 0:00.001s
21/05/17 17:00:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446988061347153790166_0004_m_000143_328
[2021-05-17 14:00:26,403] {docker.py:276} INFO - 21/05/17 17:00:26 INFO Executor: Finished task 143.0 in stage 4.0 (TID 328). 4587 bytes result sent to driver
[2021-05-17 14:00:26,404] {docker.py:276} INFO - 21/05/17 17:00:26 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 332) (bb4563f4559d, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:26,405] {docker.py:276} INFO - 21/05/17 17:00:26 INFO Executor: Running task 147.0 in stage 4.0 (TID 332)
[2021-05-17 14:00:26,406] {docker.py:276} INFO - 21/05/17 17:00:26 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 328) in 2528 ms on bb4563f4559d (executor driver) (144/200)
[2021-05-17 14:00:26,414] {docker.py:276} INFO - 21/05/17 17:00:26 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:26,415] {docker.py:276} INFO - 21/05/17 17:00:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:26,416] {docker.py:276} INFO - 21/05/17 17:00:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:26,416] {docker.py:276} INFO - 21/05/17 17:00:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448901295855151643132_0004_m_000147_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448901295855151643132_0004_m_000147_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448901295855151643132_0004}; taskId=attempt_202105171658448901295855151643132_0004_m_000147_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e805bef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:26,416] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Starting: Task committer attempt_202105171658448901295855151643132_0004_m_000147_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448901295855151643132_0004_m_000147_332
[2021-05-17 14:00:26,418] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Task committer attempt_202105171658448901295855151643132_0004_m_000147_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448901295855151643132_0004_m_000147_332 : duration 0:00.003s
[2021-05-17 14:00:26,429] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Starting: Task committer attempt_202105171658446701028734043236272_0004_m_000144_329: needsTaskCommit() Task attempt_202105171658446701028734043236272_0004_m_000144_329
[2021-05-17 14:00:26,430] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Task committer attempt_202105171658446701028734043236272_0004_m_000144_329: needsTaskCommit() Task attempt_202105171658446701028734043236272_0004_m_000144_329: duration 0:00.001s
21/05/17 17:00:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446701028734043236272_0004_m_000144_329
[2021-05-17 14:00:26,431] {docker.py:276} INFO - 21/05/17 17:00:26 INFO Executor: Finished task 144.0 in stage 4.0 (TID 329). 4587 bytes result sent to driver
[2021-05-17 14:00:26,431] {docker.py:276} INFO - 21/05/17 17:00:26 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 333) (bb4563f4559d, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:26,432] {docker.py:276} INFO - 21/05/17 17:00:26 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 329) in 2332 ms on bb4563f4559d (executor driver) (145/200)
21/05/17 17:00:26 INFO Executor: Running task 148.0 in stage 4.0 (TID 333)
[2021-05-17 14:00:26,440] {docker.py:276} INFO - 21/05/17 17:00:26 INFO ShuffleBlockFetcherIterator: Getting 4 (26.5 KiB) non-empty blocks including 4 (26.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:26,441] {docker.py:276} INFO - 21/05/17 17:00:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442558209739645899724_0004_m_000148_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442558209739645899724_0004_m_000148_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442558209739645899724_0004}; taskId=attempt_202105171658442558209739645899724_0004_m_000148_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76361351}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:26,442] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Starting: Task committer attempt_202105171658442558209739645899724_0004_m_000148_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442558209739645899724_0004_m_000148_333
[2021-05-17 14:00:26,444] {docker.py:276} INFO - 21/05/17 17:00:26 INFO StagingCommitter: Task committer attempt_202105171658442558209739645899724_0004_m_000148_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442558209739645899724_0004_m_000148_333 : duration 0:00.002s
[2021-05-17 14:00:28,223] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658446166795194710339912_0004_m_000145_330: needsTaskCommit() Task attempt_202105171658446166795194710339912_0004_m_000145_330
[2021-05-17 14:00:28,224] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Task committer attempt_202105171658446166795194710339912_0004_m_000145_330: needsTaskCommit() Task attempt_202105171658446166795194710339912_0004_m_000145_330: duration 0:00.001s
[2021-05-17 14:00:28,225] {docker.py:276} INFO - 21/05/17 17:00:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446166795194710339912_0004_m_000145_330
[2021-05-17 14:00:28,227] {docker.py:276} INFO - 21/05/17 17:00:28 INFO Executor: Finished task 145.0 in stage 4.0 (TID 330). 4587 bytes result sent to driver
[2021-05-17 14:00:28,228] {docker.py:276} INFO - 21/05/17 17:00:28 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 334) (bb4563f4559d, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:28,229] {docker.py:276} INFO - 21/05/17 17:00:28 INFO Executor: Running task 149.0 in stage 4.0 (TID 334)
21/05/17 17:00:28 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 330) in 2543 ms on bb4563f4559d (executor driver) (146/200)
[2021-05-17 14:00:28,239] {docker.py:276} INFO - 21/05/17 17:00:28 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:28,241] {docker.py:276} INFO - 21/05/17 17:00:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446164254534067363139_0004_m_000149_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446164254534067363139_0004_m_000149_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446164254534067363139_0004}; taskId=attempt_202105171658446164254534067363139_0004_m_000149_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75522f9d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658446164254534067363139_0004_m_000149_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446164254534067363139_0004_m_000149_334
[2021-05-17 14:00:28,244] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Task committer attempt_202105171658446164254534067363139_0004_m_000149_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446164254534067363139_0004_m_000149_334 : duration 0:00.003s
[2021-05-17 14:00:28,811] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658446191607610525058746_0004_m_000146_331: needsTaskCommit() Task attempt_202105171658446191607610525058746_0004_m_000146_331
[2021-05-17 14:00:28,817] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Task committer attempt_202105171658446191607610525058746_0004_m_000146_331: needsTaskCommit() Task attempt_202105171658446191607610525058746_0004_m_000146_331: duration 0:00.000s
21/05/17 17:00:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446191607610525058746_0004_m_000146_331
[2021-05-17 14:00:28,818] {docker.py:276} INFO - 21/05/17 17:00:28 INFO Executor: Finished task 146.0 in stage 4.0 (TID 331). 4587 bytes result sent to driver
[2021-05-17 14:00:28,818] {docker.py:276} INFO - 21/05/17 17:00:28 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 335) (bb4563f4559d, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:28,818] {docker.py:276} INFO - 21/05/17 17:00:28 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 331) in 2753 ms on bb4563f4559d (executor driver) (147/200)
[2021-05-17 14:00:28,819] {docker.py:276} INFO - 21/05/17 17:00:28 INFO Executor: Running task 150.0 in stage 4.0 (TID 335)
[2021-05-17 14:00:28,824] {docker.py:276} INFO - 21/05/17 17:00:28 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:28,824] {docker.py:276} INFO - 21/05/17 17:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:28,826] {docker.py:276} INFO - 21/05/17 17:00:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:28,845] {docker.py:276} INFO - 21/05/17 17:00:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:28,846] {docker.py:276} INFO - 21/05/17 17:00:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:28,846] {docker.py:276} INFO - 21/05/17 17:00:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844617168155291449189_0004_m_000150_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844617168155291449189_0004_m_000150_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844617168155291449189_0004}; taskId=attempt_20210517165844617168155291449189_0004_m_000150_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@382c4a8d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:28,846] {docker.py:276} INFO - 21/05/17 17:00:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:28 INFO StagingCommitter: Starting: Task committer attempt_20210517165844617168155291449189_0004_m_000150_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844617168155291449189_0004_m_000150_335
[2021-05-17 14:00:28,847] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Task committer attempt_20210517165844617168155291449189_0004_m_000150_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844617168155291449189_0004_m_000150_335 : duration 0:00.003s
[2021-05-17 14:00:28,907] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658442558209739645899724_0004_m_000148_333: needsTaskCommit() Task attempt_202105171658442558209739645899724_0004_m_000148_333
[2021-05-17 14:00:28,925] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Task committer attempt_202105171658442558209739645899724_0004_m_000148_333: needsTaskCommit() Task attempt_202105171658442558209739645899724_0004_m_000148_333: duration 0:00.001s
21/05/17 17:00:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442558209739645899724_0004_m_000148_333
[2021-05-17 14:00:28,926] {docker.py:276} INFO - 21/05/17 17:00:28 INFO Executor: Finished task 148.0 in stage 4.0 (TID 333). 4544 bytes result sent to driver
[2021-05-17 14:00:28,926] {docker.py:276} INFO - 21/05/17 17:00:28 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 336) (bb4563f4559d, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:28,926] {docker.py:276} INFO - 21/05/17 17:00:28 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 333) in 2482 ms on bb4563f4559d (executor driver) (148/200)
[2021-05-17 14:00:28,927] {docker.py:276} INFO - 21/05/17 17:00:28 INFO Executor: Running task 151.0 in stage 4.0 (TID 336)
[2021-05-17 14:00:28,927] {docker.py:276} INFO - 21/05/17 17:00:28 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:28,927] {docker.py:276} INFO - 21/05/17 17:00:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447006908154205042431_0004_m_000151_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447006908154205042431_0004_m_000151_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447006908154205042431_0004}; taskId=attempt_202105171658447006908154205042431_0004_m_000151_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@365731da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:28 INFO StagingCommitter: Starting: Task committer attempt_202105171658447006908154205042431_0004_m_000151_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447006908154205042431_0004_m_000151_336
[2021-05-17 14:00:28,952] {docker.py:276} INFO - 21/05/17 17:00:28 INFO StagingCommitter: Task committer attempt_202105171658447006908154205042431_0004_m_000151_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447006908154205042431_0004_m_000151_336 : duration 0:00.003s
[2021-05-17 14:00:28,995] {docker.py:276} INFO - 21/05/17 17:00:29 INFO StagingCommitter: Starting: Task committer attempt_202105171658448901295855151643132_0004_m_000147_332: needsTaskCommit() Task attempt_202105171658448901295855151643132_0004_m_000147_332
[2021-05-17 14:00:29,009] {docker.py:276} INFO - 21/05/17 17:00:29 INFO StagingCommitter: Task committer attempt_202105171658448901295855151643132_0004_m_000147_332: needsTaskCommit() Task attempt_202105171658448901295855151643132_0004_m_000147_332: duration 0:00.001s
21/05/17 17:00:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448901295855151643132_0004_m_000147_332
[2021-05-17 14:00:29,010] {docker.py:276} INFO - 21/05/17 17:00:29 INFO Executor: Finished task 147.0 in stage 4.0 (TID 332). 4544 bytes result sent to driver
[2021-05-17 14:00:29,010] {docker.py:276} INFO - 21/05/17 17:00:29 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 337) (bb4563f4559d, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:29,010] {docker.py:276} INFO - 21/05/17 17:00:29 INFO Executor: Running task 152.0 in stage 4.0 (TID 337)
[2021-05-17 14:00:29,011] {docker.py:276} INFO - 21/05/17 17:00:29 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 332) in 2599 ms on bb4563f4559d (executor driver) (149/200)
[2021-05-17 14:00:29,011] {docker.py:276} INFO - 21/05/17 17:00:29 INFO ShuffleBlockFetcherIterator: Getting 4 (25.8 KiB) non-empty blocks including 4 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:29,011] {docker.py:276} INFO - 21/05/17 17:00:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:29,012] {docker.py:276} INFO - 21/05/17 17:00:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444211588461341295466_0004_m_000152_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444211588461341295466_0004_m_000152_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444211588461341295466_0004}; taskId=attempt_202105171658444211588461341295466_0004_m_000152_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29a66bac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:29 INFO StagingCommitter: Starting: Task committer attempt_202105171658444211588461341295466_0004_m_000152_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444211588461341295466_0004_m_000152_337
[2021-05-17 14:00:29,015] {docker.py:276} INFO - 21/05/17 17:00:29 INFO StagingCommitter: Task committer attempt_202105171658444211588461341295466_0004_m_000152_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444211588461341295466_0004_m_000152_337 : duration 0:00.002s
[2021-05-17 14:00:30,694] {docker.py:276} INFO - 21/05/17 17:00:30 INFO StagingCommitter: Starting: Task committer attempt_202105171658446164254534067363139_0004_m_000149_334: needsTaskCommit() Task attempt_202105171658446164254534067363139_0004_m_000149_334
[2021-05-17 14:00:30,695] {docker.py:276} INFO - 21/05/17 17:00:30 INFO StagingCommitter: Task committer attempt_202105171658446164254534067363139_0004_m_000149_334: needsTaskCommit() Task attempt_202105171658446164254534067363139_0004_m_000149_334: duration 0:00.001s
[2021-05-17 14:00:30,695] {docker.py:276} INFO - 21/05/17 17:00:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446164254534067363139_0004_m_000149_334
[2021-05-17 14:00:30,697] {docker.py:276} INFO - 21/05/17 17:00:30 INFO Executor: Finished task 149.0 in stage 4.0 (TID 334). 4544 bytes result sent to driver
[2021-05-17 14:00:30,698] {docker.py:276} INFO - 21/05/17 17:00:30 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 338) (bb4563f4559d, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:30,699] {docker.py:276} INFO - 21/05/17 17:00:30 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 334) in 2475 ms on bb4563f4559d (executor driver) (150/200)
[2021-05-17 14:00:30,700] {docker.py:276} INFO - 21/05/17 17:00:30 INFO Executor: Running task 153.0 in stage 4.0 (TID 338)
[2021-05-17 14:00:30,706] {docker.py:276} INFO - 21/05/17 17:00:30 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:30,709] {docker.py:276} INFO - 21/05/17 17:00:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445783693166151884103_0004_m_000153_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445783693166151884103_0004_m_000153_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445783693166151884103_0004}; taskId=attempt_202105171658445783693166151884103_0004_m_000153_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@573ec920}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:30,709] {docker.py:276} INFO - 21/05/17 17:00:30 INFO StagingCommitter: Starting: Task committer attempt_202105171658445783693166151884103_0004_m_000153_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445783693166151884103_0004_m_000153_338
[2021-05-17 14:00:30,712] {docker.py:276} INFO - 21/05/17 17:00:30 INFO StagingCommitter: Task committer attempt_202105171658445783693166151884103_0004_m_000153_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445783693166151884103_0004_m_000153_338 : duration 0:00.003s
[2021-05-17 14:00:31,341] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658447006908154205042431_0004_m_000151_336: needsTaskCommit() Task attempt_202105171658447006908154205042431_0004_m_000151_336
[2021-05-17 14:00:31,342] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Task committer attempt_202105171658447006908154205042431_0004_m_000151_336: needsTaskCommit() Task attempt_202105171658447006908154205042431_0004_m_000151_336: duration 0:00.001s
21/05/17 17:00:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447006908154205042431_0004_m_000151_336
[2021-05-17 14:00:31,344] {docker.py:276} INFO - 21/05/17 17:00:31 INFO Executor: Finished task 151.0 in stage 4.0 (TID 336). 4544 bytes result sent to driver
[2021-05-17 14:00:31,345] {docker.py:276} INFO - 21/05/17 17:00:31 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 339) (bb4563f4559d, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:31,346] {docker.py:276} INFO - 21/05/17 17:00:31 INFO Executor: Running task 154.0 in stage 4.0 (TID 339)
[2021-05-17 14:00:31,347] {docker.py:276} INFO - 21/05/17 17:00:31 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 336) in 2440 ms on bb4563f4559d (executor driver) (151/200)
[2021-05-17 14:00:31,356] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Starting: Task committer attempt_20210517165844617168155291449189_0004_m_000150_335: needsTaskCommit() Task attempt_20210517165844617168155291449189_0004_m_000150_335
[2021-05-17 14:00:31,356] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Task committer attempt_20210517165844617168155291449189_0004_m_000150_335: needsTaskCommit() Task attempt_20210517165844617168155291449189_0004_m_000150_335: duration 0:00.000s
21/05/17 17:00:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844617168155291449189_0004_m_000150_335
[2021-05-17 14:00:31,358] {docker.py:276} INFO - 21/05/17 17:00:31 INFO Executor: Finished task 150.0 in stage 4.0 (TID 335). 4544 bytes result sent to driver
[2021-05-17 14:00:31,359] {docker.py:276} INFO - 21/05/17 17:00:31 INFO ShuffleBlockFetcherIterator: Getting 4 (25.2 KiB) non-empty blocks including 4 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:31,360] {docker.py:276} INFO - 21/05/17 17:00:31 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 340) (bb4563f4559d, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:31,361] {docker.py:276} INFO - 21/05/17 17:00:31 INFO Executor: Running task 155.0 in stage 4.0 (TID 340)
21/05/17 17:00:31 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 335) in 2550 ms on bb4563f4559d (executor driver) (152/200)
[2021-05-17 14:00:31,362] {docker.py:276} INFO - 21/05/17 17:00:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:31,362] {docker.py:276} INFO - 21/05/17 17:00:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:31,363] {docker.py:276} INFO - 21/05/17 17:00:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447537876316900828242_0004_m_000154_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447537876316900828242_0004_m_000154_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447537876316900828242_0004}; taskId=attempt_202105171658447537876316900828242_0004_m_000154_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9a56f45}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:31,363] {docker.py:276} INFO - 21/05/17 17:00:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:31,363] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658447537876316900828242_0004_m_000154_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447537876316900828242_0004_m_000154_339
[2021-05-17 14:00:31,366] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Task committer attempt_202105171658447537876316900828242_0004_m_000154_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447537876316900828242_0004_m_000154_339 : duration 0:00.003s
[2021-05-17 14:00:31,369] {docker.py:276} INFO - 21/05/17 17:00:31 INFO ShuffleBlockFetcherIterator: Getting 4 (26.6 KiB) non-empty blocks including 4 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:31,369] {docker.py:276} INFO - 21/05/17 17:00:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:31,370] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658444211588461341295466_0004_m_000152_337: needsTaskCommit() Task attempt_202105171658444211588461341295466_0004_m_000152_337
[2021-05-17 14:00:31,370] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Task committer attempt_202105171658444211588461341295466_0004_m_000152_337: needsTaskCommit() Task attempt_202105171658444211588461341295466_0004_m_000152_337: duration 0:00.000s
21/05/17 17:00:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444211588461341295466_0004_m_000152_337
[2021-05-17 14:00:31,371] {docker.py:276} INFO - 21/05/17 17:00:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:31,372] {docker.py:276} INFO - 21/05/17 17:00:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444537098780882848447_0004_m_000155_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444537098780882848447_0004_m_000155_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444537098780882848447_0004}; taskId=attempt_202105171658444537098780882848447_0004_m_000155_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b7a9f97}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658444537098780882848447_0004_m_000155_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444537098780882848447_0004_m_000155_340
[2021-05-17 14:00:31,375] {docker.py:276} INFO - 21/05/17 17:00:31 INFO Executor: Finished task 152.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
[2021-05-17 14:00:31,376] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Task committer attempt_202105171658444537098780882848447_0004_m_000155_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444537098780882848447_0004_m_000155_340 : duration 0:00.003s
[2021-05-17 14:00:31,379] {docker.py:276} INFO - 21/05/17 17:00:31 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 341) (bb4563f4559d, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:31,380] {docker.py:276} INFO - 21/05/17 17:00:31 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 337) in 2383 ms on bb4563f4559d (executor driver) (153/200)
[2021-05-17 14:00:31,381] {docker.py:276} INFO - 21/05/17 17:00:31 INFO Executor: Running task 156.0 in stage 4.0 (TID 341)
[2021-05-17 14:00:31,402] {docker.py:276} INFO - 21/05/17 17:00:31 INFO ShuffleBlockFetcherIterator: Getting 4 (28.3 KiB) non-empty blocks including 4 (28.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:31,404] {docker.py:276} INFO - 21/05/17 17:00:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:31,404] {docker.py:276} INFO - 21/05/17 17:00:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441810603624102125875_0004_m_000156_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441810603624102125875_0004_m_000156_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441810603624102125875_0004}; taskId=attempt_202105171658441810603624102125875_0004_m_000156_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f2aa478}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:31 INFO StagingCommitter: Starting: Task committer attempt_202105171658441810603624102125875_0004_m_000156_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441810603624102125875_0004_m_000156_341
[2021-05-17 14:00:31,407] {docker.py:276} INFO - 21/05/17 17:00:31 INFO StagingCommitter: Task committer attempt_202105171658441810603624102125875_0004_m_000156_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441810603624102125875_0004_m_000156_341 : duration 0:00.003s
[2021-05-17 14:00:33,718] {docker.py:276} INFO - 21/05/17 17:00:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658444537098780882848447_0004_m_000155_340: needsTaskCommit() Task attempt_202105171658444537098780882848447_0004_m_000155_340
21/05/17 17:00:33 INFO StagingCommitter: Task committer attempt_202105171658444537098780882848447_0004_m_000155_340: needsTaskCommit() Task attempt_202105171658444537098780882848447_0004_m_000155_340: duration 0:00.000s
21/05/17 17:00:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444537098780882848447_0004_m_000155_340
[2021-05-17 14:00:33,719] {docker.py:276} INFO - 21/05/17 17:00:33 INFO Executor: Finished task 155.0 in stage 4.0 (TID 340). 4587 bytes result sent to driver
[2021-05-17 14:00:33,720] {docker.py:276} INFO - 21/05/17 17:00:33 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 342) (bb4563f4559d, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:33,720] {docker.py:276} INFO - 21/05/17 17:00:33 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 340) in 2365 ms on bb4563f4559d (executor driver) (154/200)
21/05/17 17:00:33 INFO Executor: Running task 157.0 in stage 4.0 (TID 342)
[2021-05-17 14:00:33,728] {docker.py:276} INFO - 21/05/17 17:00:33 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:33,729] {docker.py:276} INFO - 21/05/17 17:00:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844182764612614284718_0004_m_000157_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844182764612614284718_0004_m_000157_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844182764612614284718_0004}; taskId=attempt_20210517165844182764612614284718_0004_m_000157_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@474fecb8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:33 INFO StagingCommitter: Starting: Task committer attempt_20210517165844182764612614284718_0004_m_000157_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844182764612614284718_0004_m_000157_342
[2021-05-17 14:00:33,732] {docker.py:276} INFO - 21/05/17 17:00:33 INFO StagingCommitter: Task committer attempt_20210517165844182764612614284718_0004_m_000157_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844182764612614284718_0004_m_000157_342 : duration 0:00.002s
[2021-05-17 14:00:33,762] {docker.py:276} INFO - 21/05/17 17:00:33 INFO StagingCommitter: Starting: Task committer attempt_202105171658447537876316900828242_0004_m_000154_339: needsTaskCommit() Task attempt_202105171658447537876316900828242_0004_m_000154_339
21/05/17 17:00:33 INFO StagingCommitter: Task committer attempt_202105171658447537876316900828242_0004_m_000154_339: needsTaskCommit() Task attempt_202105171658447537876316900828242_0004_m_000154_339: duration 0:00.000s
21/05/17 17:00:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447537876316900828242_0004_m_000154_339
[2021-05-17 14:00:33,764] {docker.py:276} INFO - 21/05/17 17:00:33 INFO Executor: Finished task 154.0 in stage 4.0 (TID 339). 4587 bytes result sent to driver
[2021-05-17 14:00:33,765] {docker.py:276} INFO - 21/05/17 17:00:33 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 343) (bb4563f4559d, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:33,765] {docker.py:276} INFO - 21/05/17 17:00:33 INFO Executor: Running task 158.0 in stage 4.0 (TID 343)
[2021-05-17 14:00:33,766] {docker.py:276} INFO - 21/05/17 17:00:33 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 339) in 2424 ms on bb4563f4559d (executor driver) (155/200)
[2021-05-17 14:00:33,773] {docker.py:276} INFO - 21/05/17 17:00:33 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:33,774] {docker.py:276} INFO - 21/05/17 17:00:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844141963600155394679_0004_m_000158_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844141963600155394679_0004_m_000158_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844141963600155394679_0004}; taskId=attempt_20210517165844141963600155394679_0004_m_000158_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10e88b59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:33,775] {docker.py:276} INFO - 21/05/17 17:00:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:33 INFO StagingCommitter: Starting: Task committer attempt_20210517165844141963600155394679_0004_m_000158_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844141963600155394679_0004_m_000158_343
[2021-05-17 14:00:33,777] {docker.py:276} INFO - 21/05/17 17:00:33 INFO StagingCommitter: Task committer attempt_20210517165844141963600155394679_0004_m_000158_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844141963600155394679_0004_m_000158_343 : duration 0:00.003s
[2021-05-17 14:00:34,040] {docker.py:276} INFO - 21/05/17 17:00:34 INFO StagingCommitter: Starting: Task committer attempt_202105171658441810603624102125875_0004_m_000156_341: needsTaskCommit() Task attempt_202105171658441810603624102125875_0004_m_000156_341
[2021-05-17 14:00:34,041] {docker.py:276} INFO - 21/05/17 17:00:34 INFO StagingCommitter: Task committer attempt_202105171658441810603624102125875_0004_m_000156_341: needsTaskCommit() Task attempt_202105171658441810603624102125875_0004_m_000156_341: duration 0:00.001s
21/05/17 17:00:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441810603624102125875_0004_m_000156_341
[2021-05-17 14:00:34,043] {docker.py:276} INFO - 21/05/17 17:00:34 INFO Executor: Finished task 156.0 in stage 4.0 (TID 341). 4587 bytes result sent to driver
[2021-05-17 14:00:34,044] {docker.py:276} INFO - 21/05/17 17:00:34 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 344) (bb4563f4559d, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:34,045] {docker.py:276} INFO - 21/05/17 17:00:34 INFO Executor: Running task 159.0 in stage 4.0 (TID 344)
[2021-05-17 14:00:34,046] {docker.py:276} INFO - 21/05/17 17:00:34 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 341) in 2673 ms on bb4563f4559d (executor driver) (156/200)
[2021-05-17 14:00:34,057] {docker.py:276} INFO - 21/05/17 17:00:34 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:34,059] {docker.py:276} INFO - 21/05/17 17:00:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448693177388855294595_0004_m_000159_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448693177388855294595_0004_m_000159_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448693177388855294595_0004}; taskId=attempt_202105171658448693177388855294595_0004_m_000159_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6882d6b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:34 INFO StagingCommitter: Starting: Task committer attempt_202105171658448693177388855294595_0004_m_000159_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448693177388855294595_0004_m_000159_344
[2021-05-17 14:00:34,062] {docker.py:276} INFO - 21/05/17 17:00:34 INFO StagingCommitter: Task committer attempt_202105171658448693177388855294595_0004_m_000159_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448693177388855294595_0004_m_000159_344 : duration 0:00.004s
[2021-05-17 14:00:34,308] {docker.py:276} INFO - 21/05/17 17:00:34 INFO StagingCommitter: Starting: Task committer attempt_202105171658445783693166151884103_0004_m_000153_338: needsTaskCommit() Task attempt_202105171658445783693166151884103_0004_m_000153_338
[2021-05-17 14:00:34,309] {docker.py:276} INFO - 21/05/17 17:00:34 INFO StagingCommitter: Task committer attempt_202105171658445783693166151884103_0004_m_000153_338: needsTaskCommit() Task attempt_202105171658445783693166151884103_0004_m_000153_338: duration 0:00.000s
21/05/17 17:00:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445783693166151884103_0004_m_000153_338
[2021-05-17 14:00:34,310] {docker.py:276} INFO - 21/05/17 17:00:34 INFO Executor: Finished task 153.0 in stage 4.0 (TID 338). 4587 bytes result sent to driver
[2021-05-17 14:00:34,311] {docker.py:276} INFO - 21/05/17 17:00:34 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 345) (bb4563f4559d, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:34,312] {docker.py:276} INFO - 21/05/17 17:00:34 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 338) in 3619 ms on bb4563f4559d (executor driver) (157/200)
[2021-05-17 14:00:34,312] {docker.py:276} INFO - 21/05/17 17:00:34 INFO Executor: Running task 160.0 in stage 4.0 (TID 345)
[2021-05-17 14:00:34,320] {docker.py:276} INFO - 21/05/17 17:00:34 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:34,322] {docker.py:276} INFO - 21/05/17 17:00:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:34,323] {docker.py:276} INFO - 21/05/17 17:00:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446980449460668592949_0004_m_000160_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446980449460668592949_0004_m_000160_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446980449460668592949_0004}; taskId=attempt_202105171658446980449460668592949_0004_m_000160_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1696bf33}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:34,323] {docker.py:276} INFO - 21/05/17 17:00:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:34 INFO StagingCommitter: Starting: Task committer attempt_202105171658446980449460668592949_0004_m_000160_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446980449460668592949_0004_m_000160_345
[2021-05-17 14:00:34,326] {docker.py:276} INFO - 21/05/17 17:00:34 INFO StagingCommitter: Task committer attempt_202105171658446980449460668592949_0004_m_000160_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446980449460668592949_0004_m_000160_345 : duration 0:00.003s
[2021-05-17 14:00:36,157] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_20210517165844182764612614284718_0004_m_000157_342: needsTaskCommit() Task attempt_20210517165844182764612614284718_0004_m_000157_342
[2021-05-17 14:00:36,158] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_20210517165844182764612614284718_0004_m_000157_342: needsTaskCommit() Task attempt_20210517165844182764612614284718_0004_m_000157_342: duration 0:00.001s
21/05/17 17:00:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844182764612614284718_0004_m_000157_342
[2021-05-17 14:00:36,159] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Finished task 157.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-17 14:00:36,161] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 346) (bb4563f4559d, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:36,162] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Running task 161.0 in stage 4.0 (TID 346)
21/05/17 17:00:36 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 342) in 2445 ms on bb4563f4559d (executor driver) (158/200)
[2021-05-17 14:00:36,172] {docker.py:276} INFO - 21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:36,174] {docker.py:276} INFO - 21/05/17 17:00:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442628301141453869830_0004_m_000161_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442628301141453869830_0004_m_000161_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442628301141453869830_0004}; taskId=attempt_202105171658442628301141453869830_0004_m_000161_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@292c26ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:36,174] {docker.py:276} INFO - 21/05/17 17:00:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:36,175] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658442628301141453869830_0004_m_000161_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442628301141453869830_0004_m_000161_346
[2021-05-17 14:00:36,177] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_202105171658442628301141453869830_0004_m_000161_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442628301141453869830_0004_m_000161_346 : duration 0:00.004s
[2021-05-17 14:00:36,308] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_20210517165844141963600155394679_0004_m_000158_343: needsTaskCommit() Task attempt_20210517165844141963600155394679_0004_m_000158_343
[2021-05-17 14:00:36,309] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_20210517165844141963600155394679_0004_m_000158_343: needsTaskCommit() Task attempt_20210517165844141963600155394679_0004_m_000158_343: duration 0:00.001s
21/05/17 17:00:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844141963600155394679_0004_m_000158_343
[2021-05-17 14:00:36,310] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Finished task 158.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-17 14:00:36,311] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 347) (bb4563f4559d, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:36,312] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 343) in 2550 ms on bb4563f4559d (executor driver) (159/200)
[2021-05-17 14:00:36,313] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Running task 162.0 in stage 4.0 (TID 347)
[2021-05-17 14:00:36,322] {docker.py:276} INFO - 21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Getting 4 (26.6 KiB) non-empty blocks including 4 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:36,325] {docker.py:276} INFO - 21/05/17 17:00:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:36,325] {docker.py:276} INFO - 21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442803668665327569095_0004_m_000162_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442803668665327569095_0004_m_000162_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442803668665327569095_0004}; taskId=attempt_202105171658442803668665327569095_0004_m_000162_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5754ca0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658442803668665327569095_0004_m_000162_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442803668665327569095_0004_m_000162_347
[2021-05-17 14:00:36,328] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_202105171658442803668665327569095_0004_m_000162_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442803668665327569095_0004_m_000162_347 : duration 0:00.003s
[2021-05-17 14:00:36,338] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658446980449460668592949_0004_m_000160_345: needsTaskCommit() Task attempt_202105171658446980449460668592949_0004_m_000160_345
[2021-05-17 14:00:36,338] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_202105171658446980449460668592949_0004_m_000160_345: needsTaskCommit() Task attempt_202105171658446980449460668592949_0004_m_000160_345: duration 0:00.000s
[2021-05-17 14:00:36,339] {docker.py:276} INFO - 21/05/17 17:00:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446980449460668592949_0004_m_000160_345
[2021-05-17 14:00:36,339] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Finished task 160.0 in stage 4.0 (TID 345). 4544 bytes result sent to driver
[2021-05-17 14:00:36,340] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 348) (bb4563f4559d, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:36,341] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 345) in 2032 ms on bb4563f4559d (executor driver) (160/200)
[2021-05-17 14:00:36,341] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Running task 163.0 in stage 4.0 (TID 348)
[2021-05-17 14:00:36,349] {docker.py:276} INFO - 21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:36,349] {docker.py:276} INFO - 21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:36,351] {docker.py:276} INFO - 21/05/17 17:00:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:36,352] {docker.py:276} INFO - 21/05/17 17:00:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:36,352] {docker.py:276} INFO - 21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:36,353] {docker.py:276} INFO - 21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442298820275711003736_0004_m_000163_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442298820275711003736_0004_m_000163_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442298820275711003736_0004}; taskId=attempt_202105171658442298820275711003736_0004_m_000163_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13b80d61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:36,353] {docker.py:276} INFO - 21/05/17 17:00:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:36,353] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658442298820275711003736_0004_m_000163_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442298820275711003736_0004_m_000163_348
[2021-05-17 14:00:36,356] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_202105171658442298820275711003736_0004_m_000163_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442298820275711003736_0004_m_000163_348 : duration 0:00.004s
[2021-05-17 14:00:36,572] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658448693177388855294595_0004_m_000159_344: needsTaskCommit() Task attempt_202105171658448693177388855294595_0004_m_000159_344
[2021-05-17 14:00:36,573] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_202105171658448693177388855294595_0004_m_000159_344: needsTaskCommit() Task attempt_202105171658448693177388855294595_0004_m_000159_344: duration 0:00.000s
21/05/17 17:00:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448693177388855294595_0004_m_000159_344
[2021-05-17 14:00:36,573] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Finished task 159.0 in stage 4.0 (TID 344). 4544 bytes result sent to driver
[2021-05-17 14:00:36,574] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 349) (bb4563f4559d, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:36,575] {docker.py:276} INFO - 21/05/17 17:00:36 INFO Executor: Running task 164.0 in stage 4.0 (TID 349)
[2021-05-17 14:00:36,575] {docker.py:276} INFO - 21/05/17 17:00:36 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 344) in 2535 ms on bb4563f4559d (executor driver) (161/200)
[2021-05-17 14:00:36,584] {docker.py:276} INFO - 21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:36,586] {docker.py:276} INFO - 21/05/17 17:00:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442114484232931985407_0004_m_000164_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442114484232931985407_0004_m_000164_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442114484232931985407_0004}; taskId=attempt_202105171658442114484232931985407_0004_m_000164_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a073a8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:36,586] {docker.py:276} INFO - 21/05/17 17:00:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:36 INFO StagingCommitter: Starting: Task committer attempt_202105171658442114484232931985407_0004_m_000164_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442114484232931985407_0004_m_000164_349
[2021-05-17 14:00:36,589] {docker.py:276} INFO - 21/05/17 17:00:36 INFO StagingCommitter: Task committer attempt_202105171658442114484232931985407_0004_m_000164_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442114484232931985407_0004_m_000164_349 : duration 0:00.002s
[2021-05-17 14:00:38,731] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658442114484232931985407_0004_m_000164_349: needsTaskCommit() Task attempt_202105171658442114484232931985407_0004_m_000164_349
[2021-05-17 14:00:38,732] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Task committer attempt_202105171658442114484232931985407_0004_m_000164_349: needsTaskCommit() Task attempt_202105171658442114484232931985407_0004_m_000164_349: duration 0:00.000s
21/05/17 17:00:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442114484232931985407_0004_m_000164_349
[2021-05-17 14:00:38,734] {docker.py:276} INFO - 21/05/17 17:00:38 INFO Executor: Finished task 164.0 in stage 4.0 (TID 349). 4544 bytes result sent to driver
[2021-05-17 14:00:38,735] {docker.py:276} INFO - 21/05/17 17:00:38 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 350) (bb4563f4559d, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:38,736] {docker.py:276} INFO - 21/05/17 17:00:38 INFO Executor: Running task 165.0 in stage 4.0 (TID 350)
[2021-05-17 14:00:38,737] {docker.py:276} INFO - 21/05/17 17:00:38 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 349) in 2164 ms on bb4563f4559d (executor driver) (162/200)
[2021-05-17 14:00:38,756] {docker.py:276} INFO - 21/05/17 17:00:38 INFO ShuffleBlockFetcherIterator: Getting 4 (27.8 KiB) non-empty blocks including 4 (27.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:38,758] {docker.py:276} INFO - 21/05/17 17:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448862414402636403671_0004_m_000165_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448862414402636403671_0004_m_000165_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448862414402636403671_0004}; taskId=attempt_202105171658448862414402636403671_0004_m_000165_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d097fe5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658448862414402636403671_0004_m_000165_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448862414402636403671_0004_m_000165_350
[2021-05-17 14:00:38,760] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Task committer attempt_202105171658448862414402636403671_0004_m_000165_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448862414402636403671_0004_m_000165_350 : duration 0:00.002s
[2021-05-17 14:00:38,789] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658442628301141453869830_0004_m_000161_346: needsTaskCommit() Task attempt_202105171658442628301141453869830_0004_m_000161_346
[2021-05-17 14:00:38,790] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Task committer attempt_202105171658442628301141453869830_0004_m_000161_346: needsTaskCommit() Task attempt_202105171658442628301141453869830_0004_m_000161_346: duration 0:00.000s
21/05/17 17:00:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442628301141453869830_0004_m_000161_346
[2021-05-17 14:00:38,792] {docker.py:276} INFO - 21/05/17 17:00:38 INFO Executor: Finished task 161.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-17 14:00:38,793] {docker.py:276} INFO - 21/05/17 17:00:38 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 351) (bb4563f4559d, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:38,794] {docker.py:276} INFO - 21/05/17 17:00:38 INFO Executor: Running task 166.0 in stage 4.0 (TID 351)
[2021-05-17 14:00:38,795] {docker.py:276} INFO - 21/05/17 17:00:38 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 346) in 2637 ms on bb4563f4559d (executor driver) (163/200)
[2021-05-17 14:00:38,804] {docker.py:276} INFO - 21/05/17 17:00:38 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:38,808] {docker.py:276} INFO - 21/05/17 17:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:38,809] {docker.py:276} INFO - 21/05/17 17:00:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:38,810] {docker.py:276} INFO - 21/05/17 17:00:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448380835517269223280_0004_m_000166_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448380835517269223280_0004_m_000166_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448380835517269223280_0004}; taskId=attempt_202105171658448380835517269223280_0004_m_000166_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50de712a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658448380835517269223280_0004_m_000166_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448380835517269223280_0004_m_000166_351
[2021-05-17 14:00:38,813] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Task committer attempt_202105171658448380835517269223280_0004_m_000166_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448380835517269223280_0004_m_000166_351 : duration 0:00.003s
[2021-05-17 14:00:38,908] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658442803668665327569095_0004_m_000162_347: needsTaskCommit() Task attempt_202105171658442803668665327569095_0004_m_000162_347
[2021-05-17 14:00:38,909] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Task committer attempt_202105171658442803668665327569095_0004_m_000162_347: needsTaskCommit() Task attempt_202105171658442803668665327569095_0004_m_000162_347: duration 0:00.001s
21/05/17 17:00:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442803668665327569095_0004_m_000162_347
[2021-05-17 14:00:38,911] {docker.py:276} INFO - 21/05/17 17:00:38 INFO Executor: Finished task 162.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-17 14:00:38,912] {docker.py:276} INFO - 21/05/17 17:00:38 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 352) (bb4563f4559d, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:38,914] {docker.py:276} INFO - 21/05/17 17:00:38 INFO Executor: Running task 167.0 in stage 4.0 (TID 352)
21/05/17 17:00:38 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 347) in 2606 ms on bb4563f4559d (executor driver) (164/200)
[2021-05-17 14:00:38,924] {docker.py:276} INFO - 21/05/17 17:00:38 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:38,926] {docker.py:276} INFO - 21/05/17 17:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441112547237627313957_0004_m_000167_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441112547237627313957_0004_m_000167_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441112547237627313957_0004}; taskId=attempt_202105171658441112547237627313957_0004_m_000167_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dd06b43}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:38,927] {docker.py:276} INFO - 21/05/17 17:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:38 INFO StagingCommitter: Starting: Task committer attempt_202105171658441112547237627313957_0004_m_000167_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441112547237627313957_0004_m_000167_352
[2021-05-17 14:00:38,930] {docker.py:276} INFO - 21/05/17 17:00:38 INFO StagingCommitter: Task committer attempt_202105171658441112547237627313957_0004_m_000167_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441112547237627313957_0004_m_000167_352 : duration 0:00.003s
[2021-05-17 14:00:38,980] {docker.py:276} INFO - 21/05/17 17:00:39 INFO StagingCommitter: Starting: Task committer attempt_202105171658442298820275711003736_0004_m_000163_348: needsTaskCommit() Task attempt_202105171658442298820275711003736_0004_m_000163_348
[2021-05-17 14:00:38,980] {docker.py:276} INFO - 21/05/17 17:00:39 INFO StagingCommitter: Task committer attempt_202105171658442298820275711003736_0004_m_000163_348: needsTaskCommit() Task attempt_202105171658442298820275711003736_0004_m_000163_348: duration 0:00.000s
[2021-05-17 14:00:38,981] {docker.py:276} INFO - 21/05/17 17:00:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442298820275711003736_0004_m_000163_348
[2021-05-17 14:00:38,981] {docker.py:276} INFO - 21/05/17 17:00:39 INFO Executor: Finished task 163.0 in stage 4.0 (TID 348). 4587 bytes result sent to driver
[2021-05-17 14:00:38,982] {docker.py:276} INFO - 21/05/17 17:00:39 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 353) (bb4563f4559d, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:38,983] {docker.py:276} INFO - 21/05/17 17:00:39 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 348) in 2645 ms on bb4563f4559d (executor driver) (165/200)
[2021-05-17 14:00:38,984] {docker.py:276} INFO - 21/05/17 17:00:39 INFO Executor: Running task 168.0 in stage 4.0 (TID 353)
[2021-05-17 14:00:38,993] {docker.py:276} INFO - 21/05/17 17:00:39 INFO ShuffleBlockFetcherIterator: Getting 4 (29.0 KiB) non-empty blocks including 4 (29.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:38,994] {docker.py:276} INFO - 21/05/17 17:00:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:38,995] {docker.py:276} INFO - 21/05/17 17:00:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:38,996] {docker.py:276} INFO - 21/05/17 17:00:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444952793185820818648_0004_m_000168_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444952793185820818648_0004_m_000168_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444952793185820818648_0004}; taskId=attempt_202105171658444952793185820818648_0004_m_000168_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b30beec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:38,996] {docker.py:276} INFO - 21/05/17 17:00:39 INFO StagingCommitter: Starting: Task committer attempt_202105171658444952793185820818648_0004_m_000168_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444952793185820818648_0004_m_000168_353
[2021-05-17 14:00:38,999] {docker.py:276} INFO - 21/05/17 17:00:39 INFO StagingCommitter: Task committer attempt_202105171658444952793185820818648_0004_m_000168_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444952793185820818648_0004_m_000168_353 : duration 0:00.003s
[2021-05-17 14:00:41,245] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658448862414402636403671_0004_m_000165_350: needsTaskCommit() Task attempt_202105171658448862414402636403671_0004_m_000165_350
[2021-05-17 14:00:41,246] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658448862414402636403671_0004_m_000165_350: needsTaskCommit() Task attempt_202105171658448862414402636403671_0004_m_000165_350: duration 0:00.001s
21/05/17 17:00:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448862414402636403671_0004_m_000165_350
[2021-05-17 14:00:41,248] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Finished task 165.0 in stage 4.0 (TID 350). 4587 bytes result sent to driver
[2021-05-17 14:00:41,251] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 354) (bb4563f4559d, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:41,253] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 350) in 2521 ms on bb4563f4559d (executor driver) (166/200)
[2021-05-17 14:00:41,254] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Running task 169.0 in stage 4.0 (TID 354)
[2021-05-17 14:00:41,264] {docker.py:276} INFO - 21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:41,266] {docker.py:276} INFO - 21/05/17 17:00:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442221709388508458715_0004_m_000169_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442221709388508458715_0004_m_000169_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442221709388508458715_0004}; taskId=attempt_202105171658442221709388508458715_0004_m_000169_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4dcc54c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658442221709388508458715_0004_m_000169_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442221709388508458715_0004_m_000169_354
[2021-05-17 14:00:41,268] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658442221709388508458715_0004_m_000169_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442221709388508458715_0004_m_000169_354 : duration 0:00.002s
[2021-05-17 14:00:41,311] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658448380835517269223280_0004_m_000166_351: needsTaskCommit() Task attempt_202105171658448380835517269223280_0004_m_000166_351
21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658448380835517269223280_0004_m_000166_351: needsTaskCommit() Task attempt_202105171658448380835517269223280_0004_m_000166_351: duration 0:00.000s
21/05/17 17:00:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448380835517269223280_0004_m_000166_351
[2021-05-17 14:00:41,313] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Finished task 166.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-17 14:00:41,314] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 355) (bb4563f4559d, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:41,316] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 351) in 2526 ms on bb4563f4559d (executor driver) (167/200)
[2021-05-17 14:00:41,317] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Running task 170.0 in stage 4.0 (TID 355)
[2021-05-17 14:00:41,327] {docker.py:276} INFO - 21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Getting 4 (28.3 KiB) non-empty blocks including 4 (28.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:41,329] {docker.py:276} INFO - 21/05/17 17:00:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445460460514911838380_0004_m_000170_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445460460514911838380_0004_m_000170_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445460460514911838380_0004}; taskId=attempt_202105171658445460460514911838380_0004_m_000170_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b7b4b47}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658445460460514911838380_0004_m_000170_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445460460514911838380_0004_m_000170_355
[2021-05-17 14:00:41,332] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658445460460514911838380_0004_m_000170_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445460460514911838380_0004_m_000170_355 : duration 0:00.003s
[2021-05-17 14:00:41,349] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658441112547237627313957_0004_m_000167_352: needsTaskCommit() Task attempt_202105171658441112547237627313957_0004_m_000167_352
21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658441112547237627313957_0004_m_000167_352: needsTaskCommit() Task attempt_202105171658441112547237627313957_0004_m_000167_352: duration 0:00.001s
21/05/17 17:00:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441112547237627313957_0004_m_000167_352
[2021-05-17 14:00:41,351] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Finished task 167.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-17 14:00:41,351] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 356) (bb4563f4559d, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:41,352] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Running task 171.0 in stage 4.0 (TID 356)
[2021-05-17 14:00:41,353] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 352) in 2444 ms on bb4563f4559d (executor driver) (168/200)
[2021-05-17 14:00:41,360] {docker.py:276} INFO - 21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Getting 4 (29.1 KiB) non-empty blocks including 4 (29.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:41,362] {docker.py:276} INFO - 21/05/17 17:00:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:41,363] {docker.py:276} INFO - 21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443082289241117566706_0004_m_000171_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443082289241117566706_0004_m_000171_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443082289241117566706_0004}; taskId=attempt_202105171658443082289241117566706_0004_m_000171_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6067ffac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658443082289241117566706_0004_m_000171_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443082289241117566706_0004_m_000171_356
[2021-05-17 14:00:41,366] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658443082289241117566706_0004_m_000171_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443082289241117566706_0004_m_000171_356 : duration 0:00.003s
[2021-05-17 14:00:41,488] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_202105171658444952793185820818648_0004_m_000168_353: needsTaskCommit() Task attempt_202105171658444952793185820818648_0004_m_000168_353
[2021-05-17 14:00:41,489] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_202105171658444952793185820818648_0004_m_000168_353: needsTaskCommit() Task attempt_202105171658444952793185820818648_0004_m_000168_353: duration 0:00.001s
[2021-05-17 14:00:41,490] {docker.py:276} INFO - 21/05/17 17:00:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444952793185820818648_0004_m_000168_353
[2021-05-17 14:00:41,491] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Finished task 168.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-17 14:00:41,492] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 357) (bb4563f4559d, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:41,493] {docker.py:276} INFO - 21/05/17 17:00:41 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 353) in 2513 ms on bb4563f4559d (executor driver) (169/200)
[2021-05-17 14:00:41,493] {docker.py:276} INFO - 21/05/17 17:00:41 INFO Executor: Running task 172.0 in stage 4.0 (TID 357)
[2021-05-17 14:00:41,501] {docker.py:276} INFO - 21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:41,503] {docker.py:276} INFO - 21/05/17 17:00:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517165844649908150923789880_0004_m_000172_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844649908150923789880_0004_m_000172_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517165844649908150923789880_0004}; taskId=attempt_20210517165844649908150923789880_0004_m_000172_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f0d052b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:41,503] {docker.py:276} INFO - 21/05/17 17:00:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:41 INFO StagingCommitter: Starting: Task committer attempt_20210517165844649908150923789880_0004_m_000172_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844649908150923789880_0004_m_000172_357
[2021-05-17 14:00:41,506] {docker.py:276} INFO - 21/05/17 17:00:41 INFO StagingCommitter: Task committer attempt_20210517165844649908150923789880_0004_m_000172_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_20210517165844649908150923789880_0004_m_000172_357 : duration 0:00.004s
[2021-05-17 14:00:43,748] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658442221709388508458715_0004_m_000169_354: needsTaskCommit() Task attempt_202105171658442221709388508458715_0004_m_000169_354
[2021-05-17 14:00:43,749] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Task committer attempt_202105171658442221709388508458715_0004_m_000169_354: needsTaskCommit() Task attempt_202105171658442221709388508458715_0004_m_000169_354: duration 0:00.002s
21/05/17 17:00:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442221709388508458715_0004_m_000169_354
[2021-05-17 14:00:43,750] {docker.py:276} INFO - 21/05/17 17:00:43 INFO Executor: Finished task 169.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-17 14:00:43,751] {docker.py:276} INFO - 21/05/17 17:00:43 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 358) (bb4563f4559d, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:43,752] {docker.py:276} INFO - 21/05/17 17:00:43 INFO Executor: Running task 173.0 in stage 4.0 (TID 358)
[2021-05-17 14:00:43,753] {docker.py:276} INFO - 21/05/17 17:00:43 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 354) in 2472 ms on bb4563f4559d (executor driver) (170/200)
[2021-05-17 14:00:43,763] {docker.py:276} INFO - 21/05/17 17:00:43 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:43,765] {docker.py:276} INFO - 21/05/17 17:00:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444279481241905961847_0004_m_000173_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444279481241905961847_0004_m_000173_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444279481241905961847_0004}; taskId=attempt_202105171658444279481241905961847_0004_m_000173_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c0f4b28}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658444279481241905961847_0004_m_000173_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444279481241905961847_0004_m_000173_358
[2021-05-17 14:00:43,768] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Task committer attempt_202105171658444279481241905961847_0004_m_000173_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444279481241905961847_0004_m_000173_358 : duration 0:00.003s
[2021-05-17 14:00:43,837] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658443082289241117566706_0004_m_000171_356: needsTaskCommit() Task attempt_202105171658443082289241117566706_0004_m_000171_356
[2021-05-17 14:00:43,838] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Task committer attempt_202105171658443082289241117566706_0004_m_000171_356: needsTaskCommit() Task attempt_202105171658443082289241117566706_0004_m_000171_356: duration 0:00.001s
21/05/17 17:00:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443082289241117566706_0004_m_000171_356
[2021-05-17 14:00:43,840] {docker.py:276} INFO - 21/05/17 17:00:43 INFO Executor: Finished task 171.0 in stage 4.0 (TID 356). 4544 bytes result sent to driver
[2021-05-17 14:00:43,841] {docker.py:276} INFO - 21/05/17 17:00:43 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 359) (bb4563f4559d, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:43,842] {docker.py:276} INFO - 21/05/17 17:00:43 INFO Executor: Running task 174.0 in stage 4.0 (TID 359)
21/05/17 17:00:43 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 356) in 2459 ms on bb4563f4559d (executor driver) (171/200)
[2021-05-17 14:00:43,843] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658445460460514911838380_0004_m_000170_355: needsTaskCommit() Task attempt_202105171658445460460514911838380_0004_m_000170_355
[2021-05-17 14:00:43,844] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Task committer attempt_202105171658445460460514911838380_0004_m_000170_355: needsTaskCommit() Task attempt_202105171658445460460514911838380_0004_m_000170_355: duration 0:00.001s
[2021-05-17 14:00:43,845] {docker.py:276} INFO - 21/05/17 17:00:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445460460514911838380_0004_m_000170_355
[2021-05-17 14:00:43,845] {docker.py:276} INFO - 21/05/17 17:00:43 INFO Executor: Finished task 170.0 in stage 4.0 (TID 355). 4544 bytes result sent to driver
[2021-05-17 14:00:43,846] {docker.py:276} INFO - 21/05/17 17:00:43 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 360) (bb4563f4559d, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:43,847] {docker.py:276} INFO - 21/05/17 17:00:43 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 355) in 2500 ms on bb4563f4559d (executor driver) (172/200)
21/05/17 17:00:43 INFO Executor: Running task 175.0 in stage 4.0 (TID 360)
[2021-05-17 14:00:43,865] {docker.py:276} INFO - 21/05/17 17:00:43 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 17:00:43 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:43,866] {docker.py:276} INFO - 21/05/17 17:00:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:43,867] {docker.py:276} INFO - 21/05/17 17:00:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:43,867] {docker.py:276} INFO - 21/05/17 17:00:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658445588550722610698918_0004_m_000175_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445588550722610698918_0004_m_000175_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658445588550722610698918_0004}; taskId=attempt_202105171658445588550722610698918_0004_m_000175_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12412d6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:43,867] {docker.py:276} INFO - 21/05/17 17:00:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448445995864813832173_0004_m_000174_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448445995864813832173_0004_m_000174_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448445995864813832173_0004}; taskId=attempt_202105171658448445995864813832173_0004_m_000174_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@351656b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:43,868] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658448445995864813832173_0004_m_000174_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448445995864813832173_0004_m_000174_359 
21/05/17 17:00:43 INFO StagingCommitter: Starting: Task committer attempt_202105171658445588550722610698918_0004_m_000175_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445588550722610698918_0004_m_000175_360
[2021-05-17 14:00:43,870] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Task committer attempt_202105171658445588550722610698918_0004_m_000175_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658445588550722610698918_0004_m_000175_360 : duration 0:00.003s
[2021-05-17 14:00:43,872] {docker.py:276} INFO - 21/05/17 17:00:43 INFO StagingCommitter: Task committer attempt_202105171658448445995864813832173_0004_m_000174_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448445995864813832173_0004_m_000174_359 : duration 0:00.005s
[2021-05-17 14:00:44,239] {docker.py:276} INFO - 21/05/17 17:00:44 INFO StagingCommitter: Starting: Task committer attempt_20210517165844649908150923789880_0004_m_000172_357: needsTaskCommit() Task attempt_20210517165844649908150923789880_0004_m_000172_357
[2021-05-17 14:00:44,240] {docker.py:276} INFO - 21/05/17 17:00:44 INFO StagingCommitter: Task committer attempt_20210517165844649908150923789880_0004_m_000172_357: needsTaskCommit() Task attempt_20210517165844649908150923789880_0004_m_000172_357: duration 0:00.001s
21/05/17 17:00:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517165844649908150923789880_0004_m_000172_357
[2021-05-17 14:00:44,241] {docker.py:276} INFO - 21/05/17 17:00:44 INFO Executor: Finished task 172.0 in stage 4.0 (TID 357). 4587 bytes result sent to driver
[2021-05-17 14:00:44,243] {docker.py:276} INFO - 21/05/17 17:00:44 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 361) (bb4563f4559d, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:44,244] {docker.py:276} INFO - 21/05/17 17:00:44 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 357) in 2719 ms on bb4563f4559d (executor driver) (173/200)
[2021-05-17 14:00:44,244] {docker.py:276} INFO - 21/05/17 17:00:44 INFO Executor: Running task 176.0 in stage 4.0 (TID 361)
[2021-05-17 14:00:44,253] {docker.py:276} INFO - 21/05/17 17:00:44 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:44,255] {docker.py:276} INFO - 21/05/17 17:00:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:44,255] {docker.py:276} INFO - 21/05/17 17:00:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441110240113081802492_0004_m_000176_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441110240113081802492_0004_m_000176_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441110240113081802492_0004}; taskId=attempt_202105171658441110240113081802492_0004_m_000176_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ba7f742}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:44,256] {docker.py:276} INFO - 21/05/17 17:00:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:44 INFO StagingCommitter: Starting: Task committer attempt_202105171658441110240113081802492_0004_m_000176_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441110240113081802492_0004_m_000176_361
[2021-05-17 14:00:44,259] {docker.py:276} INFO - 21/05/17 17:00:44 INFO StagingCommitter: Task committer attempt_202105171658441110240113081802492_0004_m_000176_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441110240113081802492_0004_m_000176_361 : duration 0:00.003s
[2021-05-17 14:00:46,181] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658444279481241905961847_0004_m_000173_358: needsTaskCommit() Task attempt_202105171658444279481241905961847_0004_m_000173_358
[2021-05-17 14:00:46,182] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658444279481241905961847_0004_m_000173_358: needsTaskCommit() Task attempt_202105171658444279481241905961847_0004_m_000173_358: duration 0:00.001s
[2021-05-17 14:00:46,182] {docker.py:276} INFO - 21/05/17 17:00:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444279481241905961847_0004_m_000173_358
[2021-05-17 14:00:46,183] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Finished task 173.0 in stage 4.0 (TID 358). 4587 bytes result sent to driver
[2021-05-17 14:00:46,184] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 362) (bb4563f4559d, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:46,184] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 358) in 2437 ms on bb4563f4559d (executor driver) (174/200)
[2021-05-17 14:00:46,185] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Running task 177.0 in stage 4.0 (TID 362)
[2021-05-17 14:00:46,193] {docker.py:276} INFO - 21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:46,194] {docker.py:276} INFO - 21/05/17 17:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:46,195] {docker.py:276} INFO - 21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448650158605604585606_0004_m_000177_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448650158605604585606_0004_m_000177_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448650158605604585606_0004}; taskId=attempt_202105171658448650158605604585606_0004_m_000177_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c0a3aaa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:46,195] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658448650158605604585606_0004_m_000177_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448650158605604585606_0004_m_000177_362
[2021-05-17 14:00:46,198] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658448650158605604585606_0004_m_000177_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448650158605604585606_0004_m_000177_362 : duration 0:00.003s
[2021-05-17 14:00:46,311] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658445588550722610698918_0004_m_000175_360: needsTaskCommit() Task attempt_202105171658445588550722610698918_0004_m_000175_360
[2021-05-17 14:00:46,318] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658445588550722610698918_0004_m_000175_360: needsTaskCommit() Task attempt_202105171658445588550722610698918_0004_m_000175_360: duration 0:00.001s
21/05/17 17:00:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658445588550722610698918_0004_m_000175_360
[2021-05-17 14:00:46,319] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Finished task 175.0 in stage 4.0 (TID 360). 4587 bytes result sent to driver
[2021-05-17 14:00:46,319] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 363) (bb4563f4559d, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:46,320] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 360) in 2471 ms on bb4563f4559d (executor driver) (175/200)
21/05/17 17:00:46 INFO Executor: Running task 178.0 in stage 4.0 (TID 363)
[2021-05-17 14:00:46,326] {docker.py:276} INFO - 21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:46,328] {docker.py:276} INFO - 21/05/17 17:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441288048515359147444_0004_m_000178_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441288048515359147444_0004_m_000178_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441288048515359147444_0004}; taskId=attempt_202105171658441288048515359147444_0004_m_000178_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c81868}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658441288048515359147444_0004_m_000178_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441288048515359147444_0004_m_000178_363
[2021-05-17 14:00:46,331] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658441288048515359147444_0004_m_000178_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441288048515359147444_0004_m_000178_363 : duration 0:00.003s
[2021-05-17 14:00:46,514] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658448445995864813832173_0004_m_000174_359: needsTaskCommit() Task attempt_202105171658448445995864813832173_0004_m_000174_359
[2021-05-17 14:00:46,515] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658448445995864813832173_0004_m_000174_359: needsTaskCommit() Task attempt_202105171658448445995864813832173_0004_m_000174_359: duration 0:00.001s
[2021-05-17 14:00:46,515] {docker.py:276} INFO - 21/05/17 17:00:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448445995864813832173_0004_m_000174_359
[2021-05-17 14:00:46,516] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Finished task 174.0 in stage 4.0 (TID 359). 4587 bytes result sent to driver
[2021-05-17 14:00:46,517] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 364) (bb4563f4559d, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:46,518] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Running task 179.0 in stage 4.0 (TID 364)
[2021-05-17 14:00:46,519] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 359) in 2682 ms on bb4563f4559d (executor driver) (176/200)
[2021-05-17 14:00:46,527] {docker.py:276} INFO - 21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:46,529] {docker.py:276} INFO - 21/05/17 17:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:46,529] {docker.py:276} INFO - 21/05/17 17:00:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:46,529] {docker.py:276} INFO - 21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446698406911303845120_0004_m_000179_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446698406911303845120_0004_m_000179_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446698406911303845120_0004}; taskId=attempt_202105171658446698406911303845120_0004_m_000179_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d651c3f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:46,530] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658441110240113081802492_0004_m_000176_361: needsTaskCommit() Task attempt_202105171658441110240113081802492_0004_m_000176_361
[2021-05-17 14:00:46,531] {docker.py:276} INFO - 21/05/17 17:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:46,531] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658446698406911303845120_0004_m_000179_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446698406911303845120_0004_m_000179_364
[2021-05-17 14:00:46,531] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658441110240113081802492_0004_m_000176_361: needsTaskCommit() Task attempt_202105171658441110240113081802492_0004_m_000176_361: duration 0:00.001s
[2021-05-17 14:00:46,532] {docker.py:276} INFO - 21/05/17 17:00:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441110240113081802492_0004_m_000176_361
[2021-05-17 14:00:46,532] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Finished task 176.0 in stage 4.0 (TID 361). 4544 bytes result sent to driver
[2021-05-17 14:00:46,533] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 365) (bb4563f4559d, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:46,534] {docker.py:276} INFO - 21/05/17 17:00:46 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 361) in 2295 ms on bb4563f4559d (executor driver) (177/200)
[2021-05-17 14:00:46,534] {docker.py:276} INFO - 21/05/17 17:00:46 INFO Executor: Running task 180.0 in stage 4.0 (TID 365)
[2021-05-17 14:00:46,538] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658446698406911303845120_0004_m_000179_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446698406911303845120_0004_m_000179_364 : duration 0:00.007s
[2021-05-17 14:00:46,544] {docker.py:276} INFO - 21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:46,546] {docker.py:276} INFO - 21/05/17 17:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444376158164148453273_0004_m_000180_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444376158164148453273_0004_m_000180_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444376158164148453273_0004}; taskId=attempt_202105171658444376158164148453273_0004_m_000180_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6370e04b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:46,547] {docker.py:276} INFO - 21/05/17 17:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:46,547] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Starting: Task committer attempt_202105171658444376158164148453273_0004_m_000180_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444376158164148453273_0004_m_000180_365
[2021-05-17 14:00:46,549] {docker.py:276} INFO - 21/05/17 17:00:46 INFO StagingCommitter: Task committer attempt_202105171658444376158164148453273_0004_m_000180_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444376158164148453273_0004_m_000180_365 : duration 0:00.004s
[2021-05-17 14:00:48,698] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658448650158605604585606_0004_m_000177_362: needsTaskCommit() Task attempt_202105171658448650158605604585606_0004_m_000177_362
[2021-05-17 14:00:48,699] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Task committer attempt_202105171658448650158605604585606_0004_m_000177_362: needsTaskCommit() Task attempt_202105171658448650158605604585606_0004_m_000177_362: duration 0:00.001s
21/05/17 17:00:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448650158605604585606_0004_m_000177_362
[2021-05-17 14:00:48,701] {docker.py:276} INFO - 21/05/17 17:00:48 INFO Executor: Finished task 177.0 in stage 4.0 (TID 362). 4544 bytes result sent to driver
[2021-05-17 14:00:48,702] {docker.py:276} INFO - 21/05/17 17:00:48 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 366) (bb4563f4559d, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:48,703] {docker.py:276} INFO - 21/05/17 17:00:48 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 362) in 2523 ms on bb4563f4559d (executor driver) (178/200)
[2021-05-17 14:00:48,704] {docker.py:276} INFO - 21/05/17 17:00:48 INFO Executor: Running task 181.0 in stage 4.0 (TID 366)
[2021-05-17 14:00:48,714] {docker.py:276} INFO - 21/05/17 17:00:48 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:48,715] {docker.py:276} INFO - 21/05/17 17:00:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:48,716] {docker.py:276} INFO - 21/05/17 17:00:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448398399402098440334_0004_m_000181_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448398399402098440334_0004_m_000181_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448398399402098440334_0004}; taskId=attempt_202105171658448398399402098440334_0004_m_000181_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fddb678}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658448398399402098440334_0004_m_000181_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448398399402098440334_0004_m_000181_366
[2021-05-17 14:00:48,718] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Task committer attempt_202105171658448398399402098440334_0004_m_000181_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448398399402098440334_0004_m_000181_366 : duration 0:00.003s
[2021-05-17 14:00:48,958] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658446698406911303845120_0004_m_000179_364: needsTaskCommit() Task attempt_202105171658446698406911303845120_0004_m_000179_364
[2021-05-17 14:00:48,958] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Task committer attempt_202105171658446698406911303845120_0004_m_000179_364: needsTaskCommit() Task attempt_202105171658446698406911303845120_0004_m_000179_364: duration 0:00.000s
21/05/17 17:00:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446698406911303845120_0004_m_000179_364
[2021-05-17 14:00:48,959] {docker.py:276} INFO - 21/05/17 17:00:48 INFO Executor: Finished task 179.0 in stage 4.0 (TID 364). 4544 bytes result sent to driver
[2021-05-17 14:00:48,961] {docker.py:276} INFO - 21/05/17 17:00:48 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 367) (bb4563f4559d, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:48,961] {docker.py:276} INFO - 21/05/17 17:00:48 INFO Executor: Running task 182.0 in stage 4.0 (TID 367)
[2021-05-17 14:00:48,962] {docker.py:276} INFO - 21/05/17 17:00:48 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 364) in 2447 ms on bb4563f4559d (executor driver) (179/200)
[2021-05-17 14:00:48,970] {docker.py:276} INFO - 21/05/17 17:00:48 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:48,971] {docker.py:276} INFO - 21/05/17 17:00:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:48,973] {docker.py:276} INFO - 21/05/17 17:00:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:48,974] {docker.py:276} INFO - 21/05/17 17:00:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:48,974] {docker.py:276} INFO - 21/05/17 17:00:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:48,975] {docker.py:276} INFO - 21/05/17 17:00:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442725821704946535886_0004_m_000182_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442725821704946535886_0004_m_000182_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442725821704946535886_0004}; taskId=attempt_202105171658442725821704946535886_0004_m_000182_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c211a72}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:48,975] {docker.py:276} INFO - 21/05/17 17:00:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:48,975] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Starting: Task committer attempt_202105171658442725821704946535886_0004_m_000182_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442725821704946535886_0004_m_000182_367
[2021-05-17 14:00:48,978] {docker.py:276} INFO - 21/05/17 17:00:48 INFO StagingCommitter: Task committer attempt_202105171658442725821704946535886_0004_m_000182_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442725821704946535886_0004_m_000182_367 : duration 0:00.003s
[2021-05-17 14:00:48,996] {docker.py:276} INFO - 21/05/17 17:00:49 INFO StagingCommitter: Starting: Task committer attempt_202105171658441288048515359147444_0004_m_000178_363: needsTaskCommit() Task attempt_202105171658441288048515359147444_0004_m_000178_363
[2021-05-17 14:00:48,997] {docker.py:276} INFO - 21/05/17 17:00:49 INFO StagingCommitter: Task committer attempt_202105171658441288048515359147444_0004_m_000178_363: needsTaskCommit() Task attempt_202105171658441288048515359147444_0004_m_000178_363: duration 0:00.001s
21/05/17 17:00:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441288048515359147444_0004_m_000178_363
[2021-05-17 14:00:48,998] {docker.py:276} INFO - 21/05/17 17:00:49 INFO Executor: Finished task 178.0 in stage 4.0 (TID 363). 4544 bytes result sent to driver
[2021-05-17 14:00:48,999] {docker.py:276} INFO - 21/05/17 17:00:49 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 368) (bb4563f4559d, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:49,000] {docker.py:276} INFO - 21/05/17 17:00:49 INFO Executor: Running task 183.0 in stage 4.0 (TID 368)
[2021-05-17 14:00:49,001] {docker.py:276} INFO - 21/05/17 17:00:49 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 363) in 2690 ms on bb4563f4559d (executor driver) (180/200)
[2021-05-17 14:00:49,007] {docker.py:276} INFO - 21/05/17 17:00:49 INFO ShuffleBlockFetcherIterator: Getting 4 (25.1 KiB) non-empty blocks including 4 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:49,009] {docker.py:276} INFO - 21/05/17 17:00:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:49,009] {docker.py:276} INFO - 21/05/17 17:00:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442313857510286262845_0004_m_000183_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442313857510286262845_0004_m_000183_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442313857510286262845_0004}; taskId=attempt_202105171658442313857510286262845_0004_m_000183_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60d19ea2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:49,010] {docker.py:276} INFO - 21/05/17 17:00:49 INFO StagingCommitter: Starting: Task committer attempt_202105171658442313857510286262845_0004_m_000183_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442313857510286262845_0004_m_000183_368
[2021-05-17 14:00:49,012] {docker.py:276} INFO - 21/05/17 17:00:49 INFO StagingCommitter: Task committer attempt_202105171658442313857510286262845_0004_m_000183_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442313857510286262845_0004_m_000183_368 : duration 0:00.003s
[2021-05-17 14:00:49,092] {docker.py:276} INFO - 21/05/17 17:00:49 INFO StagingCommitter: Starting: Task committer attempt_202105171658444376158164148453273_0004_m_000180_365: needsTaskCommit() Task attempt_202105171658444376158164148453273_0004_m_000180_365
21/05/17 17:00:49 INFO StagingCommitter: Task committer attempt_202105171658444376158164148453273_0004_m_000180_365: needsTaskCommit() Task attempt_202105171658444376158164148453273_0004_m_000180_365: duration 0:00.000s
[2021-05-17 14:00:49,093] {docker.py:276} INFO - 21/05/17 17:00:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444376158164148453273_0004_m_000180_365
[2021-05-17 14:00:49,094] {docker.py:276} INFO - 21/05/17 17:00:49 INFO Executor: Finished task 180.0 in stage 4.0 (TID 365). 4544 bytes result sent to driver
[2021-05-17 14:00:49,095] {docker.py:276} INFO - 21/05/17 17:00:49 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 369) (bb4563f4559d, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:49,096] {docker.py:276} INFO - 21/05/17 17:00:49 INFO Executor: Running task 184.0 in stage 4.0 (TID 369)
[2021-05-17 14:00:49,097] {docker.py:276} INFO - 21/05/17 17:00:49 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 365) in 2567 ms on bb4563f4559d (executor driver) (181/200)
[2021-05-17 14:00:49,105] {docker.py:276} INFO - 21/05/17 17:00:49 INFO ShuffleBlockFetcherIterator: Getting 4 (27.9 KiB) non-empty blocks including 4 (27.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:49,107] {docker.py:276} INFO - 21/05/17 17:00:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:49,108] {docker.py:276} INFO - 21/05/17 17:00:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658444635074811814653268_0004_m_000184_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444635074811814653268_0004_m_000184_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658444635074811814653268_0004}; taskId=attempt_202105171658444635074811814653268_0004_m_000184_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4dc191d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:49 INFO StagingCommitter: Starting: Task committer attempt_202105171658444635074811814653268_0004_m_000184_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444635074811814653268_0004_m_000184_369
[2021-05-17 14:00:49,111] {docker.py:276} INFO - 21/05/17 17:00:49 INFO StagingCommitter: Task committer attempt_202105171658444635074811814653268_0004_m_000184_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658444635074811814653268_0004_m_000184_369 : duration 0:00.003s
[2021-05-17 14:00:51,343] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658448398399402098440334_0004_m_000181_366: needsTaskCommit() Task attempt_202105171658448398399402098440334_0004_m_000181_366
21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658448398399402098440334_0004_m_000181_366: needsTaskCommit() Task attempt_202105171658448398399402098440334_0004_m_000181_366: duration 0:00.000s
21/05/17 17:00:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448398399402098440334_0004_m_000181_366
[2021-05-17 14:00:51,345] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Finished task 181.0 in stage 4.0 (TID 366). 4587 bytes result sent to driver
[2021-05-17 14:00:51,348] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 370) (bb4563f4559d, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:51,348] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Running task 185.0 in stage 4.0 (TID 370)
[2021-05-17 14:00:51,349] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 366) in 2648 ms on bb4563f4559d (executor driver) (182/200)
[2021-05-17 14:00:51,360] {docker.py:276} INFO - 21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:51,364] {docker.py:276} INFO - 21/05/17 17:00:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:51,364] {docker.py:276} INFO - 21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448885654843423685559_0004_m_000185_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448885654843423685559_0004_m_000185_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448885654843423685559_0004}; taskId=attempt_202105171658448885654843423685559_0004_m_000185_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36eb1786}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658448885654843423685559_0004_m_000185_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448885654843423685559_0004_m_000185_370
[2021-05-17 14:00:51,368] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658448885654843423685559_0004_m_000185_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448885654843423685559_0004_m_000185_370 : duration 0:00.005s
[2021-05-17 14:00:51,510] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658442313857510286262845_0004_m_000183_368: needsTaskCommit() Task attempt_202105171658442313857510286262845_0004_m_000183_368
[2021-05-17 14:00:51,510] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658442313857510286262845_0004_m_000183_368: needsTaskCommit() Task attempt_202105171658442313857510286262845_0004_m_000183_368: duration 0:00.000s
21/05/17 17:00:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442313857510286262845_0004_m_000183_368
[2021-05-17 14:00:51,511] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Finished task 183.0 in stage 4.0 (TID 368). 4587 bytes result sent to driver
[2021-05-17 14:00:51,512] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 371) (bb4563f4559d, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:51,513] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Running task 186.0 in stage 4.0 (TID 371)
[2021-05-17 14:00:51,513] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 368) in 2517 ms on bb4563f4559d (executor driver) (183/200)
[2021-05-17 14:00:51,521] {docker.py:276} INFO - 21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:51,522] {docker.py:276} INFO - 21/05/17 17:00:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:51,523] {docker.py:276} INFO - 21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443549771939074029521_0004_m_000186_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443549771939074029521_0004_m_000186_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443549771939074029521_0004}; taskId=attempt_202105171658443549771939074029521_0004_m_000186_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f9670ce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:51,523] {docker.py:276} INFO - 21/05/17 17:00:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:51,523] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658443549771939074029521_0004_m_000186_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443549771939074029521_0004_m_000186_371
[2021-05-17 14:00:51,525] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658443549771939074029521_0004_m_000186_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443549771939074029521_0004_m_000186_371 : duration 0:00.002s
[2021-05-17 14:00:51,527] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658442725821704946535886_0004_m_000182_367: needsTaskCommit() Task attempt_202105171658442725821704946535886_0004_m_000182_367
[2021-05-17 14:00:51,527] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658442725821704946535886_0004_m_000182_367: needsTaskCommit() Task attempt_202105171658442725821704946535886_0004_m_000182_367: duration 0:00.000s
21/05/17 17:00:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442725821704946535886_0004_m_000182_367
[2021-05-17 14:00:51,529] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Finished task 182.0 in stage 4.0 (TID 367). 4587 bytes result sent to driver
[2021-05-17 14:00:51,530] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 372) (bb4563f4559d, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:51,531] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 367) in 2573 ms on bb4563f4559d (executor driver) (184/200)
[2021-05-17 14:00:51,532] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Running task 187.0 in stage 4.0 (TID 372)
[2021-05-17 14:00:51,540] {docker.py:276} INFO - 21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:51,542] {docker.py:276} INFO - 21/05/17 17:00:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:51,543] {docker.py:276} INFO - 21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:51,543] {docker.py:276} INFO - 21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443550211776935306062_0004_m_000187_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443550211776935306062_0004_m_000187_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443550211776935306062_0004}; taskId=attempt_202105171658443550211776935306062_0004_m_000187_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31a453c1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:51,543] {docker.py:276} INFO - 21/05/17 17:00:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:51,544] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658443550211776935306062_0004_m_000187_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443550211776935306062_0004_m_000187_372
[2021-05-17 14:00:51,546] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658443550211776935306062_0004_m_000187_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443550211776935306062_0004_m_000187_372 : duration 0:00.003s
[2021-05-17 14:00:51,600] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658444635074811814653268_0004_m_000184_369: needsTaskCommit() Task attempt_202105171658444635074811814653268_0004_m_000184_369
[2021-05-17 14:00:51,601] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658444635074811814653268_0004_m_000184_369: needsTaskCommit() Task attempt_202105171658444635074811814653268_0004_m_000184_369: duration 0:00.000s
21/05/17 17:00:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658444635074811814653268_0004_m_000184_369
[2021-05-17 14:00:51,602] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Finished task 184.0 in stage 4.0 (TID 369). 4587 bytes result sent to driver
[2021-05-17 14:00:51,604] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 373) (bb4563f4559d, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:51,605] {docker.py:276} INFO - 21/05/17 17:00:51 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 369) in 2512 ms on bb4563f4559d (executor driver) (185/200)
[2021-05-17 14:00:51,606] {docker.py:276} INFO - 21/05/17 17:00:51 INFO Executor: Running task 188.0 in stage 4.0 (TID 373)
[2021-05-17 14:00:51,616] {docker.py:276} INFO - 21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Getting 4 (27.0 KiB) non-empty blocks including 4 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:51,616] {docker.py:276} INFO - 21/05/17 17:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:51,619] {docker.py:276} INFO - 21/05/17 17:00:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:51,619] {docker.py:276} INFO - 21/05/17 17:00:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:51,620] {docker.py:276} INFO - 21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:51,621] {docker.py:276} INFO - 21/05/17 17:00:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443490115041194282485_0004_m_000188_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443490115041194282485_0004_m_000188_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443490115041194282485_0004}; taskId=attempt_202105171658443490115041194282485_0004_m_000188_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4140dd5a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:51,621] {docker.py:276} INFO - 21/05/17 17:00:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:51 INFO StagingCommitter: Starting: Task committer attempt_202105171658443490115041194282485_0004_m_000188_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443490115041194282485_0004_m_000188_373
[2021-05-17 14:00:51,624] {docker.py:276} INFO - 21/05/17 17:00:51 INFO StagingCommitter: Task committer attempt_202105171658443490115041194282485_0004_m_000188_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443490115041194282485_0004_m_000188_373 : duration 0:00.004s
[2021-05-17 14:00:53,788] {docker.py:276} INFO - 21/05/17 17:00:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658448885654843423685559_0004_m_000185_370: needsTaskCommit() Task attempt_202105171658448885654843423685559_0004_m_000185_370
[2021-05-17 14:00:53,789] {docker.py:276} INFO - 21/05/17 17:00:53 INFO StagingCommitter: Task committer attempt_202105171658448885654843423685559_0004_m_000185_370: needsTaskCommit() Task attempt_202105171658448885654843423685559_0004_m_000185_370: duration 0:00.001s
21/05/17 17:00:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448885654843423685559_0004_m_000185_370
[2021-05-17 14:00:53,790] {docker.py:276} INFO - 21/05/17 17:00:53 INFO Executor: Finished task 185.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-17 14:00:53,792] {docker.py:276} INFO - 21/05/17 17:00:53 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 374) (bb4563f4559d, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:53,793] {docker.py:276} INFO - 21/05/17 17:00:53 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 370) in 2451 ms on bb4563f4559d (executor driver) (186/200)
[2021-05-17 14:00:53,794] {docker.py:276} INFO - 21/05/17 17:00:53 INFO Executor: Running task 189.0 in stage 4.0 (TID 374)
[2021-05-17 14:00:53,803] {docker.py:276} INFO - 21/05/17 17:00:53 INFO ShuffleBlockFetcherIterator: Getting 4 (28.5 KiB) non-empty blocks including 4 (28.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:53,805] {docker.py:276} INFO - 21/05/17 17:00:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448398746050531206361_0004_m_000189_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448398746050531206361_0004_m_000189_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448398746050531206361_0004}; taskId=attempt_202105171658448398746050531206361_0004_m_000189_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b09d892}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:53,806] {docker.py:276} INFO - 21/05/17 17:00:53 INFO StagingCommitter: Starting: Task committer attempt_202105171658448398746050531206361_0004_m_000189_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448398746050531206361_0004_m_000189_374
[2021-05-17 14:00:53,809] {docker.py:276} INFO - 21/05/17 17:00:53 INFO StagingCommitter: Task committer attempt_202105171658448398746050531206361_0004_m_000189_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448398746050531206361_0004_m_000189_374 : duration 0:00.004s
[2021-05-17 14:00:53,989] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105171658443550211776935306062_0004_m_000187_372: needsTaskCommit() Task attempt_202105171658443550211776935306062_0004_m_000187_372
[2021-05-17 14:00:53,989] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Task committer attempt_202105171658443550211776935306062_0004_m_000187_372: needsTaskCommit() Task attempt_202105171658443550211776935306062_0004_m_000187_372: duration 0:00.001s
21/05/17 17:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443550211776935306062_0004_m_000187_372
[2021-05-17 14:00:53,990] {docker.py:276} INFO - 21/05/17 17:00:54 INFO Executor: Finished task 187.0 in stage 4.0 (TID 372). 4544 bytes result sent to driver
[2021-05-17 14:00:53,992] {docker.py:276} INFO - 21/05/17 17:00:54 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 375) (bb4563f4559d, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:53,992] {docker.py:276} INFO - 21/05/17 17:00:54 INFO Executor: Running task 190.0 in stage 4.0 (TID 375)
[2021-05-17 14:00:53,993] {docker.py:276} INFO - 21/05/17 17:00:54 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 372) in 2467 ms on bb4563f4559d (executor driver) (187/200)
[2021-05-17 14:00:54,003] {docker.py:276} INFO - 21/05/17 17:00:54 INFO ShuffleBlockFetcherIterator: Getting 4 (25.7 KiB) non-empty blocks including 4 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:54,005] {docker.py:276} INFO - 21/05/17 17:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:54,006] {docker.py:276} INFO - 21/05/17 17:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:54,006] {docker.py:276} INFO - 21/05/17 17:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:54,006] {docker.py:276} INFO - 21/05/17 17:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446761478817670707677_0004_m_000190_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446761478817670707677_0004_m_000190_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446761478817670707677_0004}; taskId=attempt_202105171658446761478817670707677_0004_m_000190_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77711bb0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:54,007] {docker.py:276} INFO - 21/05/17 17:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:54,007] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105171658446761478817670707677_0004_m_000190_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446761478817670707677_0004_m_000190_375
[2021-05-17 14:00:54,009] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Task committer attempt_202105171658446761478817670707677_0004_m_000190_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446761478817670707677_0004_m_000190_375 : duration 0:00.002s
[2021-05-17 14:00:54,211] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105171658443490115041194282485_0004_m_000188_373: needsTaskCommit() Task attempt_202105171658443490115041194282485_0004_m_000188_373
[2021-05-17 14:00:54,212] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Task committer attempt_202105171658443490115041194282485_0004_m_000188_373: needsTaskCommit() Task attempt_202105171658443490115041194282485_0004_m_000188_373: duration 0:00.001s
21/05/17 17:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443490115041194282485_0004_m_000188_373
[2021-05-17 14:00:54,213] {docker.py:276} INFO - 21/05/17 17:00:54 INFO Executor: Finished task 188.0 in stage 4.0 (TID 373). 4544 bytes result sent to driver
[2021-05-17 14:00:54,214] {docker.py:276} INFO - 21/05/17 17:00:54 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 376) (bb4563f4559d, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:54,215] {docker.py:276} INFO - 21/05/17 17:00:54 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 373) in 2615 ms on bb4563f4559d (executor driver) (188/200)
[2021-05-17 14:00:54,216] {docker.py:276} INFO - 21/05/17 17:00:54 INFO Executor: Running task 191.0 in stage 4.0 (TID 376)
[2021-05-17 14:00:54,228] {docker.py:276} INFO - 21/05/17 17:00:54 INFO ShuffleBlockFetcherIterator: Getting 4 (27.2 KiB) non-empty blocks including 4 (27.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:54,231] {docker.py:276} INFO - 21/05/17 17:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:54,231] {docker.py:276} INFO - 21/05/17 17:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658442790376636790854738_0004_m_000191_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442790376636790854738_0004_m_000191_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658442790376636790854738_0004}; taskId=attempt_202105171658442790376636790854738_0004_m_000191_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d31ae4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105171658442790376636790854738_0004_m_000191_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442790376636790854738_0004_m_000191_376
[2021-05-17 14:00:54,235] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Task committer attempt_202105171658442790376636790854738_0004_m_000191_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658442790376636790854738_0004_m_000191_376 : duration 0:00.003s
[2021-05-17 14:00:54,726] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105171658443549771939074029521_0004_m_000186_371: needsTaskCommit() Task attempt_202105171658443549771939074029521_0004_m_000186_371
21/05/17 17:00:54 INFO StagingCommitter: Task committer attempt_202105171658443549771939074029521_0004_m_000186_371: needsTaskCommit() Task attempt_202105171658443549771939074029521_0004_m_000186_371: duration 0:00.001s
21/05/17 17:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443549771939074029521_0004_m_000186_371
[2021-05-17 14:00:54,729] {docker.py:276} INFO - 21/05/17 17:00:54 INFO Executor: Finished task 186.0 in stage 4.0 (TID 371). 4544 bytes result sent to driver
[2021-05-17 14:00:54,730] {docker.py:276} INFO - 21/05/17 17:00:54 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 377) (bb4563f4559d, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:54,731] {docker.py:276} INFO - 21/05/17 17:00:54 INFO Executor: Running task 192.0 in stage 4.0 (TID 377)
[2021-05-17 14:00:54,732] {docker.py:276} INFO - 21/05/17 17:00:54 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 371) in 3222 ms on bb4563f4559d (executor driver) (189/200)
[2021-05-17 14:00:54,741] {docker.py:276} INFO - 21/05/17 17:00:54 INFO ShuffleBlockFetcherIterator: Getting 4 (29.8 KiB) non-empty blocks including 4 (29.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:54,742] {docker.py:276} INFO - 21/05/17 17:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:54,743] {docker.py:276} INFO - 21/05/17 17:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 14:00:54,744] {docker.py:276} INFO - 21/05/17 17:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:54,744] {docker.py:276} INFO - 21/05/17 17:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:54,745] {docker.py:276} INFO - 21/05/17 17:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658441417510044009175663_0004_m_000192_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441417510044009175663_0004_m_000192_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658441417510044009175663_0004}; taskId=attempt_202105171658441417510044009175663_0004_m_000192_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@521254b1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:54,745] {docker.py:276} INFO - 21/05/17 17:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:54,745] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105171658441417510044009175663_0004_m_000192_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441417510044009175663_0004_m_000192_377
[2021-05-17 14:00:54,748] {docker.py:276} INFO - 21/05/17 17:00:54 INFO StagingCommitter: Task committer attempt_202105171658441417510044009175663_0004_m_000192_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658441417510044009175663_0004_m_000192_377 : duration 0:00.002s
[2021-05-17 14:00:56,203] {docker.py:276} INFO - 21/05/17 17:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658448398746050531206361_0004_m_000189_374: needsTaskCommit() Task attempt_202105171658448398746050531206361_0004_m_000189_374
[2021-05-17 14:00:56,204] {docker.py:276} INFO - 21/05/17 17:00:56 INFO StagingCommitter: Task committer attempt_202105171658448398746050531206361_0004_m_000189_374: needsTaskCommit() Task attempt_202105171658448398746050531206361_0004_m_000189_374: duration 0:00.001s
21/05/17 17:00:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448398746050531206361_0004_m_000189_374
[2021-05-17 14:00:56,206] {docker.py:276} INFO - 21/05/17 17:00:56 INFO Executor: Finished task 189.0 in stage 4.0 (TID 374). 4544 bytes result sent to driver
[2021-05-17 14:00:56,208] {docker.py:276} INFO - 21/05/17 17:00:56 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 378) (bb4563f4559d, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:56,209] {docker.py:276} INFO - 21/05/17 17:00:56 INFO Executor: Running task 193.0 in stage 4.0 (TID 378)
21/05/17 17:00:56 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 374) in 2420 ms on bb4563f4559d (executor driver) (190/200)
[2021-05-17 14:00:56,219] {docker.py:276} INFO - 21/05/17 17:00:56 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:56,221] {docker.py:276} INFO - 21/05/17 17:00:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446818965607594445263_0004_m_000193_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446818965607594445263_0004_m_000193_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446818965607594445263_0004}; taskId=attempt_202105171658446818965607594445263_0004_m_000193_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42d59b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658446818965607594445263_0004_m_000193_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446818965607594445263_0004_m_000193_378
[2021-05-17 14:00:56,224] {docker.py:276} INFO - 21/05/17 17:00:56 INFO StagingCommitter: Task committer attempt_202105171658446818965607594445263_0004_m_000193_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446818965607594445263_0004_m_000193_378 : duration 0:00.003s
[2021-05-17 14:00:56,303] {docker.py:276} INFO - 21/05/17 17:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658446761478817670707677_0004_m_000190_375: needsTaskCommit() Task attempt_202105171658446761478817670707677_0004_m_000190_375
[2021-05-17 14:00:56,304] {docker.py:276} INFO - 21/05/17 17:00:56 INFO StagingCommitter: Task committer attempt_202105171658446761478817670707677_0004_m_000190_375: needsTaskCommit() Task attempt_202105171658446761478817670707677_0004_m_000190_375: duration 0:00.000s
21/05/17 17:00:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446761478817670707677_0004_m_000190_375
[2021-05-17 14:00:56,306] {docker.py:276} INFO - 21/05/17 17:00:56 INFO Executor: Finished task 190.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
[2021-05-17 14:00:56,308] {docker.py:276} INFO - 21/05/17 17:00:56 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 379) (bb4563f4559d, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:56,309] {docker.py:276} INFO - 21/05/17 17:00:56 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 375) in 2320 ms on bb4563f4559d (executor driver) (191/200)
[2021-05-17 14:00:56,310] {docker.py:276} INFO - 21/05/17 17:00:56 INFO Executor: Running task 194.0 in stage 4.0 (TID 379)
[2021-05-17 14:00:56,328] {docker.py:276} INFO - 21/05/17 17:00:56 INFO ShuffleBlockFetcherIterator: Getting 4 (27.1 KiB) non-empty blocks including 4 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:56,329] {docker.py:276} INFO - 21/05/17 17:00:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658447482405542771347171_0004_m_000194_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447482405542771347171_0004_m_000194_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658447482405542771347171_0004}; taskId=attempt_202105171658447482405542771347171_0004_m_000194_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d6c27d5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:56,329] {docker.py:276} INFO - 21/05/17 17:00:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105171658447482405542771347171_0004_m_000194_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447482405542771347171_0004_m_000194_379
[2021-05-17 14:00:56,332] {docker.py:276} INFO - 21/05/17 17:00:56 INFO StagingCommitter: Task committer attempt_202105171658447482405542771347171_0004_m_000194_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658447482405542771347171_0004_m_000194_379 : duration 0:00.002s
[2021-05-17 14:00:57,202] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658442790376636790854738_0004_m_000191_376: needsTaskCommit() Task attempt_202105171658442790376636790854738_0004_m_000191_376
[2021-05-17 14:00:57,203] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Task committer attempt_202105171658442790376636790854738_0004_m_000191_376: needsTaskCommit() Task attempt_202105171658442790376636790854738_0004_m_000191_376: duration 0:00.000s
21/05/17 17:00:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658442790376636790854738_0004_m_000191_376
[2021-05-17 14:00:57,204] {docker.py:276} INFO - 21/05/17 17:00:57 INFO Executor: Finished task 191.0 in stage 4.0 (TID 376). 4587 bytes result sent to driver
[2021-05-17 14:00:57,207] {docker.py:276} INFO - 21/05/17 17:00:57 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 380) (bb4563f4559d, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:57,208] {docker.py:276} INFO - 21/05/17 17:00:57 INFO Executor: Running task 195.0 in stage 4.0 (TID 380)
21/05/17 17:00:57 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 376) in 2996 ms on bb4563f4559d (executor driver) (192/200)
[2021-05-17 14:00:57,220] {docker.py:276} INFO - 21/05/17 17:00:57 INFO ShuffleBlockFetcherIterator: Getting 4 (29.1 KiB) non-empty blocks including 4 (29.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:57,221] {docker.py:276} INFO - 21/05/17 17:00:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:57,221] {docker.py:276} INFO - 21/05/17 17:00:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446097803205859495546_0004_m_000195_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446097803205859495546_0004_m_000195_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446097803205859495546_0004}; taskId=attempt_202105171658446097803205859495546_0004_m_000195_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@471285a0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658446097803205859495546_0004_m_000195_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446097803205859495546_0004_m_000195_380
[2021-05-17 14:00:57,224] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Task committer attempt_202105171658446097803205859495546_0004_m_000195_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446097803205859495546_0004_m_000195_380 : duration 0:00.003s
[2021-05-17 14:00:57,386] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658441417510044009175663_0004_m_000192_377: needsTaskCommit() Task attempt_202105171658441417510044009175663_0004_m_000192_377
[2021-05-17 14:00:57,387] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Task committer attempt_202105171658441417510044009175663_0004_m_000192_377: needsTaskCommit() Task attempt_202105171658441417510044009175663_0004_m_000192_377: duration 0:00.000s
[2021-05-17 14:00:57,387] {docker.py:276} INFO - 21/05/17 17:00:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658441417510044009175663_0004_m_000192_377
[2021-05-17 14:00:57,388] {docker.py:276} INFO - 21/05/17 17:00:57 INFO Executor: Finished task 192.0 in stage 4.0 (TID 377). 4587 bytes result sent to driver
[2021-05-17 14:00:57,390] {docker.py:276} INFO - 21/05/17 17:00:57 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 381) (bb4563f4559d, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:57,390] {docker.py:276} INFO - 21/05/17 17:00:57 INFO Executor: Running task 196.0 in stage 4.0 (TID 381)
[2021-05-17 14:00:57,391] {docker.py:276} INFO - 21/05/17 17:00:57 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 377) in 2666 ms on bb4563f4559d (executor driver) (193/200)
[2021-05-17 14:00:57,399] {docker.py:276} INFO - 21/05/17 17:00:57 INFO ShuffleBlockFetcherIterator: Getting 4 (27.7 KiB) non-empty blocks including 4 (27.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:57,401] {docker.py:276} INFO - 21/05/17 17:00:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 14:00:57,401] {docker.py:276} INFO - 21/05/17 17:00:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:57,402] {docker.py:276} INFO - 21/05/17 17:00:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443774445857231172405_0004_m_000196_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443774445857231172405_0004_m_000196_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443774445857231172405_0004}; taskId=attempt_202105171658443774445857231172405_0004_m_000196_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@536b3237}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:57,402] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Starting: Task committer attempt_202105171658443774445857231172405_0004_m_000196_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443774445857231172405_0004_m_000196_381
[2021-05-17 14:00:57,405] {docker.py:276} INFO - 21/05/17 17:00:57 INFO StagingCommitter: Task committer attempt_202105171658443774445857231172405_0004_m_000196_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443774445857231172405_0004_m_000196_381 : duration 0:00.003s
[2021-05-17 14:00:58,270] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658447482405542771347171_0004_m_000194_379: needsTaskCommit() Task attempt_202105171658447482405542771347171_0004_m_000194_379
[2021-05-17 14:00:58,271] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Task committer attempt_202105171658447482405542771347171_0004_m_000194_379: needsTaskCommit() Task attempt_202105171658447482405542771347171_0004_m_000194_379: duration 0:00.001s
21/05/17 17:00:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658447482405542771347171_0004_m_000194_379
[2021-05-17 14:00:58,272] {docker.py:276} INFO - 21/05/17 17:00:58 INFO Executor: Finished task 194.0 in stage 4.0 (TID 379). 4587 bytes result sent to driver
[2021-05-17 14:00:58,273] {docker.py:276} INFO - 21/05/17 17:00:58 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 382) (bb4563f4559d, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:58,276] {docker.py:276} INFO - 21/05/17 17:00:58 INFO Executor: Running task 197.0 in stage 4.0 (TID 382)
[2021-05-17 14:00:58,277] {docker.py:276} INFO - 21/05/17 17:00:58 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 379) in 1970 ms on bb4563f4559d (executor driver) (194/200)
[2021-05-17 14:00:58,285] {docker.py:276} INFO - 21/05/17 17:00:58 INFO ShuffleBlockFetcherIterator: Getting 4 (26.4 KiB) non-empty blocks including 4 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:58,287] {docker.py:276} INFO - 21/05/17 17:00:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:58,287] {docker.py:276} INFO - 21/05/17 17:00:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658448840502901869983783_0004_m_000197_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448840502901869983783_0004_m_000197_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658448840502901869983783_0004}; taskId=attempt_202105171658448840502901869983783_0004_m_000197_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ff87987}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:58,288] {docker.py:276} INFO - 21/05/17 17:00:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 17:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658448840502901869983783_0004_m_000197_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448840502901869983783_0004_m_000197_382
[2021-05-17 14:00:58,291] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Task committer attempt_202105171658448840502901869983783_0004_m_000197_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658448840502901869983783_0004_m_000197_382 : duration 0:00.003s
[2021-05-17 14:00:58,839] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658446818965607594445263_0004_m_000193_378: needsTaskCommit() Task attempt_202105171658446818965607594445263_0004_m_000193_378
[2021-05-17 14:00:58,839] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Task committer attempt_202105171658446818965607594445263_0004_m_000193_378: needsTaskCommit() Task attempt_202105171658446818965607594445263_0004_m_000193_378: duration 0:00.000s
21/05/17 17:00:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446818965607594445263_0004_m_000193_378
[2021-05-17 14:00:58,840] {docker.py:276} INFO - 21/05/17 17:00:58 INFO Executor: Finished task 193.0 in stage 4.0 (TID 378). 4587 bytes result sent to driver
[2021-05-17 14:00:58,843] {docker.py:276} INFO - 21/05/17 17:00:58 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 383) (bb4563f4559d, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 17:00:58 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 378) in 2639 ms on bb4563f4559d (executor driver) (195/200)
[2021-05-17 14:00:58,844] {docker.py:276} INFO - 21/05/17 17:00:58 INFO Executor: Running task 198.0 in stage 4.0 (TID 383)
[2021-05-17 14:00:58,854] {docker.py:276} INFO - 21/05/17 17:00:58 INFO ShuffleBlockFetcherIterator: Getting 4 (29.0 KiB) non-empty blocks including 4 (29.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 14:00:58,855] {docker.py:276} INFO - 21/05/17 17:00:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:58,857] {docker.py:276} INFO - 21/05/17 17:00:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:58,877] {docker.py:276} INFO - 21/05/17 17:00:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658446170258179672618863_0004_m_000198_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446170258179672618863_0004_m_000198_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658446170258179672618863_0004}; taskId=attempt_202105171658446170258179672618863_0004_m_000198_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b5be2e7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 17:00:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:58,878] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105171658446170258179672618863_0004_m_000198_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446170258179672618863_0004_m_000198_383
[2021-05-17 14:00:58,878] {docker.py:276} INFO - 21/05/17 17:00:58 INFO StagingCommitter: Task committer attempt_202105171658446170258179672618863_0004_m_000198_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658446170258179672618863_0004_m_000198_383 : duration 0:00.003s
[2021-05-17 14:00:59,386] {docker.py:276} INFO - 21/05/17 17:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658446097803205859495546_0004_m_000195_380: needsTaskCommit() Task attempt_202105171658446097803205859495546_0004_m_000195_380
[2021-05-17 14:00:59,386] {docker.py:276} INFO - 21/05/17 17:00:59 INFO StagingCommitter: Task committer attempt_202105171658446097803205859495546_0004_m_000195_380: needsTaskCommit() Task attempt_202105171658446097803205859495546_0004_m_000195_380: duration 0:00.000s
21/05/17 17:00:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446097803205859495546_0004_m_000195_380
[2021-05-17 14:00:59,387] {docker.py:276} INFO - 21/05/17 17:00:59 INFO Executor: Finished task 195.0 in stage 4.0 (TID 380). 4544 bytes result sent to driver
[2021-05-17 14:00:59,389] {docker.py:276} INFO - 21/05/17 17:00:59 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 384) (bb4563f4559d, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 14:00:59,389] {docker.py:276} INFO - 21/05/17 17:00:59 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 380) in 2187 ms on bb4563f4559d (executor driver) (196/200)
[2021-05-17 14:00:59,390] {docker.py:276} INFO - 21/05/17 17:00:59 INFO Executor: Running task 199.0 in stage 4.0 (TID 384)
[2021-05-17 14:00:59,399] {docker.py:276} INFO - 21/05/17 17:00:59 INFO ShuffleBlockFetcherIterator: Getting 4 (28.4 KiB) non-empty blocks including 4 (28.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 17:00:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 14:00:59,400] {docker.py:276} INFO - 21/05/17 17:00:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 17:00:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 17:00:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 17:00:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171658443404295623185469758_0004_m_000199_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443404295623185469758_0004_m_000199_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171658443404295623185469758_0004}; taskId=attempt_202105171658443404295623185469758_0004_m_000199_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3470ed51}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0047b4e0-1c24-41eb-9424-71bc19ca2353/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 14:00:59,401] {docker.py:276} INFO - 21/05/17 17:00:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 14:00:59,401] {docker.py:276} INFO - 21/05/17 17:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658443404295623185469758_0004_m_000199_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443404295623185469758_0004_m_000199_384
[2021-05-17 14:00:59,404] {docker.py:276} INFO - 21/05/17 17:00:59 INFO StagingCommitter: Task committer attempt_202105171658443404295623185469758_0004_m_000199_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0047b4e0-1c24-41eb-9424-71bc19ca2353/_temporary/0/_temporary/attempt_202105171658443404295623185469758_0004_m_000199_384 : duration 0:00.003s
[2021-05-17 14:00:59,981] {docker.py:276} INFO - 21/05/17 17:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105171658443774445857231172405_0004_m_000196_381: needsTaskCommit() Task attempt_202105171658443774445857231172405_0004_m_000196_381
21/05/17 17:00:59 INFO StagingCommitter: Task committer attempt_202105171658443774445857231172405_0004_m_000196_381: needsTaskCommit() Task attempt_202105171658443774445857231172405_0004_m_000196_381: duration 0:00.000s
21/05/17 17:00:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443774445857231172405_0004_m_000196_381
[2021-05-17 14:00:59,983] {docker.py:276} INFO - 21/05/17 17:01:00 INFO Executor: Finished task 196.0 in stage 4.0 (TID 381). 4544 bytes result sent to driver
[2021-05-17 14:00:59,985] {docker.py:276} INFO - 21/05/17 17:01:00 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 381) in 2598 ms on bb4563f4559d (executor driver) (197/200)
[2021-05-17 14:01:00,289] {docker.py:276} INFO - 21/05/17 17:01:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658448840502901869983783_0004_m_000197_382: needsTaskCommit() Task attempt_202105171658448840502901869983783_0004_m_000197_382
[2021-05-17 14:01:00,290] {docker.py:276} INFO - 21/05/17 17:01:00 INFO StagingCommitter: Task committer attempt_202105171658448840502901869983783_0004_m_000197_382: needsTaskCommit() Task attempt_202105171658448840502901869983783_0004_m_000197_382: duration 0:00.001s
21/05/17 17:01:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658448840502901869983783_0004_m_000197_382
[2021-05-17 14:01:00,295] {docker.py:276} INFO - 21/05/17 17:01:00 INFO Executor: Finished task 197.0 in stage 4.0 (TID 382). 4544 bytes result sent to driver
[2021-05-17 14:01:00,314] {docker.py:276} INFO - 21/05/17 17:01:00 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 382) in 2024 ms on bb4563f4559d (executor driver) (198/200)
[2021-05-17 14:01:00,842] {docker.py:276} INFO - 21/05/17 17:01:00 INFO StagingCommitter: Starting: Task committer attempt_202105171658446170258179672618863_0004_m_000198_383: needsTaskCommit() Task attempt_202105171658446170258179672618863_0004_m_000198_383
[2021-05-17 14:01:00,843] {docker.py:276} INFO - 21/05/17 17:01:00 INFO StagingCommitter: Task committer attempt_202105171658446170258179672618863_0004_m_000198_383: needsTaskCommit() Task attempt_202105171658446170258179672618863_0004_m_000198_383: duration 0:00.000s
21/05/17 17:01:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658446170258179672618863_0004_m_000198_383
[2021-05-17 14:01:00,846] {docker.py:276} INFO - 21/05/17 17:01:00 INFO Executor: Finished task 198.0 in stage 4.0 (TID 383). 4544 bytes result sent to driver
[2021-05-17 14:01:00,848] {docker.py:276} INFO - 21/05/17 17:01:00 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 383) in 2009 ms on bb4563f4559d (executor driver) (199/200)
[2021-05-17 14:01:01,583] {docker.py:276} INFO - 21/05/17 17:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105171658443404295623185469758_0004_m_000199_384: needsTaskCommit() Task attempt_202105171658443404295623185469758_0004_m_000199_384
[2021-05-17 14:01:01,584] {docker.py:276} INFO - 21/05/17 17:01:01 INFO StagingCommitter: Task committer attempt_202105171658443404295623185469758_0004_m_000199_384: needsTaskCommit() Task attempt_202105171658443404295623185469758_0004_m_000199_384: duration 0:00.001s
21/05/17 17:01:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171658443404295623185469758_0004_m_000199_384
[2021-05-17 14:01:01,586] {docker.py:276} INFO - 21/05/17 17:01:01 INFO Executor: Finished task 199.0 in stage 4.0 (TID 384). 4544 bytes result sent to driver
[2021-05-17 14:01:01,587] {docker.py:276} INFO - 21/05/17 17:01:01 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 384) in 2201 ms on bb4563f4559d (executor driver) (200/200)
21/05/17 17:01:01 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-17 14:01:01,588] {docker.py:276} INFO - 21/05/17 17:01:01 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 126.410 s
[2021-05-17 14:01:01,589] {docker.py:276} INFO - 21/05/17 17:01:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-17 14:01:01,590] {docker.py:276} INFO - 21/05/17 17:01:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-17 14:01:01,590] {docker.py:276} INFO - 21/05/17 17:01:01 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 137.379965 s
[2021-05-17 14:01:01,593] {docker.py:276} INFO - 21/05/17 17:01:01 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105171658436558453948365756556_0000_m_000000_0: commitJob((no job ID))
[2021-05-17 14:01:01,612] {docker.py:276} INFO - 21/05/17 17:01:01 WARN AbstractS3ACommitter: Task committer attempt_202105171658436558453948365756556_0000_m_000000_0: No pending uploads to commit
[2021-05-17 14:01:02,117] {docker.py:276} INFO - 21/05/17 17:01:02 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/17 17:01:02 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-17 14:01:02,301] {docker.py:276} INFO - 21/05/17 17:01:02 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.185s
21/05/17 17:01:02 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.185s
[2021-05-17 14:01:02,303] {docker.py:276} INFO - 21/05/17 17:01:02 INFO AbstractS3ACommitter: Task committer attempt_202105171658436558453948365756556_0000_m_000000_0: commitJob((no job ID)): duration 0:00.711s
[2021-05-17 14:01:02,807] {docker.py:276} INFO - 21/05/17 17:01:02 INFO FileFormatWriter: Write Job 0047b4e0-1c24-41eb-9424-71bc19ca2353 committed.
[2021-05-17 14:01:02,818] {docker.py:276} INFO - 21/05/17 17:01:02 INFO FileFormatWriter: Finished processing stats for write job 0047b4e0-1c24-41eb-9424-71bc19ca2353.
[2021-05-17 14:01:02,940] {docker.py:276} INFO - 21/05/17 17:01:02 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-17 14:01:02,956] {docker.py:276} INFO - 21/05/17 17:01:02 INFO SparkUI: Stopped Spark web UI at http://bb4563f4559d:4040
[2021-05-17 14:01:02,978] {docker.py:276} INFO - 21/05/17 17:01:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-17 14:01:02,995] {docker.py:276} INFO - 21/05/17 17:01:03 INFO MemoryStore: MemoryStore cleared
[2021-05-17 14:01:02,996] {docker.py:276} INFO - 21/05/17 17:01:03 INFO BlockManager: BlockManager stopped
[2021-05-17 14:01:03,000] {docker.py:276} INFO - 21/05/17 17:01:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-17 14:01:03,004] {docker.py:276} INFO - 21/05/17 17:01:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-17 14:01:03,011] {docker.py:276} INFO - 21/05/17 17:01:03 INFO SparkContext: Successfully stopped SparkContext
[2021-05-17 14:01:03,012] {docker.py:276} INFO - 21/05/17 17:01:03 INFO ShutdownHookManager: Shutdown hook called
[2021-05-17 14:01:03,013] {docker.py:276} INFO - 21/05/17 17:01:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-028f1a58-3087-4b1f-b419-911522b87ccb
[2021-05-17 14:01:03,017] {docker.py:276} INFO - 21/05/17 17:01:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-2eab7122-7a6e-464a-9ea8-ede530e36636/pyspark-dde914bb-812a-4b47-8850-b6c94e9f3e53
[2021-05-17 14:01:03,019] {docker.py:276} INFO - 21/05/17 17:01:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-2eab7122-7a6e-464a-9ea8-ede530e36636
[2021-05-17 14:01:03,029] {docker.py:276} INFO - 21/05/17 17:01:03 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-17 14:01:03,030] {docker.py:276} INFO - 21/05/17 17:01:03 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-17 14:01:03,031] {docker.py:276} INFO - 21/05/17 17:01:03 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-17 14:01:03,300] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210517T165726, start_date=20210517T165809, end_date=20210517T170103
[2021-05-17 14:01:03,363] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-17 14:01:03,401] {local_task_job.py:146} INFO - Task exited with return code 0
