[2021-05-14 11:00:55,941] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T13:59:51.547854+00:00 [queued]>
[2021-05-14 11:00:55,947] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T13:59:51.547854+00:00 [queued]>
[2021-05-14 11:00:55,947] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 11:00:55,947] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 11:00:55,947] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 11:00:55,953] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T13:59:51.547854+00:00
[2021-05-14 11:00:55,956] {standard_task_runner.py:52} INFO - Started process 19597 to run task
[2021-05-14 11:00:55,962] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T13:59:51.547854+00:00', '--job-id', '392', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpdgq9wnt0', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp4lx7f7vc']
[2021-05-14 11:00:55,963] {standard_task_runner.py:77} INFO - Job 392: Subtask run_spark_job
[2021-05-14 11:00:55,993] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T13:59:51.547854+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-14 11:00:56,014] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T13:59:51.547854+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T13:59:51.547854+00:00
[2021-05-14 11:00:56,018] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 11:00:59,574] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 11:00:59,577] {docker.py:312} INFO - Digest: sha256:59c324d0ea6269fbd4f520bfa9d09121c4dbc9d525570712d6805ba6501a358c
[2021-05-14 11:00:59,577] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 11:00:59,581] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 11:01:01,613] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 11:01:02,151] {docker.py:276} INFO - 21/05/14 14:01:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 11:01:04,448] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 11:01:04,461] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 11:01:04,531] {docker.py:276} INFO - 21/05/14 14:01:04 INFO ResourceUtils: ==============================================================
[2021-05-14 11:01:04,532] {docker.py:276} INFO - 21/05/14 14:01:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 11:01:04,533] {docker.py:276} INFO - 21/05/14 14:01:04 INFO ResourceUtils: ==============================================================
[2021-05-14 11:01:04,534] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SparkContext: Submitted application: spark.py
[2021-05-14 11:01:04,571] {docker.py:276} INFO - 21/05/14 14:01:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 11:01:04,587] {docker.py:276} INFO - 21/05/14 14:01:04 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 11:01:04,589] {docker.py:276} INFO - 21/05/14 14:01:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 11:01:04,671] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-14 11:01:04,672] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 11:01:04,672] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SecurityManager: Changing view acls groups to:
[2021-05-14 11:01:04,672] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SecurityManager: Changing modify acls groups to:
[2021-05-14 11:01:04,673] {docker.py:276} INFO - 21/05/14 14:01:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 11:01:05,042] {docker.py:276} INFO - 21/05/14 14:01:05 INFO Utils: Successfully started service 'sparkDriver' on port 34855.
[2021-05-14 11:01:05,083] {docker.py:276} INFO - 21/05/14 14:01:05 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 11:01:05,133] {docker.py:276} INFO - 21/05/14 14:01:05 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 11:01:05,187] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 11:01:05,189] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 11:01:05,195] {docker.py:276} INFO - 21/05/14 14:01:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 11:01:05,212] {docker.py:276} INFO - 21/05/14 14:01:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ebfe667-ab05-4d11-a81a-ce680eb2351f
[2021-05-14 11:01:05,239] {docker.py:276} INFO - 21/05/14 14:01:05 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 11:01:05,262] {docker.py:276} INFO - 21/05/14 14:01:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 11:01:05,536] {docker.py:276} INFO - 21/05/14 14:01:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 11:01:05,637] {docker.py:276} INFO - 21/05/14 14:01:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3e4cb2948233:4040
[2021-05-14 11:01:05,898] {docker.py:276} INFO - 21/05/14 14:01:05 INFO Executor: Starting executor ID driver on host 3e4cb2948233
[2021-05-14 11:01:05,938] {docker.py:276} INFO - 21/05/14 14:01:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43567.
[2021-05-14 11:01:05,938] {docker.py:276} INFO - 21/05/14 14:01:05 INFO NettyBlockTransferService: Server created on 3e4cb2948233:43567
[2021-05-14 11:01:05,941] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 11:01:05,953] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3e4cb2948233, 43567, None)
[2021-05-14 11:01:05,962] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManagerMasterEndpoint: Registering block manager 3e4cb2948233:43567 with 934.4 MiB RAM, BlockManagerId(driver, 3e4cb2948233, 43567, None)
[2021-05-14 11:01:05,966] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3e4cb2948233, 43567, None)
[2021-05-14 11:01:05,968] {docker.py:276} INFO - 21/05/14 14:01:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3e4cb2948233, 43567, None)
[2021-05-14 11:01:06,545] {docker.py:276} INFO - 21/05/14 14:01:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 11:01:06,546] {docker.py:276} INFO - 21/05/14 14:01:06 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 11:01:07,601] {docker.py:276} INFO - 21/05/14 14:01:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 11:01:07,655] {docker.py:276} INFO - 21/05/14 14:01:07 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-14 11:01:07,655] {docker.py:276} INFO - 21/05/14 14:01:07 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 11:01:17,111] {docker.py:276} INFO - 21/05/14 14:01:17 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14/from_1620914410_to_1620916210.csv, s3a://udac-forex-project/1/2021-05-14/from_1620916210_to_1620918010.csv, s3a://udac-forex-project/1/2021-05-14/from_1620918010_to_1620919810.csv, s3a://udac-forex-project/1/2021-05-14/from_1620919810_to_1620921610.csv, s3a://udac-forex-project/1/2021-05-14/from_1620921610_to_1620923410.csv, s3a://udac-forex-project/1/2021-05-14/from_1620923410_to_1620925210.csv, s3a://udac-forex-project/1/2021-05-14/from_1620925210_to_1620927010.csv, s3a://udac-forex-project/1/2021-05-14/from_1620927010_to_1620928810.csv, s3a://udac-forex-project/1/2021-05-14/from_1620928810_to_1620930610.csv, s3a://udac-forex-project/1/2021-05-14/from_1620930610_to_1620932410.csv.
[2021-05-14 11:01:17,678] {docker.py:276} INFO - 21/05/14 14:01:17 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:17,706] {docker.py:276} INFO - 21/05/14 14:01:17 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-14 11:01:17,706] {docker.py:276} INFO - 21/05/14 14:01:17 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 11:01:17,712] {docker.py:276} INFO - 21/05/14 14:01:17 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 11:01:17,716] {docker.py:276} INFO - 21/05/14 14:01:17 INFO DAGScheduler: Missing parents: List()
[2021-05-14 11:01:17,724] {docker.py:276} INFO - 21/05/14 14:01:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:01:17,870] {docker.py:276} INFO - 21/05/14 14:01:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.8 KiB, free 934.3 MiB)
[2021-05-14 11:01:17,951] {docker.py:276} INFO - 21/05/14 14:01:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 934.3 MiB)
[2021-05-14 11:01:17,955] {docker.py:276} INFO - 21/05/14 14:01:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3e4cb2948233:43567 (size: 30.2 KiB, free: 934.4 MiB)
[2021-05-14 11:01:17,988] {docker.py:276} INFO - 21/05/14 14:01:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:01:18,021] {docker.py:276} INFO - 21/05/14 14:01:18 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 11:01:18,023] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-14 11:01:18,119] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3e4cb2948233, executor driver, partition 0, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,123] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (3e4cb2948233, executor driver, partition 1, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,124] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (3e4cb2948233, executor driver, partition 2, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,125] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (3e4cb2948233, executor driver, partition 3, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,154] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-14 11:01:18,154] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-14 11:01:18,155] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 11:01:18,155] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-14 11:01:18,576] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1868 bytes result sent to driver
[2021-05-14 11:01:18,585] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (3e4cb2948233, executor driver, partition 4, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,586] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-14 11:01:18,600] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 513 ms on 3e4cb2948233 (executor driver) (1/141)
[2021-05-14 11:01:18,770] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1825 bytes result sent to driver
[2021-05-14 11:01:18,772] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (3e4cb2948233, executor driver, partition 5, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,774] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-14 11:01:18,775] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 195 ms on 3e4cb2948233 (executor driver) (2/141)
[2021-05-14 11:01:18,968] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1868 bytes result sent to driver
[2021-05-14 11:01:18,971] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (3e4cb2948233, executor driver, partition 6, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:18,973] {docker.py:276} INFO - 21/05/14 14:01:18 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-14 11:01:18,974] {docker.py:276} INFO - 21/05/14 14:01:18 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 202 ms on 3e4cb2948233 (executor driver) (3/141)
[2021-05-14 11:01:19,068] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1868 bytes result sent to driver
[2021-05-14 11:01:19,071] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (3e4cb2948233, executor driver, partition 7, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,072] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 950 ms on 3e4cb2948233 (executor driver) (4/141)
[2021-05-14 11:01:19,073] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-14 11:01:19,092] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1868 bytes result sent to driver
[2021-05-14 11:01:19,094] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (3e4cb2948233, executor driver, partition 8, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,097] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-14 11:01:19,097] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 974 ms on 3e4cb2948233 (executor driver) (5/141)
[2021-05-14 11:01:19,098] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1868 bytes result sent to driver
[2021-05-14 11:01:19,098] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (3e4cb2948233, executor driver, partition 9, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,100] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 977 ms on 3e4cb2948233 (executor driver) (6/141)
[2021-05-14 11:01:19,100] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-14 11:01:19,157] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1825 bytes result sent to driver
[2021-05-14 11:01:19,159] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (3e4cb2948233, executor driver, partition 10, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,164] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 191 ms on 3e4cb2948233 (executor driver) (7/141)
[2021-05-14 11:01:19,165] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-14 11:01:19,266] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1825 bytes result sent to driver
[2021-05-14 11:01:19,269] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (3e4cb2948233, executor driver, partition 11, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,271] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 202 ms on 3e4cb2948233 (executor driver) (8/141)
[2021-05-14 11:01:19,272] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-14 11:01:19,283] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1825 bytes result sent to driver
[2021-05-14 11:01:19,284] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (3e4cb2948233, executor driver, partition 12, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,286] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 189 ms on 3e4cb2948233 (executor driver) (9/141)
[2021-05-14 11:01:19,288] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1825 bytes result sent to driver
[2021-05-14 11:01:19,288] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-14 11:01:19,290] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (3e4cb2948233, executor driver, partition 13, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,291] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 197 ms on 3e4cb2948233 (executor driver) (10/141)
[2021-05-14 11:01:19,291] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-14 11:01:19,341] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1825 bytes result sent to driver
[2021-05-14 11:01:19,343] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (3e4cb2948233, executor driver, partition 14, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,345] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
21/05/14 14:01:19 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 186 ms on 3e4cb2948233 (executor driver) (11/141)
[2021-05-14 11:01:19,452] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1825 bytes result sent to driver
[2021-05-14 11:01:19,455] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (3e4cb2948233, executor driver, partition 15, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,457] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 189 ms on 3e4cb2948233 (executor driver) (12/141)
[2021-05-14 11:01:19,459] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-14 11:01:19,467] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1825 bytes result sent to driver
[2021-05-14 11:01:19,469] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (3e4cb2948233, executor driver, partition 16, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,470] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
21/05/14 14:01:19 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 181 ms on 3e4cb2948233 (executor driver) (13/141)
[2021-05-14 11:01:19,475] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1825 bytes result sent to driver
[2021-05-14 11:01:19,476] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (3e4cb2948233, executor driver, partition 17, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,477] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 194 ms on 3e4cb2948233 (executor driver) (14/141)
[2021-05-14 11:01:19,478] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-14 11:01:19,520] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1825 bytes result sent to driver
[2021-05-14 11:01:19,522] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (3e4cb2948233, executor driver, partition 18, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,523] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
21/05/14 14:01:19 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 181 ms on 3e4cb2948233 (executor driver) (15/141)
[2021-05-14 11:01:19,646] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1825 bytes result sent to driver
[2021-05-14 11:01:19,649] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (3e4cb2948233, executor driver, partition 19, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,651] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 197 ms on 3e4cb2948233 (executor driver) (16/141)
[2021-05-14 11:01:19,651] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-14 11:01:19,667] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1825 bytes result sent to driver
[2021-05-14 11:01:19,668] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1868 bytes result sent to driver
[2021-05-14 11:01:19,669] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (3e4cb2948233, executor driver, partition 20, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,670] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-14 11:01:19,671] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (3e4cb2948233, executor driver, partition 21, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,672] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 204 ms on 3e4cb2948233 (executor driver) (17/141)
[2021-05-14 11:01:19,673] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 197 ms on 3e4cb2948233 (executor driver) (18/141)
[2021-05-14 11:01:19,673] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-14 11:01:19,704] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1868 bytes result sent to driver
[2021-05-14 11:01:19,705] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (3e4cb2948233, executor driver, partition 22, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,706] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 185 ms on 3e4cb2948233 (executor driver) (19/141)
[2021-05-14 11:01:19,711] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-14 11:01:19,859] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1825 bytes result sent to driver
[2021-05-14 11:01:19,862] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1825 bytes result sent to driver
[2021-05-14 11:01:19,863] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (3e4cb2948233, executor driver, partition 23, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,864] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 194 ms on 3e4cb2948233 (executor driver) (20/141)
[2021-05-14 11:01:19,866] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-14 11:01:19,867] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (3e4cb2948233, executor driver, partition 24, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,868] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 199 ms on 3e4cb2948233 (executor driver) (21/141)
[2021-05-14 11:01:19,869] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-14 11:01:19,870] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1868 bytes result sent to driver
[2021-05-14 11:01:19,872] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (3e4cb2948233, executor driver, partition 25, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,872] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 226 ms on 3e4cb2948233 (executor driver) (22/141)
[2021-05-14 11:01:19,873] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-14 11:01:19,945] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1825 bytes result sent to driver
[2021-05-14 11:01:19,949] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (3e4cb2948233, executor driver, partition 26, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:19,951] {docker.py:276} INFO - 21/05/14 14:01:19 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 246 ms on 3e4cb2948233 (executor driver) (23/141)
[2021-05-14 11:01:19,952] {docker.py:276} INFO - 21/05/14 14:01:19 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-14 11:01:20,055] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1825 bytes result sent to driver
[2021-05-14 11:01:20,056] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1825 bytes result sent to driver
[2021-05-14 11:01:20,057] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (3e4cb2948233, executor driver, partition 27, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,059] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1825 bytes result sent to driver
[2021-05-14 11:01:20,059] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 192 ms on 3e4cb2948233 (executor driver) (24/141)
[2021-05-14 11:01:20,061] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-14 11:01:20,061] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (3e4cb2948233, executor driver, partition 28, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,067] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 192 ms on 3e4cb2948233 (executor driver) (25/141)
[2021-05-14 11:01:20,068] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-14 11:01:20,068] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (3e4cb2948233, executor driver, partition 29, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,068] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 202 ms on 3e4cb2948233 (executor driver) (26/141)
[2021-05-14 11:01:20,069] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-14 11:01:20,130] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1825 bytes result sent to driver
[2021-05-14 11:01:20,132] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (3e4cb2948233, executor driver, partition 30, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,134] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 186 ms on 3e4cb2948233 (executor driver) (27/141)
21/05/14 14:01:20 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-14 11:01:20,239] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1825 bytes result sent to driver
[2021-05-14 11:01:20,242] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (3e4cb2948233, executor driver, partition 31, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,243] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 188 ms on 3e4cb2948233 (executor driver) (28/141)
[2021-05-14 11:01:20,244] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-14 11:01:20,245] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1825 bytes result sent to driver
[2021-05-14 11:01:20,248] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (3e4cb2948233, executor driver, partition 32, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,248] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1825 bytes result sent to driver
[2021-05-14 11:01:20,249] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-14 11:01:20,250] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (3e4cb2948233, executor driver, partition 33, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,251] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 191 ms on 3e4cb2948233 (executor driver) (29/141)
[2021-05-14 11:01:20,252] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-14 11:01:20,252] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 188 ms on 3e4cb2948233 (executor driver) (30/141)
[2021-05-14 11:01:20,314] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1868 bytes result sent to driver
[2021-05-14 11:01:20,317] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (3e4cb2948233, executor driver, partition 34, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,318] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
21/05/14 14:01:20 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 187 ms on 3e4cb2948233 (executor driver) (31/141)
[2021-05-14 11:01:20,437] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1868 bytes result sent to driver
[2021-05-14 11:01:20,438] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1868 bytes result sent to driver
[2021-05-14 11:01:20,440] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (3e4cb2948233, executor driver, partition 35, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,442] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (3e4cb2948233, executor driver, partition 36, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,443] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 201 ms on 3e4cb2948233 (executor driver) (32/141)
[2021-05-14 11:01:20,444] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 197 ms on 3e4cb2948233 (executor driver) (33/141)
[2021-05-14 11:01:20,444] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
21/05/14 14:01:20 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-14 11:01:20,451] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1868 bytes result sent to driver
[2021-05-14 11:01:20,452] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (3e4cb2948233, executor driver, partition 37, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,454] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
21/05/14 14:01:20 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 205 ms on 3e4cb2948233 (executor driver) (34/141)
[2021-05-14 11:01:20,493] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1825 bytes result sent to driver
[2021-05-14 11:01:20,495] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (3e4cb2948233, executor driver, partition 38, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,496] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 180 ms on 3e4cb2948233 (executor driver) (35/141)
[2021-05-14 11:01:20,497] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-14 11:01:20,626] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1825 bytes result sent to driver
[2021-05-14 11:01:20,628] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (3e4cb2948233, executor driver, partition 39, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,628] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1825 bytes result sent to driver
[2021-05-14 11:01:20,629] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1825 bytes result sent to driver
[2021-05-14 11:01:20,630] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 189 ms on 3e4cb2948233 (executor driver) (36/141)
[2021-05-14 11:01:20,632] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-14 11:01:20,632] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (3e4cb2948233, executor driver, partition 40, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,634] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (3e4cb2948233, executor driver, partition 41, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,635] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-14 11:01:20,636] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 197 ms on 3e4cb2948233 (executor driver) (37/141)
[2021-05-14 11:01:20,636] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-14 11:01:20,637] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 185 ms on 3e4cb2948233 (executor driver) (38/141)
[2021-05-14 11:01:20,667] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1825 bytes result sent to driver
[2021-05-14 11:01:20,668] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (3e4cb2948233, executor driver, partition 42, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,669] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 175 ms on 3e4cb2948233 (executor driver) (39/141)
[2021-05-14 11:01:20,670] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-14 11:01:20,808] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1825 bytes result sent to driver
[2021-05-14 11:01:20,811] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (3e4cb2948233, executor driver, partition 43, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,811] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 179 ms on 3e4cb2948233 (executor driver) (40/141)
[2021-05-14 11:01:20,812] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-14 11:01:20,813] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1825 bytes result sent to driver
[2021-05-14 11:01:20,814] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (3e4cb2948233, executor driver, partition 44, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,815] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 188 ms on 3e4cb2948233 (executor driver) (41/141)
[2021-05-14 11:01:20,816] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-14 11:01:20,825] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1825 bytes result sent to driver
[2021-05-14 11:01:20,826] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (3e4cb2948233, executor driver, partition 45, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,827] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 194 ms on 3e4cb2948233 (executor driver) (42/141)
[2021-05-14 11:01:20,828] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-14 11:01:20,848] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1825 bytes result sent to driver
[2021-05-14 11:01:20,849] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (3e4cb2948233, executor driver, partition 46, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,850] {docker.py:276} INFO - 21/05/14 14:01:20 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 182 ms on 3e4cb2948233 (executor driver) (43/141)
[2021-05-14 11:01:20,851] {docker.py:276} INFO - 21/05/14 14:01:20 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-14 11:01:20,989] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1825 bytes result sent to driver
[2021-05-14 11:01:20,991] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (3e4cb2948233, executor driver, partition 47, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,992] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 178 ms on 3e4cb2948233 (executor driver) (44/141)
[2021-05-14 11:01:20,993] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-14 11:01:20,994] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1825 bytes result sent to driver
[2021-05-14 11:01:20,995] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (3e4cb2948233, executor driver, partition 48, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:20,996] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 186 ms on 3e4cb2948233 (executor driver) (45/141)
[2021-05-14 11:01:20,997] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-14 11:01:21,006] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1868 bytes result sent to driver
[2021-05-14 11:01:21,007] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (3e4cb2948233, executor driver, partition 49, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,009] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
21/05/14 14:01:21 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 183 ms on 3e4cb2948233 (executor driver) (46/141)
[2021-05-14 11:01:21,029] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1868 bytes result sent to driver
[2021-05-14 11:01:21,034] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (3e4cb2948233, executor driver, partition 50, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,035] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 184 ms on 3e4cb2948233 (executor driver) (47/141)
[2021-05-14 11:01:21,035] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-14 11:01:21,186] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1868 bytes result sent to driver
[2021-05-14 11:01:21,187] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1868 bytes result sent to driver
[2021-05-14 11:01:21,189] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1825 bytes result sent to driver
[2021-05-14 11:01:21,190] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (3e4cb2948233, executor driver, partition 51, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,191] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 201 ms on 3e4cb2948233 (executor driver) (48/141)
[2021-05-14 11:01:21,193] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-14 11:01:21,195] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (3e4cb2948233, executor driver, partition 52, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,196] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-14 11:01:21,197] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (3e4cb2948233, executor driver, partition 53, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,198] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-14 11:01:21,199] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 203 ms on 3e4cb2948233 (executor driver) (49/141)
[2021-05-14 11:01:21,200] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 193 ms on 3e4cb2948233 (executor driver) (50/141)
[2021-05-14 11:01:21,213] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1825 bytes result sent to driver
[2021-05-14 11:01:21,215] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (3e4cb2948233, executor driver, partition 54, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,216] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 185 ms on 3e4cb2948233 (executor driver) (51/141)
[2021-05-14 11:01:21,217] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-14 11:01:21,377] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1825 bytes result sent to driver
[2021-05-14 11:01:21,379] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1825 bytes result sent to driver
[2021-05-14 11:01:21,381] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (3e4cb2948233, executor driver, partition 55, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,383] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-14 11:01:21,384] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (3e4cb2948233, executor driver, partition 56, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,385] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1868 bytes result sent to driver
[2021-05-14 11:01:21,386] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-14 11:01:21,386] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (3e4cb2948233, executor driver, partition 57, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,387] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 200 ms on 3e4cb2948233 (executor driver) (52/141)
[2021-05-14 11:01:21,388] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 192 ms on 3e4cb2948233 (executor driver) (53/141)
[2021-05-14 11:01:21,389] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 196 ms on 3e4cb2948233 (executor driver) (54/141)
[2021-05-14 11:01:21,391] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-14 11:01:21,394] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1825 bytes result sent to driver
[2021-05-14 11:01:21,395] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (3e4cb2948233, executor driver, partition 58, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,396] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 182 ms on 3e4cb2948233 (executor driver) (55/141)
[2021-05-14 11:01:21,397] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-14 11:01:21,557] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1825 bytes result sent to driver
[2021-05-14 11:01:21,558] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (3e4cb2948233, executor driver, partition 59, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,559] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 180 ms on 3e4cb2948233 (executor driver) (56/141)
[2021-05-14 11:01:21,560] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-14 11:01:21,575] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1825 bytes result sent to driver
[2021-05-14 11:01:21,577] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (3e4cb2948233, executor driver, partition 60, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,578] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 193 ms on 3e4cb2948233 (executor driver) (57/141)
[2021-05-14 11:01:21,578] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-14 11:01:21,580] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1825 bytes result sent to driver
[2021-05-14 11:01:21,587] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (3e4cb2948233, executor driver, partition 61, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,587] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1825 bytes result sent to driver
[2021-05-14 11:01:21,588] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-14 11:01:21,588] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (3e4cb2948233, executor driver, partition 62, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,588] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-14 11:01:21,589] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 203 ms on 3e4cb2948233 (executor driver) (58/141)
[2021-05-14 11:01:21,589] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 190 ms on 3e4cb2948233 (executor driver) (59/141)
[2021-05-14 11:01:21,732] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1868 bytes result sent to driver
[2021-05-14 11:01:21,734] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (3e4cb2948233, executor driver, partition 63, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,735] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 178 ms on 3e4cb2948233 (executor driver) (60/141)
[2021-05-14 11:01:21,736] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-14 11:01:21,763] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1868 bytes result sent to driver
[2021-05-14 11:01:21,764] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (3e4cb2948233, executor driver, partition 64, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:21 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1868 bytes result sent to driver
[2021-05-14 11:01:21,765] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-14 11:01:21,768] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (3e4cb2948233, executor driver, partition 65, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,768] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-14 11:01:21,769] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 185 ms on 3e4cb2948233 (executor driver) (61/141)
[2021-05-14 11:01:21,770] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 189 ms on 3e4cb2948233 (executor driver) (62/141)
[2021-05-14 11:01:21,863] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1868 bytes result sent to driver
[2021-05-14 11:01:21,865] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (3e4cb2948233, executor driver, partition 66, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,866] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-14 11:01:21,867] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 291 ms on 3e4cb2948233 (executor driver) (63/141)
[2021-05-14 11:01:21,909] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1825 bytes result sent to driver
[2021-05-14 11:01:21,911] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (3e4cb2948233, executor driver, partition 67, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,912] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
21/05/14 14:01:21 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 179 ms on 3e4cb2948233 (executor driver) (64/141)
[2021-05-14 11:01:21,938] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1825 bytes result sent to driver
[2021-05-14 11:01:21,939] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (3e4cb2948233, executor driver, partition 68, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,940] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 175 ms on 3e4cb2948233 (executor driver) (65/141)
[2021-05-14 11:01:21,941] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-14 11:01:21,943] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1825 bytes result sent to driver
[2021-05-14 11:01:21,945] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (3e4cb2948233, executor driver, partition 69, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:21,945] {docker.py:276} INFO - 21/05/14 14:01:21 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 182 ms on 3e4cb2948233 (executor driver) (66/141)
[2021-05-14 11:01:21,946] {docker.py:276} INFO - 21/05/14 14:01:21 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-14 11:01:22,050] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1825 bytes result sent to driver
[2021-05-14 11:01:22,052] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (3e4cb2948233, executor driver, partition 70, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,053] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 190 ms on 3e4cb2948233 (executor driver) (67/141)
[2021-05-14 11:01:22,054] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-14 11:01:22,099] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1825 bytes result sent to driver
[2021-05-14 11:01:22,101] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (3e4cb2948233, executor driver, partition 71, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,102] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-14 11:01:22,103] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 192 ms on 3e4cb2948233 (executor driver) (68/141)
[2021-05-14 11:01:22,118] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1825 bytes result sent to driver
[2021-05-14 11:01:22,119] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (3e4cb2948233, executor driver, partition 72, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,120] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
21/05/14 14:01:22 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 181 ms on 3e4cb2948233 (executor driver) (69/141)
[2021-05-14 11:01:22,123] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1825 bytes result sent to driver
[2021-05-14 11:01:22,124] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (3e4cb2948233, executor driver, partition 73, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,125] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 180 ms on 3e4cb2948233 (executor driver) (70/141)
[2021-05-14 11:01:22,125] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-14 11:01:22,236] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1825 bytes result sent to driver
[2021-05-14 11:01:22,238] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (3e4cb2948233, executor driver, partition 74, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,239] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 188 ms on 3e4cb2948233 (executor driver) (71/141)
[2021-05-14 11:01:22,239] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-14 11:01:22,280] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1825 bytes result sent to driver
[2021-05-14 11:01:22,282] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (3e4cb2948233, executor driver, partition 75, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,284] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-14 11:01:22,284] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 183 ms on 3e4cb2948233 (executor driver) (72/141)
[2021-05-14 11:01:22,294] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1825 bytes result sent to driver
[2021-05-14 11:01:22,295] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (3e4cb2948233, executor driver, partition 76, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,296] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
21/05/14 14:01:22 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 178 ms on 3e4cb2948233 (executor driver) (73/141)
[2021-05-14 11:01:22,301] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1825 bytes result sent to driver
[2021-05-14 11:01:22,301] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (3e4cb2948233, executor driver, partition 77, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,303] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-14 11:01:22,303] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 180 ms on 3e4cb2948233 (executor driver) (74/141)
[2021-05-14 11:01:22,423] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1868 bytes result sent to driver
[2021-05-14 11:01:22,424] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (3e4cb2948233, executor driver, partition 78, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,425] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-14 11:01:22,426] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 188 ms on 3e4cb2948233 (executor driver) (75/141)
[2021-05-14 11:01:22,460] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1868 bytes result sent to driver
[2021-05-14 11:01:22,462] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (3e4cb2948233, executor driver, partition 79, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,464] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 183 ms on 3e4cb2948233 (executor driver) (76/141)
[2021-05-14 11:01:22,465] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-14 11:01:22,501] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1868 bytes result sent to driver
[2021-05-14 11:01:22,503] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (3e4cb2948233, executor driver, partition 80, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,505] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 203 ms on 3e4cb2948233 (executor driver) (77/141)
21/05/14 14:01:22 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-14 11:01:22,540] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1868 bytes result sent to driver
[2021-05-14 11:01:22,544] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (3e4cb2948233, executor driver, partition 81, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,547] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-14 11:01:22,547] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 252 ms on 3e4cb2948233 (executor driver) (78/141)
[2021-05-14 11:01:22,609] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1825 bytes result sent to driver
[2021-05-14 11:01:22,611] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (3e4cb2948233, executor driver, partition 82, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,612] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-14 11:01:22,613] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 188 ms on 3e4cb2948233 (executor driver) (79/141)
[2021-05-14 11:01:22,638] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1825 bytes result sent to driver
[2021-05-14 11:01:22,641] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (3e4cb2948233, executor driver, partition 83, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,642] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-14 11:01:22,642] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 179 ms on 3e4cb2948233 (executor driver) (80/141)
[2021-05-14 11:01:22,680] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1825 bytes result sent to driver
[2021-05-14 11:01:22,681] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (3e4cb2948233, executor driver, partition 84, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,681] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 179 ms on 3e4cb2948233 (executor driver) (81/141)
[2021-05-14 11:01:22,683] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-14 11:01:22,728] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1825 bytes result sent to driver
[2021-05-14 11:01:22,729] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (3e4cb2948233, executor driver, partition 85, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,730] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 188 ms on 3e4cb2948233 (executor driver) (82/141)
[2021-05-14 11:01:22,730] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-14 11:01:22,821] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1825 bytes result sent to driver
[2021-05-14 11:01:22,822] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (3e4cb2948233, executor driver, partition 86, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,824] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-14 11:01:22,824] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 185 ms on 3e4cb2948233 (executor driver) (83/141)
[2021-05-14 11:01:22,836] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1825 bytes result sent to driver
[2021-05-14 11:01:22,838] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (3e4cb2948233, executor driver, partition 87, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,839] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 229 ms on 3e4cb2948233 (executor driver) (84/141)
21/05/14 14:01:22 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-14 11:01:22,853] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1825 bytes result sent to driver
[2021-05-14 11:01:22,854] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (3e4cb2948233, executor driver, partition 88, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,855] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 174 ms on 3e4cb2948233 (executor driver) (85/141)
[2021-05-14 11:01:22,855] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-14 11:01:22,912] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1825 bytes result sent to driver
[2021-05-14 11:01:22,913] {docker.py:276} INFO - 21/05/14 14:01:22 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (3e4cb2948233, executor driver, partition 89, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:22,914] {docker.py:276} INFO - 21/05/14 14:01:22 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
21/05/14 14:01:22 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 185 ms on 3e4cb2948233 (executor driver) (86/141)
[2021-05-14 11:01:23,001] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1825 bytes result sent to driver
[2021-05-14 11:01:23,004] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (3e4cb2948233, executor driver, partition 90, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,005] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 183 ms on 3e4cb2948233 (executor driver) (87/141)
[2021-05-14 11:01:23,006] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-14 11:01:23,020] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1825 bytes result sent to driver
[2021-05-14 11:01:23,022] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (3e4cb2948233, executor driver, partition 91, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,022] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-14 11:01:23,023] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 185 ms on 3e4cb2948233 (executor driver) (88/141)
[2021-05-14 11:01:23,085] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1825 bytes result sent to driver
[2021-05-14 11:01:23,086] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (3e4cb2948233, executor driver, partition 92, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,088] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 234 ms on 3e4cb2948233 (executor driver) (89/141)
21/05/14 14:01:23 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-14 11:01:23,109] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1825 bytes result sent to driver
[2021-05-14 11:01:23,111] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (3e4cb2948233, executor driver, partition 93, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,111] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 199 ms on 3e4cb2948233 (executor driver) (90/141)
[2021-05-14 11:01:23,119] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-14 11:01:23,180] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1868 bytes result sent to driver
[2021-05-14 11:01:23,181] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (3e4cb2948233, executor driver, partition 94, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,183] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 181 ms on 3e4cb2948233 (executor driver) (91/141)
[2021-05-14 11:01:23,184] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-14 11:01:23,200] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1868 bytes result sent to driver
[2021-05-14 11:01:23,201] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 181 ms on 3e4cb2948233 (executor driver) (92/141)
[2021-05-14 11:01:23,202] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (3e4cb2948233, executor driver, partition 95, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,203] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-14 11:01:23,260] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1868 bytes result sent to driver
[2021-05-14 11:01:23,261] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (3e4cb2948233, executor driver, partition 96, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,263] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 178 ms on 3e4cb2948233 (executor driver) (93/141)
[2021-05-14 11:01:23,263] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-14 11:01:23,308] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1825 bytes result sent to driver
21/05/14 14:01:23 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (3e4cb2948233, executor driver, partition 97, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:23 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 196 ms on 3e4cb2948233 (executor driver) (94/141)
21/05/14 14:01:23 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-14 11:01:23,362] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1825 bytes result sent to driver
[2021-05-14 11:01:23,362] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (3e4cb2948233, executor driver, partition 98, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:23 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
21/05/14 14:01:23 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 180 ms on 3e4cb2948233 (executor driver) (95/141)
[2021-05-14 11:01:23,382] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1825 bytes result sent to driver
[2021-05-14 11:01:23,385] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (3e4cb2948233, executor driver, partition 99, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:23 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 182 ms on 3e4cb2948233 (executor driver) (96/141)
21/05/14 14:01:23 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
[2021-05-14 11:01:23,435] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1825 bytes result sent to driver
[2021-05-14 11:01:23,436] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (3e4cb2948233, executor driver, partition 100, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,437] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 177 ms on 3e4cb2948233 (executor driver) (97/141)
21/05/14 14:01:23 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-14 11:01:23,496] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1825 bytes result sent to driver
[2021-05-14 11:01:23,497] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (3e4cb2948233, executor driver, partition 101, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,497] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-14 11:01:23,499] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 193 ms on 3e4cb2948233 (executor driver) (98/141)
[2021-05-14 11:01:23,538] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1825 bytes result sent to driver
[2021-05-14 11:01:23,540] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (3e4cb2948233, executor driver, partition 102, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,542] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 182 ms on 3e4cb2948233 (executor driver) (99/141)
[2021-05-14 11:01:23,543] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-14 11:01:23,562] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1825 bytes result sent to driver
[2021-05-14 11:01:23,563] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 180 ms on 3e4cb2948233 (executor driver) (100/141)
[2021-05-14 11:01:23,563] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (3e4cb2948233, executor driver, partition 103, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,564] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-14 11:01:23,610] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1825 bytes result sent to driver
[2021-05-14 11:01:23,611] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (3e4cb2948233, executor driver, partition 104, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,612] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 177 ms on 3e4cb2948233 (executor driver) (101/141)
[2021-05-14 11:01:23,614] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-14 11:01:23,677] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1825 bytes result sent to driver
[2021-05-14 11:01:23,678] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (3e4cb2948233, executor driver, partition 105, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,678] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 183 ms on 3e4cb2948233 (executor driver) (102/141)
21/05/14 14:01:23 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-14 11:01:23,714] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1825 bytes result sent to driver
[2021-05-14 11:01:23,715] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (3e4cb2948233, executor driver, partition 106, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,716] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 176 ms on 3e4cb2948233 (executor driver) (103/141)
[2021-05-14 11:01:23,716] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-14 11:01:23,739] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1825 bytes result sent to driver
[2021-05-14 11:01:23,741] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (3e4cb2948233, executor driver, partition 107, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,742] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 179 ms on 3e4cb2948233 (executor driver) (104/141)
[2021-05-14 11:01:23,743] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-14 11:01:23,787] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1825 bytes result sent to driver
[2021-05-14 11:01:23,788] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (3e4cb2948233, executor driver, partition 108, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,789] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-14 11:01:23,789] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 178 ms on 3e4cb2948233 (executor driver) (105/141)
[2021-05-14 11:01:23,863] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1868 bytes result sent to driver
[2021-05-14 11:01:23,864] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (3e4cb2948233, executor driver, partition 109, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,865] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 188 ms on 3e4cb2948233 (executor driver) (106/141)
21/05/14 14:01:23 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-14 11:01:23,890] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1868 bytes result sent to driver
[2021-05-14 11:01:23,892] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (3e4cb2948233, executor driver, partition 110, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,894] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 179 ms on 3e4cb2948233 (executor driver) (107/141)
[2021-05-14 11:01:23,894] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-14 11:01:23,922] {docker.py:276} INFO - 21/05/14 14:01:23 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1868 bytes result sent to driver
[2021-05-14 11:01:23,924] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (3e4cb2948233, executor driver, partition 111, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,926] {docker.py:276} INFO - 21/05/14 14:01:23 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 184 ms on 3e4cb2948233 (executor driver) (108/141)
21/05/14 14:01:23 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-14 11:01:23,986] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1868 bytes result sent to driver
[2021-05-14 11:01:23,988] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (3e4cb2948233, executor driver, partition 112, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:23,989] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-14 11:01:23,991] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 201 ms on 3e4cb2948233 (executor driver) (109/141)
[2021-05-14 11:01:24,047] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1825 bytes result sent to driver
[2021-05-14 11:01:24,049] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (3e4cb2948233, executor driver, partition 113, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,051] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-14 11:01:24,052] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 186 ms on 3e4cb2948233 (executor driver) (110/141)
[2021-05-14 11:01:24,072] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1825 bytes result sent to driver
[2021-05-14 11:01:24,074] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (3e4cb2948233, executor driver, partition 114, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,075] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 183 ms on 3e4cb2948233 (executor driver) (111/141)
[2021-05-14 11:01:24,076] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-14 11:01:24,107] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1825 bytes result sent to driver
[2021-05-14 11:01:24,108] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (3e4cb2948233, executor driver, partition 115, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,109] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 186 ms on 3e4cb2948233 (executor driver) (112/141)
[2021-05-14 11:01:24,111] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-14 11:01:24,168] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1825 bytes result sent to driver
[2021-05-14 11:01:24,170] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (3e4cb2948233, executor driver, partition 116, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,171] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 183 ms on 3e4cb2948233 (executor driver) (113/141)
[2021-05-14 11:01:24,172] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-14 11:01:24,229] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1825 bytes result sent to driver
[2021-05-14 11:01:24,230] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (3e4cb2948233, executor driver, partition 117, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,231] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-14 11:01:24,231] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 183 ms on 3e4cb2948233 (executor driver) (114/141)
[2021-05-14 11:01:24,256] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1825 bytes result sent to driver
[2021-05-14 11:01:24,257] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (3e4cb2948233, executor driver, partition 118, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:24 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 184 ms on 3e4cb2948233 (executor driver) (115/141)
21/05/14 14:01:24 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-14 11:01:24,286] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1825 bytes result sent to driver
[2021-05-14 11:01:24,287] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (3e4cb2948233, executor driver, partition 119, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:24 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-14 11:01:24,289] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 182 ms on 3e4cb2948233 (executor driver) (116/141)
[2021-05-14 11:01:24,342] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1825 bytes result sent to driver
[2021-05-14 11:01:24,343] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (3e4cb2948233, executor driver, partition 120, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,344] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 175 ms on 3e4cb2948233 (executor driver) (117/141)
21/05/14 14:01:24 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-14 11:01:24,410] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1825 bytes result sent to driver
[2021-05-14 11:01:24,412] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (3e4cb2948233, executor driver, partition 121, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,413] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 183 ms on 3e4cb2948233 (executor driver) (118/141)
[2021-05-14 11:01:24,414] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-14 11:01:24,434] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1825 bytes result sent to driver
[2021-05-14 11:01:24,436] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (3e4cb2948233, executor driver, partition 122, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,437] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 181 ms on 3e4cb2948233 (executor driver) (119/141)
[2021-05-14 11:01:24,438] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
[2021-05-14 11:01:24,474] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1825 bytes result sent to driver
[2021-05-14 11:01:24,475] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (3e4cb2948233, executor driver, partition 123, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,476] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 189 ms on 3e4cb2948233 (executor driver) (120/141)
[2021-05-14 11:01:24,477] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-14 11:01:24,513] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1868 bytes result sent to driver
[2021-05-14 11:01:24,514] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (3e4cb2948233, executor driver, partition 124, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,514] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 172 ms on 3e4cb2948233 (executor driver) (121/141)
[2021-05-14 11:01:24,515] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-14 11:01:24,608] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1868 bytes result sent to driver
[2021-05-14 11:01:24,610] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (3e4cb2948233, executor driver, partition 125, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,612] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 200 ms on 3e4cb2948233 (executor driver) (122/141)
[2021-05-14 11:01:24,613] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-14 11:01:24,666] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1868 bytes result sent to driver
[2021-05-14 11:01:24,670] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (3e4cb2948233, executor driver, partition 126, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,670] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 194 ms on 3e4cb2948233 (executor driver) (123/141)
[2021-05-14 11:01:24,671] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-14 11:01:24,671] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1868 bytes result sent to driver
[2021-05-14 11:01:24,673] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (3e4cb2948233, executor driver, partition 127, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,674] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 238 ms on 3e4cb2948233 (executor driver) (124/141)
[2021-05-14 11:01:24,675] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-14 11:01:24,685] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1825 bytes result sent to driver
[2021-05-14 11:01:24,686] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (3e4cb2948233, executor driver, partition 128, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,686] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-14 11:01:24,687] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 174 ms on 3e4cb2948233 (executor driver) (125/141)
[2021-05-14 11:01:24,794] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1825 bytes result sent to driver
[2021-05-14 11:01:24,795] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (3e4cb2948233, executor driver, partition 129, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,797] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 188 ms on 3e4cb2948233 (executor driver) (126/141)
[2021-05-14 11:01:24,798] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-14 11:01:24,854] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1825 bytes result sent to driver
[2021-05-14 11:01:24,856] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (3e4cb2948233, executor driver, partition 130, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,857] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 190 ms on 3e4cb2948233 (executor driver) (127/141)
[2021-05-14 11:01:24,858] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-14 11:01:24,859] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1825 bytes result sent to driver
[2021-05-14 11:01:24,864] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (3e4cb2948233, executor driver, partition 131, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:24 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1825 bytes result sent to driver
[2021-05-14 11:01:24,865] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-14 11:01:24,866] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (3e4cb2948233, executor driver, partition 132, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,867] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-14 11:01:24,868] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 181 ms on 3e4cb2948233 (executor driver) (128/141)
[2021-05-14 11:01:24,869] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 196 ms on 3e4cb2948233 (executor driver) (129/141)
[2021-05-14 11:01:24,983] {docker.py:276} INFO - 21/05/14 14:01:24 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1825 bytes result sent to driver
[2021-05-14 11:01:24,985] {docker.py:276} INFO - 21/05/14 14:01:24 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (3e4cb2948233, executor driver, partition 133, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:24,986] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-14 11:01:24,986] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 191 ms on 3e4cb2948233 (executor driver) (130/141)
[2021-05-14 11:01:25,038] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1825 bytes result sent to driver
[2021-05-14 11:01:25,040] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1825 bytes result sent to driver
21/05/14 14:01:25 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (3e4cb2948233, executor driver, partition 134, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,041] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-14 11:01:25,042] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 186 ms on 3e4cb2948233 (executor driver) (131/141)
[2021-05-14 11:01:25,043] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 178 ms on 3e4cb2948233 (executor driver) (132/141)
[2021-05-14 11:01:25,044] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (3e4cb2948233, executor driver, partition 135, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,045] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1825 bytes result sent to driver
[2021-05-14 11:01:25,046] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-14 11:01:25,047] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 182 ms on 3e4cb2948233 (executor driver) (133/141)
[2021-05-14 11:01:25,048] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (3e4cb2948233, executor driver, partition 136, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,049] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-14 11:01:25,182] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1825 bytes result sent to driver
[2021-05-14 11:01:25,184] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (3e4cb2948233, executor driver, partition 137, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,186] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 201 ms on 3e4cb2948233 (executor driver) (134/141)
[2021-05-14 11:01:25,187] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-14 11:01:25,225] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1825 bytes result sent to driver
[2021-05-14 11:01:25,227] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (3e4cb2948233, executor driver, partition 138, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,227] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 180 ms on 3e4cb2948233 (executor driver) (135/141)
[2021-05-14 11:01:25,228] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-14 11:01:25,240] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1825 bytes result sent to driver
[2021-05-14 11:01:25,242] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (3e4cb2948233, executor driver, partition 139, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:25 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-14 11:01:25,243] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1825 bytes result sent to driver
[2021-05-14 11:01:25,243] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 199 ms on 3e4cb2948233 (executor driver) (136/141)
[2021-05-14 11:01:25,244] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 205 ms on 3e4cb2948233 (executor driver) (137/141)
[2021-05-14 11:01:25,245] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (3e4cb2948233, executor driver, partition 140, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,245] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-14 11:01:25,368] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1868 bytes result sent to driver
[2021-05-14 11:01:25,378] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 195 ms on 3e4cb2948233 (executor driver) (138/141)
[2021-05-14 11:01:25,407] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1868 bytes result sent to driver
[2021-05-14 11:01:25,409] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 182 ms on 3e4cb2948233 (executor driver) (139/141)
[2021-05-14 11:01:25,424] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1868 bytes result sent to driver
[2021-05-14 11:01:25,426] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 184 ms on 3e4cb2948233 (executor driver) (140/141)
[2021-05-14 11:01:25,427] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1868 bytes result sent to driver
[2021-05-14 11:01:25,428] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 184 ms on 3e4cb2948233 (executor driver) (141/141)
[2021-05-14 11:01:25,432] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 11:01:25,433] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 7.662 s
[2021-05-14 11:01:25,442] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 11:01:25,443] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 11:01:25,446] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 7.777007 s
[2021-05-14 11:01:25,485] {docker.py:276} INFO - 21/05/14 14:01:25 INFO InMemoryFileIndex: It took 8397 ms to list leaf files for 141 paths.
[2021-05-14 11:01:25,610] {docker.py:276} INFO - 21/05/14 14:01:25 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14/from_1620914410_to_1620916210.csv, s3a://udac-forex-project/1/2021-05-14/from_1620916210_to_1620918010.csv, s3a://udac-forex-project/1/2021-05-14/from_1620918010_to_1620919810.csv, s3a://udac-forex-project/1/2021-05-14/from_1620919810_to_1620921610.csv, s3a://udac-forex-project/1/2021-05-14/from_1620921610_to_1620923410.csv, s3a://udac-forex-project/1/2021-05-14/from_1620923410_to_1620925210.csv, s3a://udac-forex-project/1/2021-05-14/from_1620925210_to_1620927010.csv, s3a://udac-forex-project/1/2021-05-14/from_1620927010_to_1620928810.csv, s3a://udac-forex-project/1/2021-05-14/from_1620928810_to_1620930610.csv, s3a://udac-forex-project/1/2021-05-14/from_1620930610_to_1620932410.csv.
[2021-05-14 11:01:25,649] {docker.py:276} INFO - 21/05/14 14:01:25 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:25,651] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/14 14:01:25 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 11:01:25,651] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 11:01:25,652] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Missing parents: List()
[2021-05-14 11:01:25,653] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:01:25,678] {docker.py:276} INFO - 21/05/14 14:01:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 84.9 KiB, free 934.2 MiB)
[2021-05-14 11:01:25,688] {docker.py:276} INFO - 21/05/14 14:01:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-14 11:01:25,689] {docker.py:276} INFO - 21/05/14 14:01:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3e4cb2948233:43567 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-14 11:01:25,689] {docker.py:276} INFO - 21/05/14 14:01:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:01:25,690] {docker.py:276} INFO - 21/05/14 14:01:25 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 14:01:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-14 11:01:25,694] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (3e4cb2948233, executor driver, partition 0, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,694] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (3e4cb2948233, executor driver, partition 1, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,695] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (3e4cb2948233, executor driver, partition 2, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,696] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (3e4cb2948233, executor driver, partition 3, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,696] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-14 11:01:25,697] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
21/05/14 14:01:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-14 11:01:25,697] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-14 11:01:25,768] {docker.py:276} INFO - 21/05/14 14:01:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3e4cb2948233:43567 in memory (size: 30.2 KiB, free: 934.4 MiB)
[2021-05-14 11:01:25,866] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1825 bytes result sent to driver
[2021-05-14 11:01:25,869] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (3e4cb2948233, executor driver, partition 4, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,871] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
21/05/14 14:01:25 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 176 ms on 3e4cb2948233 (executor driver) (1/141)
[2021-05-14 11:01:25,876] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1825 bytes result sent to driver
[2021-05-14 11:01:25,878] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (3e4cb2948233, executor driver, partition 5, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,878] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 184 ms on 3e4cb2948233 (executor driver) (2/141)
[2021-05-14 11:01:25,880] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1825 bytes result sent to driver
[2021-05-14 11:01:25,881] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (3e4cb2948233, executor driver, partition 6, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,882] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 188 ms on 3e4cb2948233 (executor driver) (3/141)
[2021-05-14 11:01:25,887] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1825 bytes result sent to driver
[2021-05-14 11:01:25,887] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-14 11:01:25,888] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-14 11:01:25,889] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (3e4cb2948233, executor driver, partition 7, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:25,891] {docker.py:276} INFO - 21/05/14 14:01:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 198 ms on 3e4cb2948233 (executor driver) (4/141)
[2021-05-14 11:01:25,892] {docker.py:276} INFO - 21/05/14 14:01:25 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-14 11:01:26,050] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1825 bytes result sent to driver
[2021-05-14 11:01:26,052] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (3e4cb2948233, executor driver, partition 8, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,053] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 183 ms on 3e4cb2948233 (executor driver) (5/141)
[2021-05-14 11:01:26,054] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-14 11:01:26,063] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1825 bytes result sent to driver
21/05/14 14:01:26 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1825 bytes result sent to driver
[2021-05-14 11:01:26,064] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1825 bytes result sent to driver
[2021-05-14 11:01:26,065] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (3e4cb2948233, executor driver, partition 9, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,066] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 177 ms on 3e4cb2948233 (executor driver) (6/141)
[2021-05-14 11:01:26,067] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (3e4cb2948233, executor driver, partition 10, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,068] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-14 11:01:26,069] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 188 ms on 3e4cb2948233 (executor driver) (7/141)
[2021-05-14 11:01:26,070] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-14 11:01:26,070] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 193 ms on 3e4cb2948233 (executor driver) (8/141)
[2021-05-14 11:01:26,072] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (3e4cb2948233, executor driver, partition 11, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,072] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-14 11:01:26,235] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1825 bytes result sent to driver
[2021-05-14 11:01:26,237] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (3e4cb2948233, executor driver, partition 12, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,238] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-14 11:01:26,238] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 188 ms on 3e4cb2948233 (executor driver) (9/141)
[2021-05-14 11:01:26,243] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1825 bytes result sent to driver
21/05/14 14:01:26 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1825 bytes result sent to driver
[2021-05-14 11:01:26,244] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (3e4cb2948233, executor driver, partition 13, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,245] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-14 11:01:26,246] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (3e4cb2948233, executor driver, partition 14, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,247] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 182 ms on 3e4cb2948233 (executor driver) (10/141)
[2021-05-14 11:01:26,248] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 176 ms on 3e4cb2948233 (executor driver) (11/141)
[2021-05-14 11:01:26,249] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
[2021-05-14 11:01:26,249] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1825 bytes result sent to driver
[2021-05-14 11:01:26,251] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (3e4cb2948233, executor driver, partition 15, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,252] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 185 ms on 3e4cb2948233 (executor driver) (12/141)
[2021-05-14 11:01:26,253] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-14 11:01:26,438] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1868 bytes result sent to driver
[2021-05-14 11:01:26,439] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (3e4cb2948233, executor driver, partition 16, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,440] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 205 ms on 3e4cb2948233 (executor driver) (13/141)
[2021-05-14 11:01:26,442] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
[2021-05-14 11:01:26,443] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1868 bytes result sent to driver
[2021-05-14 11:01:26,444] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (3e4cb2948233, executor driver, partition 17, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,445] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
21/05/14 14:01:26 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1868 bytes result sent to driver
21/05/14 14:01:26 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 202 ms on 3e4cb2948233 (executor driver) (14/141)
[2021-05-14 11:01:26,447] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (3e4cb2948233, executor driver, partition 18, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,449] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 203 ms on 3e4cb2948233 (executor driver) (15/141)
[2021-05-14 11:01:26,449] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-14 11:01:26,467] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1868 bytes result sent to driver
[2021-05-14 11:01:26,468] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (3e4cb2948233, executor driver, partition 19, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,469] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-14 11:01:26,470] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 219 ms on 3e4cb2948233 (executor driver) (16/141)
[2021-05-14 11:01:26,625] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1825 bytes result sent to driver
21/05/14 14:01:26 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1825 bytes result sent to driver
[2021-05-14 11:01:26,627] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (3e4cb2948233, executor driver, partition 20, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,629] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-14 11:01:26,630] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1825 bytes result sent to driver
[2021-05-14 11:01:26,630] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (3e4cb2948233, executor driver, partition 21, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,631] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 187 ms on 3e4cb2948233 (executor driver) (17/141)
21/05/14 14:01:26 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 192 ms on 3e4cb2948233 (executor driver) (18/141)
[2021-05-14 11:01:26,632] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-14 11:01:26,633] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (3e4cb2948233, executor driver, partition 22, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,633] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-14 11:01:26,634] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 188 ms on 3e4cb2948233 (executor driver) (19/141)
[2021-05-14 11:01:26,640] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1825 bytes result sent to driver
[2021-05-14 11:01:26,641] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (3e4cb2948233, executor driver, partition 23, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,642] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 174 ms on 3e4cb2948233 (executor driver) (20/141)
21/05/14 14:01:26 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-14 11:01:26,807] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1825 bytes result sent to driver
[2021-05-14 11:01:26,809] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (3e4cb2948233, executor driver, partition 24, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,810] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1825 bytes result sent to driver
[2021-05-14 11:01:26,811] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
[2021-05-14 11:01:26,811] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 177 ms on 3e4cb2948233 (executor driver) (21/141)
[2021-05-14 11:01:26,812] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (3e4cb2948233, executor driver, partition 25, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,813] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-14 11:01:26,814] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 184 ms on 3e4cb2948233 (executor driver) (22/141)
21/05/14 14:01:26 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1825 bytes result sent to driver
[2021-05-14 11:01:26,815] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (3e4cb2948233, executor driver, partition 26, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,816] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-14 11:01:26,817] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 176 ms on 3e4cb2948233 (executor driver) (23/141)
[2021-05-14 11:01:26,822] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1825 bytes result sent to driver
[2021-05-14 11:01:26,824] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (3e4cb2948233, executor driver, partition 27, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,824] {docker.py:276} INFO - 21/05/14 14:01:26 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 198 ms on 3e4cb2948233 (executor driver) (24/141)
[2021-05-14 11:01:26,825] {docker.py:276} INFO - 21/05/14 14:01:26 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-14 11:01:26,991] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1825 bytes result sent to driver
[2021-05-14 11:01:26,992] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (3e4cb2948233, executor driver, partition 28, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,993] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 178 ms on 3e4cb2948233 (executor driver) (25/141)
[2021-05-14 11:01:26,994] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-14 11:01:26,995] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1825 bytes result sent to driver
[2021-05-14 11:01:26,996] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (3e4cb2948233, executor driver, partition 29, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:26,997] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 189 ms on 3e4cb2948233 (executor driver) (26/141)
[2021-05-14 11:01:26,999] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-14 11:01:27,010] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1868 bytes result sent to driver
[2021-05-14 11:01:27,011] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (3e4cb2948233, executor driver, partition 30, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,012] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 200 ms on 3e4cb2948233 (executor driver) (27/141)
[2021-05-14 11:01:27,013] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-14 11:01:27,014] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1868 bytes result sent to driver
[2021-05-14 11:01:27,015] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (3e4cb2948233, executor driver, partition 31, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,016] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 192 ms on 3e4cb2948233 (executor driver) (28/141)
[2021-05-14 11:01:27,016] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-14 11:01:27,185] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1868 bytes result sent to driver
[2021-05-14 11:01:27,186] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1825 bytes result sent to driver
[2021-05-14 11:01:27,188] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (3e4cb2948233, executor driver, partition 32, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,189] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-14 11:01:27,190] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (3e4cb2948233, executor driver, partition 33, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,191] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-14 11:01:27,192] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 200 ms on 3e4cb2948233 (executor driver) (29/141)
[2021-05-14 11:01:27,192] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 182 ms on 3e4cb2948233 (executor driver) (30/141)
[2021-05-14 11:01:27,193] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1825 bytes result sent to driver
[2021-05-14 11:01:27,194] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (3e4cb2948233, executor driver, partition 34, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,195] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-14 11:01:27,196] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 200 ms on 3e4cb2948233 (executor driver) (31/141)
[2021-05-14 11:01:27,198] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1825 bytes result sent to driver
[2021-05-14 11:01:27,199] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (3e4cb2948233, executor driver, partition 35, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,201] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-14 11:01:27,201] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 186 ms on 3e4cb2948233 (executor driver) (32/141)
[2021-05-14 11:01:27,371] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1825 bytes result sent to driver
[2021-05-14 11:01:27,374] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (3e4cb2948233, executor driver, partition 36, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,375] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1825 bytes result sent to driver
[2021-05-14 11:01:27,375] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1825 bytes result sent to driver
[2021-05-14 11:01:27,376] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-14 11:01:27,377] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (3e4cb2948233, executor driver, partition 37, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,377] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-14 11:01:27,378] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (3e4cb2948233, executor driver, partition 38, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,379] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 190 ms on 3e4cb2948233 (executor driver) (33/141)
[2021-05-14 11:01:27,380] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-14 11:01:27,380] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 193 ms on 3e4cb2948233 (executor driver) (34/141)
[2021-05-14 11:01:27,381] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 186 ms on 3e4cb2948233 (executor driver) (35/141)
[2021-05-14 11:01:27,440] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1825 bytes result sent to driver
[2021-05-14 11:01:27,441] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (3e4cb2948233, executor driver, partition 39, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,442] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 243 ms on 3e4cb2948233 (executor driver) (36/141)
[2021-05-14 11:01:27,442] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-14 11:01:27,548] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1825 bytes result sent to driver
[2021-05-14 11:01:27,549] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1825 bytes result sent to driver
21/05/14 14:01:27 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (3e4cb2948233, executor driver, partition 40, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,550] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 177 ms on 3e4cb2948233 (executor driver) (37/141)
[2021-05-14 11:01:27,552] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-14 11:01:27,552] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (3e4cb2948233, executor driver, partition 41, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,554] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-14 11:01:27,554] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 175 ms on 3e4cb2948233 (executor driver) (38/141)
[2021-05-14 11:01:27,557] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1825 bytes result sent to driver
[2021-05-14 11:01:27,559] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (3e4cb2948233, executor driver, partition 42, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,560] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 184 ms on 3e4cb2948233 (executor driver) (39/141)
[2021-05-14 11:01:27,561] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-14 11:01:27,618] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1825 bytes result sent to driver
[2021-05-14 11:01:27,619] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (3e4cb2948233, executor driver, partition 43, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,620] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-14 11:01:27,621] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 179 ms on 3e4cb2948233 (executor driver) (40/141)
[2021-05-14 11:01:27,734] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1868 bytes result sent to driver
[2021-05-14 11:01:27,735] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1868 bytes result sent to driver
[2021-05-14 11:01:27,736] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1868 bytes result sent to driver
21/05/14 14:01:27 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (3e4cb2948233, executor driver, partition 44, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,737] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
21/05/14 14:01:27 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 185 ms on 3e4cb2948233 (executor driver) (41/141)
[2021-05-14 11:01:27,738] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 189 ms on 3e4cb2948233 (executor driver) (42/141)
[2021-05-14 11:01:27,740] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (3e4cb2948233, executor driver, partition 45, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,741] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
[2021-05-14 11:01:27,742] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (3e4cb2948233, executor driver, partition 46, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,743] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 184 ms on 3e4cb2948233 (executor driver) (43/141)
[2021-05-14 11:01:27,743] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-14 11:01:27,803] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1868 bytes result sent to driver
[2021-05-14 11:01:27,805] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (3e4cb2948233, executor driver, partition 47, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,806] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-14 11:01:27,806] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 187 ms on 3e4cb2948233 (executor driver) (44/141)
[2021-05-14 11:01:27,912] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1825 bytes result sent to driver
[2021-05-14 11:01:27,913] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1825 bytes result sent to driver
[2021-05-14 11:01:27,914] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (3e4cb2948233, executor driver, partition 48, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,914] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 174 ms on 3e4cb2948233 (executor driver) (45/141)
[2021-05-14 11:01:27,915] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
[2021-05-14 11:01:27,916] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (3e4cb2948233, executor driver, partition 49, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,916] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 182 ms on 3e4cb2948233 (executor driver) (46/141)
[2021-05-14 11:01:27,917] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-14 11:01:27,921] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1825 bytes result sent to driver
[2021-05-14 11:01:27,922] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (3e4cb2948233, executor driver, partition 50, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,923] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-14 11:01:27,924] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 184 ms on 3e4cb2948233 (executor driver) (47/141)
[2021-05-14 11:01:27,979] {docker.py:276} INFO - 21/05/14 14:01:27 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1825 bytes result sent to driver
[2021-05-14 11:01:27,981] {docker.py:276} INFO - 21/05/14 14:01:27 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (3e4cb2948233, executor driver, partition 51, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:27,981] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 177 ms on 3e4cb2948233 (executor driver) (48/141)
[2021-05-14 11:01:27,983] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-14 11:01:28,089] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1825 bytes result sent to driver
[2021-05-14 11:01:28,090] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (3e4cb2948233, executor driver, partition 52, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,091] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
21/05/14 14:01:28 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 176 ms on 3e4cb2948233 (executor driver) (49/141)
[2021-05-14 11:01:28,093] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1825 bytes result sent to driver
[2021-05-14 11:01:28,094] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (3e4cb2948233, executor driver, partition 53, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,094] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 173 ms on 3e4cb2948233 (executor driver) (50/141)
[2021-05-14 11:01:28,096] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-14 11:01:28,097] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1825 bytes result sent to driver
[2021-05-14 11:01:28,098] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (3e4cb2948233, executor driver, partition 54, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,099] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-14 11:01:28,099] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 186 ms on 3e4cb2948233 (executor driver) (51/141)
[2021-05-14 11:01:28,160] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1825 bytes result sent to driver
[2021-05-14 11:01:28,162] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (3e4cb2948233, executor driver, partition 55, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,164] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 184 ms on 3e4cb2948233 (executor driver) (52/141)
21/05/14 14:01:28 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
[2021-05-14 11:01:28,267] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1825 bytes result sent to driver
[2021-05-14 11:01:28,268] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (3e4cb2948233, executor driver, partition 56, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,269] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 171 ms on 3e4cb2948233 (executor driver) (53/141)
[2021-05-14 11:01:28,270] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-14 11:01:28,273] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1825 bytes result sent to driver
[2021-05-14 11:01:28,274] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (3e4cb2948233, executor driver, partition 57, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,275] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 182 ms on 3e4cb2948233 (executor driver) (54/141)
[2021-05-14 11:01:28,275] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-14 11:01:28,282] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1825 bytes result sent to driver
[2021-05-14 11:01:28,282] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (3e4cb2948233, executor driver, partition 58, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,283] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-14 11:01:28,284] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 193 ms on 3e4cb2948233 (executor driver) (55/141)
[2021-05-14 11:01:28,405] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1868 bytes result sent to driver
[2021-05-14 11:01:28,406] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (3e4cb2948233, executor driver, partition 59, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,406] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 245 ms on 3e4cb2948233 (executor driver) (56/141)
21/05/14 14:01:28 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-14 11:01:28,445] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1868 bytes result sent to driver
21/05/14 14:01:28 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1868 bytes result sent to driver
[2021-05-14 11:01:28,446] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (3e4cb2948233, executor driver, partition 60, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,447] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 179 ms on 3e4cb2948233 (executor driver) (57/141)
[2021-05-14 11:01:28,448] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-14 11:01:28,449] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (3e4cb2948233, executor driver, partition 61, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,450] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
[2021-05-14 11:01:28,450] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 176 ms on 3e4cb2948233 (executor driver) (58/141)
[2021-05-14 11:01:28,460] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1868 bytes result sent to driver
[2021-05-14 11:01:28,461] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (3e4cb2948233, executor driver, partition 62, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,462] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 180 ms on 3e4cb2948233 (executor driver) (59/141)
21/05/14 14:01:28 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-14 11:01:28,580] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1825 bytes result sent to driver
[2021-05-14 11:01:28,581] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (3e4cb2948233, executor driver, partition 63, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,582] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
21/05/14 14:01:28 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 177 ms on 3e4cb2948233 (executor driver) (60/141)
[2021-05-14 11:01:28,619] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1825 bytes result sent to driver
[2021-05-14 11:01:28,621] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (3e4cb2948233, executor driver, partition 64, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,622] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 176 ms on 3e4cb2948233 (executor driver) (61/141)
[2021-05-14 11:01:28,623] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-14 11:01:28,626] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1825 bytes result sent to driver
[2021-05-14 11:01:28,627] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (3e4cb2948233, executor driver, partition 65, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,627] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-14 11:01:28,628] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 179 ms on 3e4cb2948233 (executor driver) (62/141)
[2021-05-14 11:01:28,636] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1825 bytes result sent to driver
[2021-05-14 11:01:28,636] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (3e4cb2948233, executor driver, partition 66, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,637] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 177 ms on 3e4cb2948233 (executor driver) (63/141)
[2021-05-14 11:01:28,638] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-14 11:01:28,762] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1825 bytes result sent to driver
[2021-05-14 11:01:28,763] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (3e4cb2948233, executor driver, partition 67, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,764] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 183 ms on 3e4cb2948233 (executor driver) (64/141)
[2021-05-14 11:01:28,765] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-14 11:01:28,800] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1825 bytes result sent to driver
21/05/14 14:01:28 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1825 bytes result sent to driver
[2021-05-14 11:01:28,801] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (3e4cb2948233, executor driver, partition 68, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,802] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-14 11:01:28,803] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (3e4cb2948233, executor driver, partition 69, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,804] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 177 ms on 3e4cb2948233 (executor driver) (65/141)
21/05/14 14:01:28 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-14 11:01:28,807] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 187 ms on 3e4cb2948233 (executor driver) (66/141)
[2021-05-14 11:01:28,818] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1825 bytes result sent to driver
[2021-05-14 11:01:28,819] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (3e4cb2948233, executor driver, partition 70, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,820] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 183 ms on 3e4cb2948233 (executor driver) (67/141)
21/05/14 14:01:28 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-14 11:01:28,952] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1825 bytes result sent to driver
[2021-05-14 11:01:28,953] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (3e4cb2948233, executor driver, partition 71, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,954] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-14 11:01:28,955] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 192 ms on 3e4cb2948233 (executor driver) (68/141)
[2021-05-14 11:01:28,979] {docker.py:276} INFO - 21/05/14 14:01:28 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1825 bytes result sent to driver
[2021-05-14 11:01:28,980] {docker.py:276} INFO - 21/05/14 14:01:28 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (3e4cb2948233, executor driver, partition 72, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,981] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 178 ms on 3e4cb2948233 (executor driver) (69/141)
21/05/14 14:01:29 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-14 11:01:28,982] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1825 bytes result sent to driver
[2021-05-14 11:01:28,983] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (3e4cb2948233, executor driver, partition 73, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:28,984] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 184 ms on 3e4cb2948233 (executor driver) (70/141)
[2021-05-14 11:01:28,984] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-14 11:01:28,999] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1868 bytes result sent to driver
[2021-05-14 11:01:29,001] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (3e4cb2948233, executor driver, partition 74, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,001] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 183 ms on 3e4cb2948233 (executor driver) (71/141)
[2021-05-14 11:01:29,002] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
[2021-05-14 11:01:29,130] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1868 bytes result sent to driver
[2021-05-14 11:01:29,131] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (3e4cb2948233, executor driver, partition 75, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,132] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-14 11:01:29,132] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 180 ms on 3e4cb2948233 (executor driver) (72/141)
[2021-05-14 11:01:29,161] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1868 bytes result sent to driver
[2021-05-14 11:01:29,162] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (3e4cb2948233, executor driver, partition 76, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,163] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 180 ms on 3e4cb2948233 (executor driver) (73/141)
[2021-05-14 11:01:29,163] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-14 11:01:29,171] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1825 bytes result sent to driver
[2021-05-14 11:01:29,172] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (3e4cb2948233, executor driver, partition 77, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,173] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 173 ms on 3e4cb2948233 (executor driver) (74/141)
[2021-05-14 11:01:29,174] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-14 11:01:29,202] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1868 bytes result sent to driver
[2021-05-14 11:01:29,203] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (3e4cb2948233, executor driver, partition 78, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,204] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 225 ms on 3e4cb2948233 (executor driver) (75/141)
[2021-05-14 11:01:29,204] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-14 11:01:29,309] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1825 bytes result sent to driver
[2021-05-14 11:01:29,310] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (3e4cb2948233, executor driver, partition 79, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,311] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 180 ms on 3e4cb2948233 (executor driver) (76/141)
[2021-05-14 11:01:29,312] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
[2021-05-14 11:01:29,331] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1825 bytes result sent to driver
[2021-05-14 11:01:29,331] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (3e4cb2948233, executor driver, partition 80, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,332] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
21/05/14 14:01:29 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 170 ms on 3e4cb2948233 (executor driver) (77/141)
[2021-05-14 11:01:29,359] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1825 bytes result sent to driver
[2021-05-14 11:01:29,360] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (3e4cb2948233, executor driver, partition 81, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,361] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 189 ms on 3e4cb2948233 (executor driver) (78/141)
[2021-05-14 11:01:29,361] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-14 11:01:29,377] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1825 bytes result sent to driver
[2021-05-14 11:01:29,378] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (3e4cb2948233, executor driver, partition 82, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,379] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 176 ms on 3e4cb2948233 (executor driver) (79/141)
[2021-05-14 11:01:29,379] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
[2021-05-14 11:01:29,488] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1825 bytes result sent to driver
[2021-05-14 11:01:29,489] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (3e4cb2948233, executor driver, partition 83, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,490] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-14 11:01:29,490] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 180 ms on 3e4cb2948233 (executor driver) (80/141)
[2021-05-14 11:01:29,503] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1825 bytes result sent to driver
[2021-05-14 11:01:29,504] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (3e4cb2948233, executor driver, partition 84, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,504] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 173 ms on 3e4cb2948233 (executor driver) (81/141)
[2021-05-14 11:01:29,505] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-14 11:01:29,529] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1825 bytes result sent to driver
[2021-05-14 11:01:29,530] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (3e4cb2948233, executor driver, partition 85, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,530] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 170 ms on 3e4cb2948233 (executor driver) (82/141)
[2021-05-14 11:01:29,531] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-14 11:01:29,552] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1825 bytes result sent to driver
[2021-05-14 11:01:29,553] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (3e4cb2948233, executor driver, partition 86, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,554] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-14 11:01:29,554] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 177 ms on 3e4cb2948233 (executor driver) (83/141)
[2021-05-14 11:01:29,666] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1825 bytes result sent to driver
[2021-05-14 11:01:29,667] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (3e4cb2948233, executor driver, partition 87, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,668] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-14 11:01:29,668] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 181 ms on 3e4cb2948233 (executor driver) (84/141)
[2021-05-14 11:01:29,675] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1825 bytes result sent to driver
[2021-05-14 11:01:29,676] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (3e4cb2948233, executor driver, partition 88, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,676] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 173 ms on 3e4cb2948233 (executor driver) (85/141)
[2021-05-14 11:01:29,677] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-14 11:01:29,709] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1868 bytes result sent to driver
[2021-05-14 11:01:29,710] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (3e4cb2948233, executor driver, partition 89, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,710] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 181 ms on 3e4cb2948233 (executor driver) (86/141)
[2021-05-14 11:01:29,711] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
[2021-05-14 11:01:29,727] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1868 bytes result sent to driver
[2021-05-14 11:01:29,728] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (3e4cb2948233, executor driver, partition 90, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,729] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
21/05/14 14:01:29 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 177 ms on 3e4cb2948233 (executor driver) (87/141)
[2021-05-14 11:01:29,838] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1868 bytes result sent to driver
[2021-05-14 11:01:29,839] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (3e4cb2948233, executor driver, partition 91, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,840] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-14 11:01:29,841] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 173 ms on 3e4cb2948233 (executor driver) (88/141)
[2021-05-14 11:01:29,855] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1868 bytes result sent to driver
[2021-05-14 11:01:29,857] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (3e4cb2948233, executor driver, partition 92, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,857] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 182 ms on 3e4cb2948233 (executor driver) (89/141)
[2021-05-14 11:01:29,858] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-14 11:01:29,903] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1825 bytes result sent to driver
[2021-05-14 11:01:29,904] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (3e4cb2948233, executor driver, partition 93, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:29,905] {docker.py:276} INFO - 21/05/14 14:01:29 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 176 ms on 3e4cb2948233 (executor driver) (90/141)
[2021-05-14 11:01:29,905] {docker.py:276} INFO - 21/05/14 14:01:29 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-14 11:01:30,020] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1825 bytes result sent to driver
[2021-05-14 11:01:30,021] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (3e4cb2948233, executor driver, partition 94, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,022] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 183 ms on 3e4cb2948233 (executor driver) (91/141)
[2021-05-14 11:01:30,023] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-14 11:01:30,061] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1825 bytes result sent to driver
[2021-05-14 11:01:30,062] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (3e4cb2948233, executor driver, partition 95, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,063] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-14 11:01:30,063] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 207 ms on 3e4cb2948233 (executor driver) (92/141)
[2021-05-14 11:01:30,078] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1825 bytes result sent to driver
[2021-05-14 11:01:30,079] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (3e4cb2948233, executor driver, partition 96, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,080] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-14 11:01:30,080] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 176 ms on 3e4cb2948233 (executor driver) (93/141)
[2021-05-14 11:01:30,225] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1825 bytes result sent to driver
[2021-05-14 11:01:30,226] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 205 ms on 3e4cb2948233 (executor driver) (94/141)
[2021-05-14 11:01:30,227] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (3e4cb2948233, executor driver, partition 97, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,228] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-14 11:01:30,234] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1825 bytes result sent to driver
[2021-05-14 11:01:30,235] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (3e4cb2948233, executor driver, partition 98, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,235] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 174 ms on 3e4cb2948233 (executor driver) (95/141)
[2021-05-14 11:01:30,236] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-14 11:01:30,262] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1825 bytes result sent to driver
[2021-05-14 11:01:30,263] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (3e4cb2948233, executor driver, partition 99, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,264] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 186 ms on 3e4cb2948233 (executor driver) (96/141)
[2021-05-14 11:01:30,264] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-14 11:01:30,364] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1825 bytes result sent to driver
[2021-05-14 11:01:30,365] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (3e4cb2948233, executor driver, partition 100, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,366] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-14 11:01:30,366] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 657 ms on 3e4cb2948233 (executor driver) (97/141)
[2021-05-14 11:01:30,404] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1825 bytes result sent to driver
[2021-05-14 11:01:30,405] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (3e4cb2948233, executor driver, partition 101, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,406] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 179 ms on 3e4cb2948233 (executor driver) (98/141)
[2021-05-14 11:01:30,407] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
[2021-05-14 11:01:30,409] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1825 bytes result sent to driver
[2021-05-14 11:01:30,410] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (3e4cb2948233, executor driver, partition 102, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,412] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 177 ms on 3e4cb2948233 (executor driver) (99/141)
[2021-05-14 11:01:30,412] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-14 11:01:30,442] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1868 bytes result sent to driver
[2021-05-14 11:01:30,445] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (3e4cb2948233, executor driver, partition 103, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,446] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 182 ms on 3e4cb2948233 (executor driver) (100/141)
[2021-05-14 11:01:30,447] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-14 11:01:30,539] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1868 bytes result sent to driver
[2021-05-14 11:01:30,541] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (3e4cb2948233, executor driver, partition 104, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,542] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-14 11:01:30,543] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 177 ms on 3e4cb2948233 (executor driver) (101/141)
[2021-05-14 11:01:30,588] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1868 bytes result sent to driver
[2021-05-14 11:01:30,589] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (3e4cb2948233, executor driver, partition 105, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,590] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 185 ms on 3e4cb2948233 (executor driver) (102/141)
[2021-05-14 11:01:30,591] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-14 11:01:30,598] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1868 bytes result sent to driver
[2021-05-14 11:01:30,599] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (3e4cb2948233, executor driver, partition 106, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,599] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 190 ms on 3e4cb2948233 (executor driver) (103/141)
21/05/14 14:01:30 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-14 11:01:30,622] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1825 bytes result sent to driver
[2021-05-14 11:01:30,623] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (3e4cb2948233, executor driver, partition 107, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,624] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 180 ms on 3e4cb2948233 (executor driver) (104/141)
[2021-05-14 11:01:30,625] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-14 11:01:30,717] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1825 bytes result sent to driver
[2021-05-14 11:01:30,718] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (3e4cb2948233, executor driver, partition 108, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,719] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 178 ms on 3e4cb2948233 (executor driver) (105/141)
[2021-05-14 11:01:30,719] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-14 11:01:30,758] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1825 bytes result sent to driver
[2021-05-14 11:01:30,758] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (3e4cb2948233, executor driver, partition 109, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,759] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 170 ms on 3e4cb2948233 (executor driver) (106/141)
[2021-05-14 11:01:30,759] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-14 11:01:30,772] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1825 bytes result sent to driver
[2021-05-14 11:01:30,773] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (3e4cb2948233, executor driver, partition 110, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,774] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
21/05/14 14:01:30 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 176 ms on 3e4cb2948233 (executor driver) (107/141)
[2021-05-14 11:01:30,800] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1825 bytes result sent to driver
[2021-05-14 11:01:30,801] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (3e4cb2948233, executor driver, partition 111, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,802] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 180 ms on 3e4cb2948233 (executor driver) (108/141)
[2021-05-14 11:01:30,802] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-14 11:01:30,890] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1825 bytes result sent to driver
[2021-05-14 11:01:30,893] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (3e4cb2948233, executor driver, partition 112, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,894] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 177 ms on 3e4cb2948233 (executor driver) (109/141)
21/05/14 14:01:30 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-14 11:01:30,927] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1825 bytes result sent to driver
[2021-05-14 11:01:30,929] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (3e4cb2948233, executor driver, partition 113, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,930] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 171 ms on 3e4cb2948233 (executor driver) (110/141)
[2021-05-14 11:01:30,930] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
[2021-05-14 11:01:30,954] {docker.py:276} INFO - 21/05/14 14:01:30 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1825 bytes result sent to driver
[2021-05-14 11:01:30,956] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (3e4cb2948233, executor driver, partition 114, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,958] {docker.py:276} INFO - 21/05/14 14:01:30 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 184 ms on 3e4cb2948233 (executor driver) (111/141)
21/05/14 14:01:30 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-14 11:01:30,982] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1825 bytes result sent to driver
[2021-05-14 11:01:30,983] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (3e4cb2948233, executor driver, partition 115, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:30,983] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 182 ms on 3e4cb2948233 (executor driver) (112/141)
[2021-05-14 11:01:30,984] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-14 11:01:31,060] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1825 bytes result sent to driver
[2021-05-14 11:01:31,062] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (3e4cb2948233, executor driver, partition 116, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,062] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-14 11:01:31,063] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 171 ms on 3e4cb2948233 (executor driver) (113/141)
[2021-05-14 11:01:31,104] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1825 bytes result sent to driver
[2021-05-14 11:01:31,106] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (3e4cb2948233, executor driver, partition 117, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,107] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-14 11:01:31,108] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 178 ms on 3e4cb2948233 (executor driver) (114/141)
[2021-05-14 11:01:31,132] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1868 bytes result sent to driver
[2021-05-14 11:01:31,133] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (3e4cb2948233, executor driver, partition 118, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,133] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 179 ms on 3e4cb2948233 (executor driver) (115/141)
[2021-05-14 11:01:31,134] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-14 11:01:31,166] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1868 bytes result sent to driver
[2021-05-14 11:01:31,167] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (3e4cb2948233, executor driver, partition 119, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,168] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 185 ms on 3e4cb2948233 (executor driver) (116/141)
[2021-05-14 11:01:31,169] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-14 11:01:31,235] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1868 bytes result sent to driver
[2021-05-14 11:01:31,237] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (3e4cb2948233, executor driver, partition 120, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,238] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 177 ms on 3e4cb2948233 (executor driver) (117/141)
[2021-05-14 11:01:31,239] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-14 11:01:31,282] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1868 bytes result sent to driver
[2021-05-14 11:01:31,286] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (3e4cb2948233, executor driver, partition 121, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,286] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 180 ms on 3e4cb2948233 (executor driver) (118/141)
[2021-05-14 11:01:31,287] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-14 11:01:31,313] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1825 bytes result sent to driver
[2021-05-14 11:01:31,315] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (3e4cb2948233, executor driver, partition 122, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:01:31 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-14 11:01:31,317] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 182 ms on 3e4cb2948233 (executor driver) (119/141)
[2021-05-14 11:01:31,359] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1825 bytes result sent to driver
[2021-05-14 11:01:31,360] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (3e4cb2948233, executor driver, partition 123, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,362] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
21/05/14 14:01:31 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 195 ms on 3e4cb2948233 (executor driver) (120/141)
[2021-05-14 11:01:31,403] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1825 bytes result sent to driver
[2021-05-14 11:01:31,404] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (3e4cb2948233, executor driver, partition 124, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,405] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 168 ms on 3e4cb2948233 (executor driver) (121/141)
21/05/14 14:01:31 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
[2021-05-14 11:01:31,461] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1825 bytes result sent to driver
[2021-05-14 11:01:31,462] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (3e4cb2948233, executor driver, partition 125, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,463] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 179 ms on 3e4cb2948233 (executor driver) (122/141)
[2021-05-14 11:01:31,463] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-14 11:01:31,493] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1825 bytes result sent to driver
[2021-05-14 11:01:31,494] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (3e4cb2948233, executor driver, partition 126, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,494] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-14 11:01:31,495] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 182 ms on 3e4cb2948233 (executor driver) (123/141)
[2021-05-14 11:01:31,534] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1825 bytes result sent to driver
[2021-05-14 11:01:31,535] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (3e4cb2948233, executor driver, partition 127, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,536] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 176 ms on 3e4cb2948233 (executor driver) (124/141)
[2021-05-14 11:01:31,536] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-14 11:01:31,571] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1825 bytes result sent to driver
[2021-05-14 11:01:31,573] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (3e4cb2948233, executor driver, partition 128, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,574] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 170 ms on 3e4cb2948233 (executor driver) (125/141)
[2021-05-14 11:01:31,578] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
[2021-05-14 11:01:31,633] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1825 bytes result sent to driver
[2021-05-14 11:01:31,634] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (3e4cb2948233, executor driver, partition 129, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,635] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 173 ms on 3e4cb2948233 (executor driver) (126/141)
[2021-05-14 11:01:31,636] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-14 11:01:31,667] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1825 bytes result sent to driver
[2021-05-14 11:01:31,668] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (3e4cb2948233, executor driver, partition 130, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,668] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 175 ms on 3e4cb2948233 (executor driver) (127/141)
[2021-05-14 11:01:31,669] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-14 11:01:31,712] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1825 bytes result sent to driver
[2021-05-14 11:01:31,713] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (3e4cb2948233, executor driver, partition 131, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,714] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-14 11:01:31,715] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 181 ms on 3e4cb2948233 (executor driver) (128/141)
[2021-05-14 11:01:31,746] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1825 bytes result sent to driver
[2021-05-14 11:01:31,748] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (3e4cb2948233, executor driver, partition 132, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,748] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 176 ms on 3e4cb2948233 (executor driver) (129/141)
[2021-05-14 11:01:31,749] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-14 11:01:31,805] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1868 bytes result sent to driver
[2021-05-14 11:01:31,806] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (3e4cb2948233, executor driver, partition 133, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,807] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 173 ms on 3e4cb2948233 (executor driver) (130/141)
[2021-05-14 11:01:31,808] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-14 11:01:31,841] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1868 bytes result sent to driver
[2021-05-14 11:01:31,842] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (3e4cb2948233, executor driver, partition 134, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,842] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 175 ms on 3e4cb2948233 (executor driver) (131/141)
[2021-05-14 11:01:31,843] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
[2021-05-14 11:01:31,891] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1868 bytes result sent to driver
[2021-05-14 11:01:31,893] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (3e4cb2948233, executor driver, partition 135, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,894] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-14 11:01:31,895] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 181 ms on 3e4cb2948233 (executor driver) (132/141)
[2021-05-14 11:01:31,928] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1868 bytes result sent to driver
[2021-05-14 11:01:31,940] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (3e4cb2948233, executor driver, partition 136, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,940] {docker.py:276} INFO - 21/05/14 14:01:31 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 184 ms on 3e4cb2948233 (executor driver) (133/141)
[2021-05-14 11:01:31,941] {docker.py:276} INFO - 21/05/14 14:01:31 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-14 11:01:31,986] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1825 bytes result sent to driver
[2021-05-14 11:01:31,987] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (3e4cb2948233, executor driver, partition 137, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:31,988] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 182 ms on 3e4cb2948233 (executor driver) (134/141)
[2021-05-14 11:01:31,988] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-14 11:01:32,015] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1825 bytes result sent to driver
[2021-05-14 11:01:32,016] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (3e4cb2948233, executor driver, partition 138, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:32,017] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-14 11:01:32,017] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 176 ms on 3e4cb2948233 (executor driver) (135/141)
[2021-05-14 11:01:32,069] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1825 bytes result sent to driver
[2021-05-14 11:01:32,070] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (3e4cb2948233, executor driver, partition 139, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:32,071] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-14 11:01:32,071] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 179 ms on 3e4cb2948233 (executor driver) (136/141)
[2021-05-14 11:01:32,107] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1825 bytes result sent to driver
[2021-05-14 11:01:32,109] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (3e4cb2948233, executor driver, partition 140, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:32,110] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-14 11:01:32,111] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 181 ms on 3e4cb2948233 (executor driver) (137/141)
[2021-05-14 11:01:32,164] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1825 bytes result sent to driver
[2021-05-14 11:01:32,165] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 180 ms on 3e4cb2948233 (executor driver) (138/141)
[2021-05-14 11:01:32,191] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1825 bytes result sent to driver
[2021-05-14 11:01:32,193] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 176 ms on 3e4cb2948233 (executor driver) (139/141)
[2021-05-14 11:01:32,245] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1825 bytes result sent to driver
[2021-05-14 11:01:32,247] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 177 ms on 3e4cb2948233 (executor driver) (140/141)
[2021-05-14 11:01:32,283] {docker.py:276} INFO - 21/05/14 14:01:32 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1825 bytes result sent to driver
[2021-05-14 11:01:32,283] {docker.py:276} INFO - 21/05/14 14:01:32 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 176 ms on 3e4cb2948233 (executor driver) (141/141)
21/05/14 14:01:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 11:01:32,284] {docker.py:276} INFO - 21/05/14 14:01:32 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.632 s
[2021-05-14 11:01:32,285] {docker.py:276} INFO - 21/05/14 14:01:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 14:01:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-14 11:01:32,285] {docker.py:276} INFO - 21/05/14 14:01:32 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.644196 s
[2021-05-14 11:01:32,298] {docker.py:276} INFO - 21/05/14 14:01:32 INFO InMemoryFileIndex: It took 6697 ms to list leaf files for 141 paths.
[2021-05-14 11:01:34,854] {docker.py:276} INFO - 21/05/14 14:01:34 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 11:01:34,860] {docker.py:276} INFO - 21/05/14 14:01:34 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 11:01:34,864] {docker.py:276} INFO - 21/05/14 14:01:34 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 11:01:34,941] {docker.py:276} INFO - 21/05/14 14:01:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 3e4cb2948233:43567 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 11:01:35,355] {docker.py:276} INFO - 21/05/14 14:01:35 INFO CodeGenerator: Code generated in 260.2521 ms
[2021-05-14 11:01:35,368] {docker.py:276} INFO - 21/05/14 14:01:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.4 KiB, free 934.2 MiB)
[2021-05-14 11:01:35,386] {docker.py:276} INFO - 21/05/14 14:01:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 11:01:35,388] {docker.py:276} INFO - 21/05/14 14:01:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3e4cb2948233:43567 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 11:01:35,389] {docker.py:276} INFO - 21/05/14 14:01:35 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:35,409] {docker.py:276} INFO - 21/05/14 14:01:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 11:01:35,521] {docker.py:276} INFO - 21/05/14 14:01:35 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:35,523] {docker.py:276} INFO - 21/05/14 14:01:35 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/14 14:01:35 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 14:01:35 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 11:01:35,523] {docker.py:276} INFO - 21/05/14 14:01:35 INFO DAGScheduler: Missing parents: List()
[2021-05-14 11:01:35,525] {docker.py:276} INFO - 21/05/14 14:01:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:01:35,553] {docker.py:276} INFO - 21/05/14 14:01:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 11:01:35,582] {docker.py:276} INFO - 21/05/14 14:01:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 11:01:35,582] {docker.py:276} INFO - 21/05/14 14:01:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 3e4cb2948233:43567 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 11:01:35,584] {docker.py:276} INFO - 21/05/14 14:01:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:01:35,585] {docker.py:276} INFO - 21/05/14 14:01:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/14 14:01:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-14 11:01:35,589] {docker.py:276} INFO - 21/05/14 14:01:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (3e4cb2948233, executor driver, partition 0, PROCESS_LOCAL, 8027 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:35,590] {docker.py:276} INFO - 21/05/14 14:01:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-14 11:01:35,720] {docker.py:276} INFO - 21/05/14 14:01:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620939610_to_1620941410.csv, range: 0-111641, partition values: [empty row]
[2021-05-14 11:01:35,756] {docker.py:276} INFO - 21/05/14 14:01:35 INFO CodeGenerator: Code generated in 28.8088 ms
[2021-05-14 11:01:36,275] {docker.py:276} INFO - 21/05/14 14:01:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-14 11:01:36,277] {docker.py:276} INFO - 21/05/14 14:01:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 692 ms on 3e4cb2948233 (executor driver) (1/1)
[2021-05-14 11:01:36,278] {docker.py:276} INFO - 21/05/14 14:01:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-14 11:01:36,279] {docker.py:276} INFO - 21/05/14 14:01:36 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.750 s
[2021-05-14 11:01:36,280] {docker.py:276} INFO - 21/05/14 14:01:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 14:01:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-14 11:01:36,281] {docker.py:276} INFO - 21/05/14 14:01:36 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.760219 s
[2021-05-14 11:01:36,311] {docker.py:276} INFO - 21/05/14 14:01:36 INFO CodeGenerator: Code generated in 14.4649 ms
[2021-05-14 11:01:36,382] {docker.py:276} INFO - 21/05/14 14:01:36 INFO FileSourceStrategy: Pushed Filters: 
21/05/14 14:01:36 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 11:01:36,382] {docker.py:276} INFO - 21/05/14 14:01:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 11:01:36,388] {docker.py:276} INFO - 21/05/14 14:01:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.4 KiB, free 934.0 MiB)
[2021-05-14 11:01:36,418] {docker.py:276} INFO - 21/05/14 14:01:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 11:01:36,419] {docker.py:276} INFO - 21/05/14 14:01:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 3e4cb2948233:43567 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 11:01:36,421] {docker.py:276} INFO - 21/05/14 14:01:36 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:36,427] {docker.py:276} INFO - 21/05/14 14:01:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 11:01:36,432] {docker.py:276} INFO - 21/05/14 14:01:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 3e4cb2948233:43567 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-14 11:01:37,130] {docker.py:276} INFO - 21/05/14 14:01:37 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 11:01:37,130] {docker.py:276} INFO - 21/05/14 14:01:37 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 11:01:37,131] {docker.py:276} INFO - 21/05/14 14:01:37 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 11:01:37,798] {docker.py:276} INFO - 21/05/14 14:01:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:01:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:01:37,801] {docker.py:276} INFO - 21/05/14 14:01:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:37,802] {docker.py:276} INFO - 21/05/14 14:01:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401372772443654525411039_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401372772443654525411039_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401372772443654525411039_0000}; taskId=attempt_202105141401372772443654525411039_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20834499}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:37,803] {docker.py:276} INFO - 21/05/14 14:01:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:01:37,835] {docker.py:276} INFO - 21/05/14 14:01:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 11:01:37,960] {docker.py:276} INFO - 21/05/14 14:01:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 3e4cb2948233:43567 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 11:01:37,986] {docker.py:276} INFO - 21/05/14 14:01:38 INFO CodeGenerator: Code generated in 90.9647 ms
[2021-05-14 11:01:37,989] {docker.py:276} INFO - 21/05/14 14:01:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 11:01:38,042] {docker.py:276} INFO - 21/05/14 14:01:38 INFO CodeGenerator: Code generated in 41.0058 ms
[2021-05-14 11:01:38,045] {docker.py:276} INFO - 21/05/14 14:01:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.3 KiB, free 934.0 MiB)
[2021-05-14 11:01:38,060] {docker.py:276} INFO - 21/05/14 14:01:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 934.0 MiB)
[2021-05-14 11:01:38,061] {docker.py:276} INFO - 21/05/14 14:01:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 3e4cb2948233:43567 (size: 28.1 KiB, free: 934.3 MiB)
[2021-05-14 11:01:38,062] {docker.py:276} INFO - 21/05/14 14:01:38 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:38,071] {docker.py:276} INFO - 21/05/14 14:01:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 11:01:38,142] {docker.py:276} INFO - 21/05/14 14:01:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 3e4cb2948233:43567 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 11:01:38,205] {docker.py:276} INFO - 21/05/14 14:01:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:01:38,209] {docker.py:276} INFO - 21/05/14 14:01:38 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 11:01:38,211] {docker.py:276} INFO - 21/05/14 14:01:38 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/14 14:01:38 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 14:01:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-14 11:01:38,214] {docker.py:276} INFO - 21/05/14 14:01:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-14 11:01:38,216] {docker.py:276} INFO - 21/05/14 14:01:38 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:01:38,230] {docker.py:276} INFO - 21/05/14 14:01:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 27.9 KiB, free 934.2 MiB)
[2021-05-14 11:01:38,240] {docker.py:276} INFO - 21/05/14 14:01:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.2 MiB)
[2021-05-14 11:01:38,241] {docker.py:276} INFO - 21/05/14 14:01:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 3e4cb2948233:43567 (size: 12.0 KiB, free: 934.4 MiB)
[2021-05-14 11:01:38,242] {docker.py:276} INFO - 21/05/14 14:01:38 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:01:38,244] {docker.py:276} INFO - 21/05/14 14:01:38 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21/05/14 14:01:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-14 11:01:38,246] {docker.py:276} INFO - 21/05/14 14:01:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (3e4cb2948233, executor driver, partition 0, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:38,247] {docker.py:276} INFO - 21/05/14 14:01:38 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (3e4cb2948233, executor driver, partition 1, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:38,248] {docker.py:276} INFO - 21/05/14 14:01:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (3e4cb2948233, executor driver, partition 2, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:38,248] {docker.py:276} INFO - 21/05/14 14:01:38 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (3e4cb2948233, executor driver, partition 3, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:38,249] {docker.py:276} INFO - 21/05/14 14:01:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
[2021-05-14 11:01:38,250] {docker.py:276} INFO - 21/05/14 14:01:38 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-14 11:01:38,251] {docker.py:276} INFO - 21/05/14 14:01:38 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-14 11:01:38,255] {docker.py:276} INFO - 21/05/14 14:01:38 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-14 11:01:38,393] {docker.py:276} INFO - 21/05/14 14:01:38 INFO CodeGenerator: Code generated in 54.8049 ms
[2021-05-14 11:01:38,429] {docker.py:276} INFO - 21/05/14 14:01:38 INFO CodeGenerator: Code generated in 13.2198 ms
[2021-05-14 11:01:38,473] {docker.py:276} INFO - 21/05/14 14:01:38 INFO CodeGenerator: Code generated in 35.6867 ms
[2021-05-14 11:01:38,494] {docker.py:276} INFO - 21/05/14 14:01:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620925210_to_1620927010.csv, range: 0-104178, partition values: [empty row]
[2021-05-14 11:01:38,503] {docker.py:276} INFO - 21/05/14 14:01:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620972010_to_1620973810.csv, range: 0-104245, partition values: [empty row]
[2021-05-14 11:01:38,508] {docker.py:276} INFO - 21/05/14 14:01:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620925210_to_1620927010.csv, range: 0-104011, partition values: [empty row]
21/05/14 14:01:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620939610_to_1620941410.csv, range: 0-111641, partition values: [empty row]
[2021-05-14 11:01:39,364] {docker.py:276} INFO - 21/05/14 14:01:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620941410_to_1620943210.csv, range: 0-111641, partition values: [empty row]
[2021-05-14 11:01:39,742] {docker.py:276} INFO - 21/05/14 14:01:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620937810_to_1620939610.csv, range: 0-111604, partition values: [empty row]
[2021-05-14 11:01:39,850] {docker.py:276} INFO - 21/05/14 14:01:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620954010_to_1620955810.csv, range: 0-104006, partition values: [empty row]
[2021-05-14 11:01:39,884] {docker.py:276} INFO - 21/05/14 14:01:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620982810_to_1620984610.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 11:01:39,927] {docker.py:276} INFO - 21/05/14 14:01:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620979210_to_1620981010.csv, range: 0-104176, partition values: [empty row]
[2021-05-14 11:01:40,219] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620970210_to_1620972010.csv, range: 0-104004, partition values: [empty row]
[2021-05-14 11:01:40,274] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620943210_to_1620945010.csv, range: 0-111576, partition values: [empty row]
[2021-05-14 11:01:40,279] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620990010_to_1620991810.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 11:01:40,336] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620990010_to_1620991810.csv, range: 0-104175, partition values: [empty row]
[2021-05-14 11:01:40,663] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620995410_to_1620997210.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 11:01:40,664] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620936010_to_1620937810.csv, range: 0-111568, partition values: [empty row]
[2021-05-14 11:01:40,667] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620932410_to_1620934210.csv, range: 0-103997, partition values: [empty row]
[2021-05-14 11:01:40,689] {docker.py:276} INFO - 21/05/14 14:01:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620988210_to_1620990010.csv, range: 0-104173, partition values: [empty row]
[2021-05-14 11:01:41,042] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620923410_to_1620925210.csv, range: 0-104242, partition values: [empty row]
[2021-05-14 11:01:41,048] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620952210_to_1620954010.csv, range: 0-104169, partition values: [empty row]
[2021-05-14 11:01:41,064] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620945010_to_1620946810.csv, range: 0-111505, partition values: [empty row]
[2021-05-14 11:01:41,172] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620928810_to_1620930610.csv, range: 0-103981, partition values: [empty row]
[2021-05-14 11:01:41,423] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620928810_to_1620930610.csv, range: 0-104165, partition values: [empty row]
[2021-05-14 11:01:41,427] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620957610_to_1620959410.csv, range: 0-104238, partition values: [empty row]
[2021-05-14 11:01:41,490] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620984610_to_1620986410.csv, range: 0-104395, partition values: [empty row]
[2021-05-14 11:01:41,545] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620916210_to_1620918010.csv, range: 0-103978, partition values: [empty row]
[2021-05-14 11:01:41,780] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620946810_to_1620948610.csv, range: 0-104164, partition values: [empty row]
[2021-05-14 11:01:41,789] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620986410_to_1620988210.csv, range: 0-104236, partition values: [empty row]
[2021-05-14 11:01:41,873] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620934210_to_1620936010.csv, range: 0-104388, partition values: [empty row]
[2021-05-14 11:01:41,912] {docker.py:276} INFO - 21/05/14 14:01:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620930610_to_1620932410.csv, range: 0-103947, partition values: [empty row]
[2021-05-14 11:01:42,153] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620979210_to_1620981010.csv, range: 0-104233, partition values: [empty row]
[2021-05-14 11:01:42,176] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620921610_to_1620923410.csv, range: 0-104163, partition values: [empty row]
[2021-05-14 11:01:42,274] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620959410_to_1620961210.csv, range: 0-103945, partition values: [empty row]
[2021-05-14 11:01:42,380] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620941410_to_1620943210.csv, range: 0-104385, partition values: [empty row]
[2021-05-14 11:01:42,535] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620963010_to_1620964810.csv, range: 0-104160, partition values: [empty row]
[2021-05-14 11:01:42,539] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620961210_to_1620963010.csv, range: 0-104233, partition values: [empty row]
[2021-05-14 11:01:42,658] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620982810_to_1620984610.csv, range: 0-103938, partition values: [empty row]
[2021-05-14 11:01:42,888] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620955810_to_1620957610.csv, range: 0-104377, partition values: [empty row]
[2021-05-14 11:01:42,890] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620950410_to_1620952210.csv, range: 0-104159, partition values: [empty row]
[2021-05-14 11:01:42,894] {docker.py:276} INFO - 21/05/14 14:01:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620977410_to_1620979210.csv, range: 0-104230, partition values: [empty row]
[2021-05-14 11:01:43,025] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620995410_to_1620997210.csv, range: 0-103932, partition values: [empty row]
[2021-05-14 11:01:43,248] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620914410_to_1620916210.csv, range: 0-104158, partition values: [empty row]
[2021-05-14 11:01:43,251] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620936010_to_1620937810.csv, range: 0-104343, partition values: [empty row]
[2021-05-14 11:01:43,252] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620981010_to_1620982810.csv, range: 0-104229, partition values: [empty row]
[2021-05-14 11:01:43,417] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620941410_to_1620943210.csv, range: 0-103929, partition values: [empty row]
[2021-05-14 11:01:43,610] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620973810_to_1620975610.csv, range: 0-104158, partition values: [empty row]
[2021-05-14 11:01:43,611] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620919810_to_1620921610.csv, range: 0-104227, partition values: [empty row]
[2021-05-14 11:01:43,618] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620959410_to_1620961210.csv, range: 0-104337, partition values: [empty row]
[2021-05-14 11:01:43,808] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620964810_to_1620966610.csv, range: 0-103912, partition values: [empty row]
[2021-05-14 11:01:43,961] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620984610_to_1620986410.csv, range: 0-104226, partition values: [empty row]
[2021-05-14 11:01:43,964] {docker.py:276} INFO - 21/05/14 14:01:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620975610_to_1620977410.csv, range: 0-104158, partition values: [empty row]
[2021-05-14 11:01:43,999] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620927010_to_1620928810.csv, range: 0-104337, partition values: [empty row]
[2021-05-14 11:01:44,189] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620957610_to_1620959410.csv, range: 0-103901, partition values: [empty row]
[2021-05-14 11:01:44,324] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620928810_to_1620930610.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 11:01:44,365] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620948610_to_1620950410.csv, range: 0-104333, partition values: [empty row]
[2021-05-14 11:01:44,537] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620954010_to_1620955810.csv, range: 0-103900, partition values: [empty row]
[2021-05-14 11:01:44,693] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620975610_to_1620977410.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 11:01:44,731] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620932410_to_1620934210.csv, range: 0-104325, partition values: [empty row]
[2021-05-14 11:01:44,906] {docker.py:276} INFO - 21/05/14 14:01:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620943210_to_1620945010.csv, range: 0-103898, partition values: [empty row]
[2021-05-14 11:01:45,107] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620914410_to_1620916210.csv, range: 0-104324, partition values: [empty row]
[2021-05-14 11:01:45,119] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620973810_to_1620975610.csv, range: 0-104222, partition values: [empty row]
[2021-05-14 11:01:45,282] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620918010_to_1620919810.csv, range: 0-104148, partition values: [empty row]
[2021-05-14 11:01:45,304] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620934210_to_1620936010.csv, range: 0-103897, partition values: [empty row]
[2021-05-14 11:01:45,473] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620954010_to_1620955810.csv, range: 0-104322, partition values: [empty row]
[2021-05-14 11:01:45,478] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620930610_to_1620932410.csv, range: 0-104220, partition values: [empty row]
[2021-05-14 11:01:45,657] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620973810_to_1620975610.csv, range: 0-103897, partition values: [empty row]
[2021-05-14 11:01:45,783] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620966610_to_1620968410.csv, range: 0-104130, partition values: [empty row]
[2021-05-14 11:01:45,831] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620968410_to_1620970210.csv, range: 0-104308, partition values: [empty row]
[2021-05-14 11:01:45,991] {docker.py:276} INFO - 21/05/14 14:01:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620959410_to_1620961210.csv, range: 0-104219, partition values: [empty row]
[2021-05-14 11:01:46,007] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620988210_to_1620990010.csv, range: 0-103877, partition values: [empty row]
[2021-05-14 11:01:46,146] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620997210_to_1620999010.csv, range: 0-104129, partition values: [empty row]
[2021-05-14 11:01:46,217] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620964810_to_1620966610.csv, range: 0-104295, partition values: [empty row]
[2021-05-14 11:01:46,336] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620997210_to_1620999010.csv, range: 0-104218, partition values: [empty row]
[2021-05-14 11:01:46,359] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620952210_to_1620954010.csv, range: 0-103876, partition values: [empty row]
[2021-05-14 11:01:46,507] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620916210_to_1620918010.csv, range: 0-104127, partition values: [empty row]
[2021-05-14 11:01:46,599] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620993610_to_1620995410.csv, range: 0-104295, partition values: [empty row]
[2021-05-14 11:01:46,690] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620977410_to_1620979210.csv, range: 0-104211, partition values: [empty row]
[2021-05-14 11:01:46,730] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620955810_to_1620957610.csv, range: 0-103876, partition values: [empty row]
[2021-05-14 11:01:46,868] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620937810_to_1620939610.csv, range: 0-104119, partition values: [empty row]
[2021-05-14 11:01:46,969] {docker.py:276} INFO - 21/05/14 14:01:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620968410_to_1620970210.csv, range: 0-104290, partition values: [empty row]
[2021-05-14 11:01:47,026] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620982810_to_1620984610.csv, range: 0-104210, partition values: [empty row]
[2021-05-14 11:01:47,091] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620990010_to_1620991810.csv, range: 0-103873, partition values: [empty row]
[2021-05-14 11:01:47,220] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620952210_to_1620954010.csv, range: 0-104118, partition values: [empty row]
[2021-05-14 11:01:47,327] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620970210_to_1620972010.csv, range: 0-104286, partition values: [empty row]
[2021-05-14 11:01:47,367] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620916210_to_1620918010.csv, range: 0-104209, partition values: [empty row]
[2021-05-14 11:01:47,455] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620923410_to_1620925210.csv, range: 0-103871, partition values: [empty row]
[2021-05-14 11:01:47,558] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620930610_to_1620932410.csv, range: 0-104110, partition values: [empty row]
[2021-05-14 11:01:47,677] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620946810_to_1620948610.csv, range: 0-104282, partition values: [empty row]
[2021-05-14 11:01:47,716] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620988210_to_1620990010.csv, range: 0-104203, partition values: [empty row]
[2021-05-14 11:01:47,810] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620948610_to_1620950410.csv, range: 0-103871, partition values: [empty row]
[2021-05-14 11:01:47,905] {docker.py:276} INFO - 21/05/14 14:01:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620955810_to_1620957610.csv, range: 0-104101, partition values: [empty row]
[2021-05-14 11:01:48,038] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620970210_to_1620972010.csv, range: 0-104280, partition values: [empty row]
[2021-05-14 11:01:48,075] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620918010_to_1620919810.csv, range: 0-104202, partition values: [empty row]
[2021-05-14 11:01:48,345] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620921610_to_1620923410.csv, range: 0-103862, partition values: [empty row]
[2021-05-14 11:01:48,352] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620939610_to_1620941410.csv, range: 0-104093, partition values: [empty row]
[2021-05-14 11:01:48,407] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620995410_to_1620997210.csv, range: 0-104200, partition values: [empty row]
[2021-05-14 11:01:48,414] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620972010_to_1620973810.csv, range: 0-104273, partition values: [empty row]
[2021-05-14 11:01:48,710] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620943210_to_1620945010.csv, range: 0-104084, partition values: [empty row]
[2021-05-14 11:01:48,737] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620963010_to_1620964810.csv, range: 0-103850, partition values: [empty row]
[2021-05-14 11:01:48,764] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620945010_to_1620946810.csv, range: 0-104195, partition values: [empty row]
[2021-05-14 11:01:48,768] {docker.py:276} INFO - 21/05/14 14:01:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620993610_to_1620995410.csv, range: 0-104269, partition values: [empty row]
[2021-05-14 11:01:49,054] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620981010_to_1620982810.csv, range: 0-104071, partition values: [empty row]
[2021-05-14 11:01:49,106] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620914410_to_1620916210.csv, range: 0-103843, partition values: [empty row]
[2021-05-14 11:01:49,107] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620986410_to_1620988210.csv, range: 0-104194, partition values: [empty row]
[2021-05-14 11:01:49,119] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620961210_to_1620963010.csv, range: 0-104264, partition values: [empty row]
[2021-05-14 11:01:49,408] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620979210_to_1620981010.csv, range: 0-104058, partition values: [empty row]
[2021-05-14 11:01:49,454] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620936010_to_1620937810.csv, range: 0-103834, partition values: [empty row]
[2021-05-14 11:01:49,494] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620963010_to_1620964810.csv, range: 0-104262, partition values: [empty row]
[2021-05-14 11:01:49,498] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620991810_to_1620993610.csv, range: 0-104190, partition values: [empty row]
[2021-05-14 11:01:49,788] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620927010_to_1620928810.csv, range: 0-104052, partition values: [empty row]
[2021-05-14 11:01:49,827] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620919810_to_1620921610.csv, range: 0-103833, partition values: [empty row]
[2021-05-14 11:01:49,857] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620991810_to_1620993610.csv, range: 0-104188, partition values: [empty row]
[2021-05-14 11:01:49,864] {docker.py:276} INFO - 21/05/14 14:01:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620966610_to_1620968410.csv, range: 0-104257, partition values: [empty row]
[2021-05-14 11:01:50,144] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620919810_to_1620921610.csv, range: 0-104047, partition values: [empty row]
[2021-05-14 11:01:50,196] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620918010_to_1620919810.csv, range: 0-103829, partition values: [empty row]
[2021-05-14 11:01:50,215] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620957610_to_1620959410.csv, range: 0-104184, partition values: [empty row]
[2021-05-14 11:01:50,282] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620927010_to_1620928810.csv, range: 0-104256, partition values: [empty row]
[2021-05-14 11:01:50,488] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620964810_to_1620966610.csv, range: 0-104046, partition values: [empty row]
[2021-05-14 11:01:50,539] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620977410_to_1620979210.csv, range: 0-103827, partition values: [empty row]
[2021-05-14 11:01:50,608] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620981010_to_1620982810.csv, range: 0-104182, partition values: [empty row]
[2021-05-14 11:01:50,631] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620921610_to_1620923410.csv, range: 0-104250, partition values: [empty row]
[2021-05-14 11:01:50,827] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620925210_to_1620927010.csv, range: 0-104042, partition values: [empty row]
[2021-05-14 11:01:50,883] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620986410_to_1620988210.csv, range: 0-103827, partition values: [empty row]
[2021-05-14 11:01:50,970] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620939610_to_1620941410.csv, range: 0-104181, partition values: [empty row]
[2021-05-14 11:01:50,979] {docker.py:276} INFO - 21/05/14 14:01:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620923410_to_1620925210.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 11:01:51,233] {docker.py:276} INFO - 21/05/14 14:01:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620991810_to_1620993610.csv, range: 0-103825, partition values: [empty row]
[2021-05-14 11:01:51,262] {docker.py:276} INFO - 21/05/14 14:01:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620937810_to_1620939610.csv, range: 0-104032, partition values: [empty row]
[2021-05-14 11:01:51,660] {docker.py:276} INFO - 21/05/14 14:01:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620932410_to_1620934210.csv, range: 0-104024, partition values: [empty row]
[2021-05-14 11:01:51,951] {docker.py:276} INFO - 21/05/14 14:01:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2722 bytes result sent to driver
[2021-05-14 11:01:51,952] {docker.py:276} INFO - 21/05/14 14:01:51 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2679 bytes result sent to driver
[2021-05-14 11:01:51,952] {docker.py:276} INFO - 21/05/14 14:01:51 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2679 bytes result sent to driver
[2021-05-14 11:01:51,953] {docker.py:276} INFO - 21/05/14 14:01:51 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (3e4cb2948233, executor driver, partition 4, PROCESS_LOCAL, 6097 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:51,956] {docker.py:276} INFO - 21/05/14 14:01:51 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 13688 ms on 3e4cb2948233 (executor driver) (1/5)
[2021-05-14 11:01:51,956] {docker.py:276} INFO - 21/05/14 14:01:51 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 13690 ms on 3e4cb2948233 (executor driver) (2/5)
[2021-05-14 11:01:51,960] {docker.py:276} INFO - 21/05/14 14:01:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 13696 ms on 3e4cb2948233 (executor driver) (3/5)
[2021-05-14 11:01:51,961] {docker.py:276} INFO - 21/05/14 14:01:51 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-14 11:01:51,977] {docker.py:276} INFO - 21/05/14 14:01:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620975610_to_1620977410.csv, range: 0-103822, partition values: [empty row]
[2021-05-14 11:01:52,014] {docker.py:276} INFO - 21/05/14 14:01:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620966610_to_1620968410.csv, range: 0-104015, partition values: [empty row]
[2021-05-14 11:01:52,338] {docker.py:276} INFO - 21/05/14 14:01:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620993610_to_1620995410.csv, range: 0-103820, partition values: [empty row]
[2021-05-14 11:01:52,510] {docker.py:276} INFO - 21/05/14 14:01:52 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-14 11:01:52,511] {docker.py:276} INFO - 21/05/14 14:01:52 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 14245 ms on 3e4cb2948233 (executor driver) (4/5)
[2021-05-14 11:01:52,677] {docker.py:276} INFO - 21/05/14 14:01:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620950410_to_1620952210.csv, range: 0-103815, partition values: [empty row]
[2021-05-14 11:01:53,022] {docker.py:276} INFO - 21/05/14 14:01:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620934210_to_1620936010.csv, range: 0-103809, partition values: [empty row]
[2021-05-14 11:01:53,365] {docker.py:276} INFO - 21/05/14 14:01:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620961210_to_1620963010.csv, range: 0-103803, partition values: [empty row]
[2021-05-14 11:01:53,715] {docker.py:276} INFO - 21/05/14 14:01:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620972010_to_1620973810.csv, range: 0-103795, partition values: [empty row]
[2021-05-14 11:01:54,106] {docker.py:276} INFO - 21/05/14 14:01:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620968410_to_1620970210.csv, range: 0-103757, partition values: [empty row]
[2021-05-14 11:01:54,502] {docker.py:276} INFO - 21/05/14 14:01:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620984610_to_1620986410.csv, range: 0-103742, partition values: [empty row]
[2021-05-14 11:01:54,849] {docker.py:276} INFO - 21/05/14 14:01:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620945010_to_1620946810.csv, range: 0-103703, partition values: [empty row]
[2021-05-14 11:01:55,193] {docker.py:276} INFO - 21/05/14 14:01:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620997210_to_1620999010.csv, range: 0-103700, partition values: [empty row]
[2021-05-14 11:01:55,566] {docker.py:276} INFO - 21/05/14 14:01:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620950410_to_1620952210.csv, range: 0-103681, partition values: [empty row]
[2021-05-14 11:01:55,908] {docker.py:276} INFO - 21/05/14 14:01:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620948610_to_1620950410.csv, range: 0-103674, partition values: [empty row]
[2021-05-14 11:01:56,263] {docker.py:276} INFO - 21/05/14 14:01:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620946810_to_1620948610.csv, range: 0-103254, partition values: [empty row]
[2021-05-14 11:01:56,710] {docker.py:276} INFO - 21/05/14 14:01:56 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-14 11:01:56,714] {docker.py:276} INFO - 21/05/14 14:01:56 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 4764 ms on 3e4cb2948233 (executor driver) (5/5)
[2021-05-14 11:01:56,714] {docker.py:276} INFO - 21/05/14 14:01:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-14 11:01:56,715] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 18.477 s
[2021-05-14 11:01:56,715] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 11:01:56,716] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: running: Set()
[2021-05-14 11:01:56,717] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-14 11:01:56,718] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: failed: Set()
[2021-05-14 11:01:56,725] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:01:56,834] {docker.py:276} INFO - 21/05/14 14:01:56 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 203.6 KiB, free 934.0 MiB)
[2021-05-14 11:01:56,840] {docker.py:276} INFO - 21/05/14 14:01:56 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 75.9 KiB, free 933.9 MiB)
[2021-05-14 11:01:56,841] {docker.py:276} INFO - 21/05/14 14:01:56 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 3e4cb2948233:43567 (size: 75.9 KiB, free: 934.3 MiB)
[2021-05-14 11:01:56,843] {docker.py:276} INFO - 21/05/14 14:01:56 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:01:56,844] {docker.py:276} INFO - 21/05/14 14:01:56 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 14:01:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-14 11:01:56,854] {docker.py:276} INFO - 21/05/14 14:01:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (3e4cb2948233, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:56,855] {docker.py:276} INFO - 21/05/14 14:01:56 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (3e4cb2948233, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:56,855] {docker.py:276} INFO - 21/05/14 14:01:56 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (3e4cb2948233, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:56,855] {docker.py:276} INFO - 21/05/14 14:01:56 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (3e4cb2948233, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:01:56,856] {docker.py:276} INFO - 21/05/14 14:01:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-14 11:01:56,857] {docker.py:276} INFO - 21/05/14 14:01:56 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
[2021-05-14 11:01:56,858] {docker.py:276} INFO - 21/05/14 14:01:56 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-14 11:01:56,859] {docker.py:276} INFO - 21/05/14 14:01:56 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-14 11:01:56,971] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:01:56,973] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:01:56,974] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-14 11:01:56,974] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-14 11:01:56,975] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:01:56,975] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-14 11:01:56,976] {docker.py:276} INFO - 21/05/14 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-14 11:01:57,007] {docker.py:276} INFO - 21/05/14 14:01:57 INFO CodeGenerator: Code generated in 18.5482 ms
[2021-05-14 11:01:57,036] {docker.py:276} INFO - 21/05/14 14:01:57 INFO CodeGenerator: Code generated in 18.6945 ms
[2021-05-14 11:01:57,075] {docker.py:276} INFO - 21/05/14 14:01:57 INFO CodeGenerator: Code generated in 25.1562 ms
[2021-05-14 11:01:57,188] {docker.py:276} INFO - 21/05/14 14:01:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:01:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:01:57,189] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,189] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387819090469349822717_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387819090469349822717_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387819090469349822717_0004}; taskId=attempt_202105141401387819090469349822717_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43c0ff0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:01:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:01:57,191] {docker.py:276} INFO - 21/05/14 14:01:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:01:57,192] {docker.py:276} INFO - 21/05/14 14:01:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:01:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:01:57,193] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401387819090469349822717_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387819090469349822717_0004_m_000003_291
[2021-05-14 11:01:57,195] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,195] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388868447990424977009_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388868447990424977009_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388868447990424977009_0004}; taskId=attempt_202105141401388868447990424977009_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29e6ad87}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,196] {docker.py:276} INFO - 21/05/14 14:01:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:01:57,196] {docker.py:276} INFO - 21/05/14 14:01:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:01:57,197] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,197] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383125987255314005887_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383125987255314005887_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383125987255314005887_0004}; taskId=attempt_202105141401383125987255314005887_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d1ec42}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,197] {docker.py:276} INFO - 21/05/14 14:01:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:01:57,198] {docker.py:276} INFO - 21/05/14 14:01:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:01:57,199] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401383125987255314005887_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383125987255314005887_0004_m_000000_288
[2021-05-14 11:01:57,200] {docker.py:276} INFO - 21/05/14 14:01:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:01:57,201] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401388868447990424977009_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388868447990424977009_0004_m_000001_289
[2021-05-14 11:01:57,201] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,202] {docker.py:276} INFO - 21/05/14 14:01:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385574181232907771312_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385574181232907771312_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385574181232907771312_0004}; taskId=attempt_202105141401385574181232907771312_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d811e3c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:01:57,205] {docker.py:276} INFO - 21/05/14 14:01:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:01:57,205] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401385574181232907771312_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385574181232907771312_0004_m_000002_290
[2021-05-14 11:01:57,235] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Task committer attempt_202105141401383125987255314005887_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383125987255314005887_0004_m_000000_288 : duration 0:00.037s
[2021-05-14 11:01:57,235] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Task committer attempt_202105141401385574181232907771312_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385574181232907771312_0004_m_000002_290 : duration 0:00.031s
[2021-05-14 11:01:57,237] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Task committer attempt_202105141401387819090469349822717_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387819090469349822717_0004_m_000003_291 : duration 0:00.042s
[2021-05-14 11:01:57,239] {docker.py:276} INFO - 21/05/14 14:01:57 INFO StagingCommitter: Task committer attempt_202105141401388868447990424977009_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388868447990424977009_0004_m_000001_289 : duration 0:00.040s
[2021-05-14 11:01:57,258] {docker.py:276} INFO - 21/05/14 14:01:57 INFO CodeGenerator: Code generated in 14.3335 ms
[2021-05-14 11:01:57,277] {docker.py:276} INFO - 21/05/14 14:01:57 INFO CodeGenerator: Code generated in 13.6027 ms
[2021-05-14 11:01:57,328] {docker.py:276} INFO - 21/05/14 14:01:57 INFO CodeGenerator: Code generated in 22.1727 ms
[2021-05-14 11:02:03,922] {docker.py:276} INFO - 21/05/14 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105141401383125987255314005887_0004_m_000000_288: needsTaskCommit() Task attempt_202105141401383125987255314005887_0004_m_000000_288
[2021-05-14 11:02:03,923] {docker.py:276} INFO - 21/05/14 14:02:03 INFO StagingCommitter: Task committer attempt_202105141401383125987255314005887_0004_m_000000_288: needsTaskCommit() Task attempt_202105141401383125987255314005887_0004_m_000000_288: duration 0:00.000s
[2021-05-14 11:02:03,924] {docker.py:276} INFO - 21/05/14 14:02:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383125987255314005887_0004_m_000000_288
[2021-05-14 11:02:03,932] {docker.py:276} INFO - 21/05/14 14:02:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 5192 bytes result sent to driver
[2021-05-14 11:02:03,934] {docker.py:276} INFO - 21/05/14 14:02:03 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (3e4cb2948233, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:03,934] {docker.py:276} INFO - 21/05/14 14:02:03 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-14 11:02:03,935] {docker.py:276} INFO - 21/05/14 14:02:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 7092 ms on 3e4cb2948233 (executor driver) (1/200)
[2021-05-14 11:02:03,948] {docker.py:276} INFO - 21/05/14 14:02:03 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:03,966] {docker.py:276} INFO - 21/05/14 14:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:03,966] {docker.py:276} INFO - 21/05/14 14:02:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381553238660427566181_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381553238660427566181_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381553238660427566181_0004}; taskId=attempt_202105141401381553238660427566181_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25275b66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:03,966] {docker.py:276} INFO - 21/05/14 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105141401381553238660427566181_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381553238660427566181_0004_m_000004_292
[2021-05-14 11:02:03,972] {docker.py:276} INFO - 21/05/14 14:02:03 INFO StagingCommitter: Task committer attempt_202105141401381553238660427566181_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381553238660427566181_0004_m_000004_292 : duration 0:00.006s
[2021-05-14 11:02:04,317] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401388868447990424977009_0004_m_000001_289: needsTaskCommit() Task attempt_202105141401388868447990424977009_0004_m_000001_289
[2021-05-14 11:02:04,318] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Task committer attempt_202105141401388868447990424977009_0004_m_000001_289: needsTaskCommit() Task attempt_202105141401388868447990424977009_0004_m_000001_289: duration 0:00.002s
[2021-05-14 11:02:04,319] {docker.py:276} INFO - 21/05/14 14:02:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388868447990424977009_0004_m_000001_289
[2021-05-14 11:02:04,321] {docker.py:276} INFO - 21/05/14 14:02:04 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 5149 bytes result sent to driver
[2021-05-14 11:02:04,322] {docker.py:276} INFO - 21/05/14 14:02:04 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (3e4cb2948233, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:04,323] {docker.py:276} INFO - 21/05/14 14:02:04 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-14 11:02:04,324] {docker.py:276} INFO - 21/05/14 14:02:04 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 7478 ms on 3e4cb2948233 (executor driver) (2/200)
[2021-05-14 11:02:04,328] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401387819090469349822717_0004_m_000003_291: needsTaskCommit() Task attempt_202105141401387819090469349822717_0004_m_000003_291
[2021-05-14 11:02:04,329] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Task committer attempt_202105141401387819090469349822717_0004_m_000003_291: needsTaskCommit() Task attempt_202105141401387819090469349822717_0004_m_000003_291: duration 0:00.000s
21/05/14 14:02:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387819090469349822717_0004_m_000003_291
[2021-05-14 11:02:04,331] {docker.py:276} INFO - 21/05/14 14:02:04 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 5149 bytes result sent to driver
[2021-05-14 11:02:04,332] {docker.py:276} INFO - 21/05/14 14:02:04 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (3e4cb2948233, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:04,333] {docker.py:276} INFO - 21/05/14 14:02:04 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 7486 ms on 3e4cb2948233 (executor driver) (3/200)
[2021-05-14 11:02:04,333] {docker.py:276} INFO - 21/05/14 14:02:04 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
[2021-05-14 11:02:04,338] {docker.py:276} INFO - 21/05/14 14:02:04 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:04,339] {docker.py:276} INFO - 21/05/14 14:02:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:04,341] {docker.py:276} INFO - 21/05/14 14:02:04 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:04,342] {docker.py:276} INFO - 21/05/14 14:02:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:04,358] {docker.py:276} INFO - 21/05/14 14:02:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:04,359] {docker.py:276} INFO - 21/05/14 14:02:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:04,359] {docker.py:276} INFO - 21/05/14 14:02:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:04,360] {docker.py:276} INFO - 21/05/14 14:02:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384572914142906722451_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384572914142906722451_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384572914142906722451_0004}; taskId=attempt_202105141401384572914142906722451_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18cdb129}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:04,360] {docker.py:276} INFO - 21/05/14 14:02:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:04,360] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401384572914142906722451_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384572914142906722451_0004_m_000005_293
[2021-05-14 11:02:04,366] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Task committer attempt_202105141401384572914142906722451_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384572914142906722451_0004_m_000005_293 : duration 0:00.006s
[2021-05-14 11:02:04,371] {docker.py:276} INFO - 21/05/14 14:02:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:04,371] {docker.py:276} INFO - 21/05/14 14:02:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:04,371] {docker.py:276} INFO - 21/05/14 14:02:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385972761332035547900_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385972761332035547900_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385972761332035547900_0004}; taskId=attempt_202105141401385972761332035547900_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d05b2ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401385972761332035547900_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385972761332035547900_0004_m_000006_294
[2021-05-14 11:02:04,376] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Task committer attempt_202105141401385972761332035547900_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385972761332035547900_0004_m_000006_294 : duration 0:00.004s
[2021-05-14 11:02:04,879] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401385574181232907771312_0004_m_000002_290: needsTaskCommit() Task attempt_202105141401385574181232907771312_0004_m_000002_290
21/05/14 14:02:04 INFO StagingCommitter: Task committer attempt_202105141401385574181232907771312_0004_m_000002_290: needsTaskCommit() Task attempt_202105141401385574181232907771312_0004_m_000002_290: duration 0:00.002s
21/05/14 14:02:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385574181232907771312_0004_m_000002_290
21/05/14 14:02:04 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 5149 bytes result sent to driver
21/05/14 14:02:04 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (3e4cb2948233, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:02:04 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 8025 ms on 3e4cb2948233 (executor driver) (4/200)
21/05/14 14:02:04 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-14 11:02:04,883] {docker.py:276} INFO - 21/05/14 14:02:04 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:04,902] {docker.py:276} INFO - 21/05/14 14:02:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:04,902] {docker.py:276} INFO - 21/05/14 14:02:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:04,903] {docker.py:276} INFO - 21/05/14 14:02:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383068221724150845903_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383068221724150845903_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383068221724150845903_0004}; taskId=attempt_202105141401383068221724150845903_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@133d43c1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:04,903] {docker.py:276} INFO - 21/05/14 14:02:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:04,903] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401383068221724150845903_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383068221724150845903_0004_m_000007_295
[2021-05-14 11:02:04,909] {docker.py:276} INFO - 21/05/14 14:02:04 INFO StagingCommitter: Task committer attempt_202105141401383068221724150845903_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383068221724150845903_0004_m_000007_295 : duration 0:00.006s
[2021-05-14 11:02:11,031] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401384572914142906722451_0004_m_000005_293: needsTaskCommit() Task attempt_202105141401384572914142906722451_0004_m_000005_293
[2021-05-14 11:02:11,032] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401384572914142906722451_0004_m_000005_293: needsTaskCommit() Task attempt_202105141401384572914142906722451_0004_m_000005_293: duration 0:00.001s
[2021-05-14 11:02:11,033] {docker.py:276} INFO - 21/05/14 14:02:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384572914142906722451_0004_m_000005_293
[2021-05-14 11:02:11,036] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 5106 bytes result sent to driver
21/05/14 14:02:11 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (3e4cb2948233, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:11,037] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
21/05/14 14:02:11 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 6723 ms on 3e4cb2948233 (executor driver) (5/200)
[2021-05-14 11:02:11,049] {docker.py:276} INFO - 21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:11,067] {docker.py:276} INFO - 21/05/14 14:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385489114498502061608_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385489114498502061608_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385489114498502061608_0004}; taskId=attempt_202105141401385489114498502061608_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@639054be}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401385489114498502061608_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385489114498502061608_0004_m_000008_296
[2021-05-14 11:02:11,072] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401385489114498502061608_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385489114498502061608_0004_m_000008_296 : duration 0:00.004s
[2021-05-14 11:02:11,249] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401385972761332035547900_0004_m_000006_294: needsTaskCommit() Task attempt_202105141401385972761332035547900_0004_m_000006_294
[2021-05-14 11:02:11,250] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401385972761332035547900_0004_m_000006_294: needsTaskCommit() Task attempt_202105141401385972761332035547900_0004_m_000006_294: duration 0:00.002s
21/05/14 14:02:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385972761332035547900_0004_m_000006_294
[2021-05-14 11:02:11,252] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 5106 bytes result sent to driver
[2021-05-14 11:02:11,254] {docker.py:276} INFO - 21/05/14 14:02:11 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (3e4cb2948233, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:11,255] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-14 11:02:11,256] {docker.py:276} INFO - 21/05/14 14:02:11 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 6898 ms on 3e4cb2948233 (executor driver) (6/200)
[2021-05-14 11:02:11,264] {docker.py:276} INFO - 21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:11,280] {docker.py:276} INFO - 21/05/14 14:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388225141500860175653_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388225141500860175653_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388225141500860175653_0004}; taskId=attempt_202105141401388225141500860175653_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@756d3d60}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401388225141500860175653_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388225141500860175653_0004_m_000009_297
[2021-05-14 11:02:11,283] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401388225141500860175653_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388225141500860175653_0004_m_000009_297 : duration 0:00.004s
[2021-05-14 11:02:11,465] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401381553238660427566181_0004_m_000004_292: needsTaskCommit() Task attempt_202105141401381553238660427566181_0004_m_000004_292
[2021-05-14 11:02:11,466] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401381553238660427566181_0004_m_000004_292: needsTaskCommit() Task attempt_202105141401381553238660427566181_0004_m_000004_292: duration 0:00.001s
[2021-05-14 11:02:11,466] {docker.py:276} INFO - 21/05/14 14:02:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381553238660427566181_0004_m_000004_292
[2021-05-14 11:02:11,468] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 5106 bytes result sent to driver
[2021-05-14 11:02:11,468] {docker.py:276} INFO - 21/05/14 14:02:11 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (3e4cb2948233, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:11,469] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
[2021-05-14 11:02:11,480] {docker.py:276} INFO - 21/05/14 14:02:11 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 7521 ms on 3e4cb2948233 (executor driver) (7/200)
[2021-05-14 11:02:11,482] {docker.py:276} INFO - 21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:11,483] {docker.py:276} INFO - 21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:02:11,499] {docker.py:276} INFO - 21/05/14 14:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:11,499] {docker.py:276} INFO - 21/05/14 14:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:11,500] {docker.py:276} INFO - 21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:11,500] {docker.py:276} INFO - 21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388614183910512023449_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388614183910512023449_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388614183910512023449_0004}; taskId=attempt_202105141401388614183910512023449_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@364e7aa0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:11,501] {docker.py:276} INFO - 21/05/14 14:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:11,501] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401388614183910512023449_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388614183910512023449_0004_m_000010_298
[2021-05-14 11:02:11,506] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401388614183910512023449_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388614183910512023449_0004_m_000010_298 : duration 0:00.005s
[2021-05-14 11:02:11,881] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401383068221724150845903_0004_m_000007_295: needsTaskCommit() Task attempt_202105141401383068221724150845903_0004_m_000007_295
[2021-05-14 11:02:11,881] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401383068221724150845903_0004_m_000007_295: needsTaskCommit() Task attempt_202105141401383068221724150845903_0004_m_000007_295: duration 0:00.001s
21/05/14 14:02:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383068221724150845903_0004_m_000007_295
[2021-05-14 11:02:11,882] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 5106 bytes result sent to driver
[2021-05-14 11:02:11,883] {docker.py:276} INFO - 21/05/14 14:02:11 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (3e4cb2948233, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:11,884] {docker.py:276} INFO - 21/05/14 14:02:11 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 6988 ms on 3e4cb2948233 (executor driver) (8/200)
[2021-05-14 11:02:11,885] {docker.py:276} INFO - 21/05/14 14:02:11 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-14 11:02:11,894] {docker.py:276} INFO - 21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:11,894] {docker.py:276} INFO - 21/05/14 14:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:11,910] {docker.py:276} INFO - 21/05/14 14:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:11,911] {docker.py:276} INFO - 21/05/14 14:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:11,911] {docker.py:276} INFO - 21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:11,911] {docker.py:276} INFO - 21/05/14 14:02:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383698483462800865548_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383698483462800865548_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383698483462800865548_0004}; taskId=attempt_202105141401383698483462800865548_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60f8c185}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:11,912] {docker.py:276} INFO - 21/05/14 14:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:11,912] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401383698483462800865548_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383698483462800865548_0004_m_000011_299
[2021-05-14 11:02:11,917] {docker.py:276} INFO - 21/05/14 14:02:11 INFO StagingCommitter: Task committer attempt_202105141401383698483462800865548_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383698483462800865548_0004_m_000011_299 : duration 0:00.004s
[2021-05-14 11:02:18,010] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401385489114498502061608_0004_m_000008_296: needsTaskCommit() Task attempt_202105141401385489114498502061608_0004_m_000008_296
21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401385489114498502061608_0004_m_000008_296: needsTaskCommit() Task attempt_202105141401385489114498502061608_0004_m_000008_296: duration 0:00.003s
21/05/14 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385489114498502061608_0004_m_000008_296
[2021-05-14 11:02:18,011] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 5149 bytes result sent to driver
[2021-05-14 11:02:18,012] {docker.py:276} INFO - 21/05/14 14:02:18 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (3e4cb2948233, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:18,013] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-14 11:02:18,014] {docker.py:276} INFO - 21/05/14 14:02:18 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 6951 ms on 3e4cb2948233 (executor driver) (9/200)
[2021-05-14 11:02:18,028] {docker.py:276} INFO - 21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:18,028] {docker.py:276} INFO - 21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:18,044] {docker.py:276} INFO - 21/05/14 14:02:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:18,045] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:18,045] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387866909533930822215_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387866909533930822215_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387866909533930822215_0004}; taskId=attempt_202105141401387866909533930822215_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b4019c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:18,045] {docker.py:276} INFO - 21/05/14 14:02:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:18,046] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401387866909533930822215_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387866909533930822215_0004_m_000012_300
[2021-05-14 11:02:18,050] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401387866909533930822215_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387866909533930822215_0004_m_000012_300 : duration 0:00.005s
[2021-05-14 11:02:18,171] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401388614183910512023449_0004_m_000010_298: needsTaskCommit() Task attempt_202105141401388614183910512023449_0004_m_000010_298
[2021-05-14 11:02:18,172] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401388614183910512023449_0004_m_000010_298: needsTaskCommit() Task attempt_202105141401388614183910512023449_0004_m_000010_298: duration 0:00.003s
[2021-05-14 11:02:18,173] {docker.py:276} INFO - 21/05/14 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388614183910512023449_0004_m_000010_298
[2021-05-14 11:02:18,175] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 5149 bytes result sent to driver
[2021-05-14 11:02:18,188] {docker.py:276} INFO - 21/05/14 14:02:18 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (3e4cb2948233, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:18,189] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
21/05/14 14:02:18 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 6720 ms on 3e4cb2948233 (executor driver) (10/200)
[2021-05-14 11:02:18,190] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401388225141500860175653_0004_m_000009_297: needsTaskCommit() Task attempt_202105141401388225141500860175653_0004_m_000009_297
[2021-05-14 11:02:18,190] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401388225141500860175653_0004_m_000009_297: needsTaskCommit() Task attempt_202105141401388225141500860175653_0004_m_000009_297: duration 0:00.000s
[2021-05-14 11:02:18,191] {docker.py:276} INFO - 21/05/14 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388225141500860175653_0004_m_000009_297
[2021-05-14 11:02:18,191] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401383698483462800865548_0004_m_000011_299: needsTaskCommit() Task attempt_202105141401383698483462800865548_0004_m_000011_299
[2021-05-14 11:02:18,192] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401383698483462800865548_0004_m_000011_299: needsTaskCommit() Task attempt_202105141401383698483462800865548_0004_m_000011_299: duration 0:00.002s
[2021-05-14 11:02:18,192] {docker.py:276} INFO - 21/05/14 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383698483462800865548_0004_m_000011_299
[2021-05-14 11:02:18,194] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 5149 bytes result sent to driver
[2021-05-14 11:02:18,201] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 5149 bytes result sent to driver
[2021-05-14 11:02:18,202] {docker.py:276} INFO - 21/05/14 14:02:18 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (3e4cb2948233, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:02:18 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
21/05/14 14:02:18 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (3e4cb2948233, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:18,203] {docker.py:276} INFO - 21/05/14 14:02:18 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-14 11:02:18,204] {docker.py:276} INFO - 21/05/14 14:02:18 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 6956 ms on 3e4cb2948233 (executor driver) (11/200)
[2021-05-14 11:02:18,204] {docker.py:276} INFO - 21/05/14 14:02:18 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 6327 ms on 3e4cb2948233 (executor driver) (12/200)
[2021-05-14 11:02:18,207] {docker.py:276} INFO - 21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:02:18,214] {docker.py:276} INFO - 21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:18,218] {docker.py:276} INFO - 21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:18,229] {docker.py:276} INFO - 21/05/14 14:02:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:18,230] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:18,231] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386637652970771322799_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386637652970771322799_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386637652970771322799_0004}; taskId=attempt_202105141401386637652970771322799_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@460c940}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:18,231] {docker.py:276} INFO - 21/05/14 14:02:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:18,232] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401386637652970771322799_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386637652970771322799_0004_m_000014_302
[2021-05-14 11:02:18,238] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401386637652970771322799_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386637652970771322799_0004_m_000014_302 : duration 0:00.008s
[2021-05-14 11:02:18,242] {docker.py:276} INFO - 21/05/14 14:02:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:18,243] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:18,243] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138288080217726572755_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138288080217726572755_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138288080217726572755_0004}; taskId=attempt_20210514140138288080217726572755_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c668f6a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:18,244] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_20210514140138288080217726572755_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138288080217726572755_0004_m_000013_301
[2021-05-14 11:02:18,244] {docker.py:276} INFO - 21/05/14 14:02:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:18,245] {docker.py:276} INFO - 21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387267539323449907900_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387267539323449907900_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387267539323449907900_0004}; taskId=attempt_202105141401387267539323449907900_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e82aed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:18,245] {docker.py:276} INFO - 21/05/14 14:02:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:18,246] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401387267539323449907900_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387267539323449907900_0004_m_000015_303
[2021-05-14 11:02:18,252] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_20210514140138288080217726572755_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138288080217726572755_0004_m_000013_301 : duration 0:00.008s
[2021-05-14 11:02:18,255] {docker.py:276} INFO - 21/05/14 14:02:18 INFO StagingCommitter: Task committer attempt_202105141401387267539323449907900_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387267539323449907900_0004_m_000015_303 : duration 0:00.010s
[2021-05-14 11:02:24,947] {docker.py:276} INFO - 21/05/14 14:02:24 INFO StagingCommitter: Starting: Task committer attempt_20210514140138288080217726572755_0004_m_000013_301: needsTaskCommit() Task attempt_20210514140138288080217726572755_0004_m_000013_301
[2021-05-14 11:02:24,948] {docker.py:276} INFO - 21/05/14 14:02:24 INFO StagingCommitter: Task committer attempt_20210514140138288080217726572755_0004_m_000013_301: needsTaskCommit() Task attempt_20210514140138288080217726572755_0004_m_000013_301: duration 0:00.002s
21/05/14 14:02:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138288080217726572755_0004_m_000013_301
[2021-05-14 11:02:24,950] {docker.py:276} INFO - 21/05/14 14:02:24 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 5106 bytes result sent to driver
[2021-05-14 11:02:24,952] {docker.py:276} INFO - 21/05/14 14:02:24 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (3e4cb2948233, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:24,953] {docker.py:276} INFO - 21/05/14 14:02:24 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 6785 ms on 3e4cb2948233 (executor driver) (13/200)
[2021-05-14 11:02:24,954] {docker.py:276} INFO - 21/05/14 14:02:24 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-14 11:02:24,966] {docker.py:276} INFO - 21/05/14 14:02:24 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:24,980] {docker.py:276} INFO - 21/05/14 14:02:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:24,980] {docker.py:276} INFO - 21/05/14 14:02:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:24,981] {docker.py:276} INFO - 21/05/14 14:02:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384742053602534472135_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384742053602534472135_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384742053602534472135_0004}; taskId=attempt_202105141401384742053602534472135_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4fc265ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:24 INFO StagingCommitter: Starting: Task committer attempt_202105141401384742053602534472135_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384742053602534472135_0004_m_000016_304
[2021-05-14 11:02:24,984] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401384742053602534472135_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384742053602534472135_0004_m_000016_304 : duration 0:00.004s
[2021-05-14 11:02:25,101] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401386637652970771322799_0004_m_000014_302: needsTaskCommit() Task attempt_202105141401386637652970771322799_0004_m_000014_302
[2021-05-14 11:02:25,102] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401386637652970771322799_0004_m_000014_302: needsTaskCommit() Task attempt_202105141401386637652970771322799_0004_m_000014_302: duration 0:00.003s
21/05/14 14:02:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386637652970771322799_0004_m_000014_302
[2021-05-14 11:02:25,104] {docker.py:276} INFO - 21/05/14 14:02:25 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 5106 bytes result sent to driver
[2021-05-14 11:02:25,106] {docker.py:276} INFO - 21/05/14 14:02:25 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (3e4cb2948233, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:25,107] {docker.py:276} INFO - 21/05/14 14:02:25 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
21/05/14 14:02:25 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 6919 ms on 3e4cb2948233 (executor driver) (14/200)
[2021-05-14 11:02:25,117] {docker.py:276} INFO - 21/05/14 14:02:25 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:25,118] {docker.py:276} INFO - 21/05/14 14:02:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:25,131] {docker.py:276} INFO - 21/05/14 14:02:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:25,132] {docker.py:276} INFO - 21/05/14 14:02:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384659447472692909983_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384659447472692909983_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384659447472692909983_0004}; taskId=attempt_202105141401384659447472692909983_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d80da42}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:25,132] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401384659447472692909983_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384659447472692909983_0004_m_000017_305
[2021-05-14 11:02:25,138] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401384659447472692909983_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384659447472692909983_0004_m_000017_305 : duration 0:00.006s
[2021-05-14 11:02:25,255] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401387267539323449907900_0004_m_000015_303: needsTaskCommit() Task attempt_202105141401387267539323449907900_0004_m_000015_303
[2021-05-14 11:02:25,255] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401387267539323449907900_0004_m_000015_303: needsTaskCommit() Task attempt_202105141401387267539323449907900_0004_m_000015_303: duration 0:00.001s
[2021-05-14 11:02:25,256] {docker.py:276} INFO - 21/05/14 14:02:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387267539323449907900_0004_m_000015_303
[2021-05-14 11:02:25,257] {docker.py:276} INFO - 21/05/14 14:02:25 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 5106 bytes result sent to driver
[2021-05-14 11:02:25,258] {docker.py:276} INFO - 21/05/14 14:02:25 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (3e4cb2948233, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:25,259] {docker.py:276} INFO - 21/05/14 14:02:25 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
21/05/14 14:02:25 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 7070 ms on 3e4cb2948233 (executor driver) (15/200)
[2021-05-14 11:02:25,268] {docker.py:276} INFO - 21/05/14 14:02:25 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:25,282] {docker.py:276} INFO - 21/05/14 14:02:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:25,282] {docker.py:276} INFO - 21/05/14 14:02:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383453049217316415564_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383453049217316415564_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383453049217316415564_0004}; taskId=attempt_202105141401383453049217316415564_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@159a6804}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401383453049217316415564_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383453049217316415564_0004_m_000018_306
[2021-05-14 11:02:25,287] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401383453049217316415564_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383453049217316415564_0004_m_000018_306 : duration 0:00.005s
[2021-05-14 11:02:25,476] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401387866909533930822215_0004_m_000012_300: needsTaskCommit() Task attempt_202105141401387866909533930822215_0004_m_000012_300
21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401387866909533930822215_0004_m_000012_300: needsTaskCommit() Task attempt_202105141401387866909533930822215_0004_m_000012_300: duration 0:00.002s
[2021-05-14 11:02:25,477] {docker.py:276} INFO - 21/05/14 14:02:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387866909533930822215_0004_m_000012_300
[2021-05-14 11:02:25,479] {docker.py:276} INFO - 21/05/14 14:02:25 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 5106 bytes result sent to driver
[2021-05-14 11:02:25,481] {docker.py:276} INFO - 21/05/14 14:02:25 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (3e4cb2948233, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:25,482] {docker.py:276} INFO - 21/05/14 14:02:25 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-14 11:02:25,483] {docker.py:276} INFO - 21/05/14 14:02:25 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 7479 ms on 3e4cb2948233 (executor driver) (16/200)
[2021-05-14 11:02:25,493] {docker.py:276} INFO - 21/05/14 14:02:25 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:25,493] {docker.py:276} INFO - 21/05/14 14:02:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:25,507] {docker.py:276} INFO - 21/05/14 14:02:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:25,507] {docker.py:276} INFO - 21/05/14 14:02:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:25,508] {docker.py:276} INFO - 21/05/14 14:02:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:25,508] {docker.py:276} INFO - 21/05/14 14:02:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385861892712877651625_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385861892712877651625_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385861892712877651625_0004}; taskId=attempt_202105141401385861892712877651625_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c9cdff6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:25,508] {docker.py:276} INFO - 21/05/14 14:02:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:25,509] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401385861892712877651625_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385861892712877651625_0004_m_000019_307
[2021-05-14 11:02:25,513] {docker.py:276} INFO - 21/05/14 14:02:25 INFO StagingCommitter: Task committer attempt_202105141401385861892712877651625_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385861892712877651625_0004_m_000019_307 : duration 0:00.004s
[2021-05-14 11:02:31,680] {docker.py:276} INFO - 21/05/14 14:02:31 INFO StagingCommitter: Starting: Task committer attempt_202105141401384659447472692909983_0004_m_000017_305: needsTaskCommit() Task attempt_202105141401384659447472692909983_0004_m_000017_305
[2021-05-14 11:02:31,681] {docker.py:276} INFO - 21/05/14 14:02:31 INFO StagingCommitter: Task committer attempt_202105141401384659447472692909983_0004_m_000017_305: needsTaskCommit() Task attempt_202105141401384659447472692909983_0004_m_000017_305: duration 0:00.001s
21/05/14 14:02:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384659447472692909983_0004_m_000017_305
[2021-05-14 11:02:31,683] {docker.py:276} INFO - 21/05/14 14:02:31 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 5149 bytes result sent to driver
[2021-05-14 11:02:31,684] {docker.py:276} INFO - 21/05/14 14:02:31 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (3e4cb2948233, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:31,685] {docker.py:276} INFO - 21/05/14 14:02:31 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 6588 ms on 3e4cb2948233 (executor driver) (17/200)
21/05/14 14:02:31 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-14 11:02:31,696] {docker.py:276} INFO - 21/05/14 14:02:31 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:02:31,709] {docker.py:276} INFO - 21/05/14 14:02:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:31,710] {docker.py:276} INFO - 21/05/14 14:02:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381231729642558684075_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381231729642558684075_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381231729642558684075_0004}; taskId=attempt_202105141401381231729642558684075_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@758ee52e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:31 INFO StagingCommitter: Starting: Task committer attempt_202105141401381231729642558684075_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381231729642558684075_0004_m_000020_308
[2021-05-14 11:02:31,712] {docker.py:276} INFO - 21/05/14 14:02:31 INFO StagingCommitter: Task committer attempt_202105141401381231729642558684075_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381231729642558684075_0004_m_000020_308 : duration 0:00.003s
[2021-05-14 11:02:31,857] {docker.py:276} INFO - 21/05/14 14:02:31 INFO StagingCommitter: Starting: Task committer attempt_202105141401385861892712877651625_0004_m_000019_307: needsTaskCommit() Task attempt_202105141401385861892712877651625_0004_m_000019_307
[2021-05-14 11:02:31,858] {docker.py:276} INFO - 21/05/14 14:02:31 INFO StagingCommitter: Task committer attempt_202105141401385861892712877651625_0004_m_000019_307: needsTaskCommit() Task attempt_202105141401385861892712877651625_0004_m_000019_307: duration 0:00.002s
21/05/14 14:02:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385861892712877651625_0004_m_000019_307
[2021-05-14 11:02:31,861] {docker.py:276} INFO - 21/05/14 14:02:31 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 5149 bytes result sent to driver
[2021-05-14 11:02:31,863] {docker.py:276} INFO - 21/05/14 14:02:31 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (3e4cb2948233, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:31,864] {docker.py:276} INFO - 21/05/14 14:02:31 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
21/05/14 14:02:31 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 6391 ms on 3e4cb2948233 (executor driver) (18/200)
[2021-05-14 11:02:31,874] {docker.py:276} INFO - 21/05/14 14:02:31 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:31,875] {docker.py:276} INFO - 21/05/14 14:02:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 11:02:31,889] {docker.py:276} INFO - 21/05/14 14:02:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:31,889] {docker.py:276} INFO - 21/05/14 14:02:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382590127109505773576_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382590127109505773576_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382590127109505773576_0004}; taskId=attempt_202105141401382590127109505773576_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73b5cd36}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:31 INFO StagingCommitter: Starting: Task committer attempt_202105141401382590127109505773576_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382590127109505773576_0004_m_000021_309
[2021-05-14 11:02:31,895] {docker.py:276} INFO - 21/05/14 14:02:31 INFO StagingCommitter: Task committer attempt_202105141401382590127109505773576_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382590127109505773576_0004_m_000021_309 : duration 0:00.006s
[2021-05-14 11:02:32,173] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401383453049217316415564_0004_m_000018_306: needsTaskCommit() Task attempt_202105141401383453049217316415564_0004_m_000018_306
[2021-05-14 11:02:32,174] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Task committer attempt_202105141401383453049217316415564_0004_m_000018_306: needsTaskCommit() Task attempt_202105141401383453049217316415564_0004_m_000018_306: duration 0:00.001s
[2021-05-14 11:02:32,174] {docker.py:276} INFO - 21/05/14 14:02:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383453049217316415564_0004_m_000018_306
[2021-05-14 11:02:32,176] {docker.py:276} INFO - 21/05/14 14:02:32 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 5149 bytes result sent to driver
[2021-05-14 11:02:32,177] {docker.py:276} INFO - 21/05/14 14:02:32 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (3e4cb2948233, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:32,178] {docker.py:276} INFO - 21/05/14 14:02:32 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 6928 ms on 3e4cb2948233 (executor driver) (19/200)
[2021-05-14 11:02:32,178] {docker.py:276} INFO - 21/05/14 14:02:32 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-14 11:02:32,185] {docker.py:276} INFO - 21/05/14 14:02:32 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:32,185] {docker.py:276} INFO - 21/05/14 14:02:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:32,197] {docker.py:276} INFO - 21/05/14 14:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:32,197] {docker.py:276} INFO - 21/05/14 14:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:32,198] {docker.py:276} INFO - 21/05/14 14:02:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:32,198] {docker.py:276} INFO - 21/05/14 14:02:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385783211954571130041_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385783211954571130041_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385783211954571130041_0004}; taskId=attempt_202105141401385783211954571130041_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b16e0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:32,199] {docker.py:276} INFO - 21/05/14 14:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:32,199] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401385783211954571130041_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385783211954571130041_0004_m_000022_310
[2021-05-14 11:02:32,203] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Task committer attempt_202105141401385783211954571130041_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385783211954571130041_0004_m_000022_310 : duration 0:00.005s
[2021-05-14 11:02:32,691] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401384742053602534472135_0004_m_000016_304: needsTaskCommit() Task attempt_202105141401384742053602534472135_0004_m_000016_304
[2021-05-14 11:02:32,692] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Task committer attempt_202105141401384742053602534472135_0004_m_000016_304: needsTaskCommit() Task attempt_202105141401384742053602534472135_0004_m_000016_304: duration 0:00.001s
21/05/14 14:02:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384742053602534472135_0004_m_000016_304
[2021-05-14 11:02:32,695] {docker.py:276} INFO - 21/05/14 14:02:32 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 5192 bytes result sent to driver
[2021-05-14 11:02:32,696] {docker.py:276} INFO - 21/05/14 14:02:32 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (3e4cb2948233, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:32,697] {docker.py:276} INFO - 21/05/14 14:02:32 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 7755 ms on 3e4cb2948233 (executor driver) (20/200)
[2021-05-14 11:02:32,698] {docker.py:276} INFO - 21/05/14 14:02:32 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-14 11:02:32,709] {docker.py:276} INFO - 21/05/14 14:02:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:32,724] {docker.py:276} INFO - 21/05/14 14:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:32,724] {docker.py:276} INFO - 21/05/14 14:02:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138165303416510208217_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138165303416510208217_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138165303416510208217_0004}; taskId=attempt_20210514140138165303416510208217_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@545c6734}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:32,724] {docker.py:276} INFO - 21/05/14 14:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:32 INFO StagingCommitter: Starting: Task committer attempt_20210514140138165303416510208217_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138165303416510208217_0004_m_000023_311
[2021-05-14 11:02:32,730] {docker.py:276} INFO - 21/05/14 14:02:32 INFO StagingCommitter: Task committer attempt_20210514140138165303416510208217_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138165303416510208217_0004_m_000023_311 : duration 0:00.006s
[2021-05-14 11:02:38,336] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401382590127109505773576_0004_m_000021_309: needsTaskCommit() Task attempt_202105141401382590127109505773576_0004_m_000021_309
[2021-05-14 11:02:38,336] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Task committer attempt_202105141401382590127109505773576_0004_m_000021_309: needsTaskCommit() Task attempt_202105141401382590127109505773576_0004_m_000021_309: duration 0:00.002s
21/05/14 14:02:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382590127109505773576_0004_m_000021_309
[2021-05-14 11:02:38,338] {docker.py:276} INFO - 21/05/14 14:02:38 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 5106 bytes result sent to driver
[2021-05-14 11:02:38,340] {docker.py:276} INFO - 21/05/14 14:02:38 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (3e4cb2948233, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:38,341] {docker.py:276} INFO - 21/05/14 14:02:38 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-14 11:02:38,342] {docker.py:276} INFO - 21/05/14 14:02:38 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 6485 ms on 3e4cb2948233 (executor driver) (21/200)
[2021-05-14 11:02:38,354] {docker.py:276} INFO - 21/05/14 14:02:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:38,366] {docker.py:276} INFO - 21/05/14 14:02:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381434908767873951629_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381434908767873951629_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381434908767873951629_0004}; taskId=attempt_202105141401381434908767873951629_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73159114}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:38,366] {docker.py:276} INFO - 21/05/14 14:02:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:38,367] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401381434908767873951629_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381434908767873951629_0004_m_000024_312
[2021-05-14 11:02:38,372] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Task committer attempt_202105141401381434908767873951629_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381434908767873951629_0004_m_000024_312 : duration 0:00.005s
[2021-05-14 11:02:38,652] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401385783211954571130041_0004_m_000022_310: needsTaskCommit() Task attempt_202105141401385783211954571130041_0004_m_000022_310
[2021-05-14 11:02:38,652] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Task committer attempt_202105141401385783211954571130041_0004_m_000022_310: needsTaskCommit() Task attempt_202105141401385783211954571130041_0004_m_000022_310: duration 0:00.002s
21/05/14 14:02:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385783211954571130041_0004_m_000022_310
[2021-05-14 11:02:38,653] {docker.py:276} INFO - 21/05/14 14:02:38 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 5106 bytes result sent to driver
[2021-05-14 11:02:38,655] {docker.py:276} INFO - 21/05/14 14:02:38 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (3e4cb2948233, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:38,656] {docker.py:276} INFO - 21/05/14 14:02:38 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
[2021-05-14 11:02:38,658] {docker.py:276} INFO - 21/05/14 14:02:38 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 6487 ms on 3e4cb2948233 (executor driver) (22/200)
[2021-05-14 11:02:38,669] {docker.py:276} INFO - 21/05/14 14:02:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:38,682] {docker.py:276} INFO - 21/05/14 14:02:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:38,682] {docker.py:276} INFO - 21/05/14 14:02:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381562334785943191583_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381562334785943191583_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381562334785943191583_0004}; taskId=attempt_202105141401381562334785943191583_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d83663f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401381562334785943191583_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381562334785943191583_0004_m_000025_313
[2021-05-14 11:02:38,686] {docker.py:276} INFO - 21/05/14 14:02:38 INFO StagingCommitter: Task committer attempt_202105141401381562334785943191583_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381562334785943191583_0004_m_000025_313 : duration 0:00.004s
[2021-05-14 11:02:39,526] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401381231729642558684075_0004_m_000020_308: needsTaskCommit() Task attempt_202105141401381231729642558684075_0004_m_000020_308
[2021-05-14 11:02:39,528] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Task committer attempt_202105141401381231729642558684075_0004_m_000020_308: needsTaskCommit() Task attempt_202105141401381231729642558684075_0004_m_000020_308: duration 0:00.002s
[2021-05-14 11:02:39,528] {docker.py:276} INFO - 21/05/14 14:02:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381231729642558684075_0004_m_000020_308
[2021-05-14 11:02:39,530] {docker.py:276} INFO - 21/05/14 14:02:39 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 5106 bytes result sent to driver
[2021-05-14 11:02:39,531] {docker.py:276} INFO - 21/05/14 14:02:39 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (3e4cb2948233, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:39,532] {docker.py:276} INFO - 21/05/14 14:02:39 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-14 11:02:39,535] {docker.py:276} INFO - 21/05/14 14:02:39 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 7858 ms on 3e4cb2948233 (executor driver) (23/200)
[2021-05-14 11:02:39,544] {docker.py:276} INFO - 21/05/14 14:02:39 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:39,557] {docker.py:276} INFO - 21/05/14 14:02:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382668244277821659732_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382668244277821659732_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382668244277821659732_0004}; taskId=attempt_202105141401382668244277821659732_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d8626bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:39,557] {docker.py:276} INFO - 21/05/14 14:02:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:39,557] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401382668244277821659732_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382668244277821659732_0004_m_000026_314
[2021-05-14 11:02:39,562] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Task committer attempt_202105141401382668244277821659732_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382668244277821659732_0004_m_000026_314 : duration 0:00.005s
[2021-05-14 11:02:39,706] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Starting: Task committer attempt_20210514140138165303416510208217_0004_m_000023_311: needsTaskCommit() Task attempt_20210514140138165303416510208217_0004_m_000023_311
[2021-05-14 11:02:39,707] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Task committer attempt_20210514140138165303416510208217_0004_m_000023_311: needsTaskCommit() Task attempt_20210514140138165303416510208217_0004_m_000023_311: duration 0:00.002s
21/05/14 14:02:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138165303416510208217_0004_m_000023_311
[2021-05-14 11:02:39,709] {docker.py:276} INFO - 21/05/14 14:02:39 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 5106 bytes result sent to driver
[2021-05-14 11:02:39,712] {docker.py:276} INFO - 21/05/14 14:02:39 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (3e4cb2948233, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:39,713] {docker.py:276} INFO - 21/05/14 14:02:39 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 7026 ms on 3e4cb2948233 (executor driver) (24/200)
21/05/14 14:02:39 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
[2021-05-14 11:02:39,724] {docker.py:276} INFO - 21/05/14 14:02:39 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:39,737] {docker.py:276} INFO - 21/05/14 14:02:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:39,738] {docker.py:276} INFO - 21/05/14 14:02:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:39,738] {docker.py:276} INFO - 21/05/14 14:02:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386865230879978058171_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386865230879978058171_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386865230879978058171_0004}; taskId=attempt_202105141401386865230879978058171_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a761992}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:39,739] {docker.py:276} INFO - 21/05/14 14:02:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:39,739] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401386865230879978058171_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386865230879978058171_0004_m_000027_315
[2021-05-14 11:02:39,742] {docker.py:276} INFO - 21/05/14 14:02:39 INFO StagingCommitter: Task committer attempt_202105141401386865230879978058171_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386865230879978058171_0004_m_000027_315 : duration 0:00.003s
[2021-05-14 11:02:45,458] {docker.py:276} INFO - 21/05/14 14:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105141401381434908767873951629_0004_m_000024_312: needsTaskCommit() Task attempt_202105141401381434908767873951629_0004_m_000024_312
21/05/14 14:02:45 INFO StagingCommitter: Task committer attempt_202105141401381434908767873951629_0004_m_000024_312: needsTaskCommit() Task attempt_202105141401381434908767873951629_0004_m_000024_312: duration 0:00.002s
21/05/14 14:02:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381434908767873951629_0004_m_000024_312
[2021-05-14 11:02:45,461] {docker.py:276} INFO - 21/05/14 14:02:45 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 5106 bytes result sent to driver
[2021-05-14 11:02:45,462] {docker.py:276} INFO - 21/05/14 14:02:45 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (3e4cb2948233, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:45,462] {docker.py:276} INFO - 21/05/14 14:02:45 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-14 11:02:45,463] {docker.py:276} INFO - 21/05/14 14:02:45 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 7098 ms on 3e4cb2948233 (executor driver) (25/200)
[2021-05-14 11:02:45,473] {docker.py:276} INFO - 21/05/14 14:02:45 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:02:45,474] {docker.py:276} INFO - 21/05/14 14:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:45,486] {docker.py:276} INFO - 21/05/14 14:02:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:45,487] {docker.py:276} INFO - 21/05/14 14:02:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:45,487] {docker.py:276} INFO - 21/05/14 14:02:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:45,487] {docker.py:276} INFO - 21/05/14 14:02:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385088883102472209817_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385088883102472209817_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385088883102472209817_0004}; taskId=attempt_202105141401385088883102472209817_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f4b4a85}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:45,488] {docker.py:276} INFO - 21/05/14 14:02:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:45,488] {docker.py:276} INFO - 21/05/14 14:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105141401385088883102472209817_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385088883102472209817_0004_m_000028_316
[2021-05-14 11:02:45,491] {docker.py:276} INFO - 21/05/14 14:02:45 INFO StagingCommitter: Task committer attempt_202105141401385088883102472209817_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385088883102472209817_0004_m_000028_316 : duration 0:00.005s
[2021-05-14 11:02:45,506] {docker.py:276} INFO - 21/05/14 14:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105141401381562334785943191583_0004_m_000025_313: needsTaskCommit() Task attempt_202105141401381562334785943191583_0004_m_000025_313
[2021-05-14 11:02:45,507] {docker.py:276} INFO - 21/05/14 14:02:45 INFO StagingCommitter: Task committer attempt_202105141401381562334785943191583_0004_m_000025_313: needsTaskCommit() Task attempt_202105141401381562334785943191583_0004_m_000025_313: duration 0:00.001s
[2021-05-14 11:02:45,507] {docker.py:276} INFO - 21/05/14 14:02:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381562334785943191583_0004_m_000025_313
[2021-05-14 11:02:45,508] {docker.py:276} INFO - 21/05/14 14:02:45 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 5106 bytes result sent to driver
[2021-05-14 11:02:45,508] {docker.py:276} INFO - 21/05/14 14:02:45 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (3e4cb2948233, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:45,509] {docker.py:276} INFO - 21/05/14 14:02:45 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-14 11:02:45,510] {docker.py:276} INFO - 21/05/14 14:02:45 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 6829 ms on 3e4cb2948233 (executor driver) (26/200)
[2021-05-14 11:02:45,517] {docker.py:276} INFO - 21/05/14 14:02:45 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:45,540] {docker.py:276} INFO - 21/05/14 14:02:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386489993596658463809_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386489993596658463809_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386489993596658463809_0004}; taskId=attempt_202105141401386489993596658463809_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70a4b30f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:45,540] {docker.py:276} INFO - 21/05/14 14:02:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105141401386489993596658463809_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386489993596658463809_0004_m_000029_317
[2021-05-14 11:02:45,544] {docker.py:276} INFO - 21/05/14 14:02:45 INFO StagingCommitter: Task committer attempt_202105141401386489993596658463809_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386489993596658463809_0004_m_000029_317 : duration 0:00.004s
[2021-05-14 11:02:46,347] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401382668244277821659732_0004_m_000026_314: needsTaskCommit() Task attempt_202105141401382668244277821659732_0004_m_000026_314
[2021-05-14 11:02:46,349] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Task committer attempt_202105141401382668244277821659732_0004_m_000026_314: needsTaskCommit() Task attempt_202105141401382668244277821659732_0004_m_000026_314: duration 0:00.003s
[2021-05-14 11:02:46,350] {docker.py:276} INFO - 21/05/14 14:02:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382668244277821659732_0004_m_000026_314
[2021-05-14 11:02:46,353] {docker.py:276} INFO - 21/05/14 14:02:46 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 5149 bytes result sent to driver
[2021-05-14 11:02:46,354] {docker.py:276} INFO - 21/05/14 14:02:46 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (3e4cb2948233, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:46,355] {docker.py:276} INFO - 21/05/14 14:02:46 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 6798 ms on 3e4cb2948233 (executor driver) (27/200)
[2021-05-14 11:02:46,356] {docker.py:276} INFO - 21/05/14 14:02:46 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
[2021-05-14 11:02:46,366] {docker.py:276} INFO - 21/05/14 14:02:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:46,378] {docker.py:276} INFO - 21/05/14 14:02:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383270977973534318019_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383270977973534318019_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383270977973534318019_0004}; taskId=attempt_202105141401383270977973534318019_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c6bd275}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:46,378] {docker.py:276} INFO - 21/05/14 14:02:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:46,378] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401383270977973534318019_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383270977973534318019_0004_m_000030_318
[2021-05-14 11:02:46,382] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Task committer attempt_202105141401383270977973534318019_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383270977973534318019_0004_m_000030_318 : duration 0:00.004s
[2021-05-14 11:02:46,618] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401386865230879978058171_0004_m_000027_315: needsTaskCommit() Task attempt_202105141401386865230879978058171_0004_m_000027_315
[2021-05-14 11:02:46,619] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Task committer attempt_202105141401386865230879978058171_0004_m_000027_315: needsTaskCommit() Task attempt_202105141401386865230879978058171_0004_m_000027_315: duration 0:00.003s
21/05/14 14:02:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386865230879978058171_0004_m_000027_315
[2021-05-14 11:02:46,622] {docker.py:276} INFO - 21/05/14 14:02:46 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 5149 bytes result sent to driver
[2021-05-14 11:02:46,623] {docker.py:276} INFO - 21/05/14 14:02:46 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (3e4cb2948233, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:46,625] {docker.py:276} INFO - 21/05/14 14:02:46 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 6888 ms on 3e4cb2948233 (executor driver) (28/200)
[2021-05-14 11:02:46,625] {docker.py:276} INFO - 21/05/14 14:02:46 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
[2021-05-14 11:02:46,635] {docker.py:276} INFO - 21/05/14 14:02:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:46,647] {docker.py:276} INFO - 21/05/14 14:02:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:46,648] {docker.py:276} INFO - 21/05/14 14:02:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381907167738661766999_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381907167738661766999_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381907167738661766999_0004}; taskId=attempt_202105141401381907167738661766999_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ef2c88d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:46,648] {docker.py:276} INFO - 21/05/14 14:02:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:46,648] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401381907167738661766999_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381907167738661766999_0004_m_000031_319
[2021-05-14 11:02:46,652] {docker.py:276} INFO - 21/05/14 14:02:46 INFO StagingCommitter: Task committer attempt_202105141401381907167738661766999_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381907167738661766999_0004_m_000031_319 : duration 0:00.004s
[2021-05-14 11:02:52,141] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105141401385088883102472209817_0004_m_000028_316: needsTaskCommit() Task attempt_202105141401385088883102472209817_0004_m_000028_316
[2021-05-14 11:02:52,141] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Task committer attempt_202105141401385088883102472209817_0004_m_000028_316: needsTaskCommit() Task attempt_202105141401385088883102472209817_0004_m_000028_316: duration 0:00.002s
[2021-05-14 11:02:52,142] {docker.py:276} INFO - 21/05/14 14:02:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385088883102472209817_0004_m_000028_316
[2021-05-14 11:02:52,145] {docker.py:276} INFO - 21/05/14 14:02:52 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 5149 bytes result sent to driver
[2021-05-14 11:02:52,146] {docker.py:276} INFO - 21/05/14 14:02:52 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (3e4cb2948233, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:52,147] {docker.py:276} INFO - 21/05/14 14:02:52 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-14 11:02:52,148] {docker.py:276} INFO - 21/05/14 14:02:52 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 6694 ms on 3e4cb2948233 (executor driver) (29/200)
[2021-05-14 11:02:52,158] {docker.py:276} INFO - 21/05/14 14:02:52 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:52,170] {docker.py:276} INFO - 21/05/14 14:02:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381595495860982716692_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381595495860982716692_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381595495860982716692_0004}; taskId=attempt_202105141401381595495860982716692_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b8f10cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:52,170] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105141401381595495860982716692_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381595495860982716692_0004_m_000032_320
[2021-05-14 11:02:52,175] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Task committer attempt_202105141401381595495860982716692_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381595495860982716692_0004_m_000032_320 : duration 0:00.005s
[2021-05-14 11:02:52,587] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105141401386489993596658463809_0004_m_000029_317: needsTaskCommit() Task attempt_202105141401386489993596658463809_0004_m_000029_317
[2021-05-14 11:02:52,588] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Task committer attempt_202105141401386489993596658463809_0004_m_000029_317: needsTaskCommit() Task attempt_202105141401386489993596658463809_0004_m_000029_317: duration 0:00.003s
[2021-05-14 11:02:52,589] {docker.py:276} INFO - 21/05/14 14:02:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386489993596658463809_0004_m_000029_317
[2021-05-14 11:02:52,592] {docker.py:276} INFO - 21/05/14 14:02:52 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 5149 bytes result sent to driver
[2021-05-14 11:02:52,594] {docker.py:276} INFO - 21/05/14 14:02:52 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (3e4cb2948233, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:52,595] {docker.py:276} INFO - 21/05/14 14:02:52 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 7094 ms on 3e4cb2948233 (executor driver) (30/200)
[2021-05-14 11:02:52,596] {docker.py:276} INFO - 21/05/14 14:02:52 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-14 11:02:52,605] {docker.py:276} INFO - 21/05/14 14:02:52 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:52,616] {docker.py:276} INFO - 21/05/14 14:02:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:52,617] {docker.py:276} INFO - 21/05/14 14:02:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:52,617] {docker.py:276} INFO - 21/05/14 14:02:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387125320804128074726_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387125320804128074726_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387125320804128074726_0004}; taskId=attempt_202105141401387125320804128074726_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f26591e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:52,618] {docker.py:276} INFO - 21/05/14 14:02:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:52,618] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105141401387125320804128074726_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387125320804128074726_0004_m_000033_321
[2021-05-14 11:02:52,623] {docker.py:276} INFO - 21/05/14 14:02:52 INFO StagingCommitter: Task committer attempt_202105141401387125320804128074726_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387125320804128074726_0004_m_000033_321 : duration 0:00.005s
[2021-05-14 11:02:53,104] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401383270977973534318019_0004_m_000030_318: needsTaskCommit() Task attempt_202105141401383270977973534318019_0004_m_000030_318
[2021-05-14 11:02:53,105] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Task committer attempt_202105141401383270977973534318019_0004_m_000030_318: needsTaskCommit() Task attempt_202105141401383270977973534318019_0004_m_000030_318: duration 0:00.002s
21/05/14 14:02:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383270977973534318019_0004_m_000030_318
[2021-05-14 11:02:53,108] {docker.py:276} INFO - 21/05/14 14:02:53 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 5106 bytes result sent to driver
[2021-05-14 11:02:53,109] {docker.py:276} INFO - 21/05/14 14:02:53 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (3e4cb2948233, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:53,110] {docker.py:276} INFO - 21/05/14 14:02:53 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
[2021-05-14 11:02:53,110] {docker.py:276} INFO - 21/05/14 14:02:53 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 6764 ms on 3e4cb2948233 (executor driver) (31/200)
[2021-05-14 11:02:53,121] {docker.py:276} INFO - 21/05/14 14:02:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:53,121] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401381907167738661766999_0004_m_000031_319: needsTaskCommit() Task attempt_202105141401381907167738661766999_0004_m_000031_319
[2021-05-14 11:02:53,122] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Task committer attempt_202105141401381907167738661766999_0004_m_000031_319: needsTaskCommit() Task attempt_202105141401381907167738661766999_0004_m_000031_319: duration 0:00.001s
[2021-05-14 11:02:53,123] {docker.py:276} INFO - 21/05/14 14:02:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381907167738661766999_0004_m_000031_319
[2021-05-14 11:02:53,125] {docker.py:276} INFO - 21/05/14 14:02:53 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 5106 bytes result sent to driver
[2021-05-14 11:02:53,126] {docker.py:276} INFO - 21/05/14 14:02:53 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (3e4cb2948233, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:53,127] {docker.py:276} INFO - 21/05/14 14:02:53 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
21/05/14 14:02:53 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 6512 ms on 3e4cb2948233 (executor driver) (32/200)
[2021-05-14 11:02:53,135] {docker.py:276} INFO - 21/05/14 14:02:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 14:02:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:02:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:53,136] {docker.py:276} INFO - 21/05/14 14:02:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381639446679489001967_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381639446679489001967_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381639446679489001967_0004}; taskId=attempt_202105141401381639446679489001967_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57f4fb24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:02:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:02:53,136] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401381639446679489001967_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381639446679489001967_0004_m_000034_322
[2021-05-14 11:02:53,141] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Task committer attempt_202105141401381639446679489001967_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381639446679489001967_0004_m_000034_322 : duration 0:00.006s
[2021-05-14 11:02:53,151] {docker.py:276} INFO - 21/05/14 14:02:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:02:53,152] {docker.py:276} INFO - 21/05/14 14:02:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:53,152] {docker.py:276} INFO - 21/05/14 14:02:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:53,152] {docker.py:276} INFO - 21/05/14 14:02:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383208807833759379345_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383208807833759379345_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383208807833759379345_0004}; taskId=attempt_202105141401383208807833759379345_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30d2a74e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:53,152] {docker.py:276} INFO - 21/05/14 14:02:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401383208807833759379345_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383208807833759379345_0004_m_000035_323
[2021-05-14 11:02:53,157] {docker.py:276} INFO - 21/05/14 14:02:53 INFO StagingCommitter: Task committer attempt_202105141401383208807833759379345_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383208807833759379345_0004_m_000035_323 : duration 0:00.004s
[2021-05-14 11:02:58,211] {docker.py:276} INFO - 21/05/14 14:02:58 INFO StagingCommitter: Starting: Task committer attempt_202105141401381595495860982716692_0004_m_000032_320: needsTaskCommit() Task attempt_202105141401381595495860982716692_0004_m_000032_320
[2021-05-14 11:02:58,212] {docker.py:276} INFO - 21/05/14 14:02:58 INFO StagingCommitter: Task committer attempt_202105141401381595495860982716692_0004_m_000032_320: needsTaskCommit() Task attempt_202105141401381595495860982716692_0004_m_000032_320: duration 0:00.002s
21/05/14 14:02:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381595495860982716692_0004_m_000032_320
[2021-05-14 11:02:58,213] {docker.py:276} INFO - 21/05/14 14:02:58 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 5106 bytes result sent to driver
[2021-05-14 11:02:58,214] {docker.py:276} INFO - 21/05/14 14:02:58 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (3e4cb2948233, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:02:58,215] {docker.py:276} INFO - 21/05/14 14:02:58 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 6077 ms on 3e4cb2948233 (executor driver) (33/200)
[2021-05-14 11:02:58,216] {docker.py:276} INFO - 21/05/14 14:02:58 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-14 11:02:58,225] {docker.py:276} INFO - 21/05/14 14:02:58 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:02:58,237] {docker.py:276} INFO - 21/05/14 14:02:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:02:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:02:58,237] {docker.py:276} INFO - 21/05/14 14:02:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:02:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051414013870793195588379433_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013870793195588379433_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051414013870793195588379433_0004}; taskId=attempt_2021051414013870793195588379433_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@146f7e65}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:02:58,237] {docker.py:276} INFO - 21/05/14 14:02:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:02:58 INFO StagingCommitter: Starting: Task committer attempt_2021051414013870793195588379433_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013870793195588379433_0004_m_000036_324
[2021-05-14 11:02:58,241] {docker.py:276} INFO - 21/05/14 14:02:58 INFO StagingCommitter: Task committer attempt_2021051414013870793195588379433_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013870793195588379433_0004_m_000036_324 : duration 0:00.004s
[2021-05-14 11:03:00,047] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401383208807833759379345_0004_m_000035_323: needsTaskCommit() Task attempt_202105141401383208807833759379345_0004_m_000035_323
[2021-05-14 11:03:00,047] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Task committer attempt_202105141401383208807833759379345_0004_m_000035_323: needsTaskCommit() Task attempt_202105141401383208807833759379345_0004_m_000035_323: duration 0:00.004s
[2021-05-14 11:03:00,048] {docker.py:276} INFO - 21/05/14 14:03:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383208807833759379345_0004_m_000035_323
[2021-05-14 11:03:00,049] {docker.py:276} INFO - 21/05/14 14:03:00 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 5106 bytes result sent to driver
[2021-05-14 11:03:00,050] {docker.py:276} INFO - 21/05/14 14:03:00 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (3e4cb2948233, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:00,051] {docker.py:276} INFO - 21/05/14 14:03:00 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
[2021-05-14 11:03:00,052] {docker.py:276} INFO - 21/05/14 14:03:00 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 6933 ms on 3e4cb2948233 (executor driver) (34/200)
[2021-05-14 11:03:00,062] {docker.py:276} INFO - 21/05/14 14:03:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:00,073] {docker.py:276} INFO - 21/05/14 14:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:03:00,074] {docker.py:276} INFO - 21/05/14 14:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:00,074] {docker.py:276} INFO - 21/05/14 14:03:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388199021346201341750_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388199021346201341750_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388199021346201341750_0004}; taskId=attempt_202105141401388199021346201341750_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16f89eb4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:00,074] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401388199021346201341750_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388199021346201341750_0004_m_000037_325
[2021-05-14 11:03:00,078] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Task committer attempt_202105141401388199021346201341750_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388199021346201341750_0004_m_000037_325 : duration 0:00.004s
[2021-05-14 11:03:00,120] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401387125320804128074726_0004_m_000033_321: needsTaskCommit() Task attempt_202105141401387125320804128074726_0004_m_000033_321
[2021-05-14 11:03:00,120] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Task committer attempt_202105141401387125320804128074726_0004_m_000033_321: needsTaskCommit() Task attempt_202105141401387125320804128074726_0004_m_000033_321: duration 0:00.001s
21/05/14 14:03:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387125320804128074726_0004_m_000033_321
[2021-05-14 11:03:00,121] {docker.py:276} INFO - 21/05/14 14:03:00 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 5106 bytes result sent to driver
[2021-05-14 11:03:00,122] {docker.py:276} INFO - 21/05/14 14:03:00 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (3e4cb2948233, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:00,123] {docker.py:276} INFO - 21/05/14 14:03:00 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
21/05/14 14:03:00 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 7539 ms on 3e4cb2948233 (executor driver) (35/200)
[2021-05-14 11:03:00,144] {docker.py:276} INFO - 21/05/14 14:03:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:00,145] {docker.py:276} INFO - 21/05/14 14:03:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 11:03:00,156] {docker.py:276} INFO - 21/05/14 14:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138352131169943230250_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138352131169943230250_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138352131169943230250_0004}; taskId=attempt_20210514140138352131169943230250_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cd862b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:00,156] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Starting: Task committer attempt_20210514140138352131169943230250_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138352131169943230250_0004_m_000038_326
[2021-05-14 11:03:00,159] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Task committer attempt_20210514140138352131169943230250_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138352131169943230250_0004_m_000038_326 : duration 0:00.004s
[2021-05-14 11:03:00,411] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401381639446679489001967_0004_m_000034_322: needsTaskCommit() Task attempt_202105141401381639446679489001967_0004_m_000034_322
[2021-05-14 11:03:00,412] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Task committer attempt_202105141401381639446679489001967_0004_m_000034_322: needsTaskCommit() Task attempt_202105141401381639446679489001967_0004_m_000034_322: duration 0:00.002s
[2021-05-14 11:03:00,413] {docker.py:276} INFO - 21/05/14 14:03:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381639446679489001967_0004_m_000034_322
[2021-05-14 11:03:00,416] {docker.py:276} INFO - 21/05/14 14:03:00 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 5149 bytes result sent to driver
[2021-05-14 11:03:00,417] {docker.py:276} INFO - 21/05/14 14:03:00 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (3e4cb2948233, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:00,418] {docker.py:276} INFO - 21/05/14 14:03:00 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-14 11:03:00,418] {docker.py:276} INFO - 21/05/14 14:03:00 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 7318 ms on 3e4cb2948233 (executor driver) (36/200)
[2021-05-14 11:03:00,428] {docker.py:276} INFO - 21/05/14 14:03:00 INFO ShuffleBlockFetcherIterator: Getting 5 (40.2 KiB) non-empty blocks including 5 (40.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:00,442] {docker.py:276} INFO - 21/05/14 14:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:00,443] {docker.py:276} INFO - 21/05/14 14:03:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388926770206194323366_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388926770206194323366_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388926770206194323366_0004}; taskId=attempt_202105141401388926770206194323366_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@117383cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401388926770206194323366_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388926770206194323366_0004_m_000039_327
[2021-05-14 11:03:00,448] {docker.py:276} INFO - 21/05/14 14:03:00 INFO StagingCommitter: Task committer attempt_202105141401388926770206194323366_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388926770206194323366_0004_m_000039_327 : duration 0:00.006s
[2021-05-14 11:03:04,500] {docker.py:276} INFO - 21/05/14 14:03:04 INFO StagingCommitter: Starting: Task committer attempt_2021051414013870793195588379433_0004_m_000036_324: needsTaskCommit() Task attempt_2021051414013870793195588379433_0004_m_000036_324
[2021-05-14 11:03:04,501] {docker.py:276} INFO - 21/05/14 14:03:04 INFO StagingCommitter: Task committer attempt_2021051414013870793195588379433_0004_m_000036_324: needsTaskCommit() Task attempt_2021051414013870793195588379433_0004_m_000036_324: duration 0:00.001s
21/05/14 14:03:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051414013870793195588379433_0004_m_000036_324
[2021-05-14 11:03:04,503] {docker.py:276} INFO - 21/05/14 14:03:04 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 5149 bytes result sent to driver
[2021-05-14 11:03:04,504] {docker.py:276} INFO - 21/05/14 14:03:04 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (3e4cb2948233, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:04,505] {docker.py:276} INFO - 21/05/14 14:03:04 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
[2021-05-14 11:03:04,506] {docker.py:276} INFO - 21/05/14 14:03:04 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 6300 ms on 3e4cb2948233 (executor driver) (37/200)
[2021-05-14 11:03:04,514] {docker.py:276} INFO - 21/05/14 14:03:04 INFO ShuffleBlockFetcherIterator: Getting 5 (45.4 KiB) non-empty blocks including 5 (45.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:04,525] {docker.py:276} INFO - 21/05/14 14:03:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401389188882975379018320_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389188882975379018320_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401389188882975379018320_0004}; taskId=attempt_202105141401389188882975379018320_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@440c8deb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:04,526] {docker.py:276} INFO - 21/05/14 14:03:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401389188882975379018320_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389188882975379018320_0004_m_000040_328
[2021-05-14 11:03:04,530] {docker.py:276} INFO - 21/05/14 14:03:04 INFO StagingCommitter: Task committer attempt_202105141401389188882975379018320_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389188882975379018320_0004_m_000040_328 : duration 0:00.005s
[2021-05-14 11:03:06,947] {docker.py:276} INFO - 21/05/14 14:03:06 INFO StagingCommitter: Starting: Task committer attempt_202105141401388199021346201341750_0004_m_000037_325: needsTaskCommit() Task attempt_202105141401388199021346201341750_0004_m_000037_325
[2021-05-14 11:03:06,947] {docker.py:276} INFO - 21/05/14 14:03:06 INFO StagingCommitter: Task committer attempt_202105141401388199021346201341750_0004_m_000037_325: needsTaskCommit() Task attempt_202105141401388199021346201341750_0004_m_000037_325: duration 0:00.001s
21/05/14 14:03:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388199021346201341750_0004_m_000037_325
[2021-05-14 11:03:06,951] {docker.py:276} INFO - 21/05/14 14:03:06 INFO StagingCommitter: Starting: Task committer attempt_20210514140138352131169943230250_0004_m_000038_326: needsTaskCommit() Task attempt_20210514140138352131169943230250_0004_m_000038_326
21/05/14 14:03:06 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 5149 bytes result sent to driver
[2021-05-14 11:03:06,953] {docker.py:276} INFO - 21/05/14 14:03:06 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (3e4cb2948233, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:06,953] {docker.py:276} INFO - 21/05/14 14:03:06 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 6912 ms on 3e4cb2948233 (executor driver) (38/200)
21/05/14 14:03:06 INFO StagingCommitter: Task committer attempt_20210514140138352131169943230250_0004_m_000038_326: needsTaskCommit() Task attempt_20210514140138352131169943230250_0004_m_000038_326: duration 0:00.002s
21/05/14 14:03:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138352131169943230250_0004_m_000038_326
[2021-05-14 11:03:06,954] {docker.py:276} INFO - 21/05/14 14:03:06 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-14 11:03:06,955] {docker.py:276} INFO - 21/05/14 14:03:06 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 5149 bytes result sent to driver
[2021-05-14 11:03:06,956] {docker.py:276} INFO - 21/05/14 14:03:06 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (3e4cb2948233, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:06,957] {docker.py:276} INFO - 21/05/14 14:03:06 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
[2021-05-14 11:03:06,958] {docker.py:276} INFO - 21/05/14 14:03:06 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 6844 ms on 3e4cb2948233 (executor driver) (39/200)
[2021-05-14 11:03:06,965] {docker.py:276} INFO - 21/05/14 14:03:06 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:06,966] {docker.py:276} INFO - 21/05/14 14:03:06 INFO StagingCommitter: Starting: Task committer attempt_202105141401388926770206194323366_0004_m_000039_327: needsTaskCommit() Task attempt_202105141401388926770206194323366_0004_m_000039_327
[2021-05-14 11:03:06,966] {docker.py:276} INFO - 21/05/14 14:03:06 INFO StagingCommitter: Task committer attempt_202105141401388926770206194323366_0004_m_000039_327: needsTaskCommit() Task attempt_202105141401388926770206194323366_0004_m_000039_327: duration 0:00.000s
21/05/14 14:03:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388926770206194323366_0004_m_000039_327
[2021-05-14 11:03:06,967] {docker.py:276} INFO - 21/05/14 14:03:06 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 5106 bytes result sent to driver
[2021-05-14 11:03:06,968] {docker.py:276} INFO - 21/05/14 14:03:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
21/05/14 14:03:06 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (3e4cb2948233, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:06,969] {docker.py:276} INFO - 21/05/14 14:03:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
21/05/14 14:03:06 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 6561 ms on 3e4cb2948233 (executor driver) (40/200)
[2021-05-14 11:03:06,969] {docker.py:276} INFO - 21/05/14 14:03:06 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
[2021-05-14 11:03:06,977] {docker.py:276} INFO - 21/05/14 14:03:07 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:06,984] {docker.py:276} INFO - 21/05/14 14:03:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:06,984] {docker.py:276} INFO - 21/05/14 14:03:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:06,986] {docker.py:276} INFO - 21/05/14 14:03:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385730383443358233995_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385730383443358233995_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385730383443358233995_0004}; taskId=attempt_202105141401385730383443358233995_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31899474}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:06,987] {docker.py:276} INFO - 21/05/14 14:03:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401385730383443358233995_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385730383443358233995_0004_m_000041_329
[2021-05-14 11:03:06,990] {docker.py:276} INFO - 21/05/14 14:03:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:06,990] {docker.py:276} INFO - 21/05/14 14:03:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381373074453953168273_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381373074453953168273_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381373074453953168273_0004}; taskId=attempt_202105141401381373074453953168273_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fbe8998}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:06,991] {docker.py:276} INFO - 21/05/14 14:03:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401381373074453953168273_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381373074453953168273_0004_m_000042_330
[2021-05-14 11:03:06,999] {docker.py:276} INFO - 21/05/14 14:03:07 INFO StagingCommitter: Task committer attempt_202105141401381373074453953168273_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381373074453953168273_0004_m_000042_330 : duration 0:00.009s
[2021-05-14 11:03:07,001] {docker.py:276} INFO - 21/05/14 14:03:07 INFO StagingCommitter: Task committer attempt_202105141401385730383443358233995_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385730383443358233995_0004_m_000041_329 : duration 0:00.014s
[2021-05-14 11:03:07,006] {docker.py:276} INFO - 21/05/14 14:03:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:07,006] {docker.py:276} INFO - 21/05/14 14:03:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138765246162472627205_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138765246162472627205_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138765246162472627205_0004}; taskId=attempt_20210514140138765246162472627205_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@394bc3a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:07,007] {docker.py:276} INFO - 21/05/14 14:03:07 INFO StagingCommitter: Starting: Task committer attempt_20210514140138765246162472627205_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138765246162472627205_0004_m_000043_331
[2021-05-14 11:03:07,010] {docker.py:276} INFO - 21/05/14 14:03:07 INFO StagingCommitter: Task committer attempt_20210514140138765246162472627205_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138765246162472627205_0004_m_000043_331 : duration 0:00.005s
[2021-05-14 11:03:11,743] {docker.py:276} INFO - 21/05/14 14:03:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401389188882975379018320_0004_m_000040_328: needsTaskCommit() Task attempt_202105141401389188882975379018320_0004_m_000040_328
[2021-05-14 11:03:11,745] {docker.py:276} INFO - 21/05/14 14:03:11 INFO StagingCommitter: Task committer attempt_202105141401389188882975379018320_0004_m_000040_328: needsTaskCommit() Task attempt_202105141401389188882975379018320_0004_m_000040_328: duration 0:00.003s
21/05/14 14:03:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401389188882975379018320_0004_m_000040_328
[2021-05-14 11:03:11,747] {docker.py:276} INFO - 21/05/14 14:03:11 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 5106 bytes result sent to driver
[2021-05-14 11:03:11,749] {docker.py:276} INFO - 21/05/14 14:03:11 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (3e4cb2948233, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:11,750] {docker.py:276} INFO - 21/05/14 14:03:11 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-14 11:03:11,751] {docker.py:276} INFO - 21/05/14 14:03:11 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 7220 ms on 3e4cb2948233 (executor driver) (41/200)
[2021-05-14 11:03:11,761] {docker.py:276} INFO - 21/05/14 14:03:11 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:11,776] {docker.py:276} INFO - 21/05/14 14:03:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:11,777] {docker.py:276} INFO - 21/05/14 14:03:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388766860216310798868_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388766860216310798868_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388766860216310798868_0004}; taskId=attempt_202105141401388766860216310798868_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@184f3f8d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:11,777] {docker.py:276} INFO - 21/05/14 14:03:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401388766860216310798868_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388766860216310798868_0004_m_000044_332
[2021-05-14 11:03:11,783] {docker.py:276} INFO - 21/05/14 14:03:11 INFO StagingCommitter: Task committer attempt_202105141401388766860216310798868_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388766860216310798868_0004_m_000044_332 : duration 0:00.006s
[2021-05-14 11:03:13,772] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Starting: Task committer attempt_202105141401381373074453953168273_0004_m_000042_330: needsTaskCommit() Task attempt_202105141401381373074453953168273_0004_m_000042_330
[2021-05-14 11:03:13,773] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Task committer attempt_202105141401381373074453953168273_0004_m_000042_330: needsTaskCommit() Task attempt_202105141401381373074453953168273_0004_m_000042_330: duration 0:00.003s
21/05/14 14:03:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381373074453953168273_0004_m_000042_330
[2021-05-14 11:03:13,776] {docker.py:276} INFO - 21/05/14 14:03:13 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 5106 bytes result sent to driver
[2021-05-14 11:03:13,778] {docker.py:276} INFO - 21/05/14 14:03:13 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (3e4cb2948233, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:13,778] {docker.py:276} INFO - 21/05/14 14:03:13 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-14 11:03:13,779] {docker.py:276} INFO - 21/05/14 14:03:13 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 6795 ms on 3e4cb2948233 (executor driver) (42/200)
[2021-05-14 11:03:13,788] {docker.py:276} INFO - 21/05/14 14:03:13 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:13,800] {docker.py:276} INFO - 21/05/14 14:03:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385775227911227767053_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385775227911227767053_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385775227911227767053_0004}; taskId=attempt_202105141401385775227911227767053_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6143f7eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:13,800] {docker.py:276} INFO - 21/05/14 14:03:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:13,800] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Starting: Task committer attempt_202105141401385775227911227767053_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385775227911227767053_0004_m_000045_333
[2021-05-14 11:03:13,803] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Task committer attempt_202105141401385775227911227767053_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385775227911227767053_0004_m_000045_333 : duration 0:00.004s
[2021-05-14 11:03:13,964] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Starting: Task committer attempt_20210514140138765246162472627205_0004_m_000043_331: needsTaskCommit() Task attempt_20210514140138765246162472627205_0004_m_000043_331
[2021-05-14 11:03:13,965] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Task committer attempt_20210514140138765246162472627205_0004_m_000043_331: needsTaskCommit() Task attempt_20210514140138765246162472627205_0004_m_000043_331: duration 0:00.003s
21/05/14 14:03:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138765246162472627205_0004_m_000043_331
[2021-05-14 11:03:13,969] {docker.py:276} INFO - 21/05/14 14:03:13 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 5106 bytes result sent to driver
[2021-05-14 11:03:13,970] {docker.py:276} INFO - 21/05/14 14:03:13 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (3e4cb2948233, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:13,971] {docker.py:276} INFO - 21/05/14 14:03:13 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 6976 ms on 3e4cb2948233 (executor driver) (43/200)
[2021-05-14 11:03:13,972] {docker.py:276} INFO - 21/05/14 14:03:13 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-14 11:03:13,982] {docker.py:276} INFO - 21/05/14 14:03:13 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:13,992] {docker.py:276} INFO - 21/05/14 14:03:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387400967670294416775_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387400967670294416775_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387400967670294416775_0004}; taskId=attempt_202105141401387400967670294416775_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55df17a2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:13,992] {docker.py:276} INFO - 21/05/14 14:03:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:13 INFO StagingCommitter: Starting: Task committer attempt_202105141401387400967670294416775_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387400967670294416775_0004_m_000046_334
[2021-05-14 11:03:13,996] {docker.py:276} INFO - 21/05/14 14:03:13 INFO StagingCommitter: Task committer attempt_202105141401387400967670294416775_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387400967670294416775_0004_m_000046_334 : duration 0:00.004s
[2021-05-14 11:03:14,134] {docker.py:276} INFO - 21/05/14 14:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401385730383443358233995_0004_m_000041_329: needsTaskCommit() Task attempt_202105141401385730383443358233995_0004_m_000041_329
[2021-05-14 11:03:14,135] {docker.py:276} INFO - 21/05/14 14:03:14 INFO StagingCommitter: Task committer attempt_202105141401385730383443358233995_0004_m_000041_329: needsTaskCommit() Task attempt_202105141401385730383443358233995_0004_m_000041_329: duration 0:00.004s
21/05/14 14:03:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385730383443358233995_0004_m_000041_329
[2021-05-14 11:03:14,139] {docker.py:276} INFO - 21/05/14 14:03:14 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 5106 bytes result sent to driver
[2021-05-14 11:03:14,140] {docker.py:276} INFO - 21/05/14 14:03:14 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (3e4cb2948233, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:14,141] {docker.py:276} INFO - 21/05/14 14:03:14 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 7161 ms on 3e4cb2948233 (executor driver) (44/200)
[2021-05-14 11:03:14,142] {docker.py:276} INFO - 21/05/14 14:03:14 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-14 11:03:14,151] {docker.py:276} INFO - 21/05/14 14:03:14 INFO ShuffleBlockFetcherIterator: Getting 5 (41.5 KiB) non-empty blocks including 5 (41.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:14,162] {docker.py:276} INFO - 21/05/14 14:03:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:14,162] {docker.py:276} INFO - 21/05/14 14:03:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381407027245648995976_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381407027245648995976_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381407027245648995976_0004}; taskId=attempt_202105141401381407027245648995976_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d608d99}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:14,163] {docker.py:276} INFO - 21/05/14 14:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401381407027245648995976_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381407027245648995976_0004_m_000047_335
[2021-05-14 11:03:14,167] {docker.py:276} INFO - 21/05/14 14:03:14 INFO StagingCommitter: Task committer attempt_202105141401381407027245648995976_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381407027245648995976_0004_m_000047_335 : duration 0:00.005s
[2021-05-14 11:03:18,505] {docker.py:276} INFO - 21/05/14 14:03:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401388766860216310798868_0004_m_000044_332: needsTaskCommit() Task attempt_202105141401388766860216310798868_0004_m_000044_332
[2021-05-14 11:03:18,506] {docker.py:276} INFO - 21/05/14 14:03:18 INFO StagingCommitter: Task committer attempt_202105141401388766860216310798868_0004_m_000044_332: needsTaskCommit() Task attempt_202105141401388766860216310798868_0004_m_000044_332: duration 0:00.002s
21/05/14 14:03:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388766860216310798868_0004_m_000044_332
[2021-05-14 11:03:18,507] {docker.py:276} INFO - 21/05/14 14:03:18 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 5149 bytes result sent to driver
[2021-05-14 11:03:18,508] {docker.py:276} INFO - 21/05/14 14:03:18 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (3e4cb2948233, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:18,509] {docker.py:276} INFO - 21/05/14 14:03:18 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 6769 ms on 3e4cb2948233 (executor driver) (45/200)
[2021-05-14 11:03:18,510] {docker.py:276} INFO - 21/05/14 14:03:18 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
[2021-05-14 11:03:18,520] {docker.py:276} INFO - 21/05/14 14:03:18 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:18,529] {docker.py:276} INFO - 21/05/14 14:03:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:18,529] {docker.py:276} INFO - 21/05/14 14:03:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:18,529] {docker.py:276} INFO - 21/05/14 14:03:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388737095170240303760_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388737095170240303760_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388737095170240303760_0004}; taskId=attempt_202105141401388737095170240303760_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1dce8e0a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:18,530] {docker.py:276} INFO - 21/05/14 14:03:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:18,530] {docker.py:276} INFO - 21/05/14 14:03:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401388737095170240303760_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388737095170240303760_0004_m_000048_336
[2021-05-14 11:03:18,535] {docker.py:276} INFO - 21/05/14 14:03:18 INFO StagingCommitter: Task committer attempt_202105141401388737095170240303760_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388737095170240303760_0004_m_000048_336 : duration 0:00.006s
[2021-05-14 11:03:20,754] {docker.py:276} INFO - 21/05/14 14:03:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401387400967670294416775_0004_m_000046_334: needsTaskCommit() Task attempt_202105141401387400967670294416775_0004_m_000046_334
[2021-05-14 11:03:20,754] {docker.py:276} INFO - 21/05/14 14:03:20 INFO StagingCommitter: Task committer attempt_202105141401387400967670294416775_0004_m_000046_334: needsTaskCommit() Task attempt_202105141401387400967670294416775_0004_m_000046_334: duration 0:00.003s
21/05/14 14:03:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387400967670294416775_0004_m_000046_334
[2021-05-14 11:03:20,757] {docker.py:276} INFO - 21/05/14 14:03:20 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 5149 bytes result sent to driver
[2021-05-14 11:03:20,758] {docker.py:276} INFO - 21/05/14 14:03:20 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (3e4cb2948233, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:20,759] {docker.py:276} INFO - 21/05/14 14:03:20 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 6797 ms on 3e4cb2948233 (executor driver) (46/200)
[2021-05-14 11:03:20,760] {docker.py:276} INFO - 21/05/14 14:03:20 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-14 11:03:20,769] {docker.py:276} INFO - 21/05/14 14:03:20 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:20,769] {docker.py:276} INFO - 21/05/14 14:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:20,778] {docker.py:276} INFO - 21/05/14 14:03:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:20,779] {docker.py:276} INFO - 21/05/14 14:03:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388844099194551390835_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388844099194551390835_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388844099194551390835_0004}; taskId=attempt_202105141401388844099194551390835_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a164c12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:20,779] {docker.py:276} INFO - 21/05/14 14:03:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401388844099194551390835_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388844099194551390835_0004_m_000049_337
[2021-05-14 11:03:20,783] {docker.py:276} INFO - 21/05/14 14:03:20 INFO StagingCommitter: Task committer attempt_202105141401388844099194551390835_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388844099194551390835_0004_m_000049_337 : duration 0:00.005s
[2021-05-14 11:03:20,946] {docker.py:276} INFO - 21/05/14 14:03:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401385775227911227767053_0004_m_000045_333: needsTaskCommit() Task attempt_202105141401385775227911227767053_0004_m_000045_333
[2021-05-14 11:03:20,947] {docker.py:276} INFO - 21/05/14 14:03:20 INFO StagingCommitter: Task committer attempt_202105141401385775227911227767053_0004_m_000045_333: needsTaskCommit() Task attempt_202105141401385775227911227767053_0004_m_000045_333: duration 0:00.003s
21/05/14 14:03:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385775227911227767053_0004_m_000045_333
[2021-05-14 11:03:20,950] {docker.py:276} INFO - 21/05/14 14:03:20 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 5149 bytes result sent to driver
[2021-05-14 11:03:20,952] {docker.py:276} INFO - 21/05/14 14:03:20 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (3e4cb2948233, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:20,953] {docker.py:276} INFO - 21/05/14 14:03:20 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 7183 ms on 3e4cb2948233 (executor driver) (47/200)
[2021-05-14 11:03:20,953] {docker.py:276} INFO - 21/05/14 14:03:20 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-14 11:03:20,964] {docker.py:276} INFO - 21/05/14 14:03:20 INFO ShuffleBlockFetcherIterator: Getting 5 (40.3 KiB) non-empty blocks including 5 (40.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:20,973] {docker.py:276} INFO - 21/05/14 14:03:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:20,973] {docker.py:276} INFO - 21/05/14 14:03:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387450617150028827651_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387450617150028827651_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387450617150028827651_0004}; taskId=attempt_202105141401387450617150028827651_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70620af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401387450617150028827651_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387450617150028827651_0004_m_000050_338
[2021-05-14 11:03:20,977] {docker.py:276} INFO - 21/05/14 14:03:20 INFO StagingCommitter: Task committer attempt_202105141401387450617150028827651_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387450617150028827651_0004_m_000050_338 : duration 0:00.004s
[2021-05-14 11:03:21,120] {docker.py:276} INFO - 21/05/14 14:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401381407027245648995976_0004_m_000047_335: needsTaskCommit() Task attempt_202105141401381407027245648995976_0004_m_000047_335
[2021-05-14 11:03:21,121] {docker.py:276} INFO - 21/05/14 14:03:21 INFO StagingCommitter: Task committer attempt_202105141401381407027245648995976_0004_m_000047_335: needsTaskCommit() Task attempt_202105141401381407027245648995976_0004_m_000047_335: duration 0:00.004s
21/05/14 14:03:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381407027245648995976_0004_m_000047_335
[2021-05-14 11:03:21,124] {docker.py:276} INFO - 21/05/14 14:03:21 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 5149 bytes result sent to driver
[2021-05-14 11:03:21,125] {docker.py:276} INFO - 21/05/14 14:03:21 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (3e4cb2948233, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:21,126] {docker.py:276} INFO - 21/05/14 14:03:21 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-14 11:03:21,127] {docker.py:276} INFO - 21/05/14 14:03:21 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 6995 ms on 3e4cb2948233 (executor driver) (48/200)
[2021-05-14 11:03:21,136] {docker.py:276} INFO - 21/05/14 14:03:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:21,144] {docker.py:276} INFO - 21/05/14 14:03:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:21,145] {docker.py:276} INFO - 21/05/14 14:03:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387101063615400628559_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387101063615400628559_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387101063615400628559_0004}; taskId=attempt_202105141401387101063615400628559_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63746d8d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401387101063615400628559_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387101063615400628559_0004_m_000051_339
[2021-05-14 11:03:21,150] {docker.py:276} INFO - 21/05/14 14:03:21 INFO StagingCommitter: Task committer attempt_202105141401387101063615400628559_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387101063615400628559_0004_m_000051_339 : duration 0:00.005s
[2021-05-14 11:03:25,537] {docker.py:276} INFO - 21/05/14 14:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401388737095170240303760_0004_m_000048_336: needsTaskCommit() Task attempt_202105141401388737095170240303760_0004_m_000048_336
[2021-05-14 11:03:25,538] {docker.py:276} INFO - 21/05/14 14:03:25 INFO StagingCommitter: Task committer attempt_202105141401388737095170240303760_0004_m_000048_336: needsTaskCommit() Task attempt_202105141401388737095170240303760_0004_m_000048_336: duration 0:00.001s
21/05/14 14:03:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388737095170240303760_0004_m_000048_336
[2021-05-14 11:03:25,540] {docker.py:276} INFO - 21/05/14 14:03:25 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 5106 bytes result sent to driver
[2021-05-14 11:03:25,541] {docker.py:276} INFO - 21/05/14 14:03:25 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (3e4cb2948233, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:25,542] {docker.py:276} INFO - 21/05/14 14:03:25 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
21/05/14 14:03:25 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 7042 ms on 3e4cb2948233 (executor driver) (49/200)
[2021-05-14 11:03:25,550] {docker.py:276} INFO - 21/05/14 14:03:25 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:25,550] {docker.py:276} INFO - 21/05/14 14:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:25,559] {docker.py:276} INFO - 21/05/14 14:03:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:25,560] {docker.py:276} INFO - 21/05/14 14:03:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386878672920719963423_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386878672920719963423_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386878672920719963423_0004}; taskId=attempt_202105141401386878672920719963423_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e46007e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:25,560] {docker.py:276} INFO - 21/05/14 14:03:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:25,560] {docker.py:276} INFO - 21/05/14 14:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401386878672920719963423_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386878672920719963423_0004_m_000052_340
[2021-05-14 11:03:25,564] {docker.py:276} INFO - 21/05/14 14:03:25 INFO StagingCommitter: Task committer attempt_202105141401386878672920719963423_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386878672920719963423_0004_m_000052_340 : duration 0:00.004s
[2021-05-14 11:03:27,647] {docker.py:276} INFO - 21/05/14 14:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105141401388844099194551390835_0004_m_000049_337: needsTaskCommit() Task attempt_202105141401388844099194551390835_0004_m_000049_337
[2021-05-14 11:03:27,647] {docker.py:276} INFO - 21/05/14 14:03:27 INFO StagingCommitter: Task committer attempt_202105141401388844099194551390835_0004_m_000049_337: needsTaskCommit() Task attempt_202105141401388844099194551390835_0004_m_000049_337: duration 0:00.002s
21/05/14 14:03:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388844099194551390835_0004_m_000049_337
[2021-05-14 11:03:27,649] {docker.py:276} INFO - 21/05/14 14:03:27 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 5106 bytes result sent to driver
[2021-05-14 11:03:27,650] {docker.py:276} INFO - 21/05/14 14:03:27 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (3e4cb2948233, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:27,651] {docker.py:276} INFO - 21/05/14 14:03:27 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 6902 ms on 3e4cb2948233 (executor driver) (50/200)
[2021-05-14 11:03:27,652] {docker.py:276} INFO - 21/05/14 14:03:27 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-14 11:03:27,661] {docker.py:276} INFO - 21/05/14 14:03:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:03:27,672] {docker.py:276} INFO - 21/05/14 14:03:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384914804750300597722_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384914804750300597722_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384914804750300597722_0004}; taskId=attempt_202105141401384914804750300597722_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@119a563}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105141401384914804750300597722_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384914804750300597722_0004_m_000053_341
[2021-05-14 11:03:27,676] {docker.py:276} INFO - 21/05/14 14:03:27 INFO StagingCommitter: Task committer attempt_202105141401384914804750300597722_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384914804750300597722_0004_m_000053_341 : duration 0:00.004s
[2021-05-14 11:03:27,936] {docker.py:276} INFO - 21/05/14 14:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105141401387450617150028827651_0004_m_000050_338: needsTaskCommit() Task attempt_202105141401387450617150028827651_0004_m_000050_338
21/05/14 14:03:27 INFO StagingCommitter: Task committer attempt_202105141401387450617150028827651_0004_m_000050_338: needsTaskCommit() Task attempt_202105141401387450617150028827651_0004_m_000050_338: duration 0:00.002s
21/05/14 14:03:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387450617150028827651_0004_m_000050_338
[2021-05-14 11:03:27,937] {docker.py:276} INFO - 21/05/14 14:03:27 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 5106 bytes result sent to driver
[2021-05-14 11:03:27,939] {docker.py:276} INFO - 21/05/14 14:03:27 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (3e4cb2948233, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:27,940] {docker.py:276} INFO - 21/05/14 14:03:27 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 6997 ms on 3e4cb2948233 (executor driver) (51/200)
21/05/14 14:03:27 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-14 11:03:27,950] {docker.py:276} INFO - 21/05/14 14:03:27 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:27,960] {docker.py:276} INFO - 21/05/14 14:03:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138709401578784844897_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138709401578784844897_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138709401578784844897_0004}; taskId=attempt_20210514140138709401578784844897_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d26d9bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:27 INFO StagingCommitter: Starting: Task committer attempt_20210514140138709401578784844897_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138709401578784844897_0004_m_000054_342
[2021-05-14 11:03:27,964] {docker.py:276} INFO - 21/05/14 14:03:27 INFO StagingCommitter: Task committer attempt_20210514140138709401578784844897_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138709401578784844897_0004_m_000054_342 : duration 0:00.005s
[2021-05-14 11:03:28,165] {docker.py:276} INFO - 21/05/14 14:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401387101063615400628559_0004_m_000051_339: needsTaskCommit() Task attempt_202105141401387101063615400628559_0004_m_000051_339
[2021-05-14 11:03:28,166] {docker.py:276} INFO - 21/05/14 14:03:28 INFO StagingCommitter: Task committer attempt_202105141401387101063615400628559_0004_m_000051_339: needsTaskCommit() Task attempt_202105141401387101063615400628559_0004_m_000051_339: duration 0:00.001s
21/05/14 14:03:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387101063615400628559_0004_m_000051_339
[2021-05-14 11:03:28,166] {docker.py:276} INFO - 21/05/14 14:03:28 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 5106 bytes result sent to driver
[2021-05-14 11:03:28,167] {docker.py:276} INFO - 21/05/14 14:03:28 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (3e4cb2948233, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:28,168] {docker.py:276} INFO - 21/05/14 14:03:28 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 7053 ms on 3e4cb2948233 (executor driver) (52/200)
[2021-05-14 11:03:28,169] {docker.py:276} INFO - 21/05/14 14:03:28 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-14 11:03:28,177] {docker.py:276} INFO - 21/05/14 14:03:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:28,189] {docker.py:276} INFO - 21/05/14 14:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:28,189] {docker.py:276} INFO - 21/05/14 14:03:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384158731270549202242_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384158731270549202242_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384158731270549202242_0004}; taskId=attempt_202105141401384158731270549202242_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b4a090e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:28,190] {docker.py:276} INFO - 21/05/14 14:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401384158731270549202242_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384158731270549202242_0004_m_000055_343
[2021-05-14 11:03:28,195] {docker.py:276} INFO - 21/05/14 14:03:28 INFO StagingCommitter: Task committer attempt_202105141401384158731270549202242_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384158731270549202242_0004_m_000055_343 : duration 0:00.005s
[2021-05-14 11:03:32,308] {docker.py:276} INFO - 21/05/14 14:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401386878672920719963423_0004_m_000052_340: needsTaskCommit() Task attempt_202105141401386878672920719963423_0004_m_000052_340
21/05/14 14:03:32 INFO StagingCommitter: Task committer attempt_202105141401386878672920719963423_0004_m_000052_340: needsTaskCommit() Task attempt_202105141401386878672920719963423_0004_m_000052_340: duration 0:00.003s
21/05/14 14:03:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386878672920719963423_0004_m_000052_340
[2021-05-14 11:03:32,311] {docker.py:276} INFO - 21/05/14 14:03:32 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 5106 bytes result sent to driver
[2021-05-14 11:03:32,312] {docker.py:276} INFO - 21/05/14 14:03:32 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (3e4cb2948233, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:32,313] {docker.py:276} INFO - 21/05/14 14:03:32 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
[2021-05-14 11:03:32,314] {docker.py:276} INFO - 21/05/14 14:03:32 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 6780 ms on 3e4cb2948233 (executor driver) (53/200)
[2021-05-14 11:03:32,325] {docker.py:276} INFO - 21/05/14 14:03:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:32,347] {docker.py:276} INFO - 21/05/14 14:03:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386227315472467521543_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386227315472467521543_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386227315472467521543_0004}; taskId=attempt_202105141401386227315472467521543_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37504b81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401386227315472467521543_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386227315472467521543_0004_m_000056_344
[2021-05-14 11:03:32,350] {docker.py:276} INFO - 21/05/14 14:03:32 INFO StagingCommitter: Task committer attempt_202105141401386227315472467521543_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386227315472467521543_0004_m_000056_344 : duration 0:00.003s
[2021-05-14 11:03:34,788] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401384914804750300597722_0004_m_000053_341: needsTaskCommit() Task attempt_202105141401384914804750300597722_0004_m_000053_341
[2021-05-14 11:03:34,790] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Task committer attempt_202105141401384914804750300597722_0004_m_000053_341: needsTaskCommit() Task attempt_202105141401384914804750300597722_0004_m_000053_341: duration 0:00.003s
[2021-05-14 11:03:34,790] {docker.py:276} INFO - 21/05/14 14:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384914804750300597722_0004_m_000053_341
[2021-05-14 11:03:34,793] {docker.py:276} INFO - 21/05/14 14:03:34 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 5149 bytes result sent to driver
[2021-05-14 11:03:34,794] {docker.py:276} INFO - 21/05/14 14:03:34 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (3e4cb2948233, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:34,796] {docker.py:276} INFO - 21/05/14 14:03:34 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
[2021-05-14 11:03:34,796] {docker.py:276} INFO - 21/05/14 14:03:34 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 7154 ms on 3e4cb2948233 (executor driver) (54/200)
[2021-05-14 11:03:34,806] {docker.py:276} INFO - 21/05/14 14:03:34 INFO ShuffleBlockFetcherIterator: Getting 5 (44.1 KiB) non-empty blocks including 5 (44.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:34,806] {docker.py:276} INFO - 21/05/14 14:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:34,814] {docker.py:276} INFO - 21/05/14 14:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:34,815] {docker.py:276} INFO - 21/05/14 14:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:34,815] {docker.py:276} INFO - 21/05/14 14:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385030532893075941074_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385030532893075941074_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385030532893075941074_0004}; taskId=attempt_202105141401385030532893075941074_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66dec096}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:34,816] {docker.py:276} INFO - 21/05/14 14:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:34,816] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401385030532893075941074_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385030532893075941074_0004_m_000057_345
[2021-05-14 11:03:34,820] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Task committer attempt_202105141401385030532893075941074_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385030532893075941074_0004_m_000057_345 : duration 0:00.004s
[2021-05-14 11:03:34,856] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Starting: Task committer attempt_20210514140138709401578784844897_0004_m_000054_342: needsTaskCommit() Task attempt_20210514140138709401578784844897_0004_m_000054_342
[2021-05-14 11:03:34,856] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Task committer attempt_20210514140138709401578784844897_0004_m_000054_342: needsTaskCommit() Task attempt_20210514140138709401578784844897_0004_m_000054_342: duration 0:00.003s
21/05/14 14:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138709401578784844897_0004_m_000054_342
[2021-05-14 11:03:34,859] {docker.py:276} INFO - 21/05/14 14:03:34 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 5149 bytes result sent to driver
[2021-05-14 11:03:34,861] {docker.py:276} INFO - 21/05/14 14:03:34 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (3e4cb2948233, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:34,862] {docker.py:276} INFO - 21/05/14 14:03:34 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 6931 ms on 3e4cb2948233 (executor driver) (55/200)
[2021-05-14 11:03:34,862] {docker.py:276} INFO - 21/05/14 14:03:34 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-14 11:03:34,872] {docker.py:276} INFO - 21/05/14 14:03:34 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:34,881] {docker.py:276} INFO - 21/05/14 14:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:03:34,882] {docker.py:276} INFO - 21/05/14 14:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:34,882] {docker.py:276} INFO - 21/05/14 14:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384585587841609092865_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384585587841609092865_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384585587841609092865_0004}; taskId=attempt_202105141401384585587841609092865_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78ade3a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:34,882] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401384585587841609092865_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384585587841609092865_0004_m_000058_346
[2021-05-14 11:03:34,886] {docker.py:276} INFO - 21/05/14 14:03:34 INFO StagingCommitter: Task committer attempt_202105141401384585587841609092865_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384585587841609092865_0004_m_000058_346 : duration 0:00.005s
[2021-05-14 11:03:35,135] {docker.py:276} INFO - 21/05/14 14:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401384158731270549202242_0004_m_000055_343: needsTaskCommit() Task attempt_202105141401384158731270549202242_0004_m_000055_343
[2021-05-14 11:03:35,136] {docker.py:276} INFO - 21/05/14 14:03:35 INFO StagingCommitter: Task committer attempt_202105141401384158731270549202242_0004_m_000055_343: needsTaskCommit() Task attempt_202105141401384158731270549202242_0004_m_000055_343: duration 0:00.003s
21/05/14 14:03:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384158731270549202242_0004_m_000055_343
[2021-05-14 11:03:35,138] {docker.py:276} INFO - 21/05/14 14:03:35 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 5149 bytes result sent to driver
[2021-05-14 11:03:35,140] {docker.py:276} INFO - 21/05/14 14:03:35 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (3e4cb2948233, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:35,141] {docker.py:276} INFO - 21/05/14 14:03:35 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 6982 ms on 3e4cb2948233 (executor driver) (56/200)
21/05/14 14:03:35 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-14 11:03:35,152] {docker.py:276} INFO - 21/05/14 14:03:35 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:35,161] {docker.py:276} INFO - 21/05/14 14:03:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381886208010574193501_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381886208010574193501_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381886208010574193501_0004}; taskId=attempt_202105141401381886208010574193501_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@129c08f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:35,161] {docker.py:276} INFO - 21/05/14 14:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401381886208010574193501_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381886208010574193501_0004_m_000059_347
[2021-05-14 11:03:35,165] {docker.py:276} INFO - 21/05/14 14:03:35 INFO StagingCommitter: Task committer attempt_202105141401381886208010574193501_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381886208010574193501_0004_m_000059_347 : duration 0:00.004s
[2021-05-14 11:03:39,455] {docker.py:276} INFO - 21/05/14 14:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401386227315472467521543_0004_m_000056_344: needsTaskCommit() Task attempt_202105141401386227315472467521543_0004_m_000056_344
[2021-05-14 11:03:39,455] {docker.py:276} INFO - 21/05/14 14:03:39 INFO StagingCommitter: Task committer attempt_202105141401386227315472467521543_0004_m_000056_344: needsTaskCommit() Task attempt_202105141401386227315472467521543_0004_m_000056_344: duration 0:00.003s
21/05/14 14:03:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386227315472467521543_0004_m_000056_344
[2021-05-14 11:03:39,457] {docker.py:276} INFO - 21/05/14 14:03:39 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 5149 bytes result sent to driver
[2021-05-14 11:03:39,458] {docker.py:276} INFO - 21/05/14 14:03:39 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (3e4cb2948233, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:39,459] {docker.py:276} INFO - 21/05/14 14:03:39 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
21/05/14 14:03:39 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 7156 ms on 3e4cb2948233 (executor driver) (57/200)
[2021-05-14 11:03:39,469] {docker.py:276} INFO - 21/05/14 14:03:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:39,478] {docker.py:276} INFO - 21/05/14 14:03:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381130801289584148873_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381130801289584148873_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381130801289584148873_0004}; taskId=attempt_202105141401381130801289584148873_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d678fdd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401381130801289584148873_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381130801289584148873_0004_m_000060_348
[2021-05-14 11:03:39,482] {docker.py:276} INFO - 21/05/14 14:03:39 INFO StagingCommitter: Task committer attempt_202105141401381130801289584148873_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381130801289584148873_0004_m_000060_348 : duration 0:00.004s
[2021-05-14 11:03:41,888] {docker.py:276} INFO - 21/05/14 14:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401385030532893075941074_0004_m_000057_345: needsTaskCommit() Task attempt_202105141401385030532893075941074_0004_m_000057_345
[2021-05-14 11:03:41,889] {docker.py:276} INFO - 21/05/14 14:03:41 INFO StagingCommitter: Task committer attempt_202105141401385030532893075941074_0004_m_000057_345: needsTaskCommit() Task attempt_202105141401385030532893075941074_0004_m_000057_345: duration 0:00.002s
[2021-05-14 11:03:41,890] {docker.py:276} INFO - 21/05/14 14:03:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385030532893075941074_0004_m_000057_345
[2021-05-14 11:03:41,892] {docker.py:276} INFO - 21/05/14 14:03:41 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 5106 bytes result sent to driver
[2021-05-14 11:03:41,894] {docker.py:276} INFO - 21/05/14 14:03:41 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (3e4cb2948233, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:41,894] {docker.py:276} INFO - 21/05/14 14:03:41 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 7074 ms on 3e4cb2948233 (executor driver) (58/200)
[2021-05-14 11:03:41,896] {docker.py:276} INFO - 21/05/14 14:03:41 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-14 11:03:41,905] {docker.py:276} INFO - 21/05/14 14:03:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:41,914] {docker.py:276} INFO - 21/05/14 14:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:41,915] {docker.py:276} INFO - 21/05/14 14:03:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384665365600281189792_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384665365600281189792_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384665365600281189792_0004}; taskId=attempt_202105141401384665365600281189792_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52784506}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:41,915] {docker.py:276} INFO - 21/05/14 14:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401384665365600281189792_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384665365600281189792_0004_m_000061_349
[2021-05-14 11:03:41,920] {docker.py:276} INFO - 21/05/14 14:03:41 INFO StagingCommitter: Task committer attempt_202105141401384665365600281189792_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384665365600281189792_0004_m_000061_349 : duration 0:00.005s
[2021-05-14 11:03:41,930] {docker.py:276} INFO - 21/05/14 14:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401384585587841609092865_0004_m_000058_346: needsTaskCommit() Task attempt_202105141401384585587841609092865_0004_m_000058_346
[2021-05-14 11:03:41,931] {docker.py:276} INFO - 21/05/14 14:03:41 INFO StagingCommitter: Task committer attempt_202105141401384585587841609092865_0004_m_000058_346: needsTaskCommit() Task attempt_202105141401384585587841609092865_0004_m_000058_346: duration 0:00.001s
21/05/14 14:03:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384585587841609092865_0004_m_000058_346
[2021-05-14 11:03:41,932] {docker.py:276} INFO - 21/05/14 14:03:41 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 5106 bytes result sent to driver
[2021-05-14 11:03:41,933] {docker.py:276} INFO - 21/05/14 14:03:41 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (3e4cb2948233, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:41,934] {docker.py:276} INFO - 21/05/14 14:03:41 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-14 11:03:41,934] {docker.py:276} INFO - 21/05/14 14:03:41 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 7047 ms on 3e4cb2948233 (executor driver) (59/200)
[2021-05-14 11:03:41,941] {docker.py:276} INFO - 21/05/14 14:03:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:41,949] {docker.py:276} INFO - 21/05/14 14:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:41,949] {docker.py:276} INFO - 21/05/14 14:03:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401389110312668207555591_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389110312668207555591_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401389110312668207555591_0004}; taskId=attempt_202105141401389110312668207555591_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b065796}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:41,949] {docker.py:276} INFO - 21/05/14 14:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401389110312668207555591_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389110312668207555591_0004_m_000062_350
[2021-05-14 11:03:41,953] {docker.py:276} INFO - 21/05/14 14:03:41 INFO StagingCommitter: Task committer attempt_202105141401389110312668207555591_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389110312668207555591_0004_m_000062_350 : duration 0:00.004s
[2021-05-14 11:03:42,349] {docker.py:276} INFO - 21/05/14 14:03:42 INFO StagingCommitter: Starting: Task committer attempt_202105141401381886208010574193501_0004_m_000059_347: needsTaskCommit() Task attempt_202105141401381886208010574193501_0004_m_000059_347
[2021-05-14 11:03:42,350] {docker.py:276} INFO - 21/05/14 14:03:42 INFO StagingCommitter: Task committer attempt_202105141401381886208010574193501_0004_m_000059_347: needsTaskCommit() Task attempt_202105141401381886208010574193501_0004_m_000059_347: duration 0:00.002s
21/05/14 14:03:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381886208010574193501_0004_m_000059_347
[2021-05-14 11:03:42,353] {docker.py:276} INFO - 21/05/14 14:03:42 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 5106 bytes result sent to driver
[2021-05-14 11:03:42,354] {docker.py:276} INFO - 21/05/14 14:03:42 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (3e4cb2948233, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:42,355] {docker.py:276} INFO - 21/05/14 14:03:42 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
21/05/14 14:03:42 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 7190 ms on 3e4cb2948233 (executor driver) (60/200)
[2021-05-14 11:03:42,366] {docker.py:276} INFO - 21/05/14 14:03:42 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:42,375] {docker.py:276} INFO - 21/05/14 14:03:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:42,375] {docker.py:276} INFO - 21/05/14 14:03:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387305911534573741992_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387305911534573741992_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387305911534573741992_0004}; taskId=attempt_202105141401387305911534573741992_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58a94f04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:42 INFO StagingCommitter: Starting: Task committer attempt_202105141401387305911534573741992_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387305911534573741992_0004_m_000063_351
[2021-05-14 11:03:42,379] {docker.py:276} INFO - 21/05/14 14:03:42 INFO StagingCommitter: Task committer attempt_202105141401387305911534573741992_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387305911534573741992_0004_m_000063_351 : duration 0:00.004s
[2021-05-14 11:03:46,228] {docker.py:276} INFO - 21/05/14 14:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401381130801289584148873_0004_m_000060_348: needsTaskCommit() Task attempt_202105141401381130801289584148873_0004_m_000060_348
[2021-05-14 11:03:46,229] {docker.py:276} INFO - 21/05/14 14:03:46 INFO StagingCommitter: Task committer attempt_202105141401381130801289584148873_0004_m_000060_348: needsTaskCommit() Task attempt_202105141401381130801289584148873_0004_m_000060_348: duration 0:00.002s
21/05/14 14:03:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381130801289584148873_0004_m_000060_348
[2021-05-14 11:03:46,232] {docker.py:276} INFO - 21/05/14 14:03:46 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 5106 bytes result sent to driver
[2021-05-14 11:03:46,233] {docker.py:276} INFO - 21/05/14 14:03:46 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (3e4cb2948233, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:46,235] {docker.py:276} INFO - 21/05/14 14:03:46 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-14 11:03:46,236] {docker.py:276} INFO - 21/05/14 14:03:46 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 6751 ms on 3e4cb2948233 (executor driver) (61/200)
[2021-05-14 11:03:46,245] {docker.py:276} INFO - 21/05/14 14:03:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:46,245] {docker.py:276} INFO - 21/05/14 14:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:46,254] {docker.py:276} INFO - 21/05/14 14:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:46,255] {docker.py:276} INFO - 21/05/14 14:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138477085874616117134_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138477085874616117134_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138477085874616117134_0004}; taskId=attempt_20210514140138477085874616117134_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55236654}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:46,255] {docker.py:276} INFO - 21/05/14 14:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:46 INFO StagingCommitter: Starting: Task committer attempt_20210514140138477085874616117134_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138477085874616117134_0004_m_000064_352
[2021-05-14 11:03:46,260] {docker.py:276} INFO - 21/05/14 14:03:46 INFO StagingCommitter: Task committer attempt_20210514140138477085874616117134_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138477085874616117134_0004_m_000064_352 : duration 0:00.005s
[2021-05-14 11:03:48,811] {docker.py:276} INFO - 21/05/14 14:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401389110312668207555591_0004_m_000062_350: needsTaskCommit() Task attempt_202105141401389110312668207555591_0004_m_000062_350
[2021-05-14 11:03:48,811] {docker.py:276} INFO - 21/05/14 14:03:48 INFO StagingCommitter: Task committer attempt_202105141401389110312668207555591_0004_m_000062_350: needsTaskCommit() Task attempt_202105141401389110312668207555591_0004_m_000062_350: duration 0:00.002s
[2021-05-14 11:03:48,812] {docker.py:276} INFO - 21/05/14 14:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401389110312668207555591_0004_m_000062_350
[2021-05-14 11:03:48,812] {docker.py:276} INFO - 21/05/14 14:03:48 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 5106 bytes result sent to driver
[2021-05-14 11:03:48,814] {docker.py:276} INFO - 21/05/14 14:03:48 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (3e4cb2948233, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:48,814] {docker.py:276} INFO - 21/05/14 14:03:48 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 6889 ms on 3e4cb2948233 (executor driver) (62/200)
[2021-05-14 11:03:48,816] {docker.py:276} INFO - 21/05/14 14:03:48 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-14 11:03:48,825] {docker.py:276} INFO - 21/05/14 14:03:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:48,833] {docker.py:276} INFO - 21/05/14 14:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:48,834] {docker.py:276} INFO - 21/05/14 14:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138122749142295293029_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138122749142295293029_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138122749142295293029_0004}; taskId=attempt_20210514140138122749142295293029_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46169e9a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:48,834] {docker.py:276} INFO - 21/05/14 14:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:48 INFO StagingCommitter: Starting: Task committer attempt_20210514140138122749142295293029_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138122749142295293029_0004_m_000065_353
[2021-05-14 11:03:48,836] {docker.py:276} INFO - 21/05/14 14:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401384665365600281189792_0004_m_000061_349: needsTaskCommit() Task attempt_202105141401384665365600281189792_0004_m_000061_349
[2021-05-14 11:03:48,836] {docker.py:276} INFO - 21/05/14 14:03:48 INFO StagingCommitter: Task committer attempt_202105141401384665365600281189792_0004_m_000061_349: needsTaskCommit() Task attempt_202105141401384665365600281189792_0004_m_000061_349: duration 0:00.000s
21/05/14 14:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384665365600281189792_0004_m_000061_349
[2021-05-14 11:03:48,837] {docker.py:276} INFO - 21/05/14 14:03:48 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 5106 bytes result sent to driver
[2021-05-14 11:03:48,840] {docker.py:276} INFO - 21/05/14 14:03:48 INFO StagingCommitter: Task committer attempt_20210514140138122749142295293029_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138122749142295293029_0004_m_000065_353 : duration 0:00.006s
[2021-05-14 11:03:48,840] {docker.py:276} INFO - 21/05/14 14:03:48 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (3e4cb2948233, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:48,841] {docker.py:276} INFO - 21/05/14 14:03:48 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-14 11:03:48,842] {docker.py:276} INFO - 21/05/14 14:03:48 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 6957 ms on 3e4cb2948233 (executor driver) (63/200)
[2021-05-14 11:03:48,861] {docker.py:276} INFO - 21/05/14 14:03:48 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:48,869] {docker.py:276} INFO - 21/05/14 14:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:48,869] {docker.py:276} INFO - 21/05/14 14:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383465235966324549668_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383465235966324549668_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383465235966324549668_0004}; taskId=attempt_202105141401383465235966324549668_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25173f86}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401383465235966324549668_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383465235966324549668_0004_m_000066_354
[2021-05-14 11:03:48,873] {docker.py:276} INFO - 21/05/14 14:03:48 INFO StagingCommitter: Task committer attempt_202105141401383465235966324549668_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383465235966324549668_0004_m_000066_354 : duration 0:00.004s
[2021-05-14 11:03:49,594] {docker.py:276} INFO - 21/05/14 14:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105141401387305911534573741992_0004_m_000063_351: needsTaskCommit() Task attempt_202105141401387305911534573741992_0004_m_000063_351
[2021-05-14 11:03:49,595] {docker.py:276} INFO - 21/05/14 14:03:49 INFO StagingCommitter: Task committer attempt_202105141401387305911534573741992_0004_m_000063_351: needsTaskCommit() Task attempt_202105141401387305911534573741992_0004_m_000063_351: duration 0:00.002s
21/05/14 14:03:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387305911534573741992_0004_m_000063_351
[2021-05-14 11:03:49,597] {docker.py:276} INFO - 21/05/14 14:03:49 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 5149 bytes result sent to driver
[2021-05-14 11:03:49,598] {docker.py:276} INFO - 21/05/14 14:03:49 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (3e4cb2948233, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:49,599] {docker.py:276} INFO - 21/05/14 14:03:49 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-14 11:03:49,600] {docker.py:276} INFO - 21/05/14 14:03:49 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 7255 ms on 3e4cb2948233 (executor driver) (64/200)
[2021-05-14 11:03:49,609] {docker.py:276} INFO - 21/05/14 14:03:49 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:49,618] {docker.py:276} INFO - 21/05/14 14:03:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381723392844661992319_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381723392844661992319_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381723392844661992319_0004}; taskId=attempt_202105141401381723392844661992319_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@422345ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:49,618] {docker.py:276} INFO - 21/05/14 14:03:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105141401381723392844661992319_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381723392844661992319_0004_m_000067_355
[2021-05-14 11:03:49,622] {docker.py:276} INFO - 21/05/14 14:03:49 INFO StagingCommitter: Task committer attempt_202105141401381723392844661992319_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381723392844661992319_0004_m_000067_355 : duration 0:00.004s
[2021-05-14 11:03:53,093] {docker.py:276} INFO - 21/05/14 14:03:53 INFO StagingCommitter: Starting: Task committer attempt_20210514140138477085874616117134_0004_m_000064_352: needsTaskCommit() Task attempt_20210514140138477085874616117134_0004_m_000064_352
[2021-05-14 11:03:53,094] {docker.py:276} INFO - 21/05/14 14:03:53 INFO StagingCommitter: Task committer attempt_20210514140138477085874616117134_0004_m_000064_352: needsTaskCommit() Task attempt_20210514140138477085874616117134_0004_m_000064_352: duration 0:00.001s
21/05/14 14:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138477085874616117134_0004_m_000064_352
[2021-05-14 11:03:53,095] {docker.py:276} INFO - 21/05/14 14:03:53 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 5149 bytes result sent to driver
[2021-05-14 11:03:53,096] {docker.py:276} INFO - 21/05/14 14:03:53 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (3e4cb2948233, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:53,097] {docker.py:276} INFO - 21/05/14 14:03:53 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-14 11:03:53,097] {docker.py:276} INFO - 21/05/14 14:03:53 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 6872 ms on 3e4cb2948233 (executor driver) (65/200)
[2021-05-14 11:03:53,107] {docker.py:276} INFO - 21/05/14 14:03:53 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:03:53,107] {docker.py:276} INFO - 21/05/14 14:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:53,115] {docker.py:276} INFO - 21/05/14 14:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:03:53,115] {docker.py:276} INFO - 21/05/14 14:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387003100182485124540_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387003100182485124540_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387003100182485124540_0004}; taskId=attempt_202105141401387003100182485124540_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ba1bfa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401387003100182485124540_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387003100182485124540_0004_m_000068_356
[2021-05-14 11:03:53,119] {docker.py:276} INFO - 21/05/14 14:03:53 INFO StagingCommitter: Task committer attempt_202105141401387003100182485124540_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387003100182485124540_0004_m_000068_356 : duration 0:00.004s
[2021-05-14 11:03:55,815] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105141401383465235966324549668_0004_m_000066_354: needsTaskCommit() Task attempt_202105141401383465235966324549668_0004_m_000066_354
[2021-05-14 11:03:55,815] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Task committer attempt_202105141401383465235966324549668_0004_m_000066_354: needsTaskCommit() Task attempt_202105141401383465235966324549668_0004_m_000066_354: duration 0:00.002s
21/05/14 14:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383465235966324549668_0004_m_000066_354
[2021-05-14 11:03:55,818] {docker.py:276} INFO - 21/05/14 14:03:55 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 5149 bytes result sent to driver
[2021-05-14 11:03:55,820] {docker.py:276} INFO - 21/05/14 14:03:55 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (3e4cb2948233, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:03:55 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
21/05/14 14:03:55 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 6988 ms on 3e4cb2948233 (executor driver) (66/200)
[2021-05-14 11:03:55,829] {docker.py:276} INFO - 21/05/14 14:03:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.8 KiB) non-empty blocks including 5 (42.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:55,838] {docker.py:276} INFO - 21/05/14 14:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382764123830838765791_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382764123830838765791_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382764123830838765791_0004}; taskId=attempt_202105141401382764123830838765791_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4cdffcdf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105141401382764123830838765791_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382764123830838765791_0004_m_000069_357
[2021-05-14 11:03:55,842] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Task committer attempt_202105141401382764123830838765791_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382764123830838765791_0004_m_000069_357 : duration 0:00.004s
[2021-05-14 11:03:55,925] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Starting: Task committer attempt_20210514140138122749142295293029_0004_m_000065_353: needsTaskCommit() Task attempt_20210514140138122749142295293029_0004_m_000065_353
[2021-05-14 11:03:55,926] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Task committer attempt_20210514140138122749142295293029_0004_m_000065_353: needsTaskCommit() Task attempt_20210514140138122749142295293029_0004_m_000065_353: duration 0:00.002s
21/05/14 14:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138122749142295293029_0004_m_000065_353
[2021-05-14 11:03:55,929] {docker.py:276} INFO - 21/05/14 14:03:55 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 5149 bytes result sent to driver
[2021-05-14 11:03:55,930] {docker.py:276} INFO - 21/05/14 14:03:55 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (3e4cb2948233, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:55,931] {docker.py:276} INFO - 21/05/14 14:03:55 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 7126 ms on 3e4cb2948233 (executor driver) (67/200)
[2021-05-14 11:03:55,932] {docker.py:276} INFO - 21/05/14 14:03:55 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-14 11:03:55,941] {docker.py:276} INFO - 21/05/14 14:03:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:55,950] {docker.py:276} INFO - 21/05/14 14:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384570138562960608608_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384570138562960608608_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384570138562960608608_0004}; taskId=attempt_202105141401384570138562960608608_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45c5d197}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:03:55,950] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105141401384570138562960608608_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384570138562960608608_0004_m_000070_358
[2021-05-14 11:03:55,954] {docker.py:276} INFO - 21/05/14 14:03:55 INFO StagingCommitter: Task committer attempt_202105141401384570138562960608608_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384570138562960608608_0004_m_000070_358 : duration 0:00.005s
[2021-05-14 11:03:56,131] {docker.py:276} INFO - 21/05/14 14:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105141401381723392844661992319_0004_m_000067_355: needsTaskCommit() Task attempt_202105141401381723392844661992319_0004_m_000067_355
[2021-05-14 11:03:56,133] {docker.py:276} INFO - 21/05/14 14:03:56 INFO StagingCommitter: Task committer attempt_202105141401381723392844661992319_0004_m_000067_355: needsTaskCommit() Task attempt_202105141401381723392844661992319_0004_m_000067_355: duration 0:00.007s
[2021-05-14 11:03:56,133] {docker.py:276} INFO - 21/05/14 14:03:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381723392844661992319_0004_m_000067_355
[2021-05-14 11:03:56,136] {docker.py:276} INFO - 21/05/14 14:03:56 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 5106 bytes result sent to driver
[2021-05-14 11:03:56,137] {docker.py:276} INFO - 21/05/14 14:03:56 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (3e4cb2948233, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:03:56,138] {docker.py:276} INFO - 21/05/14 14:03:56 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 6546 ms on 3e4cb2948233 (executor driver) (68/200)
[2021-05-14 11:03:56,138] {docker.py:276} INFO - 21/05/14 14:03:56 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
[2021-05-14 11:03:56,147] {docker.py:276} INFO - 21/05/14 14:03:56 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:03:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:03:56,155] {docker.py:276} INFO - 21/05/14 14:03:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:03:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:03:56,156] {docker.py:276} INFO - 21/05/14 14:03:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:03:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382015885777769464488_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382015885777769464488_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382015885777769464488_0004}; taskId=attempt_202105141401382015885777769464488_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d1eb38b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:03:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105141401382015885777769464488_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382015885777769464488_0004_m_000071_359
[2021-05-14 11:03:56,160] {docker.py:276} INFO - 21/05/14 14:03:56 INFO StagingCommitter: Task committer attempt_202105141401382015885777769464488_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382015885777769464488_0004_m_000071_359 : duration 0:00.005s
[2021-05-14 11:04:00,019] {docker.py:276} INFO - 21/05/14 14:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401387003100182485124540_0004_m_000068_356: needsTaskCommit() Task attempt_202105141401387003100182485124540_0004_m_000068_356
[2021-05-14 11:04:00,020] {docker.py:276} INFO - 21/05/14 14:04:00 INFO StagingCommitter: Task committer attempt_202105141401387003100182485124540_0004_m_000068_356: needsTaskCommit() Task attempt_202105141401387003100182485124540_0004_m_000068_356: duration 0:00.002s
21/05/14 14:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387003100182485124540_0004_m_000068_356
[2021-05-14 11:04:00,021] {docker.py:276} INFO - 21/05/14 14:04:00 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 5106 bytes result sent to driver
[2021-05-14 11:04:00,022] {docker.py:276} INFO - 21/05/14 14:04:00 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (3e4cb2948233, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:00,023] {docker.py:276} INFO - 21/05/14 14:04:00 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 6935 ms on 3e4cb2948233 (executor driver) (69/200)
[2021-05-14 11:04:00,023] {docker.py:276} INFO - 21/05/14 14:04:00 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-14 11:04:00,032] {docker.py:276} INFO - 21/05/14 14:04:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:00,041] {docker.py:276} INFO - 21/05/14 14:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387943812882131501476_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387943812882131501476_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387943812882131501476_0004}; taskId=attempt_202105141401387943812882131501476_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c4a5b04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401387943812882131501476_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387943812882131501476_0004_m_000072_360
[2021-05-14 11:04:00,045] {docker.py:276} INFO - 21/05/14 14:04:00 INFO StagingCommitter: Task committer attempt_202105141401387943812882131501476_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387943812882131501476_0004_m_000072_360 : duration 0:00.004s
[2021-05-14 11:04:02,263] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401382764123830838765791_0004_m_000069_357: needsTaskCommit() Task attempt_202105141401382764123830838765791_0004_m_000069_357
[2021-05-14 11:04:02,264] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Task committer attempt_202105141401382764123830838765791_0004_m_000069_357: needsTaskCommit() Task attempt_202105141401382764123830838765791_0004_m_000069_357: duration 0:00.001s
21/05/14 14:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382764123830838765791_0004_m_000069_357
[2021-05-14 11:04:02,266] {docker.py:276} INFO - 21/05/14 14:04:02 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 5106 bytes result sent to driver
[2021-05-14 11:04:02,267] {docker.py:276} INFO - 21/05/14 14:04:02 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (3e4cb2948233, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:02,268] {docker.py:276} INFO - 21/05/14 14:04:02 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 6456 ms on 3e4cb2948233 (executor driver) (70/200)
21/05/14 14:04:02 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-14 11:04:02,278] {docker.py:276} INFO - 21/05/14 14:04:02 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:02,289] {docker.py:276} INFO - 21/05/14 14:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388056928204191451821_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388056928204191451821_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388056928204191451821_0004}; taskId=attempt_202105141401388056928204191451821_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd543cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401388056928204191451821_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388056928204191451821_0004_m_000073_361
[2021-05-14 11:04:02,294] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Task committer attempt_202105141401388056928204191451821_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388056928204191451821_0004_m_000073_361 : duration 0:00.004s
[2021-05-14 11:04:02,813] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401382015885777769464488_0004_m_000071_359: needsTaskCommit() Task attempt_202105141401382015885777769464488_0004_m_000071_359
[2021-05-14 11:04:02,814] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Task committer attempt_202105141401382015885777769464488_0004_m_000071_359: needsTaskCommit() Task attempt_202105141401382015885777769464488_0004_m_000071_359: duration 0:00.002s
21/05/14 14:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382015885777769464488_0004_m_000071_359
[2021-05-14 11:04:02,815] {docker.py:276} INFO - 21/05/14 14:04:02 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 5106 bytes result sent to driver
[2021-05-14 11:04:02,816] {docker.py:276} INFO - 21/05/14 14:04:02 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (3e4cb2948233, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:02,817] {docker.py:276} INFO - 21/05/14 14:04:02 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-14 11:04:02,818] {docker.py:276} INFO - 21/05/14 14:04:02 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 6690 ms on 3e4cb2948233 (executor driver) (71/200)
[2021-05-14 11:04:02,828] {docker.py:276} INFO - 21/05/14 14:04:02 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:02,828] {docker.py:276} INFO - 21/05/14 14:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:02,837] {docker.py:276} INFO - 21/05/14 14:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:04:02,838] {docker.py:276} INFO - 21/05/14 14:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:02,838] {docker.py:276} INFO - 21/05/14 14:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385180843605339100752_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385180843605339100752_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385180843605339100752_0004}; taskId=attempt_202105141401385180843605339100752_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6cc8a392}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:02,838] {docker.py:276} INFO - 21/05/14 14:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:02,838] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401385180843605339100752_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385180843605339100752_0004_m_000074_362
[2021-05-14 11:04:02,842] {docker.py:276} INFO - 21/05/14 14:04:02 INFO StagingCommitter: Task committer attempt_202105141401385180843605339100752_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385180843605339100752_0004_m_000074_362 : duration 0:00.005s
[2021-05-14 11:04:02,994] {docker.py:276} INFO - 21/05/14 14:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105141401384570138562960608608_0004_m_000070_358: needsTaskCommit() Task attempt_202105141401384570138562960608608_0004_m_000070_358
[2021-05-14 11:04:02,995] {docker.py:276} INFO - 21/05/14 14:04:03 INFO StagingCommitter: Task committer attempt_202105141401384570138562960608608_0004_m_000070_358: needsTaskCommit() Task attempt_202105141401384570138562960608608_0004_m_000070_358: duration 0:00.002s
[2021-05-14 11:04:02,996] {docker.py:276} INFO - 21/05/14 14:04:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384570138562960608608_0004_m_000070_358
[2021-05-14 11:04:02,997] {docker.py:276} INFO - 21/05/14 14:04:03 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 5106 bytes result sent to driver
[2021-05-14 11:04:02,999] {docker.py:276} INFO - 21/05/14 14:04:03 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (3e4cb2948233, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:03,001] {docker.py:276} INFO - 21/05/14 14:04:03 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 7079 ms on 3e4cb2948233 (executor driver) (72/200)
[2021-05-14 11:04:03,001] {docker.py:276} INFO - 21/05/14 14:04:03 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-14 11:04:03,010] {docker.py:276} INFO - 21/05/14 14:04:03 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:03,011] {docker.py:276} INFO - 21/05/14 14:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:03,019] {docker.py:276} INFO - 21/05/14 14:04:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:03,019] {docker.py:276} INFO - 21/05/14 14:04:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:03,020] {docker.py:276} INFO - 21/05/14 14:04:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385241438287578212680_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385241438287578212680_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385241438287578212680_0004}; taskId=attempt_202105141401385241438287578212680_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@445c74cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:03,020] {docker.py:276} INFO - 21/05/14 14:04:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:03,020] {docker.py:276} INFO - 21/05/14 14:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105141401385241438287578212680_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385241438287578212680_0004_m_000075_363
[2021-05-14 11:04:03,024] {docker.py:276} INFO - 21/05/14 14:04:03 INFO StagingCommitter: Task committer attempt_202105141401385241438287578212680_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385241438287578212680_0004_m_000075_363 : duration 0:00.004s
[2021-05-14 11:04:06,665] {docker.py:276} INFO - 21/05/14 14:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105141401387943812882131501476_0004_m_000072_360: needsTaskCommit() Task attempt_202105141401387943812882131501476_0004_m_000072_360
[2021-05-14 11:04:06,667] {docker.py:276} INFO - 21/05/14 14:04:06 INFO StagingCommitter: Task committer attempt_202105141401387943812882131501476_0004_m_000072_360: needsTaskCommit() Task attempt_202105141401387943812882131501476_0004_m_000072_360: duration 0:00.003s
21/05/14 14:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387943812882131501476_0004_m_000072_360
[2021-05-14 11:04:06,669] {docker.py:276} INFO - 21/05/14 14:04:06 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 5149 bytes result sent to driver
21/05/14 14:04:06 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (3e4cb2948233, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:06,669] {docker.py:276} INFO - 21/05/14 14:04:06 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 6655 ms on 3e4cb2948233 (executor driver) (73/200)
[2021-05-14 11:04:06,670] {docker.py:276} INFO - 21/05/14 14:04:06 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
[2021-05-14 11:04:06,679] {docker.py:276} INFO - 21/05/14 14:04:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:06,688] {docker.py:276} INFO - 21/05/14 14:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383288715444836823436_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383288715444836823436_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383288715444836823436_0004}; taskId=attempt_202105141401383288715444836823436_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d3cfa67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:06,688] {docker.py:276} INFO - 21/05/14 14:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105141401383288715444836823436_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383288715444836823436_0004_m_000076_364
[2021-05-14 11:04:06,692] {docker.py:276} INFO - 21/05/14 14:04:06 INFO StagingCommitter: Task committer attempt_202105141401383288715444836823436_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383288715444836823436_0004_m_000076_364 : duration 0:00.004s
[2021-05-14 11:04:09,936] {docker.py:276} INFO - 21/05/14 14:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401388056928204191451821_0004_m_000073_361: needsTaskCommit() Task attempt_202105141401388056928204191451821_0004_m_000073_361
[2021-05-14 11:04:09,937] {docker.py:276} INFO - 21/05/14 14:04:09 INFO StagingCommitter: Task committer attempt_202105141401388056928204191451821_0004_m_000073_361: needsTaskCommit() Task attempt_202105141401388056928204191451821_0004_m_000073_361: duration 0:00.002s
21/05/14 14:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388056928204191451821_0004_m_000073_361
[2021-05-14 11:04:09,938] {docker.py:276} INFO - 21/05/14 14:04:09 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 5149 bytes result sent to driver
[2021-05-14 11:04:09,939] {docker.py:276} INFO - 21/05/14 14:04:09 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (3e4cb2948233, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:09,940] {docker.py:276} INFO - 21/05/14 14:04:09 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
[2021-05-14 11:04:09,942] {docker.py:276} INFO - 21/05/14 14:04:09 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 7684 ms on 3e4cb2948233 (executor driver) (74/200)
[2021-05-14 11:04:09,951] {docker.py:276} INFO - 21/05/14 14:04:09 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:09,951] {docker.py:276} INFO - 21/05/14 14:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:09,960] {docker.py:276} INFO - 21/05/14 14:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:09,960] {docker.py:276} INFO - 21/05/14 14:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138675008540449856385_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138675008540449856385_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138675008540449856385_0004}; taskId=attempt_20210514140138675008540449856385_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@798a5a77}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:09 INFO StagingCommitter: Starting: Task committer attempt_20210514140138675008540449856385_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138675008540449856385_0004_m_000077_365
[2021-05-14 11:04:09,965] {docker.py:276} INFO - 21/05/14 14:04:09 INFO StagingCommitter: Task committer attempt_20210514140138675008540449856385_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138675008540449856385_0004_m_000077_365 : duration 0:00.004s
[2021-05-14 11:04:10,294] {docker.py:276} INFO - 21/05/14 14:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105141401385241438287578212680_0004_m_000075_363: needsTaskCommit() Task attempt_202105141401385241438287578212680_0004_m_000075_363
21/05/14 14:04:10 INFO StagingCommitter: Task committer attempt_202105141401385241438287578212680_0004_m_000075_363: needsTaskCommit() Task attempt_202105141401385241438287578212680_0004_m_000075_363: duration 0:00.003s
21/05/14 14:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385241438287578212680_0004_m_000075_363
[2021-05-14 11:04:10,296] {docker.py:276} INFO - 21/05/14 14:04:10 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 5149 bytes result sent to driver
[2021-05-14 11:04:10,298] {docker.py:276} INFO - 21/05/14 14:04:10 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (3e4cb2948233, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:10,299] {docker.py:276} INFO - 21/05/14 14:04:10 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 7309 ms on 3e4cb2948233 (executor driver) (75/200)
21/05/14 14:04:10 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-14 11:04:10,309] {docker.py:276} INFO - 21/05/14 14:04:10 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:10,310] {docker.py:276} INFO - 21/05/14 14:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:10,318] {docker.py:276} INFO - 21/05/14 14:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:10,318] {docker.py:276} INFO - 21/05/14 14:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386432132365519950556_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386432132365519950556_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386432132365519950556_0004}; taskId=attempt_202105141401386432132365519950556_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e19e623}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:10,319] {docker.py:276} INFO - 21/05/14 14:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105141401386432132365519950556_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386432132365519950556_0004_m_000078_366
[2021-05-14 11:04:10,323] {docker.py:276} INFO - 21/05/14 14:04:10 INFO StagingCommitter: Task committer attempt_202105141401386432132365519950556_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386432132365519950556_0004_m_000078_366 : duration 0:00.005s
[2021-05-14 11:04:10,551] {docker.py:276} INFO - 21/05/14 14:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105141401385180843605339100752_0004_m_000074_362: needsTaskCommit() Task attempt_202105141401385180843605339100752_0004_m_000074_362
21/05/14 14:04:10 INFO StagingCommitter: Task committer attempt_202105141401385180843605339100752_0004_m_000074_362: needsTaskCommit() Task attempt_202105141401385180843605339100752_0004_m_000074_362: duration 0:00.002s
21/05/14 14:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385180843605339100752_0004_m_000074_362
[2021-05-14 11:04:10,555] {docker.py:276} INFO - 21/05/14 14:04:10 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 5149 bytes result sent to driver
21/05/14 14:04:10 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (3e4cb2948233, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:10,555] {docker.py:276} INFO - 21/05/14 14:04:10 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 7747 ms on 3e4cb2948233 (executor driver) (76/200)
[2021-05-14 11:04:10,556] {docker.py:276} INFO - 21/05/14 14:04:10 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
[2021-05-14 11:04:10,566] {docker.py:276} INFO - 21/05/14 14:04:10 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:10,574] {docker.py:276} INFO - 21/05/14 14:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:10,575] {docker.py:276} INFO - 21/05/14 14:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386797680530874438546_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386797680530874438546_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386797680530874438546_0004}; taskId=attempt_202105141401386797680530874438546_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5891d7e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105141401386797680530874438546_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386797680530874438546_0004_m_000079_367
[2021-05-14 11:04:10,578] {docker.py:276} INFO - 21/05/14 14:04:10 INFO StagingCommitter: Task committer attempt_202105141401386797680530874438546_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386797680530874438546_0004_m_000079_367 : duration 0:00.003s
[2021-05-14 11:04:14,033] {docker.py:276} INFO - 21/05/14 14:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401383288715444836823436_0004_m_000076_364: needsTaskCommit() Task attempt_202105141401383288715444836823436_0004_m_000076_364
[2021-05-14 11:04:14,033] {docker.py:276} INFO - 21/05/14 14:04:14 INFO StagingCommitter: Task committer attempt_202105141401383288715444836823436_0004_m_000076_364: needsTaskCommit() Task attempt_202105141401383288715444836823436_0004_m_000076_364: duration 0:00.003s
[2021-05-14 11:04:14,034] {docker.py:276} INFO - 21/05/14 14:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383288715444836823436_0004_m_000076_364
[2021-05-14 11:04:14,037] {docker.py:276} INFO - 21/05/14 14:04:14 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 5106 bytes result sent to driver
[2021-05-14 11:04:14,038] {docker.py:276} INFO - 21/05/14 14:04:14 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (3e4cb2948233, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:14,039] {docker.py:276} INFO - 21/05/14 14:04:14 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 7343 ms on 3e4cb2948233 (executor driver) (77/200)
[2021-05-14 11:04:14,040] {docker.py:276} INFO - 21/05/14 14:04:14 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-14 11:04:14,049] {docker.py:276} INFO - 21/05/14 14:04:14 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:14,049] {docker.py:276} INFO - 21/05/14 14:04:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:14,058] {docker.py:276} INFO - 21/05/14 14:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:14,059] {docker.py:276} INFO - 21/05/14 14:04:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388374989525803047156_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388374989525803047156_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388374989525803047156_0004}; taskId=attempt_202105141401388374989525803047156_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@805ef1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:14,059] {docker.py:276} INFO - 21/05/14 14:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:14,060] {docker.py:276} INFO - 21/05/14 14:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401388374989525803047156_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388374989525803047156_0004_m_000080_368
[2021-05-14 11:04:14,066] {docker.py:276} INFO - 21/05/14 14:04:14 INFO StagingCommitter: Task committer attempt_202105141401388374989525803047156_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388374989525803047156_0004_m_000080_368 : duration 0:00.006s
[2021-05-14 11:04:16,995] {docker.py:276} INFO - 21/05/14 14:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401386797680530874438546_0004_m_000079_367: needsTaskCommit() Task attempt_202105141401386797680530874438546_0004_m_000079_367
[2021-05-14 11:04:16,995] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Task committer attempt_202105141401386797680530874438546_0004_m_000079_367: needsTaskCommit() Task attempt_202105141401386797680530874438546_0004_m_000079_367: duration 0:00.001s
21/05/14 14:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386797680530874438546_0004_m_000079_367
[2021-05-14 11:04:16,996] {docker.py:276} INFO - 21/05/14 14:04:17 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 5106 bytes result sent to driver
[2021-05-14 11:04:16,997] {docker.py:276} INFO - 21/05/14 14:04:17 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (3e4cb2948233, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:16,998] {docker.py:276} INFO - 21/05/14 14:04:17 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-14 11:04:16,999] {docker.py:276} INFO - 21/05/14 14:04:17 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 6417 ms on 3e4cb2948233 (executor driver) (78/200)
[2021-05-14 11:04:17,004] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Starting: Task committer attempt_20210514140138675008540449856385_0004_m_000077_365: needsTaskCommit() Task attempt_20210514140138675008540449856385_0004_m_000077_365
[2021-05-14 11:04:17,005] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Task committer attempt_20210514140138675008540449856385_0004_m_000077_365: needsTaskCommit() Task attempt_20210514140138675008540449856385_0004_m_000077_365: duration 0:00.000s
21/05/14 14:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138675008540449856385_0004_m_000077_365
[2021-05-14 11:04:17,006] {docker.py:276} INFO - 21/05/14 14:04:17 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 5106 bytes result sent to driver
[2021-05-14 11:04:17,007] {docker.py:276} INFO - 21/05/14 14:04:17 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (3e4cb2948233, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:17,008] {docker.py:276} INFO - 21/05/14 14:04:17 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
[2021-05-14 11:04:17,008] {docker.py:276} INFO - 21/05/14 14:04:17 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 7043 ms on 3e4cb2948233 (executor driver) (79/200)
[2021-05-14 11:04:17,012] {docker.py:276} INFO - 21/05/14 14:04:17 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:17,013] {docker.py:276} INFO - 21/05/14 14:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:17,015] {docker.py:276} INFO - 21/05/14 14:04:17 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:17,022] {docker.py:276} INFO - 21/05/14 14:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:17,023] {docker.py:276} INFO - 21/05/14 14:04:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388206786315817662744_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388206786315817662744_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388206786315817662744_0004}; taskId=attempt_202105141401388206786315817662744_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56f5b7eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:17,023] {docker.py:276} INFO - 21/05/14 14:04:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:17,024] {docker.py:276} INFO - 21/05/14 14:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105141401388206786315817662744_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388206786315817662744_0004_m_000081_369 
21/05/14 14:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:17,025] {docker.py:276} INFO - 21/05/14 14:04:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382180976091452865710_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382180976091452865710_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382180976091452865710_0004}; taskId=attempt_202105141401382180976091452865710_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@313dbfb1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:17,025] {docker.py:276} INFO - 21/05/14 14:04:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:17,026] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105141401382180976091452865710_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382180976091452865710_0004_m_000082_370
[2021-05-14 11:04:17,031] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Task committer attempt_202105141401382180976091452865710_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382180976091452865710_0004_m_000082_370 : duration 0:00.006s
[2021-05-14 11:04:17,038] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Task committer attempt_202105141401388206786315817662744_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388206786315817662744_0004_m_000081_369 : duration 0:00.013s
[2021-05-14 11:04:17,305] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105141401386432132365519950556_0004_m_000078_366: needsTaskCommit() Task attempt_202105141401386432132365519950556_0004_m_000078_366
21/05/14 14:04:17 INFO StagingCommitter: Task committer attempt_202105141401386432132365519950556_0004_m_000078_366: needsTaskCommit() Task attempt_202105141401386432132365519950556_0004_m_000078_366: duration 0:00.001s
21/05/14 14:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386432132365519950556_0004_m_000078_366
[2021-05-14 11:04:17,306] {docker.py:276} INFO - 21/05/14 14:04:17 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 5106 bytes result sent to driver
[2021-05-14 11:04:17,307] {docker.py:276} INFO - 21/05/14 14:04:17 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (3e4cb2948233, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:17,308] {docker.py:276} INFO - 21/05/14 14:04:17 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 6984 ms on 3e4cb2948233 (executor driver) (80/200)
21/05/14 14:04:17 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-14 11:04:17,317] {docker.py:276} INFO - 21/05/14 14:04:17 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:17,326] {docker.py:276} INFO - 21/05/14 14:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:17,331] {docker.py:276} INFO - 21/05/14 14:04:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383239717298312995464_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383239717298312995464_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383239717298312995464_0004}; taskId=attempt_202105141401383239717298312995464_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4afa0187}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:17,332] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105141401383239717298312995464_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383239717298312995464_0004_m_000083_371
[2021-05-14 11:04:17,332] {docker.py:276} INFO - 21/05/14 14:04:17 INFO StagingCommitter: Task committer attempt_202105141401383239717298312995464_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383239717298312995464_0004_m_000083_371 : duration 0:00.004s
[2021-05-14 11:04:21,003] {docker.py:276} INFO - 21/05/14 14:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401388374989525803047156_0004_m_000080_368: needsTaskCommit() Task attempt_202105141401388374989525803047156_0004_m_000080_368
[2021-05-14 11:04:21,005] {docker.py:276} INFO - 21/05/14 14:04:21 INFO StagingCommitter: Task committer attempt_202105141401388374989525803047156_0004_m_000080_368: needsTaskCommit() Task attempt_202105141401388374989525803047156_0004_m_000080_368: duration 0:00.003s
[2021-05-14 11:04:21,006] {docker.py:276} INFO - 21/05/14 14:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388374989525803047156_0004_m_000080_368
[2021-05-14 11:04:21,007] {docker.py:276} INFO - 21/05/14 14:04:21 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 5106 bytes result sent to driver
[2021-05-14 11:04:21,007] {docker.py:276} INFO - 21/05/14 14:04:21 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (3e4cb2948233, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:21,008] {docker.py:276} INFO - 21/05/14 14:04:21 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
[2021-05-14 11:04:21,010] {docker.py:276} INFO - 21/05/14 14:04:21 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 6980 ms on 3e4cb2948233 (executor driver) (81/200)
[2021-05-14 11:04:21,019] {docker.py:276} INFO - 21/05/14 14:04:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:21,028] {docker.py:276} INFO - 21/05/14 14:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383837710241827468314_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383837710241827468314_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383837710241827468314_0004}; taskId=attempt_202105141401383837710241827468314_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b8fff1f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401383837710241827468314_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383837710241827468314_0004_m_000084_372
[2021-05-14 11:04:21,032] {docker.py:276} INFO - 21/05/14 14:04:21 INFO StagingCommitter: Task committer attempt_202105141401383837710241827468314_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383837710241827468314_0004_m_000084_372 : duration 0:00.005s
[2021-05-14 11:04:23,883] {docker.py:276} INFO - 21/05/14 14:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401388206786315817662744_0004_m_000081_369: needsTaskCommit() Task attempt_202105141401388206786315817662744_0004_m_000081_369
21/05/14 14:04:23 INFO StagingCommitter: Task committer attempt_202105141401388206786315817662744_0004_m_000081_369: needsTaskCommit() Task attempt_202105141401388206786315817662744_0004_m_000081_369: duration 0:00.002s
21/05/14 14:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388206786315817662744_0004_m_000081_369
[2021-05-14 11:04:23,885] {docker.py:276} INFO - 21/05/14 14:04:23 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 5149 bytes result sent to driver
[2021-05-14 11:04:23,887] {docker.py:276} INFO - 21/05/14 14:04:23 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (3e4cb2948233, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:23,888] {docker.py:276} INFO - 21/05/14 14:04:23 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-14 11:04:23,889] {docker.py:276} INFO - 21/05/14 14:04:23 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 6899 ms on 3e4cb2948233 (executor driver) (82/200)
[2021-05-14 11:04:23,899] {docker.py:276} INFO - 21/05/14 14:04:23 INFO ShuffleBlockFetcherIterator: Getting 5 (40.3 KiB) non-empty blocks including 5 (40.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:23,907] {docker.py:276} INFO - 21/05/14 14:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:23,907] {docker.py:276} INFO - 21/05/14 14:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385394116306441398121_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385394116306441398121_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385394116306441398121_0004}; taskId=attempt_202105141401385394116306441398121_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dda8e54}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401385394116306441398121_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385394116306441398121_0004_m_000085_373
[2021-05-14 11:04:23,911] {docker.py:276} INFO - 21/05/14 14:04:23 INFO StagingCommitter: Task committer attempt_202105141401385394116306441398121_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385394116306441398121_0004_m_000085_373 : duration 0:00.004s
[2021-05-14 11:04:24,208] {docker.py:276} INFO - 21/05/14 14:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105141401382180976091452865710_0004_m_000082_370: needsTaskCommit() Task attempt_202105141401382180976091452865710_0004_m_000082_370
[2021-05-14 11:04:24,209] {docker.py:276} INFO - 21/05/14 14:04:24 INFO StagingCommitter: Task committer attempt_202105141401382180976091452865710_0004_m_000082_370: needsTaskCommit() Task attempt_202105141401382180976091452865710_0004_m_000082_370: duration 0:00.003s
[2021-05-14 11:04:24,210] {docker.py:276} INFO - 21/05/14 14:04:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382180976091452865710_0004_m_000082_370
[2021-05-14 11:04:24,213] {docker.py:276} INFO - 21/05/14 14:04:24 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 5149 bytes result sent to driver
[2021-05-14 11:04:24,214] {docker.py:276} INFO - 21/05/14 14:04:24 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (3e4cb2948233, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:24,215] {docker.py:276} INFO - 21/05/14 14:04:24 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-14 11:04:24,216] {docker.py:276} INFO - 21/05/14 14:04:24 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 7216 ms on 3e4cb2948233 (executor driver) (83/200)
[2021-05-14 11:04:24,225] {docker.py:276} INFO - 21/05/14 14:04:24 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:24,233] {docker.py:276} INFO - 21/05/14 14:04:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386249644103545924533_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386249644103545924533_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386249644103545924533_0004}; taskId=attempt_202105141401386249644103545924533_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66d3c561}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:24,233] {docker.py:276} INFO - 21/05/14 14:04:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105141401386249644103545924533_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386249644103545924533_0004_m_000086_374
[2021-05-14 11:04:24,238] {docker.py:276} INFO - 21/05/14 14:04:24 INFO StagingCommitter: Task committer attempt_202105141401386249644103545924533_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386249644103545924533_0004_m_000086_374 : duration 0:00.004s
[2021-05-14 11:04:26,721] {docker.py:276} INFO - 21/05/14 14:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401383239717298312995464_0004_m_000083_371: needsTaskCommit() Task attempt_202105141401383239717298312995464_0004_m_000083_371
[2021-05-14 11:04:26,722] {docker.py:276} INFO - 21/05/14 14:04:26 INFO StagingCommitter: Task committer attempt_202105141401383239717298312995464_0004_m_000083_371: needsTaskCommit() Task attempt_202105141401383239717298312995464_0004_m_000083_371: duration 0:00.003s
[2021-05-14 11:04:26,723] {docker.py:276} INFO - 21/05/14 14:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383239717298312995464_0004_m_000083_371
[2021-05-14 11:04:26,725] {docker.py:276} INFO - 21/05/14 14:04:26 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 5149 bytes result sent to driver
[2021-05-14 11:04:26,728] {docker.py:276} INFO - 21/05/14 14:04:26 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (3e4cb2948233, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:26,730] {docker.py:276} INFO - 21/05/14 14:04:26 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
21/05/14 14:04:26 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 9433 ms on 3e4cb2948233 (executor driver) (84/200)
[2021-05-14 11:04:26,739] {docker.py:276} INFO - 21/05/14 14:04:26 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:26,748] {docker.py:276} INFO - 21/05/14 14:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383675461040422761452_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383675461040422761452_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383675461040422761452_0004}; taskId=attempt_202105141401383675461040422761452_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7075abe2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401383675461040422761452_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383675461040422761452_0004_m_000087_375
[2021-05-14 11:04:26,752] {docker.py:276} INFO - 21/05/14 14:04:26 INFO StagingCommitter: Task committer attempt_202105141401383675461040422761452_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383675461040422761452_0004_m_000087_375 : duration 0:00.005s
[2021-05-14 11:04:27,983] {docker.py:276} INFO - 21/05/14 14:04:27 INFO StagingCommitter: Starting: Task committer attempt_202105141401383837710241827468314_0004_m_000084_372: needsTaskCommit() Task attempt_202105141401383837710241827468314_0004_m_000084_372
[2021-05-14 11:04:27,984] {docker.py:276} INFO - 21/05/14 14:04:28 INFO StagingCommitter: Task committer attempt_202105141401383837710241827468314_0004_m_000084_372: needsTaskCommit() Task attempt_202105141401383837710241827468314_0004_m_000084_372: duration 0:00.004s
21/05/14 14:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383837710241827468314_0004_m_000084_372
[2021-05-14 11:04:27,986] {docker.py:276} INFO - 21/05/14 14:04:28 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 5149 bytes result sent to driver
[2021-05-14 11:04:27,988] {docker.py:276} INFO - 21/05/14 14:04:28 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (3e4cb2948233, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:27,990] {docker.py:276} INFO - 21/05/14 14:04:28 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 6991 ms on 3e4cb2948233 (executor driver) (85/200)
21/05/14 14:04:28 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-14 11:04:27,999] {docker.py:276} INFO - 21/05/14 14:04:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:28,008] {docker.py:276} INFO - 21/05/14 14:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382717864160745368694_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382717864160745368694_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382717864160745368694_0004}; taskId=attempt_202105141401382717864160745368694_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48c30fcc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:28,008] {docker.py:276} INFO - 21/05/14 14:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401382717864160745368694_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382717864160745368694_0004_m_000088_376
[2021-05-14 11:04:28,012] {docker.py:276} INFO - 21/05/14 14:04:28 INFO StagingCommitter: Task committer attempt_202105141401382717864160745368694_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382717864160745368694_0004_m_000088_376 : duration 0:00.004s
[2021-05-14 11:04:30,776] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401386249644103545924533_0004_m_000086_374: needsTaskCommit() Task attempt_202105141401386249644103545924533_0004_m_000086_374
[2021-05-14 11:04:30,777] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Task committer attempt_202105141401386249644103545924533_0004_m_000086_374: needsTaskCommit() Task attempt_202105141401386249644103545924533_0004_m_000086_374: duration 0:00.002s
21/05/14 14:04:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386249644103545924533_0004_m_000086_374
[2021-05-14 11:04:30,780] {docker.py:276} INFO - 21/05/14 14:04:30 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 5106 bytes result sent to driver
[2021-05-14 11:04:30,782] {docker.py:276} INFO - 21/05/14 14:04:30 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (3e4cb2948233, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:30,783] {docker.py:276} INFO - 21/05/14 14:04:30 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 6578 ms on 3e4cb2948233 (executor driver) (86/200)
[2021-05-14 11:04:30,784] {docker.py:276} INFO - 21/05/14 14:04:30 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
[2021-05-14 11:04:30,794] {docker.py:276} INFO - 21/05/14 14:04:30 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:30,802] {docker.py:276} INFO - 21/05/14 14:04:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:30,802] {docker.py:276} INFO - 21/05/14 14:04:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387190151549314020373_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387190151549314020373_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387190151549314020373_0004}; taskId=attempt_202105141401387190151549314020373_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30cf347f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:30,802] {docker.py:276} INFO - 21/05/14 14:04:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:30,803] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401387190151549314020373_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387190151549314020373_0004_m_000089_377
[2021-05-14 11:04:30,806] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Task committer attempt_202105141401387190151549314020373_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387190151549314020373_0004_m_000089_377 : duration 0:00.004s
[2021-05-14 11:04:30,821] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401385394116306441398121_0004_m_000085_373: needsTaskCommit() Task attempt_202105141401385394116306441398121_0004_m_000085_373
[2021-05-14 11:04:30,821] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Task committer attempt_202105141401385394116306441398121_0004_m_000085_373: needsTaskCommit() Task attempt_202105141401385394116306441398121_0004_m_000085_373: duration 0:00.001s
21/05/14 14:04:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385394116306441398121_0004_m_000085_373
[2021-05-14 11:04:30,823] {docker.py:276} INFO - 21/05/14 14:04:30 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 5106 bytes result sent to driver
[2021-05-14 11:04:30,824] {docker.py:276} INFO - 21/05/14 14:04:30 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (3e4cb2948233, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:30,824] {docker.py:276} INFO - 21/05/14 14:04:30 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 6946 ms on 3e4cb2948233 (executor driver) (87/200)
[2021-05-14 11:04:30,825] {docker.py:276} INFO - 21/05/14 14:04:30 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-14 11:04:30,831] {docker.py:276} INFO - 21/05/14 14:04:30 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:30,832] {docker.py:276} INFO - 21/05/14 14:04:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:30,839] {docker.py:276} INFO - 21/05/14 14:04:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388906090753118689958_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388906090753118689958_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388906090753118689958_0004}; taskId=attempt_202105141401388906090753118689958_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a38610e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401388906090753118689958_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388906090753118689958_0004_m_000090_378
[2021-05-14 11:04:30,843] {docker.py:276} INFO - 21/05/14 14:04:30 INFO StagingCommitter: Task committer attempt_202105141401388906090753118689958_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388906090753118689958_0004_m_000090_378 : duration 0:00.004s
[2021-05-14 11:04:35,503] {docker.py:276} INFO - 21/05/14 14:04:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401383675461040422761452_0004_m_000087_375: needsTaskCommit() Task attempt_202105141401383675461040422761452_0004_m_000087_375
[2021-05-14 11:04:35,506] {docker.py:276} INFO - 21/05/14 14:04:35 INFO StagingCommitter: Task committer attempt_202105141401383675461040422761452_0004_m_000087_375: needsTaskCommit() Task attempt_202105141401383675461040422761452_0004_m_000087_375: duration 0:00.002s
21/05/14 14:04:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383675461040422761452_0004_m_000087_375
[2021-05-14 11:04:35,507] {docker.py:276} INFO - 21/05/14 14:04:35 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 5106 bytes result sent to driver
[2021-05-14 11:04:35,508] {docker.py:276} INFO - 21/05/14 14:04:35 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (3e4cb2948233, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:35,509] {docker.py:276} INFO - 21/05/14 14:04:35 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-14 11:04:35,511] {docker.py:276} INFO - 21/05/14 14:04:35 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 8794 ms on 3e4cb2948233 (executor driver) (88/200)
[2021-05-14 11:04:35,519] {docker.py:276} INFO - 21/05/14 14:04:35 INFO ShuffleBlockFetcherIterator: Getting 5 (39.8 KiB) non-empty blocks including 5 (39.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:35,530] {docker.py:276} INFO - 21/05/14 14:04:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:35,530] {docker.py:276} INFO - 21/05/14 14:04:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:35,531] {docker.py:276} INFO - 21/05/14 14:04:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387474126519271355217_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387474126519271355217_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387474126519271355217_0004}; taskId=attempt_202105141401387474126519271355217_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d4433ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:35,531] {docker.py:276} INFO - 21/05/14 14:04:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:35,531] {docker.py:276} INFO - 21/05/14 14:04:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401387474126519271355217_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387474126519271355217_0004_m_000091_379
[2021-05-14 11:04:35,538] {docker.py:276} INFO - 21/05/14 14:04:35 INFO StagingCommitter: Task committer attempt_202105141401387474126519271355217_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387474126519271355217_0004_m_000091_379 : duration 0:00.007s
[2021-05-14 11:04:36,298] {docker.py:276} INFO - 21/05/14 14:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105141401382717864160745368694_0004_m_000088_376: needsTaskCommit() Task attempt_202105141401382717864160745368694_0004_m_000088_376
[2021-05-14 11:04:36,298] {docker.py:276} INFO - 21/05/14 14:04:36 INFO StagingCommitter: Task committer attempt_202105141401382717864160745368694_0004_m_000088_376: needsTaskCommit() Task attempt_202105141401382717864160745368694_0004_m_000088_376: duration 0:00.003s
[2021-05-14 11:04:36,299] {docker.py:276} INFO - 21/05/14 14:04:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382717864160745368694_0004_m_000088_376
[2021-05-14 11:04:36,301] {docker.py:276} INFO - 21/05/14 14:04:36 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 5106 bytes result sent to driver
[2021-05-14 11:04:36,302] {docker.py:276} INFO - 21/05/14 14:04:36 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (3e4cb2948233, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:36,303] {docker.py:276} INFO - 21/05/14 14:04:36 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-14 11:04:36,304] {docker.py:276} INFO - 21/05/14 14:04:36 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 8327 ms on 3e4cb2948233 (executor driver) (89/200)
[2021-05-14 11:04:36,313] {docker.py:276} INFO - 21/05/14 14:04:36 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:36,322] {docker.py:276} INFO - 21/05/14 14:04:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401389021970427615194854_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389021970427615194854_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401389021970427615194854_0004}; taskId=attempt_202105141401389021970427615194854_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c279764}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105141401389021970427615194854_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389021970427615194854_0004_m_000092_380
[2021-05-14 11:04:36,326] {docker.py:276} INFO - 21/05/14 14:04:36 INFO StagingCommitter: Task committer attempt_202105141401389021970427615194854_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389021970427615194854_0004_m_000092_380 : duration 0:00.004s
[2021-05-14 11:04:37,041] {docker.py:276} INFO - 21/05/14 14:04:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401388906090753118689958_0004_m_000090_378: needsTaskCommit() Task attempt_202105141401388906090753118689958_0004_m_000090_378
[2021-05-14 11:04:37,042] {docker.py:276} INFO - 21/05/14 14:04:37 INFO StagingCommitter: Task committer attempt_202105141401388906090753118689958_0004_m_000090_378: needsTaskCommit() Task attempt_202105141401388906090753118689958_0004_m_000090_378: duration 0:00.002s
21/05/14 14:04:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388906090753118689958_0004_m_000090_378
[2021-05-14 11:04:37,043] {docker.py:276} INFO - 21/05/14 14:04:37 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 5106 bytes result sent to driver
[2021-05-14 11:04:37,044] {docker.py:276} INFO - 21/05/14 14:04:37 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (3e4cb2948233, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:37,046] {docker.py:276} INFO - 21/05/14 14:04:37 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 6229 ms on 3e4cb2948233 (executor driver) (90/200)
[2021-05-14 11:04:37,046] {docker.py:276} INFO - 21/05/14 14:04:37 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
[2021-05-14 11:04:37,056] {docker.py:276} INFO - 21/05/14 14:04:37 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:37,065] {docker.py:276} INFO - 21/05/14 14:04:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388735270826254006456_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388735270826254006456_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388735270826254006456_0004}; taskId=attempt_202105141401388735270826254006456_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16e60654}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:37,066] {docker.py:276} INFO - 21/05/14 14:04:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401388735270826254006456_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388735270826254006456_0004_m_000093_381
[2021-05-14 11:04:37,069] {docker.py:276} INFO - 21/05/14 14:04:37 INFO StagingCommitter: Task committer attempt_202105141401388735270826254006456_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388735270826254006456_0004_m_000093_381 : duration 0:00.004s
[2021-05-14 11:04:40,114] {docker.py:276} INFO - 21/05/14 14:04:40 INFO StagingCommitter: Starting: Task committer attempt_202105141401387190151549314020373_0004_m_000089_377: needsTaskCommit() Task attempt_202105141401387190151549314020373_0004_m_000089_377
[2021-05-14 11:04:40,115] {docker.py:276} INFO - 21/05/14 14:04:40 INFO StagingCommitter: Task committer attempt_202105141401387190151549314020373_0004_m_000089_377: needsTaskCommit() Task attempt_202105141401387190151549314020373_0004_m_000089_377: duration 0:00.004s
[2021-05-14 11:04:40,115] {docker.py:276} INFO - 21/05/14 14:04:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387190151549314020373_0004_m_000089_377
[2021-05-14 11:04:40,117] {docker.py:276} INFO - 21/05/14 14:04:40 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 5106 bytes result sent to driver
[2021-05-14 11:04:40,119] {docker.py:276} INFO - 21/05/14 14:04:40 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 9348 ms on 3e4cb2948233 (executor driver) (91/200)
[2021-05-14 11:04:40,120] {docker.py:276} INFO - 21/05/14 14:04:40 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (3e4cb2948233, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:40,121] {docker.py:276} INFO - 21/05/14 14:04:40 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-14 11:04:40,130] {docker.py:276} INFO - 21/05/14 14:04:40 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:40,139] {docker.py:276} INFO - 21/05/14 14:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387662355217087825034_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387662355217087825034_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387662355217087825034_0004}; taskId=attempt_202105141401387662355217087825034_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46251fc2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:40,139] {docker.py:276} INFO - 21/05/14 14:04:40 INFO StagingCommitter: Starting: Task committer attempt_202105141401387662355217087825034_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387662355217087825034_0004_m_000094_382
[2021-05-14 11:04:40,143] {docker.py:276} INFO - 21/05/14 14:04:40 INFO StagingCommitter: Task committer attempt_202105141401387662355217087825034_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387662355217087825034_0004_m_000094_382 : duration 0:00.004s
[2021-05-14 11:04:41,857] {docker.py:276} INFO - 21/05/14 14:04:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401387474126519271355217_0004_m_000091_379: needsTaskCommit() Task attempt_202105141401387474126519271355217_0004_m_000091_379
[2021-05-14 11:04:41,857] {docker.py:276} INFO - 21/05/14 14:04:41 INFO StagingCommitter: Task committer attempt_202105141401387474126519271355217_0004_m_000091_379: needsTaskCommit() Task attempt_202105141401387474126519271355217_0004_m_000091_379: duration 0:00.002s
[2021-05-14 11:04:41,858] {docker.py:276} INFO - 21/05/14 14:04:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387474126519271355217_0004_m_000091_379
[2021-05-14 11:04:41,860] {docker.py:276} INFO - 21/05/14 14:04:41 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 5149 bytes result sent to driver
[2021-05-14 11:04:41,862] {docker.py:276} INFO - 21/05/14 14:04:41 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (3e4cb2948233, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:41,863] {docker.py:276} INFO - 21/05/14 14:04:41 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-14 11:04:41,863] {docker.py:276} INFO - 21/05/14 14:04:41 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 6328 ms on 3e4cb2948233 (executor driver) (92/200)
[2021-05-14 11:04:41,872] {docker.py:276} INFO - 21/05/14 14:04:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:41,881] {docker.py:276} INFO - 21/05/14 14:04:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:04:41,882] {docker.py:276} INFO - 21/05/14 14:04:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:41,882] {docker.py:276} INFO - 21/05/14 14:04:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383011064196524505517_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383011064196524505517_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383011064196524505517_0004}; taskId=attempt_202105141401383011064196524505517_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11ab7b7a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:41,882] {docker.py:276} INFO - 21/05/14 14:04:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:41,883] {docker.py:276} INFO - 21/05/14 14:04:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401383011064196524505517_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383011064196524505517_0004_m_000095_383
[2021-05-14 11:04:41,889] {docker.py:276} INFO - 21/05/14 14:04:41 INFO StagingCommitter: Task committer attempt_202105141401383011064196524505517_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383011064196524505517_0004_m_000095_383 : duration 0:00.007s
[2021-05-14 11:04:46,539] {docker.py:276} INFO - 21/05/14 14:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401389021970427615194854_0004_m_000092_380: needsTaskCommit() Task attempt_202105141401389021970427615194854_0004_m_000092_380
[2021-05-14 11:04:46,540] {docker.py:276} INFO - 21/05/14 14:04:46 INFO StagingCommitter: Task committer attempt_202105141401389021970427615194854_0004_m_000092_380: needsTaskCommit() Task attempt_202105141401389021970427615194854_0004_m_000092_380: duration 0:00.003s
21/05/14 14:04:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401389021970427615194854_0004_m_000092_380
[2021-05-14 11:04:46,542] {docker.py:276} INFO - 21/05/14 14:04:46 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 5149 bytes result sent to driver
[2021-05-14 11:04:46,543] {docker.py:276} INFO - 21/05/14 14:04:46 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (3e4cb2948233, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:46,544] {docker.py:276} INFO - 21/05/14 14:04:46 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 10219 ms on 3e4cb2948233 (executor driver) (93/200)
[2021-05-14 11:04:46,545] {docker.py:276} INFO - 21/05/14 14:04:46 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-14 11:04:46,555] {docker.py:276} INFO - 21/05/14 14:04:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:46,563] {docker.py:276} INFO - 21/05/14 14:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:46,563] {docker.py:276} INFO - 21/05/14 14:04:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388909632011475747251_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388909632011475747251_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388909632011475747251_0004}; taskId=attempt_202105141401388909632011475747251_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@682704e0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:46,563] {docker.py:276} INFO - 21/05/14 14:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401388909632011475747251_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388909632011475747251_0004_m_000096_384
[2021-05-14 11:04:46,567] {docker.py:276} INFO - 21/05/14 14:04:46 INFO StagingCommitter: Task committer attempt_202105141401388909632011475747251_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388909632011475747251_0004_m_000096_384 : duration 0:00.004s
[2021-05-14 11:04:47,953] {docker.py:276} INFO - 21/05/14 14:04:47 INFO StagingCommitter: Starting: Task committer attempt_202105141401388735270826254006456_0004_m_000093_381: needsTaskCommit() Task attempt_202105141401388735270826254006456_0004_m_000093_381
[2021-05-14 11:04:47,954] {docker.py:276} INFO - 21/05/14 14:04:47 INFO StagingCommitter: Task committer attempt_202105141401388735270826254006456_0004_m_000093_381: needsTaskCommit() Task attempt_202105141401388735270826254006456_0004_m_000093_381: duration 0:00.003s
21/05/14 14:04:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388735270826254006456_0004_m_000093_381
[2021-05-14 11:04:47,956] {docker.py:276} INFO - 21/05/14 14:04:47 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 5149 bytes result sent to driver
[2021-05-14 11:04:47,957] {docker.py:276} INFO - 21/05/14 14:04:47 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (3e4cb2948233, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:47,959] {docker.py:276} INFO - 21/05/14 14:04:47 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 10892 ms on 3e4cb2948233 (executor driver) (94/200)
[2021-05-14 11:04:47,960] {docker.py:276} INFO - 21/05/14 14:04:47 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
[2021-05-14 11:04:47,970] {docker.py:276} INFO - 21/05/14 14:04:47 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:47,970] {docker.py:276} INFO - 21/05/14 14:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:47,979] {docker.py:276} INFO - 21/05/14 14:04:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:04:47,979] {docker.py:276} INFO - 21/05/14 14:04:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:47,979] {docker.py:276} INFO - 21/05/14 14:04:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138681796278644466419_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138681796278644466419_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138681796278644466419_0004}; taskId=attempt_20210514140138681796278644466419_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c260b11}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:47,980] {docker.py:276} INFO - 21/05/14 14:04:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:47,980] {docker.py:276} INFO - 21/05/14 14:04:47 INFO StagingCommitter: Starting: Task committer attempt_20210514140138681796278644466419_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138681796278644466419_0004_m_000097_385
[2021-05-14 11:04:47,984] {docker.py:276} INFO - 21/05/14 14:04:47 INFO StagingCommitter: Task committer attempt_20210514140138681796278644466419_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138681796278644466419_0004_m_000097_385 : duration 0:00.004s
[2021-05-14 11:04:48,720] {docker.py:276} INFO - 21/05/14 14:04:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401387662355217087825034_0004_m_000094_382: needsTaskCommit() Task attempt_202105141401387662355217087825034_0004_m_000094_382
[2021-05-14 11:04:48,721] {docker.py:276} INFO - 21/05/14 14:04:48 INFO StagingCommitter: Task committer attempt_202105141401387662355217087825034_0004_m_000094_382: needsTaskCommit() Task attempt_202105141401387662355217087825034_0004_m_000094_382: duration 0:00.002s
21/05/14 14:04:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387662355217087825034_0004_m_000094_382
[2021-05-14 11:04:48,722] {docker.py:276} INFO - 21/05/14 14:04:48 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 5149 bytes result sent to driver
[2021-05-14 11:04:48,724] {docker.py:276} INFO - 21/05/14 14:04:48 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (3e4cb2948233, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:48,726] {docker.py:276} INFO - 21/05/14 14:04:48 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 8580 ms on 3e4cb2948233 (executor driver) (95/200)
21/05/14 14:04:48 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-14 11:04:48,736] {docker.py:276} INFO - 21/05/14 14:04:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:48,745] {docker.py:276} INFO - 21/05/14 14:04:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381368871157994955807_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381368871157994955807_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381368871157994955807_0004}; taskId=attempt_202105141401381368871157994955807_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1da9d960}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401381368871157994955807_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381368871157994955807_0004_m_000098_386
[2021-05-14 11:04:48,750] {docker.py:276} INFO - 21/05/14 14:04:48 INFO StagingCommitter: Task committer attempt_202105141401381368871157994955807_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381368871157994955807_0004_m_000098_386 : duration 0:00.004s
[2021-05-14 11:04:51,745] {docker.py:276} INFO - 21/05/14 14:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401383011064196524505517_0004_m_000095_383: needsTaskCommit() Task attempt_202105141401383011064196524505517_0004_m_000095_383
[2021-05-14 11:04:51,746] {docker.py:276} INFO - 21/05/14 14:04:51 INFO StagingCommitter: Task committer attempt_202105141401383011064196524505517_0004_m_000095_383: needsTaskCommit() Task attempt_202105141401383011064196524505517_0004_m_000095_383: duration 0:00.003s
21/05/14 14:04:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383011064196524505517_0004_m_000095_383
[2021-05-14 11:04:51,749] {docker.py:276} INFO - 21/05/14 14:04:51 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 5106 bytes result sent to driver
[2021-05-14 11:04:51,750] {docker.py:276} INFO - 21/05/14 14:04:51 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (3e4cb2948233, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:51,751] {docker.py:276} INFO - 21/05/14 14:04:51 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 9901 ms on 3e4cb2948233 (executor driver) (96/200)
[2021-05-14 11:04:51,752] {docker.py:276} INFO - 21/05/14 14:04:51 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-14 11:04:51,760] {docker.py:276} INFO - 21/05/14 14:04:51 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:51,769] {docker.py:276} INFO - 21/05/14 14:04:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:51,770] {docker.py:276} INFO - 21/05/14 14:04:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387364070999283080690_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387364070999283080690_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387364070999283080690_0004}; taskId=attempt_202105141401387364070999283080690_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d8f4d55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:51,770] {docker.py:276} INFO - 21/05/14 14:04:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:51,771] {docker.py:276} INFO - 21/05/14 14:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401387364070999283080690_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387364070999283080690_0004_m_000099_387
[2021-05-14 11:04:51,774] {docker.py:276} INFO - 21/05/14 14:04:51 INFO StagingCommitter: Task committer attempt_202105141401387364070999283080690_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387364070999283080690_0004_m_000099_387 : duration 0:00.004s
[2021-05-14 11:04:54,222] {docker.py:276} INFO - 21/05/14 14:04:54 INFO StagingCommitter: Starting: Task committer attempt_202105141401388909632011475747251_0004_m_000096_384: needsTaskCommit() Task attempt_202105141401388909632011475747251_0004_m_000096_384
[2021-05-14 11:04:54,223] {docker.py:276} INFO - 21/05/14 14:04:54 INFO StagingCommitter: Task committer attempt_202105141401388909632011475747251_0004_m_000096_384: needsTaskCommit() Task attempt_202105141401388909632011475747251_0004_m_000096_384: duration 0:00.002s
21/05/14 14:04:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388909632011475747251_0004_m_000096_384
[2021-05-14 11:04:54,226] {docker.py:276} INFO - 21/05/14 14:04:54 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 5106 bytes result sent to driver
[2021-05-14 11:04:54,227] {docker.py:276} INFO - 21/05/14 14:04:54 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (3e4cb2948233, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:54,229] {docker.py:276} INFO - 21/05/14 14:04:54 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
[2021-05-14 11:04:54,229] {docker.py:276} INFO - 21/05/14 14:04:54 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 7695 ms on 3e4cb2948233 (executor driver) (97/200)
[2021-05-14 11:04:54,239] {docker.py:276} INFO - 21/05/14 14:04:54 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:54,247] {docker.py:276} INFO - 21/05/14 14:04:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387968992801719173334_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387968992801719173334_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387968992801719173334_0004}; taskId=attempt_202105141401387968992801719173334_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@729acefd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:54,247] {docker.py:276} INFO - 21/05/14 14:04:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:54 INFO StagingCommitter: Starting: Task committer attempt_202105141401387968992801719173334_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387968992801719173334_0004_m_000100_388
[2021-05-14 11:04:54,252] {docker.py:276} INFO - 21/05/14 14:04:54 INFO StagingCommitter: Task committer attempt_202105141401387968992801719173334_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387968992801719173334_0004_m_000100_388 : duration 0:00.005s
[2021-05-14 11:04:57,730] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Starting: Task committer attempt_20210514140138681796278644466419_0004_m_000097_385: needsTaskCommit() Task attempt_20210514140138681796278644466419_0004_m_000097_385
[2021-05-14 11:04:57,731] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Task committer attempt_20210514140138681796278644466419_0004_m_000097_385: needsTaskCommit() Task attempt_20210514140138681796278644466419_0004_m_000097_385: duration 0:00.003s
21/05/14 14:04:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138681796278644466419_0004_m_000097_385
[2021-05-14 11:04:57,733] {docker.py:276} INFO - 21/05/14 14:04:57 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 5106 bytes result sent to driver
[2021-05-14 11:04:57,734] {docker.py:276} INFO - 21/05/14 14:04:57 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (3e4cb2948233, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:57,735] {docker.py:276} INFO - 21/05/14 14:04:57 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-14 11:04:57,736] {docker.py:276} INFO - 21/05/14 14:04:57 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 9789 ms on 3e4cb2948233 (executor driver) (98/200)
[2021-05-14 11:04:57,746] {docker.py:276} INFO - 21/05/14 14:04:57 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:04:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:04:57,747] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401381368871157994955807_0004_m_000098_386: needsTaskCommit() Task attempt_202105141401381368871157994955807_0004_m_000098_386
[2021-05-14 11:04:57,748] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Task committer attempt_202105141401381368871157994955807_0004_m_000098_386: needsTaskCommit() Task attempt_202105141401381368871157994955807_0004_m_000098_386: duration 0:00.001s
21/05/14 14:04:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381368871157994955807_0004_m_000098_386
[2021-05-14 11:04:57,749] {docker.py:276} INFO - 21/05/14 14:04:57 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 5106 bytes result sent to driver
[2021-05-14 11:04:57,750] {docker.py:276} INFO - 21/05/14 14:04:57 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (3e4cb2948233, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:04:57,750] {docker.py:276} INFO - 21/05/14 14:04:57 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-14 11:04:57,751] {docker.py:276} INFO - 21/05/14 14:04:57 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 9038 ms on 3e4cb2948233 (executor driver) (99/200)
[2021-05-14 11:04:57,758] {docker.py:276} INFO - 21/05/14 14:04:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:04:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381734029835322066382_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381734029835322066382_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381734029835322066382_0004}; taskId=attempt_202105141401381734029835322066382_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@dcb6a76}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401381734029835322066382_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381734029835322066382_0004_m_000101_389
[2021-05-14 11:04:57,761] {docker.py:276} INFO - 21/05/14 14:04:57 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:04:57,761] {docker.py:276} INFO - 21/05/14 14:04:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:04:57,764] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Task committer attempt_202105141401381734029835322066382_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381734029835322066382_0004_m_000101_389 : duration 0:00.007s
[2021-05-14 11:04:57,772] {docker.py:276} INFO - 21/05/14 14:04:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:04:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:04:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:04:57,773] {docker.py:276} INFO - 21/05/14 14:04:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384343166145025038823_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384343166145025038823_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384343166145025038823_0004}; taskId=attempt_202105141401384343166145025038823_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@fca7cb9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:04:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:04:57,773] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105141401384343166145025038823_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384343166145025038823_0004_m_000102_390
[2021-05-14 11:04:57,778] {docker.py:276} INFO - 21/05/14 14:04:57 INFO StagingCommitter: Task committer attempt_202105141401384343166145025038823_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384343166145025038823_0004_m_000102_390 : duration 0:00.005s
[2021-05-14 11:05:03,989] {docker.py:276} INFO - 21/05/14 14:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401387364070999283080690_0004_m_000099_387: needsTaskCommit() Task attempt_202105141401387364070999283080690_0004_m_000099_387
21/05/14 14:05:04 INFO StagingCommitter: Task committer attempt_202105141401387364070999283080690_0004_m_000099_387: needsTaskCommit() Task attempt_202105141401387364070999283080690_0004_m_000099_387: duration 0:00.001s
21/05/14 14:05:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387364070999283080690_0004_m_000099_387
[2021-05-14 11:05:03,991] {docker.py:276} INFO - 21/05/14 14:05:04 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 5106 bytes result sent to driver
[2021-05-14 11:05:03,992] {docker.py:276} INFO - 21/05/14 14:05:04 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (3e4cb2948233, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:03,993] {docker.py:276} INFO - 21/05/14 14:05:04 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-14 11:05:03,993] {docker.py:276} INFO - 21/05/14 14:05:04 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 12258 ms on 3e4cb2948233 (executor driver) (100/200)
[2021-05-14 11:05:04,002] {docker.py:276} INFO - 21/05/14 14:05:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:04,010] {docker.py:276} INFO - 21/05/14 14:05:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:04,010] {docker.py:276} INFO - 21/05/14 14:05:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388794635424621346579_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388794635424621346579_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388794635424621346579_0004}; taskId=attempt_202105141401388794635424621346579_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@959004b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401388794635424621346579_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388794635424621346579_0004_m_000103_391
[2021-05-14 11:05:04,014] {docker.py:276} INFO - 21/05/14 14:05:04 INFO StagingCommitter: Task committer attempt_202105141401388794635424621346579_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388794635424621346579_0004_m_000103_391 : duration 0:00.005s
[2021-05-14 11:05:06,416] {docker.py:276} INFO - 21/05/14 14:05:06 INFO StagingCommitter: Starting: Task committer attempt_202105141401387968992801719173334_0004_m_000100_388: needsTaskCommit() Task attempt_202105141401387968992801719173334_0004_m_000100_388
[2021-05-14 11:05:06,417] {docker.py:276} INFO - 21/05/14 14:05:06 INFO StagingCommitter: Task committer attempt_202105141401387968992801719173334_0004_m_000100_388: needsTaskCommit() Task attempt_202105141401387968992801719173334_0004_m_000100_388: duration 0:00.003s
21/05/14 14:05:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387968992801719173334_0004_m_000100_388
[2021-05-14 11:05:06,420] {docker.py:276} INFO - 21/05/14 14:05:06 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 5106 bytes result sent to driver
[2021-05-14 11:05:06,422] {docker.py:276} INFO - 21/05/14 14:05:06 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (3e4cb2948233, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:06,423] {docker.py:276} INFO - 21/05/14 14:05:06 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 12210 ms on 3e4cb2948233 (executor driver) (101/200)
[2021-05-14 11:05:06,423] {docker.py:276} INFO - 21/05/14 14:05:06 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
[2021-05-14 11:05:06,433] {docker.py:276} INFO - 21/05/14 14:05:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:05:06,433] {docker.py:276} INFO - 21/05/14 14:05:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:06,441] {docker.py:276} INFO - 21/05/14 14:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:05:06,442] {docker.py:276} INFO - 21/05/14 14:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:06,442] {docker.py:276} INFO - 21/05/14 14:05:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387627339290995479069_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387627339290995479069_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387627339290995479069_0004}; taskId=attempt_202105141401387627339290995479069_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@254c64aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:06,442] {docker.py:276} INFO - 21/05/14 14:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:06 INFO StagingCommitter: Starting: Task committer attempt_202105141401387627339290995479069_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387627339290995479069_0004_m_000104_392
[2021-05-14 11:05:06,447] {docker.py:276} INFO - 21/05/14 14:05:06 INFO StagingCommitter: Task committer attempt_202105141401387627339290995479069_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387627339290995479069_0004_m_000104_392 : duration 0:00.005s
[2021-05-14 11:05:09,217] {docker.py:276} INFO - 21/05/14 14:05:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401384343166145025038823_0004_m_000102_390: needsTaskCommit() Task attempt_202105141401384343166145025038823_0004_m_000102_390
[2021-05-14 11:05:09,219] {docker.py:276} INFO - 21/05/14 14:05:09 INFO StagingCommitter: Task committer attempt_202105141401384343166145025038823_0004_m_000102_390: needsTaskCommit() Task attempt_202105141401384343166145025038823_0004_m_000102_390: duration 0:00.005s
21/05/14 14:05:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384343166145025038823_0004_m_000102_390
[2021-05-14 11:05:09,221] {docker.py:276} INFO - 21/05/14 14:05:09 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 5149 bytes result sent to driver
[2021-05-14 11:05:09,223] {docker.py:276} INFO - 21/05/14 14:05:09 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (3e4cb2948233, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:09,224] {docker.py:276} INFO - 21/05/14 14:05:09 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 11486 ms on 3e4cb2948233 (executor driver) (102/200)
[2021-05-14 11:05:09,224] {docker.py:276} INFO - 21/05/14 14:05:09 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-14 11:05:09,233] {docker.py:276} INFO - 21/05/14 14:05:09 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:09,241] {docker.py:276} INFO - 21/05/14 14:05:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381733040855067602431_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381733040855067602431_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381733040855067602431_0004}; taskId=attempt_202105141401381733040855067602431_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6773c034}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:09,241] {docker.py:276} INFO - 21/05/14 14:05:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401381733040855067602431_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381733040855067602431_0004_m_000105_393
[2021-05-14 11:05:09,245] {docker.py:276} INFO - 21/05/14 14:05:09 INFO StagingCommitter: Task committer attempt_202105141401381733040855067602431_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381733040855067602431_0004_m_000105_393 : duration 0:00.003s
[2021-05-14 11:05:15,478] {docker.py:276} INFO - 21/05/14 14:05:15 INFO StagingCommitter: Starting: Task committer attempt_202105141401381734029835322066382_0004_m_000101_389: needsTaskCommit() Task attempt_202105141401381734029835322066382_0004_m_000101_389
[2021-05-14 11:05:15,479] {docker.py:276} INFO - 21/05/14 14:05:15 INFO StagingCommitter: Task committer attempt_202105141401381734029835322066382_0004_m_000101_389: needsTaskCommit() Task attempt_202105141401381734029835322066382_0004_m_000101_389: duration 0:00.002s
21/05/14 14:05:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381734029835322066382_0004_m_000101_389
[2021-05-14 11:05:15,482] {docker.py:276} INFO - 21/05/14 14:05:15 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 5149 bytes result sent to driver
[2021-05-14 11:05:15,483] {docker.py:276} INFO - 21/05/14 14:05:15 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (3e4cb2948233, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:15,484] {docker.py:276} INFO - 21/05/14 14:05:15 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 17737 ms on 3e4cb2948233 (executor driver) (103/200)
[2021-05-14 11:05:15,485] {docker.py:276} INFO - 21/05/14 14:05:15 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-14 11:05:15,496] {docker.py:276} INFO - 21/05/14 14:05:15 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:15,504] {docker.py:276} INFO - 21/05/14 14:05:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:05:15,504] {docker.py:276} INFO - 21/05/14 14:05:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401389193977830507599799_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389193977830507599799_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401389193977830507599799_0004}; taskId=attempt_202105141401389193977830507599799_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52df4aa2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:15 INFO StagingCommitter: Starting: Task committer attempt_202105141401389193977830507599799_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389193977830507599799_0004_m_000106_394
[2021-05-14 11:05:15,508] {docker.py:276} INFO - 21/05/14 14:05:15 INFO StagingCommitter: Task committer attempt_202105141401389193977830507599799_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389193977830507599799_0004_m_000106_394 : duration 0:00.004s
[2021-05-14 11:05:16,634] {docker.py:276} INFO - 21/05/14 14:05:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401387627339290995479069_0004_m_000104_392: needsTaskCommit() Task attempt_202105141401387627339290995479069_0004_m_000104_392
[2021-05-14 11:05:16,634] {docker.py:276} INFO - 21/05/14 14:05:16 INFO StagingCommitter: Task committer attempt_202105141401387627339290995479069_0004_m_000104_392: needsTaskCommit() Task attempt_202105141401387627339290995479069_0004_m_000104_392: duration 0:00.001s
21/05/14 14:05:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387627339290995479069_0004_m_000104_392
[2021-05-14 11:05:16,635] {docker.py:276} INFO - 21/05/14 14:05:16 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 5149 bytes result sent to driver
[2021-05-14 11:05:16,636] {docker.py:276} INFO - 21/05/14 14:05:16 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (3e4cb2948233, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:16,638] {docker.py:276} INFO - 21/05/14 14:05:16 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
21/05/14 14:05:16 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 10194 ms on 3e4cb2948233 (executor driver) (104/200)
[2021-05-14 11:05:16,646] {docker.py:276} INFO - 21/05/14 14:05:16 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:16,657] {docker.py:276} INFO - 21/05/14 14:05:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388319310910610365182_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388319310910610365182_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388319310910610365182_0004}; taskId=attempt_202105141401388319310910610365182_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b9f57d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:16,657] {docker.py:276} INFO - 21/05/14 14:05:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401388319310910610365182_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388319310910610365182_0004_m_000107_395
[2021-05-14 11:05:16,661] {docker.py:276} INFO - 21/05/14 14:05:16 INFO StagingCommitter: Task committer attempt_202105141401388319310910610365182_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388319310910610365182_0004_m_000107_395 : duration 0:00.004s
[2021-05-14 11:05:19,628] {docker.py:276} INFO - 21/05/14 14:05:19 INFO StagingCommitter: Starting: Task committer attempt_202105141401381733040855067602431_0004_m_000105_393: needsTaskCommit() Task attempt_202105141401381733040855067602431_0004_m_000105_393
[2021-05-14 11:05:19,629] {docker.py:276} INFO - 21/05/14 14:05:19 INFO StagingCommitter: Task committer attempt_202105141401381733040855067602431_0004_m_000105_393: needsTaskCommit() Task attempt_202105141401381733040855067602431_0004_m_000105_393: duration 0:00.002s
21/05/14 14:05:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381733040855067602431_0004_m_000105_393
[2021-05-14 11:05:19,631] {docker.py:276} INFO - 21/05/14 14:05:19 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 5106 bytes result sent to driver
[2021-05-14 11:05:19,632] {docker.py:276} INFO - 21/05/14 14:05:19 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (3e4cb2948233, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:19,633] {docker.py:276} INFO - 21/05/14 14:05:19 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
[2021-05-14 11:05:19,634] {docker.py:276} INFO - 21/05/14 14:05:19 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 10389 ms on 3e4cb2948233 (executor driver) (105/200)
[2021-05-14 11:05:19,645] {docker.py:276} INFO - 21/05/14 14:05:19 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:19,653] {docker.py:276} INFO - 21/05/14 14:05:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383029286724579786144_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383029286724579786144_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383029286724579786144_0004}; taskId=attempt_202105141401383029286724579786144_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4124b34c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:19,653] {docker.py:276} INFO - 21/05/14 14:05:19 INFO StagingCommitter: Starting: Task committer attempt_202105141401383029286724579786144_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383029286724579786144_0004_m_000108_396
[2021-05-14 11:05:19,657] {docker.py:276} INFO - 21/05/14 14:05:19 INFO StagingCommitter: Task committer attempt_202105141401383029286724579786144_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383029286724579786144_0004_m_000108_396 : duration 0:00.004s
[2021-05-14 11:05:21,569] {docker.py:276} INFO - 21/05/14 14:05:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401388794635424621346579_0004_m_000103_391: needsTaskCommit() Task attempt_202105141401388794635424621346579_0004_m_000103_391
[2021-05-14 11:05:21,570] {docker.py:276} INFO - 21/05/14 14:05:21 INFO StagingCommitter: Task committer attempt_202105141401388794635424621346579_0004_m_000103_391: needsTaskCommit() Task attempt_202105141401388794635424621346579_0004_m_000103_391: duration 0:00.002s
21/05/14 14:05:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388794635424621346579_0004_m_000103_391
[2021-05-14 11:05:21,572] {docker.py:276} INFO - 21/05/14 14:05:21 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 5149 bytes result sent to driver
[2021-05-14 11:05:21,573] {docker.py:276} INFO - 21/05/14 14:05:21 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (3e4cb2948233, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:21,574] {docker.py:276} INFO - 21/05/14 14:05:21 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
21/05/14 14:05:21 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 17568 ms on 3e4cb2948233 (executor driver) (106/200)
[2021-05-14 11:05:21,583] {docker.py:276} INFO - 21/05/14 14:05:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:21,592] {docker.py:276} INFO - 21/05/14 14:05:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:21,593] {docker.py:276} INFO - 21/05/14 14:05:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387783288856277671756_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387783288856277671756_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387783288856277671756_0004}; taskId=attempt_202105141401387783288856277671756_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33aae9c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:21,593] {docker.py:276} INFO - 21/05/14 14:05:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401387783288856277671756_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387783288856277671756_0004_m_000109_397
[2021-05-14 11:05:21,597] {docker.py:276} INFO - 21/05/14 14:05:21 INFO StagingCommitter: Task committer attempt_202105141401387783288856277671756_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387783288856277671756_0004_m_000109_397 : duration 0:00.005s
[2021-05-14 11:05:22,438] {docker.py:276} INFO - 21/05/14 14:05:22 INFO StagingCommitter: Starting: Task committer attempt_202105141401389193977830507599799_0004_m_000106_394: needsTaskCommit() Task attempt_202105141401389193977830507599799_0004_m_000106_394
[2021-05-14 11:05:22,439] {docker.py:276} INFO - 21/05/14 14:05:22 INFO StagingCommitter: Task committer attempt_202105141401389193977830507599799_0004_m_000106_394: needsTaskCommit() Task attempt_202105141401389193977830507599799_0004_m_000106_394: duration 0:00.003s
[2021-05-14 11:05:22,440] {docker.py:276} INFO - 21/05/14 14:05:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401389193977830507599799_0004_m_000106_394
[2021-05-14 11:05:22,442] {docker.py:276} INFO - 21/05/14 14:05:22 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 5106 bytes result sent to driver
[2021-05-14 11:05:22,443] {docker.py:276} INFO - 21/05/14 14:05:22 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (3e4cb2948233, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:22,444] {docker.py:276} INFO - 21/05/14 14:05:22 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-14 11:05:22,445] {docker.py:276} INFO - 21/05/14 14:05:22 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 6970 ms on 3e4cb2948233 (executor driver) (107/200)
[2021-05-14 11:05:22,454] {docker.py:276} INFO - 21/05/14 14:05:22 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:22,463] {docker.py:276} INFO - 21/05/14 14:05:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:22,463] {docker.py:276} INFO - 21/05/14 14:05:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385938056533476156176_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385938056533476156176_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385938056533476156176_0004}; taskId=attempt_202105141401385938056533476156176_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a7cbfbc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:22,464] {docker.py:276} INFO - 21/05/14 14:05:22 INFO StagingCommitter: Starting: Task committer attempt_202105141401385938056533476156176_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385938056533476156176_0004_m_000110_398
[2021-05-14 11:05:22,467] {docker.py:276} INFO - 21/05/14 14:05:22 INFO StagingCommitter: Task committer attempt_202105141401385938056533476156176_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385938056533476156176_0004_m_000110_398 : duration 0:00.005s
[2021-05-14 11:05:23,660] {docker.py:276} INFO - 21/05/14 14:05:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401388319310910610365182_0004_m_000107_395: needsTaskCommit() Task attempt_202105141401388319310910610365182_0004_m_000107_395
[2021-05-14 11:05:23,660] {docker.py:276} INFO - 21/05/14 14:05:23 INFO StagingCommitter: Task committer attempt_202105141401388319310910610365182_0004_m_000107_395: needsTaskCommit() Task attempt_202105141401388319310910610365182_0004_m_000107_395: duration 0:00.001s
21/05/14 14:05:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388319310910610365182_0004_m_000107_395
[2021-05-14 11:05:23,661] {docker.py:276} INFO - 21/05/14 14:05:23 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 5106 bytes result sent to driver
[2021-05-14 11:05:23,663] {docker.py:276} INFO - 21/05/14 14:05:23 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (3e4cb2948233, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:23,663] {docker.py:276} INFO - 21/05/14 14:05:23 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-14 11:05:23,664] {docker.py:276} INFO - 21/05/14 14:05:23 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 7036 ms on 3e4cb2948233 (executor driver) (108/200)
[2021-05-14 11:05:23,673] {docker.py:276} INFO - 21/05/14 14:05:23 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:23,681] {docker.py:276} INFO - 21/05/14 14:05:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382019739137226410018_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382019739137226410018_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382019739137226410018_0004}; taskId=attempt_202105141401382019739137226410018_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69142e57}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:23,681] {docker.py:276} INFO - 21/05/14 14:05:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401382019739137226410018_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382019739137226410018_0004_m_000111_399
[2021-05-14 11:05:23,685] {docker.py:276} INFO - 21/05/14 14:05:23 INFO StagingCommitter: Task committer attempt_202105141401382019739137226410018_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382019739137226410018_0004_m_000111_399 : duration 0:00.004s
[2021-05-14 11:05:26,436] {docker.py:276} INFO - 21/05/14 14:05:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401383029286724579786144_0004_m_000108_396: needsTaskCommit() Task attempt_202105141401383029286724579786144_0004_m_000108_396
[2021-05-14 11:05:26,444] {docker.py:276} INFO - 21/05/14 14:05:26 INFO StagingCommitter: Task committer attempt_202105141401383029286724579786144_0004_m_000108_396: needsTaskCommit() Task attempt_202105141401383029286724579786144_0004_m_000108_396: duration 0:00.004s
21/05/14 14:05:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383029286724579786144_0004_m_000108_396
[2021-05-14 11:05:26,446] {docker.py:276} INFO - 21/05/14 14:05:26 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 5106 bytes result sent to driver
[2021-05-14 11:05:26,449] {docker.py:276} INFO - 21/05/14 14:05:26 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (3e4cb2948233, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:26,450] {docker.py:276} INFO - 21/05/14 14:05:26 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
21/05/14 14:05:26 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 6824 ms on 3e4cb2948233 (executor driver) (109/200)
[2021-05-14 11:05:26,459] {docker.py:276} INFO - 21/05/14 14:05:26 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:26,467] {docker.py:276} INFO - 21/05/14 14:05:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387380104688038777164_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387380104688038777164_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387380104688038777164_0004}; taskId=attempt_202105141401387380104688038777164_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74271ded}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401387380104688038777164_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387380104688038777164_0004_m_000112_400
[2021-05-14 11:05:26,473] {docker.py:276} INFO - 21/05/14 14:05:26 INFO StagingCommitter: Task committer attempt_202105141401387380104688038777164_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387380104688038777164_0004_m_000112_400 : duration 0:00.005s
[2021-05-14 11:05:28,978] {docker.py:276} INFO - 21/05/14 14:05:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401387783288856277671756_0004_m_000109_397: needsTaskCommit() Task attempt_202105141401387783288856277671756_0004_m_000109_397
[2021-05-14 11:05:28,979] {docker.py:276} INFO - 21/05/14 14:05:28 INFO StagingCommitter: Task committer attempt_202105141401387783288856277671756_0004_m_000109_397: needsTaskCommit() Task attempt_202105141401387783288856277671756_0004_m_000109_397: duration 0:00.002s
21/05/14 14:05:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387783288856277671756_0004_m_000109_397
[2021-05-14 11:05:28,980] {docker.py:276} INFO - 21/05/14 14:05:28 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 5106 bytes result sent to driver
[2021-05-14 11:05:28,981] {docker.py:276} INFO - 21/05/14 14:05:29 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (3e4cb2948233, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:28,982] {docker.py:276} INFO - 21/05/14 14:05:29 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 7417 ms on 3e4cb2948233 (executor driver) (110/200)
21/05/14 14:05:29 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-14 11:05:28,992] {docker.py:276} INFO - 21/05/14 14:05:29 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:29,001] {docker.py:276} INFO - 21/05/14 14:05:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:29,001] {docker.py:276} INFO - 21/05/14 14:05:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381690906102751453826_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381690906102751453826_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381690906102751453826_0004}; taskId=attempt_202105141401381690906102751453826_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55f0c0af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:29 INFO StagingCommitter: Starting: Task committer attempt_202105141401381690906102751453826_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381690906102751453826_0004_m_000113_401
[2021-05-14 11:05:29,006] {docker.py:276} INFO - 21/05/14 14:05:29 INFO StagingCommitter: Task committer attempt_202105141401381690906102751453826_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381690906102751453826_0004_m_000113_401 : duration 0:00.004s
[2021-05-14 11:05:31,730] {docker.py:276} INFO - 21/05/14 14:05:31 INFO StagingCommitter: Starting: Task committer attempt_202105141401385938056533476156176_0004_m_000110_398: needsTaskCommit() Task attempt_202105141401385938056533476156176_0004_m_000110_398
21/05/14 14:05:31 INFO StagingCommitter: Task committer attempt_202105141401385938056533476156176_0004_m_000110_398: needsTaskCommit() Task attempt_202105141401385938056533476156176_0004_m_000110_398: duration 0:00.003s
21/05/14 14:05:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385938056533476156176_0004_m_000110_398
[2021-05-14 11:05:31,732] {docker.py:276} INFO - 21/05/14 14:05:31 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 5149 bytes result sent to driver
[2021-05-14 11:05:31,734] {docker.py:276} INFO - 21/05/14 14:05:31 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (3e4cb2948233, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:31,735] {docker.py:276} INFO - 21/05/14 14:05:31 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 9302 ms on 3e4cb2948233 (executor driver) (111/200)
[2021-05-14 11:05:31,736] {docker.py:276} INFO - 21/05/14 14:05:31 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-14 11:05:31,746] {docker.py:276} INFO - 21/05/14 14:05:31 INFO ShuffleBlockFetcherIterator: Getting 5 (40.8 KiB) non-empty blocks including 5 (40.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:31,754] {docker.py:276} INFO - 21/05/14 14:05:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:31,754] {docker.py:276} INFO - 21/05/14 14:05:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138705356107511122937_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138705356107511122937_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138705356107511122937_0004}; taskId=attempt_20210514140138705356107511122937_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9efa6f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:31 INFO StagingCommitter: Starting: Task committer attempt_20210514140138705356107511122937_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138705356107511122937_0004_m_000114_402
[2021-05-14 11:05:31,758] {docker.py:276} INFO - 21/05/14 14:05:31 INFO StagingCommitter: Task committer attempt_20210514140138705356107511122937_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138705356107511122937_0004_m_000114_402 : duration 0:00.004s
[2021-05-14 11:05:35,181] {docker.py:276} INFO - 21/05/14 14:05:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401382019739137226410018_0004_m_000111_399: needsTaskCommit() Task attempt_202105141401382019739137226410018_0004_m_000111_399
21/05/14 14:05:35 INFO StagingCommitter: Task committer attempt_202105141401382019739137226410018_0004_m_000111_399: needsTaskCommit() Task attempt_202105141401382019739137226410018_0004_m_000111_399: duration 0:00.004s
21/05/14 14:05:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382019739137226410018_0004_m_000111_399
[2021-05-14 11:05:35,183] {docker.py:276} INFO - 21/05/14 14:05:35 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 5149 bytes result sent to driver
[2021-05-14 11:05:35,185] {docker.py:276} INFO - 21/05/14 14:05:35 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (3e4cb2948233, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:35,186] {docker.py:276} INFO - 21/05/14 14:05:35 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
[2021-05-14 11:05:35,187] {docker.py:276} INFO - 21/05/14 14:05:35 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 11538 ms on 3e4cb2948233 (executor driver) (112/200)
[2021-05-14 11:05:35,196] {docker.py:276} INFO - 21/05/14 14:05:35 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:35,204] {docker.py:276} INFO - 21/05/14 14:05:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383597444992964925116_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383597444992964925116_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383597444992964925116_0004}; taskId=attempt_202105141401383597444992964925116_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61b34d9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401383597444992964925116_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383597444992964925116_0004_m_000115_403
[2021-05-14 11:05:35,208] {docker.py:276} INFO - 21/05/14 14:05:35 INFO StagingCommitter: Task committer attempt_202105141401383597444992964925116_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383597444992964925116_0004_m_000115_403 : duration 0:00.004s
[2021-05-14 11:05:37,519] {docker.py:276} INFO - 21/05/14 14:05:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401387380104688038777164_0004_m_000112_400: needsTaskCommit() Task attempt_202105141401387380104688038777164_0004_m_000112_400
21/05/14 14:05:37 INFO StagingCommitter: Task committer attempt_202105141401387380104688038777164_0004_m_000112_400: needsTaskCommit() Task attempt_202105141401387380104688038777164_0004_m_000112_400: duration 0:00.002s
21/05/14 14:05:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387380104688038777164_0004_m_000112_400
[2021-05-14 11:05:37,521] {docker.py:276} INFO - 21/05/14 14:05:37 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 5149 bytes result sent to driver
[2021-05-14 11:05:37,523] {docker.py:276} INFO - 21/05/14 14:05:37 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (3e4cb2948233, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:37,524] {docker.py:276} INFO - 21/05/14 14:05:37 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
21/05/14 14:05:37 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 11089 ms on 3e4cb2948233 (executor driver) (113/200)
[2021-05-14 11:05:37,534] {docker.py:276} INFO - 21/05/14 14:05:37 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:37,544] {docker.py:276} INFO - 21/05/14 14:05:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386176287553127343678_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386176287553127343678_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386176287553127343678_0004}; taskId=attempt_202105141401386176287553127343678_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7938ea63}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401386176287553127343678_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386176287553127343678_0004_m_000116_404
[2021-05-14 11:05:37,548] {docker.py:276} INFO - 21/05/14 14:05:37 INFO StagingCommitter: Task committer attempt_202105141401386176287553127343678_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386176287553127343678_0004_m_000116_404 : duration 0:00.005s
[2021-05-14 11:05:38,067] {docker.py:276} INFO - 21/05/14 14:05:38 INFO StagingCommitter: Starting: Task committer attempt_20210514140138705356107511122937_0004_m_000114_402: needsTaskCommit() Task attempt_20210514140138705356107511122937_0004_m_000114_402
[2021-05-14 11:05:38,067] {docker.py:276} INFO - 21/05/14 14:05:38 INFO StagingCommitter: Task committer attempt_20210514140138705356107511122937_0004_m_000114_402: needsTaskCommit() Task attempt_20210514140138705356107511122937_0004_m_000114_402: duration 0:00.002s
[2021-05-14 11:05:38,068] {docker.py:276} INFO - 21/05/14 14:05:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138705356107511122937_0004_m_000114_402
[2021-05-14 11:05:38,070] {docker.py:276} INFO - 21/05/14 14:05:38 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 5106 bytes result sent to driver
21/05/14 14:05:38 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (3e4cb2948233, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:38,071] {docker.py:276} INFO - 21/05/14 14:05:38 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 6345 ms on 3e4cb2948233 (executor driver) (114/200)
[2021-05-14 11:05:38,072] {docker.py:276} INFO - 21/05/14 14:05:38 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
[2021-05-14 11:05:38,082] {docker.py:276} INFO - 21/05/14 14:05:38 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:38,090] {docker.py:276} INFO - 21/05/14 14:05:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383427718016198361108_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383427718016198361108_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383427718016198361108_0004}; taskId=attempt_202105141401383427718016198361108_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21a1130}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401383427718016198361108_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383427718016198361108_0004_m_000117_405
[2021-05-14 11:05:38,094] {docker.py:276} INFO - 21/05/14 14:05:38 INFO StagingCommitter: Task committer attempt_202105141401383427718016198361108_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383427718016198361108_0004_m_000117_405 : duration 0:00.003s
[2021-05-14 11:05:38,112] {docker.py:276} INFO - 21/05/14 14:05:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401381690906102751453826_0004_m_000113_401: needsTaskCommit() Task attempt_202105141401381690906102751453826_0004_m_000113_401
[2021-05-14 11:05:38,113] {docker.py:276} INFO - 21/05/14 14:05:38 INFO StagingCommitter: Task committer attempt_202105141401381690906102751453826_0004_m_000113_401: needsTaskCommit() Task attempt_202105141401381690906102751453826_0004_m_000113_401: duration 0:00.001s
21/05/14 14:05:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381690906102751453826_0004_m_000113_401
[2021-05-14 11:05:38,115] {docker.py:276} INFO - 21/05/14 14:05:38 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 5149 bytes result sent to driver
[2021-05-14 11:05:38,116] {docker.py:276} INFO - 21/05/14 14:05:38 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (3e4cb2948233, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:38,117] {docker.py:276} INFO - 21/05/14 14:05:38 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-14 11:05:38,117] {docker.py:276} INFO - 21/05/14 14:05:38 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 9147 ms on 3e4cb2948233 (executor driver) (115/200)
[2021-05-14 11:05:38,126] {docker.py:276} INFO - 21/05/14 14:05:38 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:05:38,126] {docker.py:276} INFO - 21/05/14 14:05:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:38,135] {docker.py:276} INFO - 21/05/14 14:05:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:38,136] {docker.py:276} INFO - 21/05/14 14:05:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381233808825550289092_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381233808825550289092_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381233808825550289092_0004}; taskId=attempt_202105141401381233808825550289092_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7aaac7a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401381233808825550289092_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381233808825550289092_0004_m_000118_406
[2021-05-14 11:05:38,140] {docker.py:276} INFO - 21/05/14 14:05:38 INFO StagingCommitter: Task committer attempt_202105141401381233808825550289092_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381233808825550289092_0004_m_000118_406 : duration 0:00.004s
[2021-05-14 11:05:44,754] {docker.py:276} INFO - 21/05/14 14:05:44 INFO StagingCommitter: Starting: Task committer attempt_202105141401381233808825550289092_0004_m_000118_406: needsTaskCommit() Task attempt_202105141401381233808825550289092_0004_m_000118_406
21/05/14 14:05:44 INFO StagingCommitter: Task committer attempt_202105141401381233808825550289092_0004_m_000118_406: needsTaskCommit() Task attempt_202105141401381233808825550289092_0004_m_000118_406: duration 0:00.001s
[2021-05-14 11:05:44,755] {docker.py:276} INFO - 21/05/14 14:05:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381233808825550289092_0004_m_000118_406
[2021-05-14 11:05:44,757] {docker.py:276} INFO - 21/05/14 14:05:44 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 5106 bytes result sent to driver
[2021-05-14 11:05:44,759] {docker.py:276} INFO - 21/05/14 14:05:44 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (3e4cb2948233, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:44,764] {docker.py:276} INFO - 21/05/14 14:05:44 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
[2021-05-14 11:05:44,764] {docker.py:276} INFO - 21/05/14 14:05:44 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 6619 ms on 3e4cb2948233 (executor driver) (116/200)
[2021-05-14 11:05:44,769] {docker.py:276} INFO - 21/05/14 14:05:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:44,777] {docker.py:276} INFO - 21/05/14 14:05:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:44,777] {docker.py:276} INFO - 21/05/14 14:05:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386742362487589005147_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386742362487589005147_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386742362487589005147_0004}; taskId=attempt_202105141401386742362487589005147_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cb6517d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:44 INFO StagingCommitter: Starting: Task committer attempt_202105141401386742362487589005147_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386742362487589005147_0004_m_000119_407
[2021-05-14 11:05:44,781] {docker.py:276} INFO - 21/05/14 14:05:44 INFO StagingCommitter: Task committer attempt_202105141401386742362487589005147_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386742362487589005147_0004_m_000119_407 : duration 0:00.003s
[2021-05-14 11:05:46,504] {docker.py:276} INFO - 21/05/14 14:05:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401383597444992964925116_0004_m_000115_403: needsTaskCommit() Task attempt_202105141401383597444992964925116_0004_m_000115_403
21/05/14 14:05:46 INFO StagingCommitter: Task committer attempt_202105141401383597444992964925116_0004_m_000115_403: needsTaskCommit() Task attempt_202105141401383597444992964925116_0004_m_000115_403: duration 0:00.001s
21/05/14 14:05:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383597444992964925116_0004_m_000115_403
[2021-05-14 11:05:46,508] {docker.py:276} INFO - 21/05/14 14:05:46 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 5106 bytes result sent to driver
[2021-05-14 11:05:46,509] {docker.py:276} INFO - 21/05/14 14:05:46 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (3e4cb2948233, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:46,510] {docker.py:276} INFO - 21/05/14 14:05:46 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 11304 ms on 3e4cb2948233 (executor driver) (117/200)
[2021-05-14 11:05:46,511] {docker.py:276} INFO - 21/05/14 14:05:46 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-14 11:05:46,520] {docker.py:276} INFO - 21/05/14 14:05:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:46,529] {docker.py:276} INFO - 21/05/14 14:05:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:05:46,530] {docker.py:276} INFO - 21/05/14 14:05:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382026363690786245401_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382026363690786245401_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382026363690786245401_0004}; taskId=attempt_202105141401382026363690786245401_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19698cb7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:46,530] {docker.py:276} INFO - 21/05/14 14:05:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401382026363690786245401_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382026363690786245401_0004_m_000120_408
[2021-05-14 11:05:46,534] {docker.py:276} INFO - 21/05/14 14:05:46 INFO StagingCommitter: Task committer attempt_202105141401382026363690786245401_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382026363690786245401_0004_m_000120_408 : duration 0:00.004s
[2021-05-14 11:05:51,141] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401386176287553127343678_0004_m_000116_404: needsTaskCommit() Task attempt_202105141401386176287553127343678_0004_m_000116_404
[2021-05-14 11:05:51,142] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Task committer attempt_202105141401386176287553127343678_0004_m_000116_404: needsTaskCommit() Task attempt_202105141401386176287553127343678_0004_m_000116_404: duration 0:00.002s
21/05/14 14:05:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386176287553127343678_0004_m_000116_404
[2021-05-14 11:05:51,144] {docker.py:276} INFO - 21/05/14 14:05:51 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 5106 bytes result sent to driver
[2021-05-14 11:05:51,145] {docker.py:276} INFO - 21/05/14 14:05:51 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (3e4cb2948233, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:51,146] {docker.py:276} INFO - 21/05/14 14:05:51 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 13604 ms on 3e4cb2948233 (executor driver) (118/200)
[2021-05-14 11:05:51,147] {docker.py:276} INFO - 21/05/14 14:05:51 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
[2021-05-14 11:05:51,158] {docker.py:276} INFO - 21/05/14 14:05:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:05:51,158] {docker.py:276} INFO - 21/05/14 14:05:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:51,166] {docker.py:276} INFO - 21/05/14 14:05:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381285017616677891945_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381285017616677891945_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381285017616677891945_0004}; taskId=attempt_202105141401381285017616677891945_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@768a86c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:51,167] {docker.py:276} INFO - 21/05/14 14:05:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:51,167] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401381285017616677891945_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381285017616677891945_0004_m_000121_409
[2021-05-14 11:05:51,171] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Task committer attempt_202105141401381285017616677891945_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381285017616677891945_0004_m_000121_409 : duration 0:00.003s
[2021-05-14 11:05:51,202] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401386742362487589005147_0004_m_000119_407: needsTaskCommit() Task attempt_202105141401386742362487589005147_0004_m_000119_407
[2021-05-14 11:05:51,203] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Task committer attempt_202105141401386742362487589005147_0004_m_000119_407: needsTaskCommit() Task attempt_202105141401386742362487589005147_0004_m_000119_407: duration 0:00.002s
21/05/14 14:05:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386742362487589005147_0004_m_000119_407
[2021-05-14 11:05:51,205] {docker.py:276} INFO - 21/05/14 14:05:51 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 5106 bytes result sent to driver
[2021-05-14 11:05:51,206] {docker.py:276} INFO - 21/05/14 14:05:51 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (3e4cb2948233, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:51,207] {docker.py:276} INFO - 21/05/14 14:05:51 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
[2021-05-14 11:05:51,207] {docker.py:276} INFO - 21/05/14 14:05:51 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 6457 ms on 3e4cb2948233 (executor driver) (119/200)
[2021-05-14 11:05:51,217] {docker.py:276} INFO - 21/05/14 14:05:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:51,225] {docker.py:276} INFO - 21/05/14 14:05:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138753723146425020541_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138753723146425020541_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138753723146425020541_0004}; taskId=attempt_20210514140138753723146425020541_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e0831eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:51,225] {docker.py:276} INFO - 21/05/14 14:05:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:51 INFO StagingCommitter: Starting: Task committer attempt_20210514140138753723146425020541_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138753723146425020541_0004_m_000122_410
[2021-05-14 11:05:51,229] {docker.py:276} INFO - 21/05/14 14:05:51 INFO StagingCommitter: Task committer attempt_20210514140138753723146425020541_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138753723146425020541_0004_m_000122_410 : duration 0:00.004s
[2021-05-14 11:05:53,119] {docker.py:276} INFO - 21/05/14 14:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401383427718016198361108_0004_m_000117_405: needsTaskCommit() Task attempt_202105141401383427718016198361108_0004_m_000117_405
[2021-05-14 11:05:53,120] {docker.py:276} INFO - 21/05/14 14:05:53 INFO StagingCommitter: Task committer attempt_202105141401383427718016198361108_0004_m_000117_405: needsTaskCommit() Task attempt_202105141401383427718016198361108_0004_m_000117_405: duration 0:00.002s
21/05/14 14:05:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383427718016198361108_0004_m_000117_405
[2021-05-14 11:05:53,121] {docker.py:276} INFO - 21/05/14 14:05:53 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 5106 bytes result sent to driver
[2021-05-14 11:05:53,122] {docker.py:276} INFO - 21/05/14 14:05:53 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (3e4cb2948233, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:53,123] {docker.py:276} INFO - 21/05/14 14:05:53 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 15037 ms on 3e4cb2948233 (executor driver) (120/200)
[2021-05-14 11:05:53,124] {docker.py:276} INFO - 21/05/14 14:05:53 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
[2021-05-14 11:05:53,132] {docker.py:276} INFO - 21/05/14 14:05:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:53,140] {docker.py:276} INFO - 21/05/14 14:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:05:53,141] {docker.py:276} INFO - 21/05/14 14:05:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387749978959760433705_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387749978959760433705_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387749978959760433705_0004}; taskId=attempt_202105141401387749978959760433705_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48318b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:53,141] {docker.py:276} INFO - 21/05/14 14:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401387749978959760433705_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387749978959760433705_0004_m_000123_411
[2021-05-14 11:05:53,145] {docker.py:276} INFO - 21/05/14 14:05:53 INFO StagingCommitter: Task committer attempt_202105141401387749978959760433705_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387749978959760433705_0004_m_000123_411 : duration 0:00.004s
[2021-05-14 11:05:53,489] {docker.py:276} INFO - 21/05/14 14:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401382026363690786245401_0004_m_000120_408: needsTaskCommit() Task attempt_202105141401382026363690786245401_0004_m_000120_408
21/05/14 14:05:53 INFO StagingCommitter: Task committer attempt_202105141401382026363690786245401_0004_m_000120_408: needsTaskCommit() Task attempt_202105141401382026363690786245401_0004_m_000120_408: duration 0:00.002s
21/05/14 14:05:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382026363690786245401_0004_m_000120_408
[2021-05-14 11:05:53,491] {docker.py:276} INFO - 21/05/14 14:05:53 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 5106 bytes result sent to driver
[2021-05-14 11:05:53,493] {docker.py:276} INFO - 21/05/14 14:05:53 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (3e4cb2948233, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:53,494] {docker.py:276} INFO - 21/05/14 14:05:53 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
21/05/14 14:05:53 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 6993 ms on 3e4cb2948233 (executor driver) (121/200)
[2021-05-14 11:05:53,518] {docker.py:276} INFO - 21/05/14 14:05:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:05:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:53,525] {docker.py:276} INFO - 21/05/14 14:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:05:53,525] {docker.py:276} INFO - 21/05/14 14:05:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:05:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386290402127288780608_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386290402127288780608_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386290402127288780608_0004}; taskId=attempt_202105141401386290402127288780608_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1542e3f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401386290402127288780608_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386290402127288780608_0004_m_000124_412
[2021-05-14 11:05:53,529] {docker.py:276} INFO - 21/05/14 14:05:53 INFO StagingCommitter: Task committer attempt_202105141401386290402127288780608_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386290402127288780608_0004_m_000124_412 : duration 0:00.004s
[2021-05-14 11:05:58,023] {docker.py:276} INFO - 21/05/14 14:05:58 INFO StagingCommitter: Starting: Task committer attempt_202105141401381285017616677891945_0004_m_000121_409: needsTaskCommit() Task attempt_202105141401381285017616677891945_0004_m_000121_409
21/05/14 14:05:58 INFO StagingCommitter: Task committer attempt_202105141401381285017616677891945_0004_m_000121_409: needsTaskCommit() Task attempt_202105141401381285017616677891945_0004_m_000121_409: duration 0:00.002s
21/05/14 14:05:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381285017616677891945_0004_m_000121_409
[2021-05-14 11:05:58,025] {docker.py:276} INFO - 21/05/14 14:05:58 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 5149 bytes result sent to driver
[2021-05-14 11:05:58,026] {docker.py:276} INFO - 21/05/14 14:05:58 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (3e4cb2948233, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:05:58,027] {docker.py:276} INFO - 21/05/14 14:05:58 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-14 11:05:58,028] {docker.py:276} INFO - 21/05/14 14:05:58 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 6890 ms on 3e4cb2948233 (executor driver) (122/200)
[2021-05-14 11:05:58,037] {docker.py:276} INFO - 21/05/14 14:05:58 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:05:58,037] {docker.py:276} INFO - 21/05/14 14:05:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:05:58,046] {docker.py:276} INFO - 21/05/14 14:05:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:05:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:05:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:05:58,046] {docker.py:276} INFO - 21/05/14 14:05:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386184324290766200134_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386184324290766200134_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386184324290766200134_0004}; taskId=attempt_202105141401386184324290766200134_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@485b2251}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:05:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:05:58,047] {docker.py:276} INFO - 21/05/14 14:05:58 INFO StagingCommitter: Starting: Task committer attempt_202105141401386184324290766200134_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386184324290766200134_0004_m_000125_413
[2021-05-14 11:05:58,052] {docker.py:276} INFO - 21/05/14 14:05:58 INFO StagingCommitter: Task committer attempt_202105141401386184324290766200134_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386184324290766200134_0004_m_000125_413 : duration 0:00.005s
[2021-05-14 11:06:04,585] {docker.py:276} INFO - 21/05/14 14:06:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401386184324290766200134_0004_m_000125_413: needsTaskCommit() Task attempt_202105141401386184324290766200134_0004_m_000125_413
[2021-05-14 11:06:04,586] {docker.py:276} INFO - 21/05/14 14:06:04 INFO StagingCommitter: Task committer attempt_202105141401386184324290766200134_0004_m_000125_413: needsTaskCommit() Task attempt_202105141401386184324290766200134_0004_m_000125_413: duration 0:00.001s
21/05/14 14:06:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386184324290766200134_0004_m_000125_413
[2021-05-14 11:06:04,588] {docker.py:276} INFO - 21/05/14 14:06:04 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 5106 bytes result sent to driver
[2021-05-14 11:06:04,590] {docker.py:276} INFO - 21/05/14 14:06:04 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (3e4cb2948233, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:04,591] {docker.py:276} INFO - 21/05/14 14:06:04 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
[2021-05-14 11:06:04,592] {docker.py:276} INFO - 21/05/14 14:06:04 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 6574 ms on 3e4cb2948233 (executor driver) (123/200)
[2021-05-14 11:06:04,601] {docker.py:276} INFO - 21/05/14 14:06:04 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:04,610] {docker.py:276} INFO - 21/05/14 14:06:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:06:04,610] {docker.py:276} INFO - 21/05/14 14:06:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386493897086355580173_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386493897086355580173_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386493897086355580173_0004}; taskId=attempt_202105141401386493897086355580173_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@632b41f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:06:04,610] {docker.py:276} INFO - 21/05/14 14:06:04 INFO StagingCommitter: Starting: Task committer attempt_202105141401386493897086355580173_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386493897086355580173_0004_m_000126_414
[2021-05-14 11:06:04,614] {docker.py:276} INFO - 21/05/14 14:06:04 INFO StagingCommitter: Task committer attempt_202105141401386493897086355580173_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386493897086355580173_0004_m_000126_414 : duration 0:00.004s
[2021-05-14 11:06:11,246] {docker.py:276} INFO - 21/05/14 14:06:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401386493897086355580173_0004_m_000126_414: needsTaskCommit() Task attempt_202105141401386493897086355580173_0004_m_000126_414
[2021-05-14 11:06:11,247] {docker.py:276} INFO - 21/05/14 14:06:11 INFO StagingCommitter: Task committer attempt_202105141401386493897086355580173_0004_m_000126_414: needsTaskCommit() Task attempt_202105141401386493897086355580173_0004_m_000126_414: duration 0:00.001s
21/05/14 14:06:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386493897086355580173_0004_m_000126_414
[2021-05-14 11:06:11,250] {docker.py:276} INFO - 21/05/14 14:06:11 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 5106 bytes result sent to driver
[2021-05-14 11:06:11,252] {docker.py:276} INFO - 21/05/14 14:06:11 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (3e4cb2948233, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:11,253] {docker.py:276} INFO - 21/05/14 14:06:11 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 6637 ms on 3e4cb2948233 (executor driver) (124/200)
[2021-05-14 11:06:11,254] {docker.py:276} INFO - 21/05/14 14:06:11 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-14 11:06:11,264] {docker.py:276} INFO - 21/05/14 14:06:11 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:11,274] {docker.py:276} INFO - 21/05/14 14:06:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:06:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:06:11,274] {docker.py:276} INFO - 21/05/14 14:06:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388676963031541082008_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388676963031541082008_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388676963031541082008_0004}; taskId=attempt_202105141401388676963031541082008_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60e08795}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:06:11 INFO StagingCommitter: Starting: Task committer attempt_202105141401388676963031541082008_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388676963031541082008_0004_m_000127_415
[2021-05-14 11:06:11,279] {docker.py:276} INFO - 21/05/14 14:06:11 INFO StagingCommitter: Task committer attempt_202105141401388676963031541082008_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388676963031541082008_0004_m_000127_415 : duration 0:00.004s
[2021-05-14 11:06:18,220] {docker.py:276} INFO - 21/05/14 14:06:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401388676963031541082008_0004_m_000127_415: needsTaskCommit() Task attempt_202105141401388676963031541082008_0004_m_000127_415
21/05/14 14:06:18 INFO StagingCommitter: Task committer attempt_202105141401388676963031541082008_0004_m_000127_415: needsTaskCommit() Task attempt_202105141401388676963031541082008_0004_m_000127_415: duration 0:00.001s
21/05/14 14:06:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388676963031541082008_0004_m_000127_415
[2021-05-14 11:06:18,222] {docker.py:276} INFO - 21/05/14 14:06:18 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 5106 bytes result sent to driver
[2021-05-14 11:06:18,224] {docker.py:276} INFO - 21/05/14 14:06:18 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (3e4cb2948233, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:18,225] {docker.py:276} INFO - 21/05/14 14:06:18 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
[2021-05-14 11:06:18,225] {docker.py:276} INFO - 21/05/14 14:06:18 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 6983 ms on 3e4cb2948233 (executor driver) (125/200)
[2021-05-14 11:06:18,235] {docker.py:276} INFO - 21/05/14 14:06:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:18,243] {docker.py:276} INFO - 21/05/14 14:06:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:06:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384350833135854242915_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384350833135854242915_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384350833135854242915_0004}; taskId=attempt_202105141401384350833135854242915_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61bd3e49}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:06:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401384350833135854242915_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384350833135854242915_0004_m_000128_416
[2021-05-14 11:06:18,250] {docker.py:276} INFO - 21/05/14 14:06:18 INFO StagingCommitter: Task committer attempt_202105141401384350833135854242915_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384350833135854242915_0004_m_000128_416 : duration 0:00.006s
[2021-05-14 11:06:25,288] {docker.py:276} INFO - 21/05/14 14:06:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401384350833135854242915_0004_m_000128_416: needsTaskCommit() Task attempt_202105141401384350833135854242915_0004_m_000128_416
[2021-05-14 11:06:25,289] {docker.py:276} INFO - 21/05/14 14:06:25 INFO StagingCommitter: Task committer attempt_202105141401384350833135854242915_0004_m_000128_416: needsTaskCommit() Task attempt_202105141401384350833135854242915_0004_m_000128_416: duration 0:00.000s
21/05/14 14:06:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384350833135854242915_0004_m_000128_416
[2021-05-14 11:06:25,291] {docker.py:276} INFO - 21/05/14 14:06:25 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 5106 bytes result sent to driver
[2021-05-14 11:06:25,292] {docker.py:276} INFO - 21/05/14 14:06:25 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (3e4cb2948233, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:25,294] {docker.py:276} INFO - 21/05/14 14:06:25 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 7080 ms on 3e4cb2948233 (executor driver) (126/200)
21/05/14 14:06:25 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-14 11:06:25,303] {docker.py:276} INFO - 21/05/14 14:06:25 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:25,312] {docker.py:276} INFO - 21/05/14 14:06:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:06:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383733575960404660350_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383733575960404660350_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383733575960404660350_0004}; taskId=attempt_202105141401383733575960404660350_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a779c14}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:06:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401383733575960404660350_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383733575960404660350_0004_m_000129_417
[2021-05-14 11:06:25,316] {docker.py:276} INFO - 21/05/14 14:06:25 INFO StagingCommitter: Task committer attempt_202105141401383733575960404660350_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383733575960404660350_0004_m_000129_417 : duration 0:00.005s
[2021-05-14 11:06:32,515] {docker.py:276} INFO - 21/05/14 14:06:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401383733575960404660350_0004_m_000129_417: needsTaskCommit() Task attempt_202105141401383733575960404660350_0004_m_000129_417
[2021-05-14 11:06:32,516] {docker.py:276} INFO - 21/05/14 14:06:32 INFO StagingCommitter: Task committer attempt_202105141401383733575960404660350_0004_m_000129_417: needsTaskCommit() Task attempt_202105141401383733575960404660350_0004_m_000129_417: duration 0:00.001s
21/05/14 14:06:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383733575960404660350_0004_m_000129_417
[2021-05-14 11:06:32,518] {docker.py:276} INFO - 21/05/14 14:06:32 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 5106 bytes result sent to driver
[2021-05-14 11:06:32,520] {docker.py:276} INFO - 21/05/14 14:06:32 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (3e4cb2948233, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:32,522] {docker.py:276} INFO - 21/05/14 14:06:32 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 7237 ms on 3e4cb2948233 (executor driver) (127/200)
[2021-05-14 11:06:32,522] {docker.py:276} INFO - 21/05/14 14:06:32 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-14 11:06:32,531] {docker.py:276} INFO - 21/05/14 14:06:32 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:32,539] {docker.py:276} INFO - 21/05/14 14:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:06:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381158382635214944102_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381158382635214944102_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381158382635214944102_0004}; taskId=attempt_202105141401381158382635214944102_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5302622e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:06:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401381158382635214944102_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381158382635214944102_0004_m_000130_418
[2021-05-14 11:06:32,543] {docker.py:276} INFO - 21/05/14 14:06:32 INFO StagingCommitter: Task committer attempt_202105141401381158382635214944102_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381158382635214944102_0004_m_000130_418 : duration 0:00.004s
[2021-05-14 11:06:39,509] {docker.py:276} INFO - 21/05/14 14:06:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401381158382635214944102_0004_m_000130_418: needsTaskCommit() Task attempt_202105141401381158382635214944102_0004_m_000130_418
[2021-05-14 11:06:39,510] {docker.py:276} INFO - 21/05/14 14:06:39 INFO StagingCommitter: Task committer attempt_202105141401381158382635214944102_0004_m_000130_418: needsTaskCommit() Task attempt_202105141401381158382635214944102_0004_m_000130_418: duration 0:00.000s
21/05/14 14:06:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381158382635214944102_0004_m_000130_418
[2021-05-14 11:06:39,512] {docker.py:276} INFO - 21/05/14 14:06:39 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 5106 bytes result sent to driver
[2021-05-14 11:06:39,513] {docker.py:276} INFO - 21/05/14 14:06:39 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (3e4cb2948233, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:39,514] {docker.py:276} INFO - 21/05/14 14:06:39 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
21/05/14 14:06:39 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 7002 ms on 3e4cb2948233 (executor driver) (128/200)
[2021-05-14 11:06:39,524] {docker.py:276} INFO - 21/05/14 14:06:39 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:39,532] {docker.py:276} INFO - 21/05/14 14:06:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:06:39,533] {docker.py:276} INFO - 21/05/14 14:06:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387151012279088598009_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387151012279088598009_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387151012279088598009_0004}; taskId=attempt_202105141401387151012279088598009_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@317cd67f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:06:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401387151012279088598009_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387151012279088598009_0004_m_000131_419
[2021-05-14 11:06:39,536] {docker.py:276} INFO - 21/05/14 14:06:39 INFO StagingCommitter: Task committer attempt_202105141401387151012279088598009_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387151012279088598009_0004_m_000131_419 : duration 0:00.004s
[2021-05-14 11:06:46,561] {docker.py:276} INFO - 21/05/14 14:06:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401387151012279088598009_0004_m_000131_419: needsTaskCommit() Task attempt_202105141401387151012279088598009_0004_m_000131_419
[2021-05-14 11:06:46,561] {docker.py:276} INFO - 21/05/14 14:06:46 INFO StagingCommitter: Task committer attempt_202105141401387151012279088598009_0004_m_000131_419: needsTaskCommit() Task attempt_202105141401387151012279088598009_0004_m_000131_419: duration 0:00.001s
21/05/14 14:06:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387151012279088598009_0004_m_000131_419
[2021-05-14 11:06:46,563] {docker.py:276} INFO - 21/05/14 14:06:46 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 5106 bytes result sent to driver
[2021-05-14 11:06:46,565] {docker.py:276} INFO - 21/05/14 14:06:46 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (3e4cb2948233, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:46,566] {docker.py:276} INFO - 21/05/14 14:06:46 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 7028 ms on 3e4cb2948233 (executor driver) (129/200)
[2021-05-14 11:06:46,566] {docker.py:276} INFO - 21/05/14 14:06:46 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-14 11:06:46,574] {docker.py:276} INFO - 21/05/14 14:06:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:46,582] {docker.py:276} INFO - 21/05/14 14:06:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:06:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388113416548648634339_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388113416548648634339_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388113416548648634339_0004}; taskId=attempt_202105141401388113416548648634339_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ca72ed3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:06:46 INFO StagingCommitter: Starting: Task committer attempt_202105141401388113416548648634339_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388113416548648634339_0004_m_000132_420
[2021-05-14 11:06:46,586] {docker.py:276} INFO - 21/05/14 14:06:46 INFO StagingCommitter: Task committer attempt_202105141401388113416548648634339_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388113416548648634339_0004_m_000132_420 : duration 0:00.004s
[2021-05-14 11:06:53,553] {docker.py:276} INFO - 21/05/14 14:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401388113416548648634339_0004_m_000132_420: needsTaskCommit() Task attempt_202105141401388113416548648634339_0004_m_000132_420
[2021-05-14 11:06:53,555] {docker.py:276} INFO - 21/05/14 14:06:53 INFO StagingCommitter: Task committer attempt_202105141401388113416548648634339_0004_m_000132_420: needsTaskCommit() Task attempt_202105141401388113416548648634339_0004_m_000132_420: duration 0:00.001s
21/05/14 14:06:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388113416548648634339_0004_m_000132_420
[2021-05-14 11:06:53,557] {docker.py:276} INFO - 21/05/14 14:06:53 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 5106 bytes result sent to driver
[2021-05-14 11:06:53,559] {docker.py:276} INFO - 21/05/14 14:06:53 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (3e4cb2948233, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:06:53,560] {docker.py:276} INFO - 21/05/14 14:06:53 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
21/05/14 14:06:53 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 7003 ms on 3e4cb2948233 (executor driver) (130/200)
[2021-05-14 11:06:53,569] {docker.py:276} INFO - 21/05/14 14:06:53 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:06:53,591] {docker.py:276} INFO - 21/05/14 14:06:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:06:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:06:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:06:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384625562554356004419_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384625562554356004419_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384625562554356004419_0004}; taskId=attempt_202105141401384625562554356004419_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@197d9f5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:06:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:06:53,591] {docker.py:276} INFO - 21/05/14 14:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401384625562554356004419_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384625562554356004419_0004_m_000133_421
[2021-05-14 11:06:53,596] {docker.py:276} INFO - 21/05/14 14:06:53 INFO StagingCommitter: Task committer attempt_202105141401384625562554356004419_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384625562554356004419_0004_m_000133_421 : duration 0:00.005s
[2021-05-14 11:07:00,381] {docker.py:276} INFO - 21/05/14 14:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401384625562554356004419_0004_m_000133_421: needsTaskCommit() Task attempt_202105141401384625562554356004419_0004_m_000133_421
[2021-05-14 11:07:00,382] {docker.py:276} INFO - 21/05/14 14:07:00 INFO StagingCommitter: Task committer attempt_202105141401384625562554356004419_0004_m_000133_421: needsTaskCommit() Task attempt_202105141401384625562554356004419_0004_m_000133_421: duration 0:00.000s
[2021-05-14 11:07:00,383] {docker.py:276} INFO - 21/05/14 14:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384625562554356004419_0004_m_000133_421
[2021-05-14 11:07:00,385] {docker.py:276} INFO - 21/05/14 14:07:00 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 5149 bytes result sent to driver
[2021-05-14 11:07:00,387] {docker.py:276} INFO - 21/05/14 14:07:00 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (3e4cb2948233, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:00,388] {docker.py:276} INFO - 21/05/14 14:07:00 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 6838 ms on 3e4cb2948233 (executor driver) (131/200)
[2021-05-14 11:07:00,389] {docker.py:276} INFO - 21/05/14 14:07:00 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
[2021-05-14 11:07:00,398] {docker.py:276} INFO - 21/05/14 14:07:00 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:00,406] {docker.py:276} INFO - 21/05/14 14:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:00,407] {docker.py:276} INFO - 21/05/14 14:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386522736132753447171_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386522736132753447171_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386522736132753447171_0004}; taskId=attempt_202105141401386522736132753447171_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9a67ade}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401386522736132753447171_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386522736132753447171_0004_m_000134_422
[2021-05-14 11:07:00,412] {docker.py:276} INFO - 21/05/14 14:07:00 INFO StagingCommitter: Task committer attempt_202105141401386522736132753447171_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386522736132753447171_0004_m_000134_422 : duration 0:00.006s
[2021-05-14 11:07:07,319] {docker.py:276} INFO - 21/05/14 14:07:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401386522736132753447171_0004_m_000134_422: needsTaskCommit() Task attempt_202105141401386522736132753447171_0004_m_000134_422
[2021-05-14 11:07:07,321] {docker.py:276} INFO - 21/05/14 14:07:07 INFO StagingCommitter: Task committer attempt_202105141401386522736132753447171_0004_m_000134_422: needsTaskCommit() Task attempt_202105141401386522736132753447171_0004_m_000134_422: duration 0:00.001s
[2021-05-14 11:07:07,322] {docker.py:276} INFO - 21/05/14 14:07:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386522736132753447171_0004_m_000134_422
[2021-05-14 11:07:07,322] {docker.py:276} INFO - 21/05/14 14:07:07 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 5106 bytes result sent to driver
[2021-05-14 11:07:07,323] {docker.py:276} INFO - 21/05/14 14:07:07 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (3e4cb2948233, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:07,323] {docker.py:276} INFO - 21/05/14 14:07:07 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-14 11:07:07,324] {docker.py:276} INFO - 21/05/14 14:07:07 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 6945 ms on 3e4cb2948233 (executor driver) (132/200)
[2021-05-14 11:07:07,335] {docker.py:276} INFO - 21/05/14 14:07:07 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:07:07,336] {docker.py:276} INFO - 21/05/14 14:07:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:07,346] {docker.py:276} INFO - 21/05/14 14:07:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:07:07,347] {docker.py:276} INFO - 21/05/14 14:07:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:07:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138636170682473604823_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138636170682473604823_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138636170682473604823_0004}; taskId=attempt_20210514140138636170682473604823_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c8921e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:07:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:07:07 INFO StagingCommitter: Starting: Task committer attempt_20210514140138636170682473604823_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138636170682473604823_0004_m_000135_423
[2021-05-14 11:07:07,353] {docker.py:276} INFO - 21/05/14 14:07:07 INFO StagingCommitter: Task committer attempt_20210514140138636170682473604823_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138636170682473604823_0004_m_000135_423 : duration 0:00.005s
[2021-05-14 11:07:14,361] {docker.py:276} INFO - 21/05/14 14:07:14 INFO StagingCommitter: Starting: Task committer attempt_20210514140138636170682473604823_0004_m_000135_423: needsTaskCommit() Task attempt_20210514140138636170682473604823_0004_m_000135_423
[2021-05-14 11:07:14,361] {docker.py:276} INFO - 21/05/14 14:07:14 INFO StagingCommitter: Task committer attempt_20210514140138636170682473604823_0004_m_000135_423: needsTaskCommit() Task attempt_20210514140138636170682473604823_0004_m_000135_423: duration 0:00.000s
21/05/14 14:07:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138636170682473604823_0004_m_000135_423
[2021-05-14 11:07:14,363] {docker.py:276} INFO - 21/05/14 14:07:14 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 5106 bytes result sent to driver
[2021-05-14 11:07:14,365] {docker.py:276} INFO - 21/05/14 14:07:14 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (3e4cb2948233, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:14,365] {docker.py:276} INFO - 21/05/14 14:07:14 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
21/05/14 14:07:14 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 7017 ms on 3e4cb2948233 (executor driver) (133/200)
[2021-05-14 11:07:14,372] {docker.py:276} INFO - 21/05/14 14:07:14 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:14,380] {docker.py:276} INFO - 21/05/14 14:07:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:14,381] {docker.py:276} INFO - 21/05/14 14:07:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388648090030482143026_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388648090030482143026_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388648090030482143026_0004}; taskId=attempt_202105141401388648090030482143026_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@659730e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:07:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:07:14,381] {docker.py:276} INFO - 21/05/14 14:07:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401388648090030482143026_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388648090030482143026_0004_m_000136_424
[2021-05-14 11:07:14,386] {docker.py:276} INFO - 21/05/14 14:07:14 INFO StagingCommitter: Task committer attempt_202105141401388648090030482143026_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388648090030482143026_0004_m_000136_424 : duration 0:00.005s
[2021-05-14 11:07:21,131] {docker.py:276} INFO - 21/05/14 14:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401388648090030482143026_0004_m_000136_424: needsTaskCommit() Task attempt_202105141401388648090030482143026_0004_m_000136_424
[2021-05-14 11:07:21,133] {docker.py:276} INFO - 21/05/14 14:07:21 INFO StagingCommitter: Task committer attempt_202105141401388648090030482143026_0004_m_000136_424: needsTaskCommit() Task attempt_202105141401388648090030482143026_0004_m_000136_424: duration 0:00.001s
21/05/14 14:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388648090030482143026_0004_m_000136_424
[2021-05-14 11:07:21,135] {docker.py:276} INFO - 21/05/14 14:07:21 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 5106 bytes result sent to driver
[2021-05-14 11:07:21,139] {docker.py:276} INFO - 21/05/14 14:07:21 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (3e4cb2948233, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:07:21 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
21/05/14 14:07:21 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 6782 ms on 3e4cb2948233 (executor driver) (134/200)
[2021-05-14 11:07:21,147] {docker.py:276} INFO - 21/05/14 14:07:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:21,155] {docker.py:276} INFO - 21/05/14 14:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383992378223615921338_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383992378223615921338_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383992378223615921338_0004}; taskId=attempt_202105141401383992378223615921338_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5cf4b682}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:21,155] {docker.py:276} INFO - 21/05/14 14:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401383992378223615921338_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383992378223615921338_0004_m_000137_425
[2021-05-14 11:07:21,159] {docker.py:276} INFO - 21/05/14 14:07:21 INFO StagingCommitter: Task committer attempt_202105141401383992378223615921338_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383992378223615921338_0004_m_000137_425 : duration 0:00.004s
[2021-05-14 11:07:28,060] {docker.py:276} INFO - 21/05/14 14:07:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401383992378223615921338_0004_m_000137_425: needsTaskCommit() Task attempt_202105141401383992378223615921338_0004_m_000137_425
[2021-05-14 11:07:28,061] {docker.py:276} INFO - 21/05/14 14:07:28 INFO StagingCommitter: Task committer attempt_202105141401383992378223615921338_0004_m_000137_425: needsTaskCommit() Task attempt_202105141401383992378223615921338_0004_m_000137_425: duration 0:00.001s
21/05/14 14:07:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383992378223615921338_0004_m_000137_425
[2021-05-14 11:07:28,062] {docker.py:276} INFO - 21/05/14 14:07:28 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 5106 bytes result sent to driver
[2021-05-14 11:07:28,064] {docker.py:276} INFO - 21/05/14 14:07:28 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (3e4cb2948233, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:28,065] {docker.py:276} INFO - 21/05/14 14:07:28 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
[2021-05-14 11:07:28,066] {docker.py:276} INFO - 21/05/14 14:07:28 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 6938 ms on 3e4cb2948233 (executor driver) (135/200)
[2021-05-14 11:07:28,075] {docker.py:276} INFO - 21/05/14 14:07:28 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:28,083] {docker.py:276} INFO - 21/05/14 14:07:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:28,084] {docker.py:276} INFO - 21/05/14 14:07:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386740253107261397862_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386740253107261397862_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386740253107261397862_0004}; taskId=attempt_202105141401386740253107261397862_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47484162}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:07:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:07:28,084] {docker.py:276} INFO - 21/05/14 14:07:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401386740253107261397862_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386740253107261397862_0004_m_000138_426
[2021-05-14 11:07:28,089] {docker.py:276} INFO - 21/05/14 14:07:28 INFO StagingCommitter: Task committer attempt_202105141401386740253107261397862_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386740253107261397862_0004_m_000138_426 : duration 0:00.005s
[2021-05-14 11:07:35,060] {docker.py:276} INFO - 21/05/14 14:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401386740253107261397862_0004_m_000138_426: needsTaskCommit() Task attempt_202105141401386740253107261397862_0004_m_000138_426
[2021-05-14 11:07:35,061] {docker.py:276} INFO - 21/05/14 14:07:35 INFO StagingCommitter: Task committer attempt_202105141401386740253107261397862_0004_m_000138_426: needsTaskCommit() Task attempt_202105141401386740253107261397862_0004_m_000138_426: duration 0:00.000s
21/05/14 14:07:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386740253107261397862_0004_m_000138_426
[2021-05-14 11:07:35,062] {docker.py:276} INFO - 21/05/14 14:07:35 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 5106 bytes result sent to driver
[2021-05-14 11:07:35,064] {docker.py:276} INFO - 21/05/14 14:07:35 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (3e4cb2948233, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:35,064] {docker.py:276} INFO - 21/05/14 14:07:35 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 7010 ms on 3e4cb2948233 (executor driver) (136/200)
21/05/14 14:07:35 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
[2021-05-14 11:07:35,081] {docker.py:276} INFO - 21/05/14 14:07:35 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 14:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:35,082] {docker.py:276} INFO - 21/05/14 14:07:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388235325471272116891_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388235325471272116891_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388235325471272116891_0004}; taskId=attempt_202105141401388235325471272116891_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4795deb0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:35,083] {docker.py:276} INFO - 21/05/14 14:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:07:35,083] {docker.py:276} INFO - 21/05/14 14:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401388235325471272116891_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388235325471272116891_0004_m_000139_427
[2021-05-14 11:07:35,087] {docker.py:276} INFO - 21/05/14 14:07:35 INFO StagingCommitter: Task committer attempt_202105141401388235325471272116891_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388235325471272116891_0004_m_000139_427 : duration 0:00.004s
[2021-05-14 11:07:42,066] {docker.py:276} INFO - 21/05/14 14:07:42 INFO StagingCommitter: Starting: Task committer attempt_202105141401388235325471272116891_0004_m_000139_427: needsTaskCommit() Task attempt_202105141401388235325471272116891_0004_m_000139_427
[2021-05-14 11:07:42,068] {docker.py:276} INFO - 21/05/14 14:07:42 INFO StagingCommitter: Task committer attempt_202105141401388235325471272116891_0004_m_000139_427: needsTaskCommit() Task attempt_202105141401388235325471272116891_0004_m_000139_427: duration 0:00.001s
21/05/14 14:07:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388235325471272116891_0004_m_000139_427
[2021-05-14 11:07:42,070] {docker.py:276} INFO - 21/05/14 14:07:42 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 5106 bytes result sent to driver
[2021-05-14 11:07:42,074] {docker.py:276} INFO - 21/05/14 14:07:42 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (3e4cb2948233, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:42,074] {docker.py:276} INFO - 21/05/14 14:07:42 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-14 11:07:42,074] {docker.py:276} INFO - 21/05/14 14:07:42 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 6984 ms on 3e4cb2948233 (executor driver) (137/200)
[2021-05-14 11:07:42,082] {docker.py:276} INFO - 21/05/14 14:07:42 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:42,091] {docker.py:276} INFO - 21/05/14 14:07:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:07:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382291984403023766223_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382291984403023766223_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382291984403023766223_0004}; taskId=attempt_202105141401382291984403023766223_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72241a5c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:07:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:07:42 INFO StagingCommitter: Starting: Task committer attempt_202105141401382291984403023766223_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382291984403023766223_0004_m_000140_428
[2021-05-14 11:07:42,095] {docker.py:276} INFO - 21/05/14 14:07:42 INFO StagingCommitter: Task committer attempt_202105141401382291984403023766223_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382291984403023766223_0004_m_000140_428 : duration 0:00.004s
[2021-05-14 11:07:49,052] {docker.py:276} INFO - 21/05/14 14:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105141401382291984403023766223_0004_m_000140_428: needsTaskCommit() Task attempt_202105141401382291984403023766223_0004_m_000140_428
[2021-05-14 11:07:49,054] {docker.py:276} INFO - 21/05/14 14:07:49 INFO StagingCommitter: Task committer attempt_202105141401382291984403023766223_0004_m_000140_428: needsTaskCommit() Task attempt_202105141401382291984403023766223_0004_m_000140_428: duration 0:00.002s
21/05/14 14:07:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382291984403023766223_0004_m_000140_428
[2021-05-14 11:07:49,055] {docker.py:276} INFO - 21/05/14 14:07:49 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 5106 bytes result sent to driver
[2021-05-14 11:07:49,057] {docker.py:276} INFO - 21/05/14 14:07:49 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (3e4cb2948233, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:49,059] {docker.py:276} INFO - 21/05/14 14:07:49 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 6997 ms on 3e4cb2948233 (executor driver) (138/200)
21/05/14 14:07:49 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
[2021-05-14 11:07:49,068] {docker.py:276} INFO - 21/05/14 14:07:49 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:49,076] {docker.py:276} INFO - 21/05/14 14:07:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:49,076] {docker.py:276} INFO - 21/05/14 14:07:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138868915764507927725_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138868915764507927725_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138868915764507927725_0004}; taskId=attempt_20210514140138868915764507927725_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b201946}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:07:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:07:49 INFO StagingCommitter: Starting: Task committer attempt_20210514140138868915764507927725_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138868915764507927725_0004_m_000141_429
[2021-05-14 11:07:49,080] {docker.py:276} INFO - 21/05/14 14:07:49 INFO StagingCommitter: Task committer attempt_20210514140138868915764507927725_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138868915764507927725_0004_m_000141_429 : duration 0:00.004s
[2021-05-14 11:07:55,999] {docker.py:276} INFO - 21/05/14 14:07:56 INFO StagingCommitter: Starting: Task committer attempt_20210514140138868915764507927725_0004_m_000141_429: needsTaskCommit() Task attempt_20210514140138868915764507927725_0004_m_000141_429
[2021-05-14 11:07:56,000] {docker.py:276} INFO - 21/05/14 14:07:56 INFO StagingCommitter: Task committer attempt_20210514140138868915764507927725_0004_m_000141_429: needsTaskCommit() Task attempt_20210514140138868915764507927725_0004_m_000141_429: duration 0:00.001s
21/05/14 14:07:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138868915764507927725_0004_m_000141_429
[2021-05-14 11:07:56,002] {docker.py:276} INFO - 21/05/14 14:07:56 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 5106 bytes result sent to driver
[2021-05-14 11:07:56,005] {docker.py:276} INFO - 21/05/14 14:07:56 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (3e4cb2948233, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:07:56,006] {docker.py:276} INFO - 21/05/14 14:07:56 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 6956 ms on 3e4cb2948233 (executor driver) (139/200)
21/05/14 14:07:56 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-14 11:07:56,015] {docker.py:276} INFO - 21/05/14 14:07:56 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:07:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:07:56,023] {docker.py:276} INFO - 21/05/14 14:07:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:07:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:07:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:07:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383255981668156553317_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383255981668156553317_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383255981668156553317_0004}; taskId=attempt_202105141401383255981668156553317_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fc2a0ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:07:56,023] {docker.py:276} INFO - 21/05/14 14:07:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105141401383255981668156553317_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383255981668156553317_0004_m_000142_430
[2021-05-14 11:07:56,027] {docker.py:276} INFO - 21/05/14 14:07:56 INFO StagingCommitter: Task committer attempt_202105141401383255981668156553317_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383255981668156553317_0004_m_000142_430 : duration 0:00.005s
[2021-05-14 11:08:02,223] {docker.py:276} INFO - 21/05/14 14:08:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401383255981668156553317_0004_m_000142_430: needsTaskCommit() Task attempt_202105141401383255981668156553317_0004_m_000142_430
[2021-05-14 11:08:02,224] {docker.py:276} INFO - 21/05/14 14:08:02 INFO StagingCommitter: Task committer attempt_202105141401383255981668156553317_0004_m_000142_430: needsTaskCommit() Task attempt_202105141401383255981668156553317_0004_m_000142_430: duration 0:00.002s
21/05/14 14:08:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383255981668156553317_0004_m_000142_430
[2021-05-14 11:08:02,227] {docker.py:276} INFO - 21/05/14 14:08:02 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 5106 bytes result sent to driver
[2021-05-14 11:08:02,228] {docker.py:276} INFO - 21/05/14 14:08:02 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (3e4cb2948233, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:02,229] {docker.py:276} INFO - 21/05/14 14:08:02 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
[2021-05-14 11:08:02,230] {docker.py:276} INFO - 21/05/14 14:08:02 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 6233 ms on 3e4cb2948233 (executor driver) (140/200)
[2021-05-14 11:08:02,255] {docker.py:276} INFO - 21/05/14 14:08:02 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:08:02,256] {docker.py:276} INFO - 21/05/14 14:08:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:02,263] {docker.py:276} INFO - 21/05/14 14:08:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:08:02,264] {docker.py:276} INFO - 21/05/14 14:08:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:08:02,264] {docker.py:276} INFO - 21/05/14 14:08:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:08:02,264] {docker.py:276} INFO - 21/05/14 14:08:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384260337704571822088_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384260337704571822088_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384260337704571822088_0004}; taskId=attempt_202105141401384260337704571822088_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41d1452b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:08:02,264] {docker.py:276} INFO - 21/05/14 14:08:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:02,265] {docker.py:276} INFO - 21/05/14 14:08:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401384260337704571822088_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384260337704571822088_0004_m_000143_431
[2021-05-14 11:08:02,268] {docker.py:276} INFO - 21/05/14 14:08:02 INFO StagingCommitter: Task committer attempt_202105141401384260337704571822088_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384260337704571822088_0004_m_000143_431 : duration 0:00.004s
[2021-05-14 11:08:09,373] {docker.py:276} INFO - 21/05/14 14:08:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401384260337704571822088_0004_m_000143_431: needsTaskCommit() Task attempt_202105141401384260337704571822088_0004_m_000143_431
[2021-05-14 11:08:09,375] {docker.py:276} INFO - 21/05/14 14:08:09 INFO StagingCommitter: Task committer attempt_202105141401384260337704571822088_0004_m_000143_431: needsTaskCommit() Task attempt_202105141401384260337704571822088_0004_m_000143_431: duration 0:00.002s
21/05/14 14:08:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384260337704571822088_0004_m_000143_431
[2021-05-14 11:08:09,376] {docker.py:276} INFO - 21/05/14 14:08:09 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 5149 bytes result sent to driver
[2021-05-14 11:08:09,379] {docker.py:276} INFO - 21/05/14 14:08:09 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (3e4cb2948233, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:09,380] {docker.py:276} INFO - 21/05/14 14:08:09 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 7162 ms on 3e4cb2948233 (executor driver) (141/200)
21/05/14 14:08:09 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-14 11:08:09,390] {docker.py:276} INFO - 21/05/14 14:08:09 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:09,398] {docker.py:276} INFO - 21/05/14 14:08:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:08:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138267979164330920560_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138267979164330920560_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138267979164330920560_0004}; taskId=attempt_20210514140138267979164330920560_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@323d3f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:08:09 INFO StagingCommitter: Starting: Task committer attempt_20210514140138267979164330920560_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138267979164330920560_0004_m_000144_432
[2021-05-14 11:08:09,402] {docker.py:276} INFO - 21/05/14 14:08:09 INFO StagingCommitter: Task committer attempt_20210514140138267979164330920560_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138267979164330920560_0004_m_000144_432 : duration 0:00.004s
[2021-05-14 11:08:16,444] {docker.py:276} INFO - 21/05/14 14:08:16 INFO StagingCommitter: Starting: Task committer attempt_20210514140138267979164330920560_0004_m_000144_432: needsTaskCommit() Task attempt_20210514140138267979164330920560_0004_m_000144_432
[2021-05-14 11:08:16,444] {docker.py:276} INFO - 21/05/14 14:08:16 INFO StagingCommitter: Task committer attempt_20210514140138267979164330920560_0004_m_000144_432: needsTaskCommit() Task attempt_20210514140138267979164330920560_0004_m_000144_432: duration 0:00.000s
21/05/14 14:08:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138267979164330920560_0004_m_000144_432
[2021-05-14 11:08:16,447] {docker.py:276} INFO - 21/05/14 14:08:16 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 5106 bytes result sent to driver
[2021-05-14 11:08:16,449] {docker.py:276} INFO - 21/05/14 14:08:16 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (3e4cb2948233, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:16,450] {docker.py:276} INFO - 21/05/14 14:08:16 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 7046 ms on 3e4cb2948233 (executor driver) (142/200)
21/05/14 14:08:16 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
[2021-05-14 11:08:16,460] {docker.py:276} INFO - 21/05/14 14:08:16 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:16,467] {docker.py:276} INFO - 21/05/14 14:08:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:08:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384987058967325093366_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384987058967325093366_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384987058967325093366_0004}; taskId=attempt_202105141401384987058967325093366_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@167db504}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:08:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401384987058967325093366_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384987058967325093366_0004_m_000145_433
[2021-05-14 11:08:16,472] {docker.py:276} INFO - 21/05/14 14:08:16 INFO StagingCommitter: Task committer attempt_202105141401384987058967325093366_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384987058967325093366_0004_m_000145_433 : duration 0:00.005s
[2021-05-14 11:08:23,699] {docker.py:276} INFO - 21/05/14 14:08:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401384987058967325093366_0004_m_000145_433: needsTaskCommit() Task attempt_202105141401384987058967325093366_0004_m_000145_433
[2021-05-14 11:08:23,700] {docker.py:276} INFO - 21/05/14 14:08:23 INFO StagingCommitter: Task committer attempt_202105141401384987058967325093366_0004_m_000145_433: needsTaskCommit() Task attempt_202105141401384987058967325093366_0004_m_000145_433: duration 0:00.000s
21/05/14 14:08:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384987058967325093366_0004_m_000145_433
[2021-05-14 11:08:23,701] {docker.py:276} INFO - 21/05/14 14:08:23 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 5106 bytes result sent to driver
[2021-05-14 11:08:23,704] {docker.py:276} INFO - 21/05/14 14:08:23 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (3e4cb2948233, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:23,705] {docker.py:276} INFO - 21/05/14 14:08:23 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
21/05/14 14:08:23 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 7267 ms on 3e4cb2948233 (executor driver) (143/200)
[2021-05-14 11:08:23,715] {docker.py:276} INFO - 21/05/14 14:08:23 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:23,722] {docker.py:276} INFO - 21/05/14 14:08:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:08:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382429891129525937080_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382429891129525937080_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382429891129525937080_0004}; taskId=attempt_202105141401382429891129525937080_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27fee9e6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:23,723] {docker.py:276} INFO - 21/05/14 14:08:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401382429891129525937080_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382429891129525937080_0004_m_000146_434
[2021-05-14 11:08:23,727] {docker.py:276} INFO - 21/05/14 14:08:23 INFO StagingCommitter: Task committer attempt_202105141401382429891129525937080_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382429891129525937080_0004_m_000146_434 : duration 0:00.005s
[2021-05-14 11:08:30,704] {docker.py:276} INFO - 21/05/14 14:08:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401382429891129525937080_0004_m_000146_434: needsTaskCommit() Task attempt_202105141401382429891129525937080_0004_m_000146_434
[2021-05-14 11:08:30,706] {docker.py:276} INFO - 21/05/14 14:08:30 INFO StagingCommitter: Task committer attempt_202105141401382429891129525937080_0004_m_000146_434: needsTaskCommit() Task attempt_202105141401382429891129525937080_0004_m_000146_434: duration 0:00.002s
21/05/14 14:08:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382429891129525937080_0004_m_000146_434
[2021-05-14 11:08:30,708] {docker.py:276} INFO - 21/05/14 14:08:30 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 5106 bytes result sent to driver
[2021-05-14 11:08:30,711] {docker.py:276} INFO - 21/05/14 14:08:30 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (3e4cb2948233, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:30,711] {docker.py:276} INFO - 21/05/14 14:08:30 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 7017 ms on 3e4cb2948233 (executor driver) (144/200)
21/05/14 14:08:30 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-14 11:08:30,721] {docker.py:276} INFO - 21/05/14 14:08:30 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:30,728] {docker.py:276} INFO - 21/05/14 14:08:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:08:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383680689552499748413_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383680689552499748413_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383680689552499748413_0004}; taskId=attempt_202105141401383680689552499748413_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@769b576b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:30,729] {docker.py:276} INFO - 21/05/14 14:08:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401383680689552499748413_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383680689552499748413_0004_m_000147_435
[2021-05-14 11:08:30,733] {docker.py:276} INFO - 21/05/14 14:08:30 INFO StagingCommitter: Task committer attempt_202105141401383680689552499748413_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383680689552499748413_0004_m_000147_435 : duration 0:00.005s
[2021-05-14 11:08:37,824] {docker.py:276} INFO - 21/05/14 14:08:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401383680689552499748413_0004_m_000147_435: needsTaskCommit() Task attempt_202105141401383680689552499748413_0004_m_000147_435
[2021-05-14 11:08:37,825] {docker.py:276} INFO - 21/05/14 14:08:37 INFO StagingCommitter: Task committer attempt_202105141401383680689552499748413_0004_m_000147_435: needsTaskCommit() Task attempt_202105141401383680689552499748413_0004_m_000147_435: duration 0:00.001s
[2021-05-14 11:08:37,826] {docker.py:276} INFO - 21/05/14 14:08:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383680689552499748413_0004_m_000147_435
[2021-05-14 11:08:37,826] {docker.py:276} INFO - 21/05/14 14:08:37 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 5106 bytes result sent to driver
[2021-05-14 11:08:37,829] {docker.py:276} INFO - 21/05/14 14:08:37 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (3e4cb2948233, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:37,830] {docker.py:276} INFO - 21/05/14 14:08:37 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
[2021-05-14 11:08:37,830] {docker.py:276} INFO - 21/05/14 14:08:37 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 7131 ms on 3e4cb2948233 (executor driver) (145/200)
[2021-05-14 11:08:37,839] {docker.py:276} INFO - 21/05/14 14:08:37 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:37,848] {docker.py:276} INFO - 21/05/14 14:08:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:08:37,848] {docker.py:276} INFO - 21/05/14 14:08:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:08:37,848] {docker.py:276} INFO - 21/05/14 14:08:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386821258341226428529_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386821258341226428529_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386821258341226428529_0004}; taskId=attempt_202105141401386821258341226428529_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f82878e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:37,849] {docker.py:276} INFO - 21/05/14 14:08:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401386821258341226428529_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386821258341226428529_0004_m_000148_436
[2021-05-14 11:08:37,854] {docker.py:276} INFO - 21/05/14 14:08:37 INFO StagingCommitter: Task committer attempt_202105141401386821258341226428529_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386821258341226428529_0004_m_000148_436 : duration 0:00.005s
[2021-05-14 11:08:44,762] {docker.py:276} INFO - 21/05/14 14:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105141401386821258341226428529_0004_m_000148_436: needsTaskCommit() Task attempt_202105141401386821258341226428529_0004_m_000148_436
[2021-05-14 11:08:44,764] {docker.py:276} INFO - 21/05/14 14:08:44 INFO StagingCommitter: Task committer attempt_202105141401386821258341226428529_0004_m_000148_436: needsTaskCommit() Task attempt_202105141401386821258341226428529_0004_m_000148_436: duration 0:00.002s
[2021-05-14 11:08:44,764] {docker.py:276} INFO - 21/05/14 14:08:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386821258341226428529_0004_m_000148_436
[2021-05-14 11:08:44,769] {docker.py:276} INFO - 21/05/14 14:08:44 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 5106 bytes result sent to driver
[2021-05-14 11:08:44,771] {docker.py:276} INFO - 21/05/14 14:08:44 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (3e4cb2948233, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:44,771] {docker.py:276} INFO - 21/05/14 14:08:44 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 6918 ms on 3e4cb2948233 (executor driver) (146/200)
[2021-05-14 11:08:44,773] {docker.py:276} INFO - 21/05/14 14:08:44 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-14 11:08:44,781] {docker.py:276} INFO - 21/05/14 14:08:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:44,789] {docker.py:276} INFO - 21/05/14 14:08:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:08:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382493307664274477980_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382493307664274477980_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382493307664274477980_0004}; taskId=attempt_202105141401382493307664274477980_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53fc6489}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:44,789] {docker.py:276} INFO - 21/05/14 14:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105141401382493307664274477980_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382493307664274477980_0004_m_000149_437
[2021-05-14 11:08:44,794] {docker.py:276} INFO - 21/05/14 14:08:44 INFO StagingCommitter: Task committer attempt_202105141401382493307664274477980_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382493307664274477980_0004_m_000149_437 : duration 0:00.005s
[2021-05-14 11:08:51,669] {docker.py:276} INFO - 21/05/14 14:08:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401382493307664274477980_0004_m_000149_437: needsTaskCommit() Task attempt_202105141401382493307664274477980_0004_m_000149_437
[2021-05-14 11:08:51,669] {docker.py:276} INFO - 21/05/14 14:08:51 INFO StagingCommitter: Task committer attempt_202105141401382493307664274477980_0004_m_000149_437: needsTaskCommit() Task attempt_202105141401382493307664274477980_0004_m_000149_437: duration 0:00.000s
21/05/14 14:08:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382493307664274477980_0004_m_000149_437
[2021-05-14 11:08:51,672] {docker.py:276} INFO - 21/05/14 14:08:51 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 5106 bytes result sent to driver
[2021-05-14 11:08:51,673] {docker.py:276} INFO - 21/05/14 14:08:51 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (3e4cb2948233, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:51,674] {docker.py:276} INFO - 21/05/14 14:08:51 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
21/05/14 14:08:51 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 6912 ms on 3e4cb2948233 (executor driver) (147/200)
[2021-05-14 11:08:51,681] {docker.py:276} INFO - 21/05/14 14:08:51 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:08:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:51,689] {docker.py:276} INFO - 21/05/14 14:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:08:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385591583876543508595_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385591583876543508595_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385591583876543508595_0004}; taskId=attempt_202105141401385591583876543508595_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31ca1427}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:51,689] {docker.py:276} INFO - 21/05/14 14:08:51 INFO StagingCommitter: Starting: Task committer attempt_202105141401385591583876543508595_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385591583876543508595_0004_m_000150_438
[2021-05-14 11:08:51,693] {docker.py:276} INFO - 21/05/14 14:08:51 INFO StagingCommitter: Task committer attempt_202105141401385591583876543508595_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385591583876543508595_0004_m_000150_438 : duration 0:00.004s
[2021-05-14 11:08:58,493] {docker.py:276} INFO - 21/05/14 14:08:58 INFO StagingCommitter: Starting: Task committer attempt_202105141401385591583876543508595_0004_m_000150_438: needsTaskCommit() Task attempt_202105141401385591583876543508595_0004_m_000150_438
21/05/14 14:08:58 INFO StagingCommitter: Task committer attempt_202105141401385591583876543508595_0004_m_000150_438: needsTaskCommit() Task attempt_202105141401385591583876543508595_0004_m_000150_438: duration 0:00.001s
21/05/14 14:08:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385591583876543508595_0004_m_000150_438
[2021-05-14 11:08:58,495] {docker.py:276} INFO - 21/05/14 14:08:58 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 5106 bytes result sent to driver
[2021-05-14 11:08:58,497] {docker.py:276} INFO - 21/05/14 14:08:58 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (3e4cb2948233, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:08:58,498] {docker.py:276} INFO - 21/05/14 14:08:58 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 6834 ms on 3e4cb2948233 (executor driver) (148/200)
[2021-05-14 11:08:58,498] {docker.py:276} INFO - 21/05/14 14:08:58 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-14 11:08:58,506] {docker.py:276} INFO - 21/05/14 14:08:58 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:08:58,507] {docker.py:276} INFO - 21/05/14 14:08:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:08:58,516] {docker.py:276} INFO - 21/05/14 14:08:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:08:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:08:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:08:58,516] {docker.py:276} INFO - 21/05/14 14:08:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051414013874013931392662824_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013874013931392662824_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051414013874013931392662824_0004}; taskId=attempt_2021051414013874013931392662824_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7eb943bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:08:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:08:58,517] {docker.py:276} INFO - 21/05/14 14:08:58 INFO StagingCommitter: Starting: Task committer attempt_2021051414013874013931392662824_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013874013931392662824_0004_m_000151_439
[2021-05-14 11:08:58,520] {docker.py:276} INFO - 21/05/14 14:08:58 INFO StagingCommitter: Task committer attempt_2021051414013874013931392662824_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013874013931392662824_0004_m_000151_439 : duration 0:00.004s
[2021-05-14 11:09:05,320] {docker.py:276} INFO - 21/05/14 14:09:05 INFO StagingCommitter: Starting: Task committer attempt_2021051414013874013931392662824_0004_m_000151_439: needsTaskCommit() Task attempt_2021051414013874013931392662824_0004_m_000151_439
[2021-05-14 11:09:05,321] {docker.py:276} INFO - 21/05/14 14:09:05 INFO StagingCommitter: Task committer attempt_2021051414013874013931392662824_0004_m_000151_439: needsTaskCommit() Task attempt_2021051414013874013931392662824_0004_m_000151_439: duration 0:00.001s
21/05/14 14:09:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051414013874013931392662824_0004_m_000151_439
[2021-05-14 11:09:05,323] {docker.py:276} INFO - 21/05/14 14:09:05 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 5106 bytes result sent to driver
[2021-05-14 11:09:05,324] {docker.py:276} INFO - 21/05/14 14:09:05 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (3e4cb2948233, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:05,324] {docker.py:276} INFO - 21/05/14 14:09:05 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
[2021-05-14 11:09:05,325] {docker.py:276} INFO - 21/05/14 14:09:05 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 6837 ms on 3e4cb2948233 (executor driver) (149/200)
[2021-05-14 11:09:05,333] {docker.py:276} INFO - 21/05/14 14:09:05 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:05,341] {docker.py:276} INFO - 21/05/14 14:09:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:05,341] {docker.py:276} INFO - 21/05/14 14:09:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382115423381839037904_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382115423381839037904_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382115423381839037904_0004}; taskId=attempt_202105141401382115423381839037904_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53b1b3f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:05 INFO StagingCommitter: Starting: Task committer attempt_202105141401382115423381839037904_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382115423381839037904_0004_m_000152_440
[2021-05-14 11:09:05,345] {docker.py:276} INFO - 21/05/14 14:09:05 INFO StagingCommitter: Task committer attempt_202105141401382115423381839037904_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382115423381839037904_0004_m_000152_440 : duration 0:00.004s
[2021-05-14 11:09:12,435] {docker.py:276} INFO - 21/05/14 14:09:12 INFO StagingCommitter: Starting: Task committer attempt_202105141401382115423381839037904_0004_m_000152_440: needsTaskCommit() Task attempt_202105141401382115423381839037904_0004_m_000152_440
[2021-05-14 11:09:12,436] {docker.py:276} INFO - 21/05/14 14:09:12 INFO StagingCommitter: Task committer attempt_202105141401382115423381839037904_0004_m_000152_440: needsTaskCommit() Task attempt_202105141401382115423381839037904_0004_m_000152_440: duration 0:00.001s
21/05/14 14:09:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382115423381839037904_0004_m_000152_440
[2021-05-14 11:09:12,439] {docker.py:276} INFO - 21/05/14 14:09:12 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 5149 bytes result sent to driver
[2021-05-14 11:09:12,441] {docker.py:276} INFO - 21/05/14 14:09:12 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (3e4cb2948233, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:12,442] {docker.py:276} INFO - 21/05/14 14:09:12 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 7092 ms on 3e4cb2948233 (executor driver) (150/200)
21/05/14 14:09:12 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-14 11:09:12,451] {docker.py:276} INFO - 21/05/14 14:09:12 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:12,460] {docker.py:276} INFO - 21/05/14 14:09:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:09:12,461] {docker.py:276} INFO - 21/05/14 14:09:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383052694743049969807_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383052694743049969807_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383052694743049969807_0004}; taskId=attempt_202105141401383052694743049969807_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d8782ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:12,462] {docker.py:276} INFO - 21/05/14 14:09:12 INFO StagingCommitter: Starting: Task committer attempt_202105141401383052694743049969807_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383052694743049969807_0004_m_000153_441
[2021-05-14 11:09:12,467] {docker.py:276} INFO - 21/05/14 14:09:12 INFO StagingCommitter: Task committer attempt_202105141401383052694743049969807_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383052694743049969807_0004_m_000153_441 : duration 0:00.006s
[2021-05-14 11:09:18,184] {docker.py:276} INFO - 21/05/14 14:09:18 INFO StagingCommitter: Starting: Task committer attempt_20210514140138753723146425020541_0004_m_000122_410: needsTaskCommit() Task attempt_20210514140138753723146425020541_0004_m_000122_410
[2021-05-14 11:09:18,185] {docker.py:276} INFO - 21/05/14 14:09:18 INFO StagingCommitter: Task committer attempt_20210514140138753723146425020541_0004_m_000122_410: needsTaskCommit() Task attempt_20210514140138753723146425020541_0004_m_000122_410: duration 0:00.002s
21/05/14 14:09:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138753723146425020541_0004_m_000122_410
[2021-05-14 11:09:18,186] {docker.py:276} INFO - 21/05/14 14:09:18 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 5149 bytes result sent to driver
[2021-05-14 11:09:18,188] {docker.py:276} INFO - 21/05/14 14:09:18 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (3e4cb2948233, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:18,188] {docker.py:276} INFO - 21/05/14 14:09:18 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 206980 ms on 3e4cb2948233 (executor driver) (151/200)
[2021-05-14 11:09:18,189] {docker.py:276} INFO - 21/05/14 14:09:18 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-14 11:09:18,197] {docker.py:276} INFO - 21/05/14 14:09:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:18,206] {docker.py:276} INFO - 21/05/14 14:09:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:18,207] {docker.py:276} INFO - 21/05/14 14:09:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388458021308842623016_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388458021308842623016_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388458021308842623016_0004}; taskId=attempt_202105141401388458021308842623016_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65c0bfb3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:18,207] {docker.py:276} INFO - 21/05/14 14:09:18 INFO StagingCommitter: Starting: Task committer attempt_202105141401388458021308842623016_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388458021308842623016_0004_m_000154_442
[2021-05-14 11:09:18,212] {docker.py:276} INFO - 21/05/14 14:09:18 INFO StagingCommitter: Task committer attempt_202105141401388458021308842623016_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388458021308842623016_0004_m_000154_442 : duration 0:00.006s
[2021-05-14 11:09:19,331] {docker.py:276} INFO - 21/05/14 14:09:19 INFO StagingCommitter: Starting: Task committer attempt_202105141401383052694743049969807_0004_m_000153_441: needsTaskCommit() Task attempt_202105141401383052694743049969807_0004_m_000153_441
[2021-05-14 11:09:19,332] {docker.py:276} INFO - 21/05/14 14:09:19 INFO StagingCommitter: Task committer attempt_202105141401383052694743049969807_0004_m_000153_441: needsTaskCommit() Task attempt_202105141401383052694743049969807_0004_m_000153_441: duration 0:00.001s
[2021-05-14 11:09:19,333] {docker.py:276} INFO - 21/05/14 14:09:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383052694743049969807_0004_m_000153_441
[2021-05-14 11:09:19,334] {docker.py:276} INFO - 21/05/14 14:09:19 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 5106 bytes result sent to driver
[2021-05-14 11:09:19,334] {docker.py:276} INFO - 21/05/14 14:09:19 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (3e4cb2948233, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:19,335] {docker.py:276} INFO - 21/05/14 14:09:19 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
21/05/14 14:09:19 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 6904 ms on 3e4cb2948233 (executor driver) (152/200)
[2021-05-14 11:09:19,344] {docker.py:276} INFO - 21/05/14 14:09:19 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:19,355] {docker.py:276} INFO - 21/05/14 14:09:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381731426933102157616_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381731426933102157616_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381731426933102157616_0004}; taskId=attempt_202105141401381731426933102157616_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4750c57a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:19,355] {docker.py:276} INFO - 21/05/14 14:09:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:19 INFO StagingCommitter: Starting: Task committer attempt_202105141401381731426933102157616_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381731426933102157616_0004_m_000155_443
[2021-05-14 11:09:19,360] {docker.py:276} INFO - 21/05/14 14:09:19 INFO StagingCommitter: Task committer attempt_202105141401381731426933102157616_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381731426933102157616_0004_m_000155_443 : duration 0:00.005s
[2021-05-14 11:09:20,471] {docker.py:276} INFO - 21/05/14 14:09:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401387749978959760433705_0004_m_000123_411: needsTaskCommit() Task attempt_202105141401387749978959760433705_0004_m_000123_411
21/05/14 14:09:20 INFO StagingCommitter: Task committer attempt_202105141401387749978959760433705_0004_m_000123_411: needsTaskCommit() Task attempt_202105141401387749978959760433705_0004_m_000123_411: duration 0:00.002s
[2021-05-14 11:09:20,471] {docker.py:276} INFO - 21/05/14 14:09:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387749978959760433705_0004_m_000123_411
[2021-05-14 11:09:20,474] {docker.py:276} INFO - 21/05/14 14:09:20 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 5149 bytes result sent to driver
[2021-05-14 11:09:20,474] {docker.py:276} INFO - 21/05/14 14:09:20 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (3e4cb2948233, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:20,475] {docker.py:276} INFO - 21/05/14 14:09:20 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
21/05/14 14:09:20 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 207350 ms on 3e4cb2948233 (executor driver) (153/200)
[2021-05-14 11:09:20,484] {docker.py:276} INFO - 21/05/14 14:09:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:20,493] {docker.py:276} INFO - 21/05/14 14:09:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388400167578073568598_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388400167578073568598_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388400167578073568598_0004}; taskId=attempt_202105141401388400167578073568598_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5224b4dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401388400167578073568598_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388400167578073568598_0004_m_000156_444
[2021-05-14 11:09:20,499] {docker.py:276} INFO - 21/05/14 14:09:20 INFO StagingCommitter: Task committer attempt_202105141401388400167578073568598_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388400167578073568598_0004_m_000156_444 : duration 0:00.006s
[2021-05-14 11:09:20,699] {docker.py:276} INFO - 21/05/14 14:09:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401386290402127288780608_0004_m_000124_412: needsTaskCommit() Task attempt_202105141401386290402127288780608_0004_m_000124_412
[2021-05-14 11:09:20,700] {docker.py:276} INFO - 21/05/14 14:09:20 INFO StagingCommitter: Task committer attempt_202105141401386290402127288780608_0004_m_000124_412: needsTaskCommit() Task attempt_202105141401386290402127288780608_0004_m_000124_412: duration 0:00.001s
21/05/14 14:09:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386290402127288780608_0004_m_000124_412
[2021-05-14 11:09:20,701] {docker.py:276} INFO - 21/05/14 14:09:20 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 5149 bytes result sent to driver
[2021-05-14 11:09:20,702] {docker.py:276} INFO - 21/05/14 14:09:20 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (3e4cb2948233, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:20,703] {docker.py:276} INFO - 21/05/14 14:09:20 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-14 11:09:20,704] {docker.py:276} INFO - 21/05/14 14:09:20 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 207209 ms on 3e4cb2948233 (executor driver) (154/200)
[2021-05-14 11:09:20,716] {docker.py:276} INFO - 21/05/14 14:09:20 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:20,725] {docker.py:276} INFO - 21/05/14 14:09:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385429148138992601994_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385429148138992601994_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385429148138992601994_0004}; taskId=attempt_202105141401385429148138992601994_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5540df58}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:20,726] {docker.py:276} INFO - 21/05/14 14:09:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:20,726] {docker.py:276} INFO - 21/05/14 14:09:20 INFO StagingCommitter: Starting: Task committer attempt_202105141401385429148138992601994_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385429148138992601994_0004_m_000157_445
[2021-05-14 11:09:20,732] {docker.py:276} INFO - 21/05/14 14:09:20 INFO StagingCommitter: Task committer attempt_202105141401385429148138992601994_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385429148138992601994_0004_m_000157_445 : duration 0:00.006s
[2021-05-14 11:09:25,381] {docker.py:276} INFO - 21/05/14 14:09:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401388458021308842623016_0004_m_000154_442: needsTaskCommit() Task attempt_202105141401388458021308842623016_0004_m_000154_442
21/05/14 14:09:25 INFO StagingCommitter: Task committer attempt_202105141401388458021308842623016_0004_m_000154_442: needsTaskCommit() Task attempt_202105141401388458021308842623016_0004_m_000154_442: duration 0:00.003s
[2021-05-14 11:09:25,382] {docker.py:276} INFO - 21/05/14 14:09:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388458021308842623016_0004_m_000154_442
[2021-05-14 11:09:25,383] {docker.py:276} INFO - 21/05/14 14:09:25 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 5106 bytes result sent to driver
[2021-05-14 11:09:25,384] {docker.py:276} INFO - 21/05/14 14:09:25 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (3e4cb2948233, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:25,385] {docker.py:276} INFO - 21/05/14 14:09:25 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-14 11:09:25,386] {docker.py:276} INFO - 21/05/14 14:09:25 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 7206 ms on 3e4cb2948233 (executor driver) (155/200)
[2021-05-14 11:09:25,395] {docker.py:276} INFO - 21/05/14 14:09:25 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:25,403] {docker.py:276} INFO - 21/05/14 14:09:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382099202539705734320_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382099202539705734320_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382099202539705734320_0004}; taskId=attempt_202105141401382099202539705734320_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@390d6b96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:25 INFO StagingCommitter: Starting: Task committer attempt_202105141401382099202539705734320_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382099202539705734320_0004_m_000158_446
[2021-05-14 11:09:25,408] {docker.py:276} INFO - 21/05/14 14:09:25 INFO StagingCommitter: Task committer attempt_202105141401382099202539705734320_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382099202539705734320_0004_m_000158_446 : duration 0:00.004s
[2021-05-14 11:09:26,832] {docker.py:276} INFO - 21/05/14 14:09:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401388400167578073568598_0004_m_000156_444: needsTaskCommit() Task attempt_202105141401388400167578073568598_0004_m_000156_444
21/05/14 14:09:26 INFO StagingCommitter: Task committer attempt_202105141401388400167578073568598_0004_m_000156_444: needsTaskCommit() Task attempt_202105141401388400167578073568598_0004_m_000156_444: duration 0:00.002s
[2021-05-14 11:09:26,833] {docker.py:276} INFO - 21/05/14 14:09:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388400167578073568598_0004_m_000156_444
[2021-05-14 11:09:26,834] {docker.py:276} INFO - 21/05/14 14:09:26 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 5106 bytes result sent to driver
[2021-05-14 11:09:26,835] {docker.py:276} INFO - 21/05/14 14:09:26 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (3e4cb2948233, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:26,836] {docker.py:276} INFO - 21/05/14 14:09:26 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 6370 ms on 3e4cb2948233 (executor driver) (156/200)
[2021-05-14 11:09:26,837] {docker.py:276} INFO - 21/05/14 14:09:26 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
[2021-05-14 11:09:26,840] {docker.py:276} INFO - 21/05/14 14:09:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401381731426933102157616_0004_m_000155_443: needsTaskCommit() Task attempt_202105141401381731426933102157616_0004_m_000155_443
[2021-05-14 11:09:26,841] {docker.py:276} INFO - 21/05/14 14:09:26 INFO StagingCommitter: Task committer attempt_202105141401381731426933102157616_0004_m_000155_443: needsTaskCommit() Task attempt_202105141401381731426933102157616_0004_m_000155_443: duration 0:00.001s
[2021-05-14 11:09:26,841] {docker.py:276} INFO - 21/05/14 14:09:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381731426933102157616_0004_m_000155_443
[2021-05-14 11:09:26,842] {docker.py:276} INFO - 21/05/14 14:09:26 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 5106 bytes result sent to driver
[2021-05-14 11:09:26,842] {docker.py:276} INFO - 21/05/14 14:09:26 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (3e4cb2948233, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:26,843] {docker.py:276} INFO - 21/05/14 14:09:26 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 7518 ms on 3e4cb2948233 (executor driver) (157/200)
[2021-05-14 11:09:26,845] {docker.py:276} INFO - 21/05/14 14:09:26 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
[2021-05-14 11:09:26,852] {docker.py:276} INFO - 21/05/14 14:09:26 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:26,855] {docker.py:276} INFO - 21/05/14 14:09:26 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:26,860] {docker.py:276} INFO - 21/05/14 14:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388625904452340854856_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388625904452340854856_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388625904452340854856_0004}; taskId=attempt_202105141401388625904452340854856_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d70b1d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401388625904452340854856_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388625904452340854856_0004_m_000159_447
[2021-05-14 11:09:26,865] {docker.py:276} INFO - 21/05/14 14:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:09:26,865] {docker.py:276} INFO - 21/05/14 14:09:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388555741934273477515_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388555741934273477515_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388555741934273477515_0004}; taskId=attempt_202105141401388555741934273477515_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e29b97b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:26 INFO StagingCommitter: Starting: Task committer attempt_202105141401388555741934273477515_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388555741934273477515_0004_m_000160_448
[2021-05-14 11:09:26,866] {docker.py:276} INFO - 21/05/14 14:09:26 INFO StagingCommitter: Task committer attempt_202105141401388625904452340854856_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388625904452340854856_0004_m_000159_447 : duration 0:00.005s
[2021-05-14 11:09:26,870] {docker.py:276} INFO - 21/05/14 14:09:26 INFO StagingCommitter: Task committer attempt_202105141401388555741934273477515_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388555741934273477515_0004_m_000160_448 : duration 0:00.005s
[2021-05-14 11:09:27,840] {docker.py:276} INFO - 21/05/14 14:09:27 INFO StagingCommitter: Starting: Task committer attempt_202105141401385429148138992601994_0004_m_000157_445: needsTaskCommit() Task attempt_202105141401385429148138992601994_0004_m_000157_445
[2021-05-14 11:09:27,841] {docker.py:276} INFO - 21/05/14 14:09:27 INFO StagingCommitter: Task committer attempt_202105141401385429148138992601994_0004_m_000157_445: needsTaskCommit() Task attempt_202105141401385429148138992601994_0004_m_000157_445: duration 0:00.001s
21/05/14 14:09:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385429148138992601994_0004_m_000157_445
[2021-05-14 11:09:27,843] {docker.py:276} INFO - 21/05/14 14:09:27 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 5106 bytes result sent to driver
[2021-05-14 11:09:27,844] {docker.py:276} INFO - 21/05/14 14:09:27 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (3e4cb2948233, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:27,845] {docker.py:276} INFO - 21/05/14 14:09:27 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
21/05/14 14:09:27 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 7152 ms on 3e4cb2948233 (executor driver) (158/200)
[2021-05-14 11:09:27,853] {docker.py:276} INFO - 21/05/14 14:09:27 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:09:27,853] {docker.py:276} INFO - 21/05/14 14:09:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:27,862] {docker.py:276} INFO - 21/05/14 14:09:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:09:27,862] {docker.py:276} INFO - 21/05/14 14:09:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:09:27,863] {docker.py:276} INFO - 21/05/14 14:09:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:27,863] {docker.py:276} INFO - 21/05/14 14:09:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384262847793803233746_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384262847793803233746_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384262847793803233746_0004}; taskId=attempt_202105141401384262847793803233746_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3093af96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:27,863] {docker.py:276} INFO - 21/05/14 14:09:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:27,863] {docker.py:276} INFO - 21/05/14 14:09:27 INFO StagingCommitter: Starting: Task committer attempt_202105141401384262847793803233746_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384262847793803233746_0004_m_000161_449
[2021-05-14 11:09:27,867] {docker.py:276} INFO - 21/05/14 14:09:27 INFO StagingCommitter: Task committer attempt_202105141401384262847793803233746_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384262847793803233746_0004_m_000161_449 : duration 0:00.004s
[2021-05-14 11:09:32,160] {docker.py:276} INFO - 21/05/14 14:09:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401382099202539705734320_0004_m_000158_446: needsTaskCommit() Task attempt_202105141401382099202539705734320_0004_m_000158_446
[2021-05-14 11:09:32,161] {docker.py:276} INFO - 21/05/14 14:09:32 INFO StagingCommitter: Task committer attempt_202105141401382099202539705734320_0004_m_000158_446: needsTaskCommit() Task attempt_202105141401382099202539705734320_0004_m_000158_446: duration 0:00.001s
21/05/14 14:09:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382099202539705734320_0004_m_000158_446
[2021-05-14 11:09:32,163] {docker.py:276} INFO - 21/05/14 14:09:32 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 5149 bytes result sent to driver
[2021-05-14 11:09:32,164] {docker.py:276} INFO - 21/05/14 14:09:32 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (3e4cb2948233, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:32,165] {docker.py:276} INFO - 21/05/14 14:09:32 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-14 11:09:32,166] {docker.py:276} INFO - 21/05/14 14:09:32 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 6790 ms on 3e4cb2948233 (executor driver) (159/200)
[2021-05-14 11:09:32,173] {docker.py:276} INFO - 21/05/14 14:09:32 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:32,182] {docker.py:276} INFO - 21/05/14 14:09:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:32,183] {docker.py:276} INFO - 21/05/14 14:09:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385204354471461248244_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385204354471461248244_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385204354471461248244_0004}; taskId=attempt_202105141401385204354471461248244_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b3ab011}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:32 INFO StagingCommitter: Starting: Task committer attempt_202105141401385204354471461248244_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385204354471461248244_0004_m_000162_450
[2021-05-14 11:09:32,187] {docker.py:276} INFO - 21/05/14 14:09:32 INFO StagingCommitter: Task committer attempt_202105141401385204354471461248244_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385204354471461248244_0004_m_000162_450 : duration 0:00.005s
[2021-05-14 11:09:33,506] {docker.py:276} INFO - 21/05/14 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105141401388625904452340854856_0004_m_000159_447: needsTaskCommit() Task attempt_202105141401388625904452340854856_0004_m_000159_447
[2021-05-14 11:09:33,507] {docker.py:276} INFO - 21/05/14 14:09:33 INFO StagingCommitter: Task committer attempt_202105141401388625904452340854856_0004_m_000159_447: needsTaskCommit() Task attempt_202105141401388625904452340854856_0004_m_000159_447: duration 0:00.001s
21/05/14 14:09:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388625904452340854856_0004_m_000159_447
[2021-05-14 11:09:33,508] {docker.py:276} INFO - 21/05/14 14:09:33 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 5149 bytes result sent to driver
[2021-05-14 11:09:33,509] {docker.py:276} INFO - 21/05/14 14:09:33 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (3e4cb2948233, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:33,510] {docker.py:276} INFO - 21/05/14 14:09:33 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-14 11:09:33,510] {docker.py:276} INFO - 21/05/14 14:09:33 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 6683 ms on 3e4cb2948233 (executor driver) (160/200)
[2021-05-14 11:09:33,518] {docker.py:276} INFO - 21/05/14 14:09:33 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:09:33,518] {docker.py:276} INFO - 21/05/14 14:09:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:33,525] {docker.py:276} INFO - 21/05/14 14:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:09:33,525] {docker.py:276} INFO - 21/05/14 14:09:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:33,526] {docker.py:276} INFO - 21/05/14 14:09:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382484439000470455810_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382484439000470455810_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382484439000470455810_0004}; taskId=attempt_202105141401382484439000470455810_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b332afd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:33,526] {docker.py:276} INFO - 21/05/14 14:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:33,526] {docker.py:276} INFO - 21/05/14 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105141401382484439000470455810_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382484439000470455810_0004_m_000163_451
[2021-05-14 11:09:33,529] {docker.py:276} INFO - 21/05/14 14:09:33 INFO StagingCommitter: Task committer attempt_202105141401382484439000470455810_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382484439000470455810_0004_m_000163_451 : duration 0:00.004s
[2021-05-14 11:09:33,995] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401388555741934273477515_0004_m_000160_448: needsTaskCommit() Task attempt_202105141401388555741934273477515_0004_m_000160_448
[2021-05-14 11:09:33,996] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Task committer attempt_202105141401388555741934273477515_0004_m_000160_448: needsTaskCommit() Task attempt_202105141401388555741934273477515_0004_m_000160_448: duration 0:00.003s
21/05/14 14:09:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388555741934273477515_0004_m_000160_448
[2021-05-14 11:09:33,998] {docker.py:276} INFO - 21/05/14 14:09:34 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 5149 bytes result sent to driver
[2021-05-14 11:09:33,998] {docker.py:276} INFO - 21/05/14 14:09:34 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (3e4cb2948233, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:34,000] {docker.py:276} INFO - 21/05/14 14:09:34 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 7166 ms on 3e4cb2948233 (executor driver) (161/200)
[2021-05-14 11:09:34,001] {docker.py:276} INFO - 21/05/14 14:09:34 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-14 11:09:34,011] {docker.py:276} INFO - 21/05/14 14:09:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:34,019] {docker.py:276} INFO - 21/05/14 14:09:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386640440180055790756_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386640440180055790756_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386640440180055790756_0004}; taskId=attempt_202105141401386640440180055790756_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3955b80b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401386640440180055790756_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386640440180055790756_0004_m_000164_452
[2021-05-14 11:09:34,026] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Task committer attempt_202105141401386640440180055790756_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386640440180055790756_0004_m_000164_452 : duration 0:00.008s
[2021-05-14 11:09:34,541] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401384262847793803233746_0004_m_000161_449: needsTaskCommit() Task attempt_202105141401384262847793803233746_0004_m_000161_449
[2021-05-14 11:09:34,542] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Task committer attempt_202105141401384262847793803233746_0004_m_000161_449: needsTaskCommit() Task attempt_202105141401384262847793803233746_0004_m_000161_449: duration 0:00.002s
[2021-05-14 11:09:34,542] {docker.py:276} INFO - 21/05/14 14:09:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384262847793803233746_0004_m_000161_449
[2021-05-14 11:09:34,544] {docker.py:276} INFO - 21/05/14 14:09:34 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 5149 bytes result sent to driver
[2021-05-14 11:09:34,545] {docker.py:276} INFO - 21/05/14 14:09:34 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (3e4cb2948233, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:34,547] {docker.py:276} INFO - 21/05/14 14:09:34 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
[2021-05-14 11:09:34,548] {docker.py:276} INFO - 21/05/14 14:09:34 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 6711 ms on 3e4cb2948233 (executor driver) (162/200)
[2021-05-14 11:09:34,557] {docker.py:276} INFO - 21/05/14 14:09:34 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:34,565] {docker.py:276} INFO - 21/05/14 14:09:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:09:34,566] {docker.py:276} INFO - 21/05/14 14:09:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:09:34,566] {docker.py:276} INFO - 21/05/14 14:09:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:34,566] {docker.py:276} INFO - 21/05/14 14:09:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386031773774183631487_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386031773774183631487_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386031773774183631487_0004}; taskId=attempt_202105141401386031773774183631487_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@241d11b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:34,567] {docker.py:276} INFO - 21/05/14 14:09:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:34,567] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Starting: Task committer attempt_202105141401386031773774183631487_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386031773774183631487_0004_m_000165_453
[2021-05-14 11:09:34,570] {docker.py:276} INFO - 21/05/14 14:09:34 INFO StagingCommitter: Task committer attempt_202105141401386031773774183631487_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386031773774183631487_0004_m_000165_453 : duration 0:00.004s
[2021-05-14 11:09:39,272] {docker.py:276} INFO - 21/05/14 14:09:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401385204354471461248244_0004_m_000162_450: needsTaskCommit() Task attempt_202105141401385204354471461248244_0004_m_000162_450
[2021-05-14 11:09:39,274] {docker.py:276} INFO - 21/05/14 14:09:39 INFO StagingCommitter: Task committer attempt_202105141401385204354471461248244_0004_m_000162_450: needsTaskCommit() Task attempt_202105141401385204354471461248244_0004_m_000162_450: duration 0:00.002s
21/05/14 14:09:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385204354471461248244_0004_m_000162_450
[2021-05-14 11:09:39,275] {docker.py:276} INFO - 21/05/14 14:09:39 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 5106 bytes result sent to driver
[2021-05-14 11:09:39,276] {docker.py:276} INFO - 21/05/14 14:09:39 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (3e4cb2948233, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:09:39 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 7120 ms on 3e4cb2948233 (executor driver) (163/200)
21/05/14 14:09:39 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
[2021-05-14 11:09:39,285] {docker.py:276} INFO - 21/05/14 14:09:39 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:39,293] {docker.py:276} INFO - 21/05/14 14:09:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:39,294] {docker.py:276} INFO - 21/05/14 14:09:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383543771708214812956_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383543771708214812956_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383543771708214812956_0004}; taskId=attempt_202105141401383543771708214812956_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63a077a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:39 INFO StagingCommitter: Starting: Task committer attempt_202105141401383543771708214812956_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383543771708214812956_0004_m_000166_454
[2021-05-14 11:09:39,298] {docker.py:276} INFO - 21/05/14 14:09:39 INFO StagingCommitter: Task committer attempt_202105141401383543771708214812956_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383543771708214812956_0004_m_000166_454 : duration 0:00.005s
[2021-05-14 11:09:39,986] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Starting: Task committer attempt_202105141401382484439000470455810_0004_m_000163_451: needsTaskCommit() Task attempt_202105141401382484439000470455810_0004_m_000163_451
[2021-05-14 11:09:39,987] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Task committer attempt_202105141401382484439000470455810_0004_m_000163_451: needsTaskCommit() Task attempt_202105141401382484439000470455810_0004_m_000163_451: duration 0:00.002s
21/05/14 14:09:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382484439000470455810_0004_m_000163_451
[2021-05-14 11:09:39,989] {docker.py:276} INFO - 21/05/14 14:09:40 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 5106 bytes result sent to driver
[2021-05-14 11:09:39,990] {docker.py:276} INFO - 21/05/14 14:09:40 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (3e4cb2948233, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:39,991] {docker.py:276} INFO - 21/05/14 14:09:40 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 6488 ms on 3e4cb2948233 (executor driver) (164/200)
21/05/14 14:09:40 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-14 11:09:40,001] {docker.py:276} INFO - 21/05/14 14:09:40 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:40,009] {docker.py:276} INFO - 21/05/14 14:09:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138610521296207583959_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138610521296207583959_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138610521296207583959_0004}; taskId=attempt_20210514140138610521296207583959_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@694406fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:40 INFO StagingCommitter: Starting: Task committer attempt_20210514140138610521296207583959_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138610521296207583959_0004_m_000167_455
[2021-05-14 11:09:40,013] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Task committer attempt_20210514140138610521296207583959_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138610521296207583959_0004_m_000167_455 : duration 0:00.004s
[2021-05-14 11:09:40,926] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Starting: Task committer attempt_202105141401386640440180055790756_0004_m_000164_452: needsTaskCommit() Task attempt_202105141401386640440180055790756_0004_m_000164_452
[2021-05-14 11:09:40,933] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Task committer attempt_202105141401386640440180055790756_0004_m_000164_452: needsTaskCommit() Task attempt_202105141401386640440180055790756_0004_m_000164_452: duration 0:00.003s
21/05/14 14:09:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386640440180055790756_0004_m_000164_452
[2021-05-14 11:09:40,934] {docker.py:276} INFO - 21/05/14 14:09:40 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 5106 bytes result sent to driver
[2021-05-14 11:09:40,934] {docker.py:276} INFO - 21/05/14 14:09:40 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (3e4cb2948233, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:40,935] {docker.py:276} INFO - 21/05/14 14:09:40 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
[2021-05-14 11:09:40,935] {docker.py:276} INFO - 21/05/14 14:09:40 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 6907 ms on 3e4cb2948233 (executor driver) (165/200)
[2021-05-14 11:09:40,950] {docker.py:276} INFO - 21/05/14 14:09:40 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:40,961] {docker.py:276} INFO - 21/05/14 14:09:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386065058213459788837_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386065058213459788837_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386065058213459788837_0004}; taskId=attempt_202105141401386065058213459788837_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e023ca3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:40,962] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Starting: Task committer attempt_202105141401386065058213459788837_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386065058213459788837_0004_m_000168_456
[2021-05-14 11:09:40,966] {docker.py:276} INFO - 21/05/14 14:09:40 INFO StagingCommitter: Task committer attempt_202105141401386065058213459788837_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386065058213459788837_0004_m_000168_456 : duration 0:00.005s
[2021-05-14 11:09:41,402] {docker.py:276} INFO - 21/05/14 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401386031773774183631487_0004_m_000165_453: needsTaskCommit() Task attempt_202105141401386031773774183631487_0004_m_000165_453
[2021-05-14 11:09:41,403] {docker.py:276} INFO - 21/05/14 14:09:41 INFO StagingCommitter: Task committer attempt_202105141401386031773774183631487_0004_m_000165_453: needsTaskCommit() Task attempt_202105141401386031773774183631487_0004_m_000165_453: duration 0:00.001s
21/05/14 14:09:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386031773774183631487_0004_m_000165_453
[2021-05-14 11:09:41,405] {docker.py:276} INFO - 21/05/14 14:09:41 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 5106 bytes result sent to driver
21/05/14 14:09:41 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (3e4cb2948233, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:41,406] {docker.py:276} INFO - 21/05/14 14:09:41 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-14 11:09:41,407] {docker.py:276} INFO - 21/05/14 14:09:41 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 6835 ms on 3e4cb2948233 (executor driver) (166/200)
[2021-05-14 11:09:41,415] {docker.py:276} INFO - 21/05/14 14:09:41 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:41,423] {docker.py:276} INFO - 21/05/14 14:09:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386094793253776070205_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386094793253776070205_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386094793253776070205_0004}; taskId=attempt_202105141401386094793253776070205_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@260d1103}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105141401386094793253776070205_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386094793253776070205_0004_m_000169_457
[2021-05-14 11:09:41,428] {docker.py:276} INFO - 21/05/14 14:09:41 INFO StagingCommitter: Task committer attempt_202105141401386094793253776070205_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386094793253776070205_0004_m_000169_457 : duration 0:00.005s
[2021-05-14 11:09:45,964] {docker.py:276} INFO - 21/05/14 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105141401383543771708214812956_0004_m_000166_454: needsTaskCommit() Task attempt_202105141401383543771708214812956_0004_m_000166_454
[2021-05-14 11:09:45,965] {docker.py:276} INFO - 21/05/14 14:09:45 INFO StagingCommitter: Task committer attempt_202105141401383543771708214812956_0004_m_000166_454: needsTaskCommit() Task attempt_202105141401383543771708214812956_0004_m_000166_454: duration 0:00.001s
21/05/14 14:09:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383543771708214812956_0004_m_000166_454
[2021-05-14 11:09:45,968] {docker.py:276} INFO - 21/05/14 14:09:45 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 5106 bytes result sent to driver
[2021-05-14 11:09:45,969] {docker.py:276} INFO - 21/05/14 14:09:45 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (3e4cb2948233, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:09:45 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
[2021-05-14 11:09:45,970] {docker.py:276} INFO - 21/05/14 14:09:45 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 6668 ms on 3e4cb2948233 (executor driver) (167/200)
[2021-05-14 11:09:45,977] {docker.py:276} INFO - 21/05/14 14:09:45 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:45,985] {docker.py:276} INFO - 21/05/14 14:09:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387363462315979701636_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387363462315979701636_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387363462315979701636_0004}; taskId=attempt_202105141401387363462315979701636_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48a30816}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105141401387363462315979701636_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387363462315979701636_0004_m_000170_458
[2021-05-14 11:09:45,988] {docker.py:276} INFO - 21/05/14 14:09:45 INFO StagingCommitter: Task committer attempt_202105141401387363462315979701636_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387363462315979701636_0004_m_000170_458 : duration 0:00.003s
[2021-05-14 11:09:47,000] {docker.py:276} INFO - 21/05/14 14:09:47 INFO StagingCommitter: Starting: Task committer attempt_20210514140138610521296207583959_0004_m_000167_455: needsTaskCommit() Task attempt_20210514140138610521296207583959_0004_m_000167_455
[2021-05-14 11:09:47,001] {docker.py:276} INFO - 21/05/14 14:09:47 INFO StagingCommitter: Task committer attempt_20210514140138610521296207583959_0004_m_000167_455: needsTaskCommit() Task attempt_20210514140138610521296207583959_0004_m_000167_455: duration 0:00.002s
[2021-05-14 11:09:47,001] {docker.py:276} INFO - 21/05/14 14:09:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138610521296207583959_0004_m_000167_455
[2021-05-14 11:09:47,002] {docker.py:276} INFO - 21/05/14 14:09:47 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 5106 bytes result sent to driver
[2021-05-14 11:09:47,003] {docker.py:276} INFO - 21/05/14 14:09:47 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (3e4cb2948233, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:47,004] {docker.py:276} INFO - 21/05/14 14:09:47 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
[2021-05-14 11:09:47,004] {docker.py:276} INFO - 21/05/14 14:09:47 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 6988 ms on 3e4cb2948233 (executor driver) (168/200)
[2021-05-14 11:09:47,013] {docker.py:276} INFO - 21/05/14 14:09:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:47,020] {docker.py:276} INFO - 21/05/14 14:09:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386179480349845173057_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386179480349845173057_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386179480349845173057_0004}; taskId=attempt_202105141401386179480349845173057_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60ae1895}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:47 INFO StagingCommitter: Starting: Task committer attempt_202105141401386179480349845173057_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386179480349845173057_0004_m_000171_459
[2021-05-14 11:09:47,024] {docker.py:276} INFO - 21/05/14 14:09:47 INFO StagingCommitter: Task committer attempt_202105141401386179480349845173057_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386179480349845173057_0004_m_000171_459 : duration 0:00.004s
[2021-05-14 11:09:48,028] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401386065058213459788837_0004_m_000168_456: needsTaskCommit() Task attempt_202105141401386065058213459788837_0004_m_000168_456
[2021-05-14 11:09:48,029] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Task committer attempt_202105141401386065058213459788837_0004_m_000168_456: needsTaskCommit() Task attempt_202105141401386065058213459788837_0004_m_000168_456: duration 0:00.002s
[2021-05-14 11:09:48,029] {docker.py:276} INFO - 21/05/14 14:09:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386065058213459788837_0004_m_000168_456
[2021-05-14 11:09:48,030] {docker.py:276} INFO - 21/05/14 14:09:48 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 5106 bytes result sent to driver
[2021-05-14 11:09:48,031] {docker.py:276} INFO - 21/05/14 14:09:48 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (3e4cb2948233, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:48,032] {docker.py:276} INFO - 21/05/14 14:09:48 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 7110 ms on 3e4cb2948233 (executor driver) (169/200)
[2021-05-14 11:09:48,033] {docker.py:276} INFO - 21/05/14 14:09:48 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
[2021-05-14 11:09:48,054] {docker.py:276} INFO - 21/05/14 14:09:48 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:48,064] {docker.py:276} INFO - 21/05/14 14:09:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:48,065] {docker.py:276} INFO - 21/05/14 14:09:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138461970573334681316_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138461970573334681316_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138461970573334681316_0004}; taskId=attempt_20210514140138461970573334681316_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@120ddbf0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:48,065] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_20210514140138461970573334681316_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138461970573334681316_0004_m_000172_460
[2021-05-14 11:09:48,071] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Task committer attempt_20210514140138461970573334681316_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138461970573334681316_0004_m_000172_460 : duration 0:00.006s
[2021-05-14 11:09:48,244] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401386094793253776070205_0004_m_000169_457: needsTaskCommit() Task attempt_202105141401386094793253776070205_0004_m_000169_457
[2021-05-14 11:09:48,245] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Task committer attempt_202105141401386094793253776070205_0004_m_000169_457: needsTaskCommit() Task attempt_202105141401386094793253776070205_0004_m_000169_457: duration 0:00.003s
21/05/14 14:09:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386094793253776070205_0004_m_000169_457
[2021-05-14 11:09:48,247] {docker.py:276} INFO - 21/05/14 14:09:48 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 5149 bytes result sent to driver
[2021-05-14 11:09:48,248] {docker.py:276} INFO - 21/05/14 14:09:48 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (3e4cb2948233, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:48,249] {docker.py:276} INFO - 21/05/14 14:09:48 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
[2021-05-14 11:09:48,250] {docker.py:276} INFO - 21/05/14 14:09:48 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 6852 ms on 3e4cb2948233 (executor driver) (170/200)
[2021-05-14 11:09:48,259] {docker.py:276} INFO - 21/05/14 14:09:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:48,269] {docker.py:276} INFO - 21/05/14 14:09:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:09:48,270] {docker.py:276} INFO - 21/05/14 14:09:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388823458631503252452_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388823458631503252452_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388823458631503252452_0004}; taskId=attempt_202105141401388823458631503252452_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10c4fb3f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:48,270] {docker.py:276} INFO - 21/05/14 14:09:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:48,270] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105141401388823458631503252452_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388823458631503252452_0004_m_000173_461
[2021-05-14 11:09:48,274] {docker.py:276} INFO - 21/05/14 14:09:48 INFO StagingCommitter: Task committer attempt_202105141401388823458631503252452_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388823458631503252452_0004_m_000173_461 : duration 0:00.004s
[2021-05-14 11:09:53,451] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401387363462315979701636_0004_m_000170_458: needsTaskCommit() Task attempt_202105141401387363462315979701636_0004_m_000170_458
21/05/14 14:09:53 INFO StagingCommitter: Task committer attempt_202105141401387363462315979701636_0004_m_000170_458: needsTaskCommit() Task attempt_202105141401387363462315979701636_0004_m_000170_458: duration 0:00.002s
21/05/14 14:09:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387363462315979701636_0004_m_000170_458
[2021-05-14 11:09:53,456] {docker.py:276} INFO - 21/05/14 14:09:53 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 5149 bytes result sent to driver
[2021-05-14 11:09:53,457] {docker.py:276} INFO - 21/05/14 14:09:53 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (3e4cb2948233, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:53,458] {docker.py:276} INFO - 21/05/14 14:09:53 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-14 11:09:53,458] {docker.py:276} INFO - 21/05/14 14:09:53 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 7498 ms on 3e4cb2948233 (executor driver) (171/200)
[2021-05-14 11:09:53,470] {docker.py:276} INFO - 21/05/14 14:09:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:53,481] {docker.py:276} INFO - 21/05/14 14:09:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386355373939311709466_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386355373939311709466_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386355373939311709466_0004}; taskId=attempt_202105141401386355373939311709466_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d928ce9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:53,482] {docker.py:276} INFO - 21/05/14 14:09:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:53,482] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401386355373939311709466_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386355373939311709466_0004_m_000174_462
[2021-05-14 11:09:53,486] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Task committer attempt_202105141401386355373939311709466_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386355373939311709466_0004_m_000174_462 : duration 0:00.004s
[2021-05-14 11:09:53,626] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401386179480349845173057_0004_m_000171_459: needsTaskCommit() Task attempt_202105141401386179480349845173057_0004_m_000171_459
[2021-05-14 11:09:53,627] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Task committer attempt_202105141401386179480349845173057_0004_m_000171_459: needsTaskCommit() Task attempt_202105141401386179480349845173057_0004_m_000171_459: duration 0:00.002s
21/05/14 14:09:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386179480349845173057_0004_m_000171_459
[2021-05-14 11:09:53,634] {docker.py:276} INFO - 21/05/14 14:09:53 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 5149 bytes result sent to driver
[2021-05-14 11:09:53,636] {docker.py:276} INFO - 21/05/14 14:09:53 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (3e4cb2948233, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:53,636] {docker.py:276} INFO - 21/05/14 14:09:53 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-14 11:09:53,637] {docker.py:276} INFO - 21/05/14 14:09:53 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 6642 ms on 3e4cb2948233 (executor driver) (172/200)
[2021-05-14 11:09:53,646] {docker.py:276} INFO - 21/05/14 14:09:53 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:53,654] {docker.py:276} INFO - 21/05/14 14:09:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384055923201160972589_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384055923201160972589_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384055923201160972589_0004}; taskId=attempt_202105141401384055923201160972589_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f5eeaa9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:09:53,654] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105141401384055923201160972589_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384055923201160972589_0004_m_000175_463
[2021-05-14 11:09:53,659] {docker.py:276} INFO - 21/05/14 14:09:53 INFO StagingCommitter: Task committer attempt_202105141401384055923201160972589_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384055923201160972589_0004_m_000175_463 : duration 0:00.005s
[2021-05-14 11:09:55,567] {docker.py:276} INFO - 21/05/14 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105141401388823458631503252452_0004_m_000173_461: needsTaskCommit() Task attempt_202105141401388823458631503252452_0004_m_000173_461
[2021-05-14 11:09:55,567] {docker.py:276} INFO - 21/05/14 14:09:55 INFO StagingCommitter: Task committer attempt_202105141401388823458631503252452_0004_m_000173_461: needsTaskCommit() Task attempt_202105141401388823458631503252452_0004_m_000173_461: duration 0:00.001s
21/05/14 14:09:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388823458631503252452_0004_m_000173_461
[2021-05-14 11:09:55,570] {docker.py:276} INFO - 21/05/14 14:09:55 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 5106 bytes result sent to driver
21/05/14 14:09:55 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (3e4cb2948233, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:55,571] {docker.py:276} INFO - 21/05/14 14:09:55 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-14 11:09:55,572] {docker.py:276} INFO - 21/05/14 14:09:55 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 7332 ms on 3e4cb2948233 (executor driver) (173/200)
[2021-05-14 11:09:55,582] {docker.py:276} INFO - 21/05/14 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:55,589] {docker.py:276} INFO - 21/05/14 14:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:09:55,589] {docker.py:276} INFO - 21/05/14 14:09:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388658815479088025416_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388658815479088025416_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388658815479088025416_0004}; taskId=attempt_202105141401388658815479088025416_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@787c7f8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105141401388658815479088025416_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388658815479088025416_0004_m_000176_464
[2021-05-14 11:09:55,592] {docker.py:276} INFO - 21/05/14 14:09:55 INFO StagingCommitter: Task committer attempt_202105141401388658815479088025416_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388658815479088025416_0004_m_000176_464 : duration 0:00.003s
[2021-05-14 11:09:55,761] {docker.py:276} INFO - 21/05/14 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_20210514140138461970573334681316_0004_m_000172_460: needsTaskCommit() Task attempt_20210514140138461970573334681316_0004_m_000172_460
[2021-05-14 11:09:55,761] {docker.py:276} INFO - 21/05/14 14:09:55 INFO StagingCommitter: Task committer attempt_20210514140138461970573334681316_0004_m_000172_460: needsTaskCommit() Task attempt_20210514140138461970573334681316_0004_m_000172_460: duration 0:00.003s
21/05/14 14:09:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138461970573334681316_0004_m_000172_460
[2021-05-14 11:09:55,763] {docker.py:276} INFO - 21/05/14 14:09:55 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 5149 bytes result sent to driver
[2021-05-14 11:09:55,764] {docker.py:276} INFO - 21/05/14 14:09:55 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (3e4cb2948233, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:09:55,765] {docker.py:276} INFO - 21/05/14 14:09:55 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
21/05/14 14:09:55 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 7744 ms on 3e4cb2948233 (executor driver) (174/200)
[2021-05-14 11:09:55,775] {docker.py:276} INFO - 21/05/14 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:09:55,783] {docker.py:276} INFO - 21/05/14 14:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:09:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:09:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401388516574754736705551_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388516574754736705551_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401388516574754736705551_0004}; taskId=attempt_202105141401388516574754736705551_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bf68a0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105141401388516574754736705551_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388516574754736705551_0004_m_000177_465
[2021-05-14 11:09:55,786] {docker.py:276} INFO - 21/05/14 14:09:55 INFO StagingCommitter: Task committer attempt_202105141401388516574754736705551_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401388516574754736705551_0004_m_000177_465 : duration 0:00.004s
[2021-05-14 11:10:00,521] {docker.py:276} INFO - 21/05/14 14:10:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401384055923201160972589_0004_m_000175_463: needsTaskCommit() Task attempt_202105141401384055923201160972589_0004_m_000175_463
[2021-05-14 11:10:00,522] {docker.py:276} INFO - 21/05/14 14:10:00 INFO StagingCommitter: Task committer attempt_202105141401384055923201160972589_0004_m_000175_463: needsTaskCommit() Task attempt_202105141401384055923201160972589_0004_m_000175_463: duration 0:00.001s
21/05/14 14:10:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384055923201160972589_0004_m_000175_463
[2021-05-14 11:10:00,524] {docker.py:276} INFO - 21/05/14 14:10:00 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 5106 bytes result sent to driver
[2021-05-14 11:10:00,524] {docker.py:276} INFO - 21/05/14 14:10:00 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (3e4cb2948233, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:00,526] {docker.py:276} INFO - 21/05/14 14:10:00 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
21/05/14 14:10:00 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 6898 ms on 3e4cb2948233 (executor driver) (175/200)
[2021-05-14 11:10:00,535] {docker.py:276} INFO - 21/05/14 14:10:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:00,544] {docker.py:276} INFO - 21/05/14 14:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387762676276924762190_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387762676276924762190_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387762676276924762190_0004}; taskId=attempt_202105141401387762676276924762190_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7feff7c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:00,545] {docker.py:276} INFO - 21/05/14 14:10:00 INFO StagingCommitter: Starting: Task committer attempt_202105141401387762676276924762190_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387762676276924762190_0004_m_000178_466
[2021-05-14 11:10:00,549] {docker.py:276} INFO - 21/05/14 14:10:00 INFO StagingCommitter: Task committer attempt_202105141401387762676276924762190_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387762676276924762190_0004_m_000178_466 : duration 0:00.005s
[2021-05-14 11:10:01,070] {docker.py:276} INFO - 21/05/14 14:10:01 INFO StagingCommitter: Starting: Task committer attempt_202105141401386355373939311709466_0004_m_000174_462: needsTaskCommit() Task attempt_202105141401386355373939311709466_0004_m_000174_462
[2021-05-14 11:10:01,071] {docker.py:276} INFO - 21/05/14 14:10:01 INFO StagingCommitter: Task committer attempt_202105141401386355373939311709466_0004_m_000174_462: needsTaskCommit() Task attempt_202105141401386355373939311709466_0004_m_000174_462: duration 0:00.001s
21/05/14 14:10:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386355373939311709466_0004_m_000174_462
[2021-05-14 11:10:01,075] {docker.py:276} INFO - 21/05/14 14:10:01 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 5106 bytes result sent to driver
21/05/14 14:10:01 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (3e4cb2948233, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:10:01 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
21/05/14 14:10:01 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 7629 ms on 3e4cb2948233 (executor driver) (176/200)
[2021-05-14 11:10:01,095] {docker.py:276} INFO - 21/05/14 14:10:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:10:01,112] {docker.py:276} INFO - 21/05/14 14:10:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:01,113] {docker.py:276} INFO - 21/05/14 14:10:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:01,113] {docker.py:276} INFO - 21/05/14 14:10:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401384655811070452746307_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384655811070452746307_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401384655811070452746307_0004}; taskId=attempt_202105141401384655811070452746307_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f6de5dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:01,114] {docker.py:276} INFO - 21/05/14 14:10:01 INFO StagingCommitter: Starting: Task committer attempt_202105141401384655811070452746307_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384655811070452746307_0004_m_000179_467
[2021-05-14 11:10:01,119] {docker.py:276} INFO - 21/05/14 14:10:01 INFO StagingCommitter: Task committer attempt_202105141401384655811070452746307_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401384655811070452746307_0004_m_000179_467 : duration 0:00.005s
[2021-05-14 11:10:02,796] {docker.py:276} INFO - 21/05/14 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401388516574754736705551_0004_m_000177_465: needsTaskCommit() Task attempt_202105141401388516574754736705551_0004_m_000177_465
[2021-05-14 11:10:02,797] {docker.py:276} INFO - 21/05/14 14:10:02 INFO StagingCommitter: Task committer attempt_202105141401388516574754736705551_0004_m_000177_465: needsTaskCommit() Task attempt_202105141401388516574754736705551_0004_m_000177_465: duration 0:00.002s
21/05/14 14:10:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388516574754736705551_0004_m_000177_465
[2021-05-14 11:10:02,799] {docker.py:276} INFO - 21/05/14 14:10:02 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 5106 bytes result sent to driver
[2021-05-14 11:10:02,799] {docker.py:276} INFO - 21/05/14 14:10:02 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (3e4cb2948233, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:02,800] {docker.py:276} INFO - 21/05/14 14:10:02 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
[2021-05-14 11:10:02,801] {docker.py:276} INFO - 21/05/14 14:10:02 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 7045 ms on 3e4cb2948233 (executor driver) (177/200)
[2021-05-14 11:10:02,808] {docker.py:276} INFO - 21/05/14 14:10:02 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:02,816] {docker.py:276} INFO - 21/05/14 14:10:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:02,816] {docker.py:276} INFO - 21/05/14 14:10:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401385073037781488337331_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385073037781488337331_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401385073037781488337331_0004}; taskId=attempt_202105141401385073037781488337331_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5179e085}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:02,816] {docker.py:276} INFO - 21/05/14 14:10:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105141401385073037781488337331_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385073037781488337331_0004_m_000180_468
[2021-05-14 11:10:02,820] {docker.py:276} INFO - 21/05/14 14:10:02 INFO StagingCommitter: Task committer attempt_202105141401385073037781488337331_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401385073037781488337331_0004_m_000180_468 : duration 0:00.004s
[2021-05-14 11:10:03,042] {docker.py:276} INFO - 21/05/14 14:10:03 INFO StagingCommitter: Starting: Task committer attempt_202105141401388658815479088025416_0004_m_000176_464: needsTaskCommit() Task attempt_202105141401388658815479088025416_0004_m_000176_464
[2021-05-14 11:10:03,043] {docker.py:276} INFO - 21/05/14 14:10:03 INFO StagingCommitter: Task committer attempt_202105141401388658815479088025416_0004_m_000176_464: needsTaskCommit() Task attempt_202105141401388658815479088025416_0004_m_000176_464: duration 0:00.002s
21/05/14 14:10:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401388658815479088025416_0004_m_000176_464
[2021-05-14 11:10:03,047] {docker.py:276} INFO - 21/05/14 14:10:03 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 5106 bytes result sent to driver
[2021-05-14 11:10:03,048] {docker.py:276} INFO - 21/05/14 14:10:03 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (3e4cb2948233, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:03,048] {docker.py:276} INFO - 21/05/14 14:10:03 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
[2021-05-14 11:10:03,049] {docker.py:276} INFO - 21/05/14 14:10:03 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 7488 ms on 3e4cb2948233 (executor driver) (178/200)
[2021-05-14 11:10:03,061] {docker.py:276} INFO - 21/05/14 14:10:03 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:03,069] {docker.py:276} INFO - 21/05/14 14:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:03,069] {docker.py:276} INFO - 21/05/14 14:10:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138424208084235068261_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138424208084235068261_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138424208084235068261_0004}; taskId=attempt_20210514140138424208084235068261_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e12f5f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:03 INFO StagingCommitter: Starting: Task committer attempt_20210514140138424208084235068261_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138424208084235068261_0004_m_000181_469
[2021-05-14 11:10:03,073] {docker.py:276} INFO - 21/05/14 14:10:03 INFO StagingCommitter: Task committer attempt_20210514140138424208084235068261_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138424208084235068261_0004_m_000181_469 : duration 0:00.004s
[2021-05-14 11:10:07,560] {docker.py:276} INFO - 21/05/14 14:10:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401384655811070452746307_0004_m_000179_467: needsTaskCommit() Task attempt_202105141401384655811070452746307_0004_m_000179_467
[2021-05-14 11:10:07,561] {docker.py:276} INFO - 21/05/14 14:10:07 INFO StagingCommitter: Task committer attempt_202105141401384655811070452746307_0004_m_000179_467: needsTaskCommit() Task attempt_202105141401384655811070452746307_0004_m_000179_467: duration 0:00.002s
[2021-05-14 11:10:07,562] {docker.py:276} INFO - 21/05/14 14:10:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401384655811070452746307_0004_m_000179_467
[2021-05-14 11:10:07,563] {docker.py:276} INFO - 21/05/14 14:10:07 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 5149 bytes result sent to driver
[2021-05-14 11:10:07,564] {docker.py:276} INFO - 21/05/14 14:10:07 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (3e4cb2948233, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:07,565] {docker.py:276} INFO - 21/05/14 14:10:07 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 6499 ms on 3e4cb2948233 (executor driver) (179/200)
[2021-05-14 11:10:07,565] {docker.py:276} INFO - 21/05/14 14:10:07 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
[2021-05-14 11:10:07,575] {docker.py:276} INFO - 21/05/14 14:10:07 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:07,584] {docker.py:276} INFO - 21/05/14 14:10:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387835211179606838886_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387835211179606838886_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387835211179606838886_0004}; taskId=attempt_202105141401387835211179606838886_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4654eb50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401387835211179606838886_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387835211179606838886_0004_m_000182_470
[2021-05-14 11:10:07,588] {docker.py:276} INFO - 21/05/14 14:10:07 INFO StagingCommitter: Task committer attempt_202105141401387835211179606838886_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387835211179606838886_0004_m_000182_470 : duration 0:00.004s
[2021-05-14 11:10:07,771] {docker.py:276} INFO - 21/05/14 14:10:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401387762676276924762190_0004_m_000178_466: needsTaskCommit() Task attempt_202105141401387762676276924762190_0004_m_000178_466
[2021-05-14 11:10:07,772] {docker.py:276} INFO - 21/05/14 14:10:07 INFO StagingCommitter: Task committer attempt_202105141401387762676276924762190_0004_m_000178_466: needsTaskCommit() Task attempt_202105141401387762676276924762190_0004_m_000178_466: duration 0:00.004s
21/05/14 14:10:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387762676276924762190_0004_m_000178_466
[2021-05-14 11:10:07,775] {docker.py:276} INFO - 21/05/14 14:10:07 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 5149 bytes result sent to driver
[2021-05-14 11:10:07,775] {docker.py:276} INFO - 21/05/14 14:10:07 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (3e4cb2948233, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:07,776] {docker.py:276} INFO - 21/05/14 14:10:07 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
21/05/14 14:10:07 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 7260 ms on 3e4cb2948233 (executor driver) (180/200)
[2021-05-14 11:10:07,785] {docker.py:276} INFO - 21/05/14 14:10:07 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:07,793] {docker.py:276} INFO - 21/05/14 14:10:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401389152264763527812533_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389152264763527812533_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401389152264763527812533_0004}; taskId=attempt_202105141401389152264763527812533_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1959ebdd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:07,793] {docker.py:276} INFO - 21/05/14 14:10:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:07 INFO StagingCommitter: Starting: Task committer attempt_202105141401389152264763527812533_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389152264763527812533_0004_m_000183_471
[2021-05-14 11:10:07,797] {docker.py:276} INFO - 21/05/14 14:10:07 INFO StagingCommitter: Task committer attempt_202105141401389152264763527812533_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389152264763527812533_0004_m_000183_471 : duration 0:00.003s
[2021-05-14 11:10:09,633] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Starting: Task committer attempt_20210514140138424208084235068261_0004_m_000181_469: needsTaskCommit() Task attempt_20210514140138424208084235068261_0004_m_000181_469
[2021-05-14 11:10:09,634] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Task committer attempt_20210514140138424208084235068261_0004_m_000181_469: needsTaskCommit() Task attempt_20210514140138424208084235068261_0004_m_000181_469: duration 0:00.002s
21/05/14 14:10:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138424208084235068261_0004_m_000181_469
[2021-05-14 11:10:09,635] {docker.py:276} INFO - 21/05/14 14:10:09 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 5149 bytes result sent to driver
[2021-05-14 11:10:09,636] {docker.py:276} INFO - 21/05/14 14:10:09 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (3e4cb2948233, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:09,637] {docker.py:276} INFO - 21/05/14 14:10:09 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 6598 ms on 3e4cb2948233 (executor driver) (181/200)
[2021-05-14 11:10:09,638] {docker.py:276} INFO - 21/05/14 14:10:09 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-14 11:10:09,666] {docker.py:276} INFO - 21/05/14 14:10:09 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:10:09,667] {docker.py:276} INFO - 21/05/14 14:10:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:09,674] {docker.py:276} INFO - 21/05/14 14:10:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:09,675] {docker.py:276} INFO - 21/05/14 14:10:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:09,676] {docker.py:276} INFO - 21/05/14 14:10:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401387222659013405419515_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387222659013405419515_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401387222659013405419515_0004}; taskId=attempt_202105141401387222659013405419515_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bac4423}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:09,676] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401387222659013405419515_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387222659013405419515_0004_m_000184_472
[2021-05-14 11:10:09,681] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Task committer attempt_202105141401387222659013405419515_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401387222659013405419515_0004_m_000184_472 : duration 0:00.005s
[2021-05-14 11:10:09,699] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401385073037781488337331_0004_m_000180_468: needsTaskCommit() Task attempt_202105141401385073037781488337331_0004_m_000180_468
[2021-05-14 11:10:09,700] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Task committer attempt_202105141401385073037781488337331_0004_m_000180_468: needsTaskCommit() Task attempt_202105141401385073037781488337331_0004_m_000180_468: duration 0:00.001s
21/05/14 14:10:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401385073037781488337331_0004_m_000180_468
[2021-05-14 11:10:09,701] {docker.py:276} INFO - 21/05/14 14:10:09 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 5149 bytes result sent to driver
[2021-05-14 11:10:09,703] {docker.py:276} INFO - 21/05/14 14:10:09 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (3e4cb2948233, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:09,705] {docker.py:276} INFO - 21/05/14 14:10:09 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 6912 ms on 3e4cb2948233 (executor driver) (182/200)
21/05/14 14:10:09 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-14 11:10:09,714] {docker.py:276} INFO - 21/05/14 14:10:09 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:09,723] {docker.py:276} INFO - 21/05/14 14:10:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:10:09,723] {docker.py:276} INFO - 21/05/14 14:10:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:09,723] {docker.py:276} INFO - 21/05/14 14:10:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:09,724] {docker.py:276} INFO - 21/05/14 14:10:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401381104651219124539208_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381104651219124539208_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401381104651219124539208_0004}; taskId=attempt_202105141401381104651219124539208_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@fcca574}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:09,724] {docker.py:276} INFO - 21/05/14 14:10:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:09,724] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Starting: Task committer attempt_202105141401381104651219124539208_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381104651219124539208_0004_m_000185_473
[2021-05-14 11:10:09,727] {docker.py:276} INFO - 21/05/14 14:10:09 INFO StagingCommitter: Task committer attempt_202105141401381104651219124539208_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401381104651219124539208_0004_m_000185_473 : duration 0:00.004s
[2021-05-14 11:10:14,720] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401387835211179606838886_0004_m_000182_470: needsTaskCommit() Task attempt_202105141401387835211179606838886_0004_m_000182_470
[2021-05-14 11:10:14,722] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Task committer attempt_202105141401387835211179606838886_0004_m_000182_470: needsTaskCommit() Task attempt_202105141401387835211179606838886_0004_m_000182_470: duration 0:00.002s
21/05/14 14:10:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387835211179606838886_0004_m_000182_470
[2021-05-14 11:10:14,723] {docker.py:276} INFO - 21/05/14 14:10:14 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 5106 bytes result sent to driver
[2021-05-14 11:10:14,724] {docker.py:276} INFO - 21/05/14 14:10:14 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (3e4cb2948233, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:14,725] {docker.py:276} INFO - 21/05/14 14:10:14 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
21/05/14 14:10:14 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 7137 ms on 3e4cb2948233 (executor driver) (183/200)
[2021-05-14 11:10:14,735] {docker.py:276} INFO - 21/05/14 14:10:14 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:10:14,735] {docker.py:276} INFO - 21/05/14 14:10:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:14,746] {docker.py:276} INFO - 21/05/14 14:10:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:10:14,747] {docker.py:276} INFO - 21/05/14 14:10:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:14,748] {docker.py:276} INFO - 21/05/14 14:10:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:14,748] {docker.py:276} INFO - 21/05/14 14:10:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051414013816890136829493786_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013816890136829493786_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051414013816890136829493786_0004}; taskId=attempt_2021051414013816890136829493786_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58d25c6d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:14,749] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Starting: Task committer attempt_2021051414013816890136829493786_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013816890136829493786_0004_m_000186_474
[2021-05-14 11:10:14,753] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Task committer attempt_2021051414013816890136829493786_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_2021051414013816890136829493786_0004_m_000186_474 : duration 0:00.005s
[2021-05-14 11:10:14,902] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401389152264763527812533_0004_m_000183_471: needsTaskCommit() Task attempt_202105141401389152264763527812533_0004_m_000183_471
[2021-05-14 11:10:14,903] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Task committer attempt_202105141401389152264763527812533_0004_m_000183_471: needsTaskCommit() Task attempt_202105141401389152264763527812533_0004_m_000183_471: duration 0:00.002s
21/05/14 14:10:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401389152264763527812533_0004_m_000183_471
[2021-05-14 11:10:14,904] {docker.py:276} INFO - 21/05/14 14:10:14 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 5106 bytes result sent to driver
[2021-05-14 11:10:14,906] {docker.py:276} INFO - 21/05/14 14:10:14 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (3e4cb2948233, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:14,907] {docker.py:276} INFO - 21/05/14 14:10:14 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 7105 ms on 3e4cb2948233 (executor driver) (184/200)
[2021-05-14 11:10:14,907] {docker.py:276} INFO - 21/05/14 14:10:14 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
[2021-05-14 11:10:14,917] {docker.py:276} INFO - 21/05/14 14:10:14 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:14,925] {docker.py:276} INFO - 21/05/14 14:10:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383354509921972581275_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383354509921972581275_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383354509921972581275_0004}; taskId=attempt_202105141401383354509921972581275_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ff1e2d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:14,925] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Starting: Task committer attempt_202105141401383354509921972581275_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383354509921972581275_0004_m_000187_475
[2021-05-14 11:10:14,929] {docker.py:276} INFO - 21/05/14 14:10:14 INFO StagingCommitter: Task committer attempt_202105141401383354509921972581275_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383354509921972581275_0004_m_000187_475 : duration 0:00.004s
[2021-05-14 11:10:16,055] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401381104651219124539208_0004_m_000185_473: needsTaskCommit() Task attempt_202105141401381104651219124539208_0004_m_000185_473
[2021-05-14 11:10:16,056] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Task committer attempt_202105141401381104651219124539208_0004_m_000185_473: needsTaskCommit() Task attempt_202105141401381104651219124539208_0004_m_000185_473: duration 0:00.001s
21/05/14 14:10:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401381104651219124539208_0004_m_000185_473
[2021-05-14 11:10:16,058] {docker.py:276} INFO - 21/05/14 14:10:16 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 5106 bytes result sent to driver
[2021-05-14 11:10:16,058] {docker.py:276} INFO - 21/05/14 14:10:16 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (3e4cb2948233, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:16,059] {docker.py:276} INFO - 21/05/14 14:10:16 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-14 11:10:16,060] {docker.py:276} INFO - 21/05/14 14:10:16 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 6330 ms on 3e4cb2948233 (executor driver) (185/200)
[2021-05-14 11:10:16,068] {docker.py:276} INFO - 21/05/14 14:10:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:16,082] {docker.py:276} INFO - 21/05/14 14:10:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:16,083] {docker.py:276} INFO - 21/05/14 14:10:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:16,083] {docker.py:276} INFO - 21/05/14 14:10:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386489488587919358696_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386489488587919358696_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386489488587919358696_0004}; taskId=attempt_202105141401386489488587919358696_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59caa596}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:16,084] {docker.py:276} INFO - 21/05/14 14:10:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:16,084] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401386489488587919358696_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386489488587919358696_0004_m_000188_476
[2021-05-14 11:10:16,088] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Task committer attempt_202105141401386489488587919358696_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386489488587919358696_0004_m_000188_476 : duration 0:00.004s
[2021-05-14 11:10:16,963] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401387222659013405419515_0004_m_000184_472: needsTaskCommit() Task attempt_202105141401387222659013405419515_0004_m_000184_472
[2021-05-14 11:10:16,963] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Task committer attempt_202105141401387222659013405419515_0004_m_000184_472: needsTaskCommit() Task attempt_202105141401387222659013405419515_0004_m_000184_472: duration 0:00.002s
21/05/14 14:10:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401387222659013405419515_0004_m_000184_472
[2021-05-14 11:10:16,965] {docker.py:276} INFO - 21/05/14 14:10:16 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 5106 bytes result sent to driver
[2021-05-14 11:10:16,966] {docker.py:276} INFO - 21/05/14 14:10:16 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (3e4cb2948233, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:16,967] {docker.py:276} INFO - 21/05/14 14:10:16 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
21/05/14 14:10:16 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 7305 ms on 3e4cb2948233 (executor driver) (186/200)
[2021-05-14 11:10:16,977] {docker.py:276} INFO - 21/05/14 14:10:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:16,985] {docker.py:276} INFO - 21/05/14 14:10:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383040560500664681412_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383040560500664681412_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383040560500664681412_0004}; taskId=attempt_202105141401383040560500664681412_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@948fd64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:16 INFO StagingCommitter: Starting: Task committer attempt_202105141401383040560500664681412_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383040560500664681412_0004_m_000189_477
[2021-05-14 11:10:16,990] {docker.py:276} INFO - 21/05/14 14:10:16 INFO StagingCommitter: Task committer attempt_202105141401383040560500664681412_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383040560500664681412_0004_m_000189_477 : duration 0:00.005s
[2021-05-14 11:10:21,613] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Starting: Task committer attempt_202105141401383354509921972581275_0004_m_000187_475: needsTaskCommit() Task attempt_202105141401383354509921972581275_0004_m_000187_475
[2021-05-14 11:10:21,614] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Task committer attempt_202105141401383354509921972581275_0004_m_000187_475: needsTaskCommit() Task attempt_202105141401383354509921972581275_0004_m_000187_475: duration 0:00.002s
[2021-05-14 11:10:21,615] {docker.py:276} INFO - 21/05/14 14:10:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383354509921972581275_0004_m_000187_475
[2021-05-14 11:10:21,616] {docker.py:276} INFO - 21/05/14 14:10:21 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 5106 bytes result sent to driver
[2021-05-14 11:10:21,618] {docker.py:276} INFO - 21/05/14 14:10:21 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (3e4cb2948233, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:21,618] {docker.py:276} INFO - 21/05/14 14:10:21 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-14 11:10:21,619] {docker.py:276} INFO - 21/05/14 14:10:21 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 6721 ms on 3e4cb2948233 (executor driver) (187/200)
[2021-05-14 11:10:21,627] {docker.py:276} INFO - 21/05/14 14:10:21 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:10:21,627] {docker.py:276} INFO - 21/05/14 14:10:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:21,637] {docker.py:276} INFO - 21/05/14 14:10:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:10:21,638] {docker.py:276} INFO - 21/05/14 14:10:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:21,639] {docker.py:276} INFO - 21/05/14 14:10:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:21,640] {docker.py:276} INFO - 21/05/14 14:10:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138685720883658621677_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138685720883658621677_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138685720883658621677_0004}; taskId=attempt_20210514140138685720883658621677_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a48bdb6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:21 INFO StagingCommitter: Starting: Task committer attempt_20210514140138685720883658621677_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138685720883658621677_0004_m_000190_478
[2021-05-14 11:10:21,644] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Task committer attempt_20210514140138685720883658621677_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138685720883658621677_0004_m_000190_478 : duration 0:00.004s
[2021-05-14 11:10:21,754] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Starting: Task committer attempt_2021051414013816890136829493786_0004_m_000186_474: needsTaskCommit() Task attempt_2021051414013816890136829493786_0004_m_000186_474
[2021-05-14 11:10:21,754] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Task committer attempt_2021051414013816890136829493786_0004_m_000186_474: needsTaskCommit() Task attempt_2021051414013816890136829493786_0004_m_000186_474: duration 0:00.001s
21/05/14 14:10:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051414013816890136829493786_0004_m_000186_474
[2021-05-14 11:10:21,756] {docker.py:276} INFO - 21/05/14 14:10:21 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 5106 bytes result sent to driver
[2021-05-14 11:10:21,757] {docker.py:276} INFO - 21/05/14 14:10:21 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (3e4cb2948233, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:21,758] {docker.py:276} INFO - 21/05/14 14:10:21 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 7042 ms on 3e4cb2948233 (executor driver) (188/200)
[2021-05-14 11:10:21,758] {docker.py:276} INFO - 21/05/14 14:10:21 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-14 11:10:21,781] {docker.py:276} INFO - 21/05/14 14:10:21 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:10:21,784] {docker.py:276} INFO - 21/05/14 14:10:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 11:10:21,801] {docker.py:276} INFO - 21/05/14 14:10:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138851299729258331407_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138851299729258331407_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138851299729258331407_0004}; taskId=attempt_20210514140138851299729258331407_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23a78cef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:21,802] {docker.py:276} INFO - 21/05/14 14:10:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:21,802] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Starting: Task committer attempt_20210514140138851299729258331407_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138851299729258331407_0004_m_000191_479
[2021-05-14 11:10:21,809] {docker.py:276} INFO - 21/05/14 14:10:21 INFO StagingCommitter: Task committer attempt_20210514140138851299729258331407_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138851299729258331407_0004_m_000191_479 : duration 0:00.007s
[2021-05-14 11:10:23,085] {docker.py:276} INFO - 21/05/14 14:10:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401386489488587919358696_0004_m_000188_476: needsTaskCommit() Task attempt_202105141401386489488587919358696_0004_m_000188_476
[2021-05-14 11:10:23,086] {docker.py:276} INFO - 21/05/14 14:10:23 INFO StagingCommitter: Task committer attempt_202105141401386489488587919358696_0004_m_000188_476: needsTaskCommit() Task attempt_202105141401386489488587919358696_0004_m_000188_476: duration 0:00.002s
[2021-05-14 11:10:23,087] {docker.py:276} INFO - 21/05/14 14:10:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386489488587919358696_0004_m_000188_476
[2021-05-14 11:10:23,089] {docker.py:276} INFO - 21/05/14 14:10:23 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 5106 bytes result sent to driver
[2021-05-14 11:10:23,090] {docker.py:276} INFO - 21/05/14 14:10:23 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (3e4cb2948233, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:23,091] {docker.py:276} INFO - 21/05/14 14:10:23 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-14 11:10:23,091] {docker.py:276} INFO - 21/05/14 14:10:23 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 7042 ms on 3e4cb2948233 (executor driver) (189/200)
[2021-05-14 11:10:23,114] {docker.py:276} INFO - 21/05/14 14:10:23 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:23,123] {docker.py:276} INFO - 21/05/14 14:10:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383607445676084602946_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383607445676084602946_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383607445676084602946_0004}; taskId=attempt_202105141401383607445676084602946_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b99f4a6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:23,124] {docker.py:276} INFO - 21/05/14 14:10:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401383607445676084602946_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383607445676084602946_0004_m_000192_480
[2021-05-14 11:10:23,128] {docker.py:276} INFO - 21/05/14 14:10:23 INFO StagingCommitter: Task committer attempt_202105141401383607445676084602946_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383607445676084602946_0004_m_000192_480 : duration 0:00.004s
[2021-05-14 11:10:23,837] {docker.py:276} INFO - 21/05/14 14:10:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401383040560500664681412_0004_m_000189_477: needsTaskCommit() Task attempt_202105141401383040560500664681412_0004_m_000189_477
21/05/14 14:10:23 INFO StagingCommitter: Task committer attempt_202105141401383040560500664681412_0004_m_000189_477: needsTaskCommit() Task attempt_202105141401383040560500664681412_0004_m_000189_477: duration 0:00.002s
21/05/14 14:10:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383040560500664681412_0004_m_000189_477
[2021-05-14 11:10:23,839] {docker.py:276} INFO - 21/05/14 14:10:23 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 5149 bytes result sent to driver
[2021-05-14 11:10:23,841] {docker.py:276} INFO - 21/05/14 14:10:23 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (3e4cb2948233, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:23,842] {docker.py:276} INFO - 21/05/14 14:10:23 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-14 11:10:23,843] {docker.py:276} INFO - 21/05/14 14:10:23 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 6884 ms on 3e4cb2948233 (executor driver) (190/200)
[2021-05-14 11:10:23,851] {docker.py:276} INFO - 21/05/14 14:10:23 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:23,860] {docker.py:276} INFO - 21/05/14 14:10:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383754247311040566875_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383754247311040566875_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383754247311040566875_0004}; taskId=attempt_202105141401383754247311040566875_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27986837}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:23,860] {docker.py:276} INFO - 21/05/14 14:10:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:23,860] {docker.py:276} INFO - 21/05/14 14:10:23 INFO StagingCommitter: Starting: Task committer attempt_202105141401383754247311040566875_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383754247311040566875_0004_m_000193_481
[2021-05-14 11:10:23,864] {docker.py:276} INFO - 21/05/14 14:10:23 INFO StagingCommitter: Task committer attempt_202105141401383754247311040566875_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383754247311040566875_0004_m_000193_481 : duration 0:00.004s
[2021-05-14 11:10:28,468] {docker.py:276} INFO - 21/05/14 14:10:28 INFO StagingCommitter: Starting: Task committer attempt_20210514140138851299729258331407_0004_m_000191_479: needsTaskCommit() Task attempt_20210514140138851299729258331407_0004_m_000191_479
[2021-05-14 11:10:28,469] {docker.py:276} INFO - 21/05/14 14:10:28 INFO StagingCommitter: Task committer attempt_20210514140138851299729258331407_0004_m_000191_479: needsTaskCommit() Task attempt_20210514140138851299729258331407_0004_m_000191_479: duration 0:00.001s
21/05/14 14:10:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138851299729258331407_0004_m_000191_479
[2021-05-14 11:10:28,470] {docker.py:276} INFO - 21/05/14 14:10:28 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 5149 bytes result sent to driver
[2021-05-14 11:10:28,471] {docker.py:276} INFO - 21/05/14 14:10:28 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (3e4cb2948233, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:28,472] {docker.py:276} INFO - 21/05/14 14:10:28 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 6722 ms on 3e4cb2948233 (executor driver) (191/200)
[2021-05-14 11:10:28,473] {docker.py:276} INFO - 21/05/14 14:10:28 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-14 11:10:28,480] {docker.py:276} INFO - 21/05/14 14:10:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:28,487] {docker.py:276} INFO - 21/05/14 14:10:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401386344630675379704118_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386344630675379704118_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401386344630675379704118_0004}; taskId=attempt_202105141401386344630675379704118_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fbd3bde}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:28 INFO StagingCommitter: Starting: Task committer attempt_202105141401386344630675379704118_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386344630675379704118_0004_m_000194_482
[2021-05-14 11:10:28,491] {docker.py:276} INFO - 21/05/14 14:10:28 INFO StagingCommitter: Task committer attempt_202105141401386344630675379704118_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401386344630675379704118_0004_m_000194_482 : duration 0:00.004s
[2021-05-14 11:10:28,779] {docker.py:276} INFO - 21/05/14 14:10:28 INFO StagingCommitter: Starting: Task committer attempt_20210514140138685720883658621677_0004_m_000190_478: needsTaskCommit() Task attempt_20210514140138685720883658621677_0004_m_000190_478
21/05/14 14:10:28 INFO StagingCommitter: Task committer attempt_20210514140138685720883658621677_0004_m_000190_478: needsTaskCommit() Task attempt_20210514140138685720883658621677_0004_m_000190_478: duration 0:00.001s
21/05/14 14:10:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138685720883658621677_0004_m_000190_478
[2021-05-14 11:10:28,780] {docker.py:276} INFO - 21/05/14 14:10:28 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 5149 bytes result sent to driver
[2021-05-14 11:10:28,781] {docker.py:276} INFO - 21/05/14 14:10:28 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (3e4cb2948233, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:28,782] {docker.py:276} INFO - 21/05/14 14:10:28 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-14 11:10:28,783] {docker.py:276} INFO - 21/05/14 14:10:28 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 7174 ms on 3e4cb2948233 (executor driver) (192/200)
[2021-05-14 11:10:28,790] {docker.py:276} INFO - 21/05/14 14:10:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:28,797] {docker.py:276} INFO - 21/05/14 14:10:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514140138640280992063603688_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138640280992063603688_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514140138640280992063603688_0004}; taskId=attempt_20210514140138640280992063603688_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18ceb7e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:28,798] {docker.py:276} INFO - 21/05/14 14:10:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:28 INFO StagingCommitter: Starting: Task committer attempt_20210514140138640280992063603688_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138640280992063603688_0004_m_000195_483
[2021-05-14 11:10:28,802] {docker.py:276} INFO - 21/05/14 14:10:28 INFO StagingCommitter: Task committer attempt_20210514140138640280992063603688_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_20210514140138640280992063603688_0004_m_000195_483 : duration 0:00.004s
[2021-05-14 11:10:30,101] {docker.py:276} INFO - 21/05/14 14:10:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401383607445676084602946_0004_m_000192_480: needsTaskCommit() Task attempt_202105141401383607445676084602946_0004_m_000192_480
21/05/14 14:10:30 INFO StagingCommitter: Task committer attempt_202105141401383607445676084602946_0004_m_000192_480: needsTaskCommit() Task attempt_202105141401383607445676084602946_0004_m_000192_480: duration 0:00.002s
21/05/14 14:10:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383607445676084602946_0004_m_000192_480
[2021-05-14 11:10:30,104] {docker.py:276} INFO - 21/05/14 14:10:30 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 5149 bytes result sent to driver
[2021-05-14 11:10:30,105] {docker.py:276} INFO - 21/05/14 14:10:30 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (3e4cb2948233, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:30,106] {docker.py:276} INFO - 21/05/14 14:10:30 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-14 11:10:30,107] {docker.py:276} INFO - 21/05/14 14:10:30 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 7025 ms on 3e4cb2948233 (executor driver) (193/200)
[2021-05-14 11:10:30,116] {docker.py:276} INFO - 21/05/14 14:10:30 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:30,125] {docker.py:276} INFO - 21/05/14 14:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:30,125] {docker.py:276} INFO - 21/05/14 14:10:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401382778550411607187672_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382778550411607187672_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401382778550411607187672_0004}; taskId=attempt_202105141401382778550411607187672_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e9600da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:30,125] {docker.py:276} INFO - 21/05/14 14:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:30,126] {docker.py:276} INFO - 21/05/14 14:10:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401382778550411607187672_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382778550411607187672_0004_m_000196_484
[2021-05-14 11:10:30,129] {docker.py:276} INFO - 21/05/14 14:10:30 INFO StagingCommitter: Task committer attempt_202105141401382778550411607187672_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401382778550411607187672_0004_m_000196_484 : duration 0:00.004s
[2021-05-14 11:10:30,292] {docker.py:276} INFO - 21/05/14 14:10:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401383754247311040566875_0004_m_000193_481: needsTaskCommit() Task attempt_202105141401383754247311040566875_0004_m_000193_481
21/05/14 14:10:30 INFO StagingCommitter: Task committer attempt_202105141401383754247311040566875_0004_m_000193_481: needsTaskCommit() Task attempt_202105141401383754247311040566875_0004_m_000193_481: duration 0:00.001s
21/05/14 14:10:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383754247311040566875_0004_m_000193_481
[2021-05-14 11:10:30,295] {docker.py:276} INFO - 21/05/14 14:10:30 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 5106 bytes result sent to driver
[2021-05-14 11:10:30,296] {docker.py:276} INFO - 21/05/14 14:10:30 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (3e4cb2948233, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:30,297] {docker.py:276} INFO - 21/05/14 14:10:30 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-14 11:10:30,298] {docker.py:276} INFO - 21/05/14 14:10:30 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 6464 ms on 3e4cb2948233 (executor driver) (194/200)
[2021-05-14 11:10:30,306] {docker.py:276} INFO - 21/05/14 14:10:30 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:10:30,306] {docker.py:276} INFO - 21/05/14 14:10:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:30,314] {docker.py:276} INFO - 21/05/14 14:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:10:30,315] {docker.py:276} INFO - 21/05/14 14:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:10:30,315] {docker.py:276} INFO - 21/05/14 14:10:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:30,316] {docker.py:276} INFO - 21/05/14 14:10:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401389187900516220759360_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389187900516220759360_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401389187900516220759360_0004}; taskId=attempt_202105141401389187900516220759360_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35d31548}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:10:30,316] {docker.py:276} INFO - 21/05/14 14:10:30 INFO StagingCommitter: Starting: Task committer attempt_202105141401389187900516220759360_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389187900516220759360_0004_m_000197_485
[2021-05-14 11:10:30,320] {docker.py:276} INFO - 21/05/14 14:10:30 INFO StagingCommitter: Task committer attempt_202105141401389187900516220759360_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401389187900516220759360_0004_m_000197_485 : duration 0:00.004s
[2021-05-14 11:10:35,474] {docker.py:276} INFO - 21/05/14 14:10:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401386344630675379704118_0004_m_000194_482: needsTaskCommit() Task attempt_202105141401386344630675379704118_0004_m_000194_482
[2021-05-14 11:10:35,476] {docker.py:276} INFO - 21/05/14 14:10:35 INFO StagingCommitter: Task committer attempt_202105141401386344630675379704118_0004_m_000194_482: needsTaskCommit() Task attempt_202105141401386344630675379704118_0004_m_000194_482: duration 0:00.004s
21/05/14 14:10:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401386344630675379704118_0004_m_000194_482
[2021-05-14 11:10:35,479] {docker.py:276} INFO - 21/05/14 14:10:35 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 5106 bytes result sent to driver
[2021-05-14 11:10:35,482] {docker.py:276} INFO - 21/05/14 14:10:35 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (3e4cb2948233, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:35,482] {docker.py:276} INFO - 21/05/14 14:10:35 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-14 11:10:35,483] {docker.py:276} INFO - 21/05/14 14:10:35 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 7019 ms on 3e4cb2948233 (executor driver) (195/200)
[2021-05-14 11:10:35,497] {docker.py:276} INFO - 21/05/14 14:10:35 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:35,506] {docker.py:276} INFO - 21/05/14 14:10:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383093573903021544484_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383093573903021544484_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383093573903021544484_0004}; taskId=attempt_202105141401383093573903021544484_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bb516bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401383093573903021544484_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383093573903021544484_0004_m_000198_486
[2021-05-14 11:10:35,512] {docker.py:276} INFO - 21/05/14 14:10:35 INFO StagingCommitter: Task committer attempt_202105141401383093573903021544484_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383093573903021544484_0004_m_000198_486 : duration 0:00.005s
[2021-05-14 11:10:35,846] {docker.py:276} INFO - 21/05/14 14:10:35 INFO StagingCommitter: Starting: Task committer attempt_20210514140138640280992063603688_0004_m_000195_483: needsTaskCommit() Task attempt_20210514140138640280992063603688_0004_m_000195_483
21/05/14 14:10:35 INFO StagingCommitter: Task committer attempt_20210514140138640280992063603688_0004_m_000195_483: needsTaskCommit() Task attempt_20210514140138640280992063603688_0004_m_000195_483: duration 0:00.003s
21/05/14 14:10:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514140138640280992063603688_0004_m_000195_483
[2021-05-14 11:10:35,847] {docker.py:276} INFO - 21/05/14 14:10:35 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 5106 bytes result sent to driver
[2021-05-14 11:10:35,848] {docker.py:276} INFO - 21/05/14 14:10:35 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (3e4cb2948233, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:10:35,849] {docker.py:276} INFO - 21/05/14 14:10:35 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 7076 ms on 3e4cb2948233 (executor driver) (196/200)
[2021-05-14 11:10:35,850] {docker.py:276} INFO - 21/05/14 14:10:35 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-14 11:10:35,861] {docker.py:276} INFO - 21/05/14 14:10:35 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:10:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:10:35,871] {docker.py:276} INFO - 21/05/14 14:10:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:10:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:10:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:10:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141401383723755084764100435_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383723755084764100435_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141401383723755084764100435_0004}; taskId=attempt_202105141401383723755084764100435_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ba018c0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/9d8ed931-feb1-44ea-aef3-e352a91bbc35/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:10:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:10:35 INFO StagingCommitter: Starting: Task committer attempt_202105141401383723755084764100435_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383723755084764100435_0004_m_000199_487
[2021-05-14 11:10:35,875] {docker.py:276} INFO - 21/05/14 14:10:35 INFO StagingCommitter: Task committer attempt_202105141401383723755084764100435_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/9d8ed931-feb1-44ea-aef3-e352a91bbc35/_temporary/0/_temporary/attempt_202105141401383723755084764100435_0004_m_000199_487 : duration 0:00.004s
[2021-05-14 11:10:37,539] {docker.py:276} INFO - 21/05/14 14:10:37 INFO StagingCommitter: Starting: Task committer attempt_202105141401389187900516220759360_0004_m_000197_485: needsTaskCommit() Task attempt_202105141401389187900516220759360_0004_m_000197_485
[2021-05-14 11:10:37,539] {docker.py:276} INFO - 21/05/14 14:10:37 INFO StagingCommitter: Task committer attempt_202105141401389187900516220759360_0004_m_000197_485: needsTaskCommit() Task attempt_202105141401389187900516220759360_0004_m_000197_485: duration 0:00.001s
21/05/14 14:10:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401389187900516220759360_0004_m_000197_485
[2021-05-14 11:10:37,540] {docker.py:276} INFO - 21/05/14 14:10:37 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 5106 bytes result sent to driver
[2021-05-14 11:10:37,542] {docker.py:276} INFO - 21/05/14 14:10:37 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 7256 ms on 3e4cb2948233 (executor driver) (197/200)
[2021-05-14 11:10:38,412] {docker.py:276} INFO - 21/05/14 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105141401382778550411607187672_0004_m_000196_484: needsTaskCommit() Task attempt_202105141401382778550411607187672_0004_m_000196_484
[2021-05-14 11:10:38,413] {docker.py:276} INFO - 21/05/14 14:10:38 INFO StagingCommitter: Task committer attempt_202105141401382778550411607187672_0004_m_000196_484: needsTaskCommit() Task attempt_202105141401382778550411607187672_0004_m_000196_484: duration 0:00.001s
21/05/14 14:10:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401382778550411607187672_0004_m_000196_484
[2021-05-14 11:10:38,414] {docker.py:276} INFO - 21/05/14 14:10:38 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 5106 bytes result sent to driver
[2021-05-14 11:10:38,415] {docker.py:276} INFO - 21/05/14 14:10:38 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 8319 ms on 3e4cb2948233 (executor driver) (198/200)
[2021-05-14 11:10:42,164] {docker.py:276} INFO - 21/05/14 14:10:42 INFO StagingCommitter: Starting: Task committer attempt_202105141401383723755084764100435_0004_m_000199_487: needsTaskCommit() Task attempt_202105141401383723755084764100435_0004_m_000199_487
[2021-05-14 11:10:42,165] {docker.py:276} INFO - 21/05/14 14:10:42 INFO StagingCommitter: Task committer attempt_202105141401383723755084764100435_0004_m_000199_487: needsTaskCommit() Task attempt_202105141401383723755084764100435_0004_m_000199_487: duration 0:00.001s
21/05/14 14:10:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383723755084764100435_0004_m_000199_487
[2021-05-14 11:10:42,169] {docker.py:276} INFO - 21/05/14 14:10:42 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 5106 bytes result sent to driver
[2021-05-14 11:10:42,171] {docker.py:276} INFO - 21/05/14 14:10:42 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 6296 ms on 3e4cb2948233 (executor driver) (199/200)
[2021-05-14 11:10:42,645] {docker.py:276} INFO - 21/05/14 14:10:42 INFO StagingCommitter: Starting: Task committer attempt_202105141401383093573903021544484_0004_m_000198_486: needsTaskCommit() Task attempt_202105141401383093573903021544484_0004_m_000198_486
21/05/14 14:10:42 INFO StagingCommitter: Task committer attempt_202105141401383093573903021544484_0004_m_000198_486: needsTaskCommit() Task attempt_202105141401383093573903021544484_0004_m_000198_486: duration 0:00.001s
21/05/14 14:10:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141401383093573903021544484_0004_m_000198_486
[2021-05-14 11:10:42,648] {docker.py:276} INFO - 21/05/14 14:10:42 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 5106 bytes result sent to driver
[2021-05-14 11:10:42,648] {docker.py:276} INFO - 21/05/14 14:10:42 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 7142 ms on 3e4cb2948233 (executor driver) (200/200)
[2021-05-14 11:10:42,650] {docker.py:276} INFO - 21/05/14 14:10:42 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-14 11:10:42,651] {docker.py:276} INFO - 21/05/14 14:10:42 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 525.851 s
[2021-05-14 11:10:42,651] {docker.py:276} INFO - 21/05/14 14:10:42 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 14:10:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-14 11:10:42,652] {docker.py:276} INFO - 21/05/14 14:10:42 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 545.073470 s
[2021-05-14 11:10:42,654] {docker.py:276} INFO - 21/05/14 14:10:42 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105141401372772443654525411039_0000_m_000000_0: commitJob((no job ID))
[2021-05-14 11:10:42,671] {docker.py:276} INFO - 21/05/14 14:10:42 WARN AbstractS3ACommitter: Task committer attempt_202105141401372772443654525411039_0000_m_000000_0: No pending uploads to commit
[2021-05-14 11:10:44,291] {docker.py:276} INFO - 21/05/14 14:10:44 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/14 14:10:44 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-14 11:10:44,473] {docker.py:276} INFO - 21/05/14 14:10:44 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.182s
21/05/14 14:10:44 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.182s
[2021-05-14 11:10:44,474] {docker.py:276} INFO - 21/05/14 14:10:44 INFO AbstractS3ACommitter: Task committer attempt_202105141401372772443654525411039_0000_m_000000_0: commitJob((no job ID)): duration 0:01.822s
[2021-05-14 11:10:44,989] {docker.py:276} INFO - 21/05/14 14:10:44 INFO FileFormatWriter: Write Job 9d8ed931-feb1-44ea-aef3-e352a91bbc35 committed.
[2021-05-14 11:10:44,998] {docker.py:276} INFO - 21/05/14 14:10:45 INFO FileFormatWriter: Finished processing stats for write job 9d8ed931-feb1-44ea-aef3-e352a91bbc35.
[2021-05-14 11:10:45,102] {docker.py:276} INFO - 21/05/14 14:10:45 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-14 11:10:45,117] {docker.py:276} INFO - 21/05/14 14:10:45 INFO SparkUI: Stopped Spark web UI at http://3e4cb2948233:4040
[2021-05-14 11:10:45,141] {docker.py:276} INFO - 21/05/14 14:10:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-14 11:10:45,158] {docker.py:276} INFO - 21/05/14 14:10:45 INFO MemoryStore: MemoryStore cleared
[2021-05-14 11:10:45,159] {docker.py:276} INFO - 21/05/14 14:10:45 INFO BlockManager: BlockManager stopped
[2021-05-14 11:10:45,162] {docker.py:276} INFO - 21/05/14 14:10:45 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-14 11:10:45,167] {docker.py:276} INFO - 21/05/14 14:10:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-14 11:10:45,175] {docker.py:276} INFO - 21/05/14 14:10:45 INFO SparkContext: Successfully stopped SparkContext
[2021-05-14 11:10:45,176] {docker.py:276} INFO - 21/05/14 14:10:45 INFO ShutdownHookManager: Shutdown hook called
[2021-05-14 11:10:45,177] {docker.py:276} INFO - 21/05/14 14:10:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-32105048-129f-4657-b35b-61564e319f51
[2021-05-14 11:10:45,179] {docker.py:276} INFO - 21/05/14 14:10:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-ddaa6649-d9a7-44b2-8cf1-9cc65859bedc
[2021-05-14 11:10:45,182] {docker.py:276} INFO - 21/05/14 14:10:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-32105048-129f-4657-b35b-61564e319f51/pyspark-9d46381f-1711-42e0-9b23-7a470e1bad43
[2021-05-14 11:10:45,187] {docker.py:276} INFO - 21/05/14 14:10:45 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-14 11:10:45,188] {docker.py:276} INFO - 21/05/14 14:10:45 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-14 11:10:45,189] {docker.py:276} INFO - 21/05/14 14:10:45 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-14 11:10:45,435] {xcom.py:238} ERROR - Could not serialize the XCom value into JSON. If you are using pickles instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2021-05-14 11:10:45,436] {taskinstance.py:1482} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1138, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1344, in _execute_task
    self.xcom_push(key=XCOM_RETURN_KEY, value=result)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1928, in xcom_push
    session=session,
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/xcom.py", line 88, in set
    value = XCom.serialize_value(value)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/xcom.py", line 235, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type bytes is not JSON serializable
[2021-05-14 11:10:45,445] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=etl, task_id=run_spark_job, execution_date=20210514T135951, start_date=20210514T140055, end_date=20210514T141045
[2021-05-14 11:10:45,480] {local_task_job.py:146} INFO - Task exited with return code 1
