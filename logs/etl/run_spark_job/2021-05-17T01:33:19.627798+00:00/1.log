[2021-05-16 22:34:06,773] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T01:33:19.627798+00:00 [queued]>
[2021-05-16 22:34:06,779] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T01:33:19.627798+00:00 [queued]>
[2021-05-16 22:34:06,779] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-16 22:34:06,779] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-16 22:34:06,779] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-16 22:34:06,784] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-17T01:33:19.627798+00:00
[2021-05-16 22:34:06,787] {standard_task_runner.py:52} INFO - Started process 32778 to run task
[2021-05-16 22:34:06,794] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-17T01:33:19.627798+00:00', '--job-id', '838', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpwjm16r6i', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpg711spf3']
[2021-05-16 22:34:06,796] {standard_task_runner.py:77} INFO - Job 838: Subtask run_spark_job
[2021-05-16 22:34:06,825] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-17T01:33:19.627798+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-16 22:34:06,848] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-17T01:33:19.627798+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-17T01:33:19.627798+00:00
[2021-05-16 22:34:06,851] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-16 22:34:10,086] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-16 22:34:10,088] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-16 22:34:10,088] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-16 22:34:10,092] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-16 22:34:12,316] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-16 22:34:12,917] {docker.py:276} INFO - 21/05/17 01:34:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-16 22:34:15,172] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-16 22:34:15,187] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SparkContext: Running Spark version 3.1.1
[2021-05-16 22:34:15,290] {docker.py:276} INFO - 21/05/17 01:34:15 INFO ResourceUtils: ==============================================================
[2021-05-16 22:34:15,291] {docker.py:276} INFO - 21/05/17 01:34:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-16 22:34:15,293] {docker.py:276} INFO - 21/05/17 01:34:15 INFO ResourceUtils: ==============================================================
[2021-05-16 22:34:15,295] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SparkContext: Submitted application: spark.py
[2021-05-16 22:34:15,351] {docker.py:276} INFO - 21/05/17 01:34:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-16 22:34:15,372] {docker.py:276} INFO - 21/05/17 01:34:15 INFO ResourceProfile: Limiting resource is cpu
[2021-05-16 22:34:15,373] {docker.py:276} INFO - 21/05/17 01:34:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-16 22:34:15,458] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-16 22:34:15,459] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-16 22:34:15,459] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SecurityManager: Changing view acls groups to:
[2021-05-16 22:34:15,459] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SecurityManager: Changing modify acls groups to:
[2021-05-16 22:34:15,459] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-16 22:34:15,831] {docker.py:276} INFO - 21/05/17 01:34:15 INFO Utils: Successfully started service 'sparkDriver' on port 44051.
[2021-05-16 22:34:15,874] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SparkEnv: Registering MapOutputTracker
[2021-05-16 22:34:15,924] {docker.py:276} INFO - 21/05/17 01:34:15 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-16 22:34:15,975] {docker.py:276} INFO - 21/05/17 01:34:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-16 22:34:15,977] {docker.py:276} INFO - 21/05/17 01:34:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-16 22:34:15,983] {docker.py:276} INFO - 21/05/17 01:34:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-16 22:34:16,004] {docker.py:276} INFO - 21/05/17 01:34:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f6c621be-d9b5-4749-9762-22918acb6170
[2021-05-16 22:34:16,035] {docker.py:276} INFO - 21/05/17 01:34:16 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-16 22:34:16,059] {docker.py:276} INFO - 21/05/17 01:34:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-16 22:34:16,341] {docker.py:276} INFO - 21/05/17 01:34:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-16 22:34:16,445] {docker.py:276} INFO - 21/05/17 01:34:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://5deb0a1bddc0:4040
[2021-05-16 22:34:16,703] {docker.py:276} INFO - 21/05/17 01:34:16 INFO Executor: Starting executor ID driver on host 5deb0a1bddc0
[2021-05-16 22:34:16,743] {docker.py:276} INFO - 21/05/17 01:34:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42667.
21/05/17 01:34:16 INFO NettyBlockTransferService: Server created on 5deb0a1bddc0:42667
[2021-05-16 22:34:16,746] {docker.py:276} INFO - 21/05/17 01:34:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-16 22:34:16,758] {docker.py:276} INFO - 21/05/17 01:34:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5deb0a1bddc0, 42667, None)
[2021-05-16 22:34:16,767] {docker.py:276} INFO - 21/05/17 01:34:16 INFO BlockManagerMasterEndpoint: Registering block manager 5deb0a1bddc0:42667 with 934.4 MiB RAM, BlockManagerId(driver, 5deb0a1bddc0, 42667, None)
[2021-05-16 22:34:16,772] {docker.py:276} INFO - 21/05/17 01:34:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5deb0a1bddc0, 42667, None)
[2021-05-16 22:34:16,774] {docker.py:276} INFO - 21/05/17 01:34:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5deb0a1bddc0, 42667, None)
[2021-05-16 22:34:17,388] {docker.py:276} INFO - 21/05/17 01:34:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-16 22:34:17,388] {docker.py:276} INFO - 21/05/17 01:34:17 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-16 22:34:18,594] {docker.py:276} INFO - 21/05/17 01:34:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-16 22:34:18,658] {docker.py:276} INFO - 21/05/17 01:34:18 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-16 22:34:18,658] {docker.py:276} INFO - 21/05/17 01:34:18 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-16 22:34:24,531] {docker.py:276} INFO - 21/05/17 01:34:24 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621128813_to_1621130613.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621130613_to_1621132413.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621132413_to_1621134213.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621134213_to_1621136013.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621136013_to_1621137813.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621137813_to_1621139613.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621139613_to_1621141413.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621141413_to_1621143213.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621143213_to_1621145013.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621145013_to_1621146813.csv.
[2021-05-16 22:34:25,097] {docker.py:276} INFO - 21/05/17 01:34:25 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:25,127] {docker.py:276} INFO - 21/05/17 01:34:25 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-16 22:34:25,128] {docker.py:276} INFO - 21/05/17 01:34:25 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 22:34:25,135] {docker.py:276} INFO - 21/05/17 01:34:25 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 22:34:25,138] {docker.py:276} INFO - 21/05/17 01:34:25 INFO DAGScheduler: Missing parents: List()
[2021-05-16 22:34:25,147] {docker.py:276} INFO - 21/05/17 01:34:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 22:34:25,299] {docker.py:276} INFO - 21/05/17 01:34:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-16 22:34:25,373] {docker.py:276} INFO - 21/05/17 01:34:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-16 22:34:25,378] {docker.py:276} INFO - 21/05/17 01:34:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5deb0a1bddc0:42667 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-16 22:34:25,384] {docker.py:276} INFO - 21/05/17 01:34:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-16 22:34:25,424] {docker.py:276} INFO - 21/05/17 01:34:25 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-16 22:34:25,426] {docker.py:276} INFO - 21/05/17 01:34:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-16 22:34:25,536] {docker.py:276} INFO - 21/05/17 01:34:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (5deb0a1bddc0, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:25,540] {docker.py:276} INFO - 21/05/17 01:34:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (5deb0a1bddc0, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:25,541] {docker.py:276} INFO - 21/05/17 01:34:25 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (5deb0a1bddc0, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:25,542] {docker.py:276} INFO - 21/05/17 01:34:25 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (5deb0a1bddc0, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:25,567] {docker.py:276} INFO - 21/05/17 01:34:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-16 22:34:25,567] {docker.py:276} INFO - 21/05/17 01:34:25 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
21/05/17 01:34:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-16 22:34:25,568] {docker.py:276} INFO - 21/05/17 01:34:25 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-16 22:34:26,014] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-16 22:34:26,020] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (5deb0a1bddc0, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,022] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-16 22:34:26,028] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 485 ms on 5deb0a1bddc0 (executor driver) (1/141)
[2021-05-16 22:34:26,224] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1886 bytes result sent to driver
[2021-05-16 22:34:26,226] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (5deb0a1bddc0, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,228] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-16 22:34:26,229] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 210 ms on 5deb0a1bddc0 (executor driver) (2/141)
[2021-05-16 22:34:26,418] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-16 22:34:26,420] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (5deb0a1bddc0, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,422] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-16 22:34:26,423] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 198 ms on 5deb0a1bddc0 (executor driver) (3/141)
[2021-05-16 22:34:26,510] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-16 22:34:26,512] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (5deb0a1bddc0, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,514] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
21/05/17 01:34:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1017 ms on 5deb0a1bddc0 (executor driver) (4/141)
[2021-05-16 22:34:26,524] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-16 22:34:26,525] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (5deb0a1bddc0, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,526] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
21/05/17 01:34:26 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 987 ms on 5deb0a1bddc0 (executor driver) (5/141)
[2021-05-16 22:34:26,589] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-16 22:34:26,590] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (5deb0a1bddc0, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,591] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1053 ms on 5deb0a1bddc0 (executor driver) (6/141)
[2021-05-16 22:34:26,592] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-16 22:34:26,603] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-16 22:34:26,605] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (5deb0a1bddc0, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,606] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-16 22:34:26,606] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 187 ms on 5deb0a1bddc0 (executor driver) (7/141)
[2021-05-16 22:34:26,705] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-16 22:34:26,707] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (5deb0a1bddc0, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,709] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-16 22:34:26,709] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 184 ms on 5deb0a1bddc0 (executor driver) (8/141)
[2021-05-16 22:34:26,710] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-16 22:34:26,712] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (5deb0a1bddc0, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,713] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 202 ms on 5deb0a1bddc0 (executor driver) (9/141)
[2021-05-16 22:34:26,714] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-16 22:34:26,773] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-16 22:34:26,775] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (5deb0a1bddc0, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,776] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
21/05/17 01:34:26 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 187 ms on 5deb0a1bddc0 (executor driver) (10/141)
[2021-05-16 22:34:26,780] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-16 22:34:26,781] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (5deb0a1bddc0, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,783] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 178 ms on 5deb0a1bddc0 (executor driver) (11/141)
21/05/17 01:34:26 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-16 22:34:26,892] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-16 22:34:26,896] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-16 22:34:26,897] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (5deb0a1bddc0, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,900] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-16 22:34:26,901] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (5deb0a1bddc0, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,903] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 191 ms on 5deb0a1bddc0 (executor driver) (12/141)
[2021-05-16 22:34:26,904] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 197 ms on 5deb0a1bddc0 (executor driver) (13/141)
[2021-05-16 22:34:26,904] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-16 22:34:26,961] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1886 bytes result sent to driver
[2021-05-16 22:34:26,962] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (5deb0a1bddc0, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,963] {docker.py:276} INFO - 21/05/17 01:34:26 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 182 ms on 5deb0a1bddc0 (executor driver) (14/141)
[2021-05-16 22:34:26,964] {docker.py:276} INFO - 21/05/17 01:34:26 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-16 22:34:26,981] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1886 bytes result sent to driver
[2021-05-16 22:34:26,982] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (5deb0a1bddc0, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:26,984] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 210 ms on 5deb0a1bddc0 (executor driver) (15/141)
[2021-05-16 22:34:26,995] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-16 22:34:27,092] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1886 bytes result sent to driver
[2021-05-16 22:34:27,093] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1886 bytes result sent to driver
[2021-05-16 22:34:27,094] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (5deb0a1bddc0, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,095] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-16 22:34:27,096] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (5deb0a1bddc0, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,097] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 198 ms on 5deb0a1bddc0 (executor driver) (16/141)
[2021-05-16 22:34:27,097] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 203 ms on 5deb0a1bddc0 (executor driver) (17/141)
[2021-05-16 22:34:27,099] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-16 22:34:27,141] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-16 22:34:27,142] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (5deb0a1bddc0, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,144] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 182 ms on 5deb0a1bddc0 (executor driver) (18/141)
[2021-05-16 22:34:27,144] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-16 22:34:27,170] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-16 22:34:27,171] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (5deb0a1bddc0, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,172] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-16 22:34:27,172] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 191 ms on 5deb0a1bddc0 (executor driver) (19/141)
[2021-05-16 22:34:27,270] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-16 22:34:27,272] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
[2021-05-16 22:34:27,273] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (5deb0a1bddc0, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,276] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
21/05/17 01:34:27 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (5deb0a1bddc0, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,277] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 184 ms on 5deb0a1bddc0 (executor driver) (20/141)
[2021-05-16 22:34:27,278] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-16 22:34:27,279] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 183 ms on 5deb0a1bddc0 (executor driver) (21/141)
[2021-05-16 22:34:27,320] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-16 22:34:27,321] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (5deb0a1bddc0, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,324] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 181 ms on 5deb0a1bddc0 (executor driver) (22/141)
[2021-05-16 22:34:27,324] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-16 22:34:27,344] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-16 22:34:27,346] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (5deb0a1bddc0, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,348] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-16 22:34:27,348] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 177 ms on 5deb0a1bddc0 (executor driver) (23/141)
[2021-05-16 22:34:27,450] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-16 22:34:27,452] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (5deb0a1bddc0, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,453] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 181 ms on 5deb0a1bddc0 (executor driver) (24/141)
[2021-05-16 22:34:27,454] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-16 22:34:27,457] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-16 22:34:27,465] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (5deb0a1bddc0, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,467] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-16 22:34:27,468] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 192 ms on 5deb0a1bddc0 (executor driver) (25/141)
[2021-05-16 22:34:27,500] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1886 bytes result sent to driver
[2021-05-16 22:34:27,501] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (5deb0a1bddc0, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,502] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
21/05/17 01:34:27 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 182 ms on 5deb0a1bddc0 (executor driver) (26/141)
[2021-05-16 22:34:27,521] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1886 bytes result sent to driver
[2021-05-16 22:34:27,523] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (5deb0a1bddc0, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,523] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 179 ms on 5deb0a1bddc0 (executor driver) (27/141)
[2021-05-16 22:34:27,526] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-16 22:34:27,635] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1886 bytes result sent to driver
[2021-05-16 22:34:27,637] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (5deb0a1bddc0, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,638] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 187 ms on 5deb0a1bddc0 (executor driver) (28/141)
21/05/17 01:34:27 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-16 22:34:27,639] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-16 22:34:27,640] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (5deb0a1bddc0, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,642] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-16 22:34:27,643] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 184 ms on 5deb0a1bddc0 (executor driver) (29/141)
[2021-05-16 22:34:27,678] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-16 22:34:27,679] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (5deb0a1bddc0, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,681] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 180 ms on 5deb0a1bddc0 (executor driver) (30/141)
[2021-05-16 22:34:27,681] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-16 22:34:27,698] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-16 22:34:27,700] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (5deb0a1bddc0, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,701] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-16 22:34:27,702] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 179 ms on 5deb0a1bddc0 (executor driver) (31/141)
[2021-05-16 22:34:27,813] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-16 22:34:27,814] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (5deb0a1bddc0, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,815] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-16 22:34:27,816] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-16 22:34:27,817] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 181 ms on 5deb0a1bddc0 (executor driver) (32/141)
[2021-05-16 22:34:27,818] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 179 ms on 5deb0a1bddc0 (executor driver) (33/141)
[2021-05-16 22:34:27,820] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (5deb0a1bddc0, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,821] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-16 22:34:27,857] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-16 22:34:27,858] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (5deb0a1bddc0, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,859] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 181 ms on 5deb0a1bddc0 (executor driver) (34/141)
[2021-05-16 22:34:27,859] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-16 22:34:27,875] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-16 22:34:27,876] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (5deb0a1bddc0, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,877] {docker.py:276} INFO - 21/05/17 01:34:27 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-16 22:34:27,877] {docker.py:276} INFO - 21/05/17 01:34:27 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 178 ms on 5deb0a1bddc0 (executor driver) (35/141)
[2021-05-16 22:34:27,989] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-16 22:34:27,990] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (5deb0a1bddc0, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,991] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 177 ms on 5deb0a1bddc0 (executor driver) (36/141)
[2021-05-16 22:34:27,991] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-16 22:34:27,993] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-16 22:34:27,994] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (5deb0a1bddc0, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:27,995] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 176 ms on 5deb0a1bddc0 (executor driver) (37/141)
[2021-05-16 22:34:27,996] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-16 22:34:28,033] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-16 22:34:28,035] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (5deb0a1bddc0, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,035] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-16 22:34:28,036] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 177 ms on 5deb0a1bddc0 (executor driver) (38/141)
[2021-05-16 22:34:28,052] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1886 bytes result sent to driver
[2021-05-16 22:34:28,053] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (5deb0a1bddc0, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,054] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-16 22:34:28,055] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 179 ms on 5deb0a1bddc0 (executor driver) (39/141)
[2021-05-16 22:34:28,163] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1886 bytes result sent to driver
[2021-05-16 22:34:28,165] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (5deb0a1bddc0, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,166] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 177 ms on 5deb0a1bddc0 (executor driver) (40/141)
[2021-05-16 22:34:28,166] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1886 bytes result sent to driver
[2021-05-16 22:34:28,167] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-16 22:34:28,170] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (5deb0a1bddc0, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,170] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 176 ms on 5deb0a1bddc0 (executor driver) (41/141)
[2021-05-16 22:34:28,172] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-16 22:34:28,217] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1886 bytes result sent to driver
[2021-05-16 22:34:28,218] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (5deb0a1bddc0, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,219] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-16 22:34:28,220] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 186 ms on 5deb0a1bddc0 (executor driver) (42/141)
[2021-05-16 22:34:28,228] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-16 22:34:28,229] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (5deb0a1bddc0, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,231] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 178 ms on 5deb0a1bddc0 (executor driver) (43/141)
21/05/17 01:34:28 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-16 22:34:28,340] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-16 22:34:28,341] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (5deb0a1bddc0, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,342] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 177 ms on 5deb0a1bddc0 (executor driver) (44/141)
[2021-05-16 22:34:28,343] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-16 22:34:28,345] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-16 22:34:28,346] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (5deb0a1bddc0, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,347] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 178 ms on 5deb0a1bddc0 (executor driver) (45/141)
[2021-05-16 22:34:28,348] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-16 22:34:28,395] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-16 22:34:28,396] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (5deb0a1bddc0, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,397] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 180 ms on 5deb0a1bddc0 (executor driver) (46/141)
[2021-05-16 22:34:28,398] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-16 22:34:28,406] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-16 22:34:28,407] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (5deb0a1bddc0, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,408] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-16 22:34:28,409] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 179 ms on 5deb0a1bddc0 (executor driver) (47/141)
[2021-05-16 22:34:28,517] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-16 22:34:28,518] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-16 22:34:28,519] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (5deb0a1bddc0, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,521] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 175 ms on 5deb0a1bddc0 (executor driver) (48/141)
[2021-05-16 22:34:28,522] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (5deb0a1bddc0, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,523] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-16 22:34:28,524] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-16 22:34:28,525] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 184 ms on 5deb0a1bddc0 (executor driver) (49/141)
[2021-05-16 22:34:28,579] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-16 22:34:28,581] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (5deb0a1bddc0, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,582] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-16 22:34:28,584] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 187 ms on 5deb0a1bddc0 (executor driver) (50/141)
[2021-05-16 22:34:28,586] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-16 22:34:28,587] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (5deb0a1bddc0, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,588] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-16 22:34:28,589] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 182 ms on 5deb0a1bddc0 (executor driver) (51/141)
[2021-05-16 22:34:28,729] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1886 bytes result sent to driver
[2021-05-16 22:34:28,730] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1886 bytes result sent to driver
[2021-05-16 22:34:28,731] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (5deb0a1bddc0, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,733] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 179 ms on 5deb0a1bddc0 (executor driver) (52/141)
[2021-05-16 22:34:28,734] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
21/05/17 01:34:28 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (5deb0a1bddc0, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,735] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 179 ms on 5deb0a1bddc0 (executor driver) (53/141)
[2021-05-16 22:34:28,735] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-16 22:34:28,777] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1886 bytes result sent to driver
[2021-05-16 22:34:28,779] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1886 bytes result sent to driver
[2021-05-16 22:34:28,780] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (5deb0a1bddc0, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,782] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-16 22:34:28,783] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (5deb0a1bddc0, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,784] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 169 ms on 5deb0a1bddc0 (executor driver) (54/141)
[2021-05-16 22:34:28,785] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-16 22:34:28,786] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 164 ms on 5deb0a1bddc0 (executor driver) (55/141)
[2021-05-16 22:34:28,905] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
[2021-05-16 22:34:28,907] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (5deb0a1bddc0, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,908] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
[2021-05-16 22:34:28,910] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
21/05/17 01:34:28 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (5deb0a1bddc0, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,911] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 177 ms on 5deb0a1bddc0 (executor driver) (56/141)
[2021-05-16 22:34:28,912] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 181 ms on 5deb0a1bddc0 (executor driver) (57/141)
[2021-05-16 22:34:28,914] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-16 22:34:28,962] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-16 22:34:28,963] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-16 22:34:28,964] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (5deb0a1bddc0, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,966] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
21/05/17 01:34:28 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 187 ms on 5deb0a1bddc0 (executor driver) (58/141)
[2021-05-16 22:34:28,968] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (5deb0a1bddc0, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:28,970] {docker.py:276} INFO - 21/05/17 01:34:28 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-16 22:34:28,971] {docker.py:276} INFO - 21/05/17 01:34:28 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 189 ms on 5deb0a1bddc0 (executor driver) (59/141)
[2021-05-16 22:34:29,086] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-16 22:34:29,088] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-16 22:34:29,089] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (5deb0a1bddc0, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,092] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (5deb0a1bddc0, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,093] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 183 ms on 5deb0a1bddc0 (executor driver) (60/141)
[2021-05-16 22:34:29,094] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 187 ms on 5deb0a1bddc0 (executor driver) (61/141)
21/05/17 01:34:29 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-16 22:34:29,095] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-16 22:34:29,146] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-16 22:34:29,147] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-16 22:34:29,149] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (5deb0a1bddc0, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,151] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-16 22:34:29,152] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (5deb0a1bddc0, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,152] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 185 ms on 5deb0a1bddc0 (executor driver) (62/141)
[2021-05-16 22:34:29,153] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-16 22:34:29,153] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 190 ms on 5deb0a1bddc0 (executor driver) (63/141)
[2021-05-16 22:34:29,270] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-16 22:34:29,272] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (5deb0a1bddc0, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,274] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-16 22:34:29,274] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 183 ms on 5deb0a1bddc0 (executor driver) (64/141)
[2021-05-16 22:34:29,275] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-16 22:34:29,277] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (5deb0a1bddc0, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,279] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-16 22:34:29,280] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 191 ms on 5deb0a1bddc0 (executor driver) (65/141)
[2021-05-16 22:34:29,328] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1886 bytes result sent to driver
[2021-05-16 22:34:29,329] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (5deb0a1bddc0, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,330] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 183 ms on 5deb0a1bddc0 (executor driver) (66/141)
[2021-05-16 22:34:29,331] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-16 22:34:29,333] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1886 bytes result sent to driver
[2021-05-16 22:34:29,344] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (5deb0a1bddc0, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,345] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 194 ms on 5deb0a1bddc0 (executor driver) (67/141)
[2021-05-16 22:34:29,346] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-16 22:34:29,458] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1886 bytes result sent to driver
[2021-05-16 22:34:29,459] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1886 bytes result sent to driver
[2021-05-16 22:34:29,461] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (5deb0a1bddc0, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,463] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 191 ms on 5deb0a1bddc0 (executor driver) (68/141)
[2021-05-16 22:34:29,464] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-16 22:34:29,465] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (5deb0a1bddc0, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,467] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 190 ms on 5deb0a1bddc0 (executor driver) (69/141)
[2021-05-16 22:34:29,467] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-16 22:34:29,527] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-16 22:34:29,529] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (5deb0a1bddc0, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,531] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 201 ms on 5deb0a1bddc0 (executor driver) (70/141)
21/05/17 01:34:29 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-16 22:34:29,532] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-16 22:34:29,533] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (5deb0a1bddc0, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,534] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 190 ms on 5deb0a1bddc0 (executor driver) (71/141)
[2021-05-16 22:34:29,535] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-16 22:34:29,642] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-16 22:34:29,643] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-16 22:34:29,644] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (5deb0a1bddc0, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,646] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 186 ms on 5deb0a1bddc0 (executor driver) (72/141)
[2021-05-16 22:34:29,647] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-16 22:34:29,649] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (5deb0a1bddc0, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,650] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 185 ms on 5deb0a1bddc0 (executor driver) (73/141)
[2021-05-16 22:34:29,651] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-16 22:34:29,713] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1843 bytes result sent to driver
[2021-05-16 22:34:29,714] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-16 22:34:29,715] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (5deb0a1bddc0, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,716] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-16 22:34:29,717] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (5deb0a1bddc0, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,718] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 186 ms on 5deb0a1bddc0 (executor driver) (74/141)
[2021-05-16 22:34:29,719] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 191 ms on 5deb0a1bddc0 (executor driver) (75/141)
[2021-05-16 22:34:29,720] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-16 22:34:29,818] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1843 bytes result sent to driver
[2021-05-16 22:34:29,819] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (5deb0a1bddc0, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,820] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-16 22:34:29,821] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 178 ms on 5deb0a1bddc0 (executor driver) (76/141)
[2021-05-16 22:34:29,829] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-16 22:34:29,830] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (5deb0a1bddc0, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,831] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 183 ms on 5deb0a1bddc0 (executor driver) (77/141)
[2021-05-16 22:34:29,833] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-16 22:34:29,894] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-16 22:34:29,896] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-16 22:34:29,898] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (5deb0a1bddc0, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,899] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 185 ms on 5deb0a1bddc0 (executor driver) (78/141)
[2021-05-16 22:34:29,900] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (5deb0a1bddc0, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,900] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-16 22:34:29,901] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 185 ms on 5deb0a1bddc0 (executor driver) (79/141)
[2021-05-16 22:34:29,903] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-16 22:34:29,986] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1886 bytes result sent to driver
[2021-05-16 22:34:29,988] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (5deb0a1bddc0, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:29,989] {docker.py:276} INFO - 21/05/17 01:34:29 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-16 22:34:29,990] {docker.py:276} INFO - 21/05/17 01:34:29 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 171 ms on 5deb0a1bddc0 (executor driver) (80/141)
[2021-05-16 22:34:30,002] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1886 bytes result sent to driver
[2021-05-16 22:34:30,003] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (5deb0a1bddc0, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,004] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 174 ms on 5deb0a1bddc0 (executor driver) (81/141)
[2021-05-16 22:34:30,005] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-16 22:34:30,077] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1886 bytes result sent to driver
[2021-05-16 22:34:30,079] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (5deb0a1bddc0, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,081] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 182 ms on 5deb0a1bddc0 (executor driver) (82/141)
21/05/17 01:34:30 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-16 22:34:30,088] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1886 bytes result sent to driver
[2021-05-16 22:34:30,089] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (5deb0a1bddc0, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,090] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-16 22:34:30,091] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 193 ms on 5deb0a1bddc0 (executor driver) (83/141)
[2021-05-16 22:34:30,157] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-16 22:34:30,159] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (5deb0a1bddc0, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,160] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-16 22:34:30,160] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 173 ms on 5deb0a1bddc0 (executor driver) (84/141)
[2021-05-16 22:34:30,171] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-16 22:34:30,173] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (5deb0a1bddc0, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,174] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 172 ms on 5deb0a1bddc0 (executor driver) (85/141)
[2021-05-16 22:34:30,174] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-16 22:34:30,268] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-16 22:34:30,269] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (5deb0a1bddc0, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,270] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-16 22:34:30,271] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 192 ms on 5deb0a1bddc0 (executor driver) (86/141)
[2021-05-16 22:34:30,275] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
[2021-05-16 22:34:30,276] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (5deb0a1bddc0, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,277] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 188 ms on 5deb0a1bddc0 (executor driver) (87/141)
[2021-05-16 22:34:30,278] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-16 22:34:30,335] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1843 bytes result sent to driver
[2021-05-16 22:34:30,339] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (5deb0a1bddc0, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,340] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 182 ms on 5deb0a1bddc0 (executor driver) (88/141)
[2021-05-16 22:34:30,340] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-16 22:34:30,345] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1843 bytes result sent to driver
[2021-05-16 22:34:30,347] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (5deb0a1bddc0, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,348] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-16 22:34:30,348] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 176 ms on 5deb0a1bddc0 (executor driver) (89/141)
[2021-05-16 22:34:30,458] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1843 bytes result sent to driver
[2021-05-16 22:34:30,460] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (5deb0a1bddc0, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,461] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-16 22:34:30,461] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 192 ms on 5deb0a1bddc0 (executor driver) (90/141)
[2021-05-16 22:34:30,475] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1843 bytes result sent to driver
[2021-05-16 22:34:30,477] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (5deb0a1bddc0, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,478] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 202 ms on 5deb0a1bddc0 (executor driver) (91/141)
[2021-05-16 22:34:30,478] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-16 22:34:30,514] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1843 bytes result sent to driver
[2021-05-16 22:34:30,515] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (5deb0a1bddc0, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,516] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-16 22:34:30,517] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 178 ms on 5deb0a1bddc0 (executor driver) (92/141)
[2021-05-16 22:34:30,518] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1843 bytes result sent to driver
[2021-05-16 22:34:30,520] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (5deb0a1bddc0, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,521] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 175 ms on 5deb0a1bddc0 (executor driver) (93/141)
21/05/17 01:34:30 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-16 22:34:30,637] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1886 bytes result sent to driver
[2021-05-16 22:34:30,639] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (5deb0a1bddc0, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,641] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 182 ms on 5deb0a1bddc0 (executor driver) (94/141)
[2021-05-16 22:34:30,642] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-16 22:34:30,654] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1886 bytes result sent to driver
[2021-05-16 22:34:30,655] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (5deb0a1bddc0, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,657] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 181 ms on 5deb0a1bddc0 (executor driver) (95/141)
[2021-05-16 22:34:30,657] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-16 22:34:30,697] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1886 bytes result sent to driver
[2021-05-16 22:34:30,700] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (5deb0a1bddc0, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,701] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 186 ms on 5deb0a1bddc0 (executor driver) (96/141)
[2021-05-16 22:34:30,702] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1886 bytes result sent to driver
[2021-05-16 22:34:30,703] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
[2021-05-16 22:34:30,704] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (5deb0a1bddc0, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,705] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-16 22:34:30,705] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 185 ms on 5deb0a1bddc0 (executor driver) (97/141)
[2021-05-16 22:34:30,819] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1843 bytes result sent to driver
[2021-05-16 22:34:30,822] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (5deb0a1bddc0, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,823] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 185 ms on 5deb0a1bddc0 (executor driver) (98/141)
[2021-05-16 22:34:30,825] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-16 22:34:30,832] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1843 bytes result sent to driver
[2021-05-16 22:34:30,833] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (5deb0a1bddc0, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,834] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-16 22:34:30,836] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 181 ms on 5deb0a1bddc0 (executor driver) (99/141)
[2021-05-16 22:34:30,874] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1843 bytes result sent to driver
[2021-05-16 22:34:30,875] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (5deb0a1bddc0, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,876] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 174 ms on 5deb0a1bddc0 (executor driver) (100/141)
[2021-05-16 22:34:30,877] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-16 22:34:30,879] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1843 bytes result sent to driver
[2021-05-16 22:34:30,880] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (5deb0a1bddc0, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:30,880] {docker.py:276} INFO - 21/05/17 01:34:30 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-16 22:34:30,881] {docker.py:276} INFO - 21/05/17 01:34:30 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 182 ms on 5deb0a1bddc0 (executor driver) (101/141)
[2021-05-16 22:34:31,000] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1843 bytes result sent to driver
[2021-05-16 22:34:31,002] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (5deb0a1bddc0, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,002] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-16 22:34:31,003] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 182 ms on 5deb0a1bddc0 (executor driver) (102/141)
[2021-05-16 22:34:31,007] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1843 bytes result sent to driver
[2021-05-16 22:34:31,008] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (5deb0a1bddc0, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,009] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-16 22:34:31,009] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 177 ms on 5deb0a1bddc0 (executor driver) (103/141)
[2021-05-16 22:34:31,047] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1843 bytes result sent to driver
[2021-05-16 22:34:31,047] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1843 bytes result sent to driver
[2021-05-16 22:34:31,048] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (5deb0a1bddc0, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,049] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 169 ms on 5deb0a1bddc0 (executor driver) (104/141)
[2021-05-16 22:34:31,049] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-16 22:34:31,050] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (5deb0a1bddc0, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,051] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 176 ms on 5deb0a1bddc0 (executor driver) (105/141)
[2021-05-16 22:34:31,052] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-16 22:34:31,188] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1843 bytes result sent to driver
[2021-05-16 22:34:31,189] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1843 bytes result sent to driver
[2021-05-16 22:34:31,191] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (5deb0a1bddc0, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,192] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 191 ms on 5deb0a1bddc0 (executor driver) (106/141)
[2021-05-16 22:34:31,193] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
21/05/17 01:34:31 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 185 ms on 5deb0a1bddc0 (executor driver) (107/141)
[2021-05-16 22:34:31,195] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (5deb0a1bddc0, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,195] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-16 22:34:31,222] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1843 bytes result sent to driver
[2021-05-16 22:34:31,222] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1843 bytes result sent to driver
[2021-05-16 22:34:31,223] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (5deb0a1bddc0, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,224] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
21/05/17 01:34:31 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (5deb0a1bddc0, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,225] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 178 ms on 5deb0a1bddc0 (executor driver) (108/141)
[2021-05-16 22:34:31,226] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 176 ms on 5deb0a1bddc0 (executor driver) (109/141)
[2021-05-16 22:34:31,227] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-16 22:34:31,370] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1886 bytes result sent to driver
[2021-05-16 22:34:31,372] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (5deb0a1bddc0, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,373] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 183 ms on 5deb0a1bddc0 (executor driver) (110/141)
[2021-05-16 22:34:31,374] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-16 22:34:31,375] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1886 bytes result sent to driver
[2021-05-16 22:34:31,376] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (5deb0a1bddc0, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,378] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 184 ms on 5deb0a1bddc0 (executor driver) (111/141)
[2021-05-16 22:34:31,378] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-16 22:34:31,399] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1886 bytes result sent to driver
[2021-05-16 22:34:31,400] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (5deb0a1bddc0, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,401] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 177 ms on 5deb0a1bddc0 (executor driver) (112/141)
[2021-05-16 22:34:31,401] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-16 22:34:31,402] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1886 bytes result sent to driver
[2021-05-16 22:34:31,403] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (5deb0a1bddc0, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,404] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 181 ms on 5deb0a1bddc0 (executor driver) (113/141)
21/05/17 01:34:31 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-16 22:34:31,554] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1843 bytes result sent to driver
[2021-05-16 22:34:31,555] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (5deb0a1bddc0, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,557] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 184 ms on 5deb0a1bddc0 (executor driver) (114/141)
[2021-05-16 22:34:31,558] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-16 22:34:31,560] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1843 bytes result sent to driver
[2021-05-16 22:34:31,561] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (5deb0a1bddc0, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,562] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 187 ms on 5deb0a1bddc0 (executor driver) (115/141)
[2021-05-16 22:34:31,563] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-16 22:34:31,570] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1843 bytes result sent to driver
[2021-05-16 22:34:31,572] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (5deb0a1bddc0, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,573] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 170 ms on 5deb0a1bddc0 (executor driver) (116/141)
[2021-05-16 22:34:31,573] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-16 22:34:31,575] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-16 22:34:31,575] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (5deb0a1bddc0, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,576] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
21/05/17 01:34:31 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 177 ms on 5deb0a1bddc0 (executor driver) (117/141)
[2021-05-16 22:34:31,736] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1843 bytes result sent to driver
[2021-05-16 22:34:31,738] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (5deb0a1bddc0, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,739] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1843 bytes result sent to driver
[2021-05-16 22:34:31,740] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-16 22:34:31,741] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (5deb0a1bddc0, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,742] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 181 ms on 5deb0a1bddc0 (executor driver) (118/141)
[2021-05-16 22:34:31,743] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 188 ms on 5deb0a1bddc0 (executor driver) (119/141)
[2021-05-16 22:34:31,744] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
[2021-05-16 22:34:31,746] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1843 bytes result sent to driver
[2021-05-16 22:34:31,747] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (5deb0a1bddc0, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,749] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 177 ms on 5deb0a1bddc0 (executor driver) (120/141)
[2021-05-16 22:34:31,749] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-16 22:34:31,752] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1843 bytes result sent to driver
[2021-05-16 22:34:31,753] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (5deb0a1bddc0, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,754] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-16 22:34:31,755] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 179 ms on 5deb0a1bddc0 (executor driver) (121/141)
[2021-05-16 22:34:31,922] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1843 bytes result sent to driver
[2021-05-16 22:34:31,923] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1843 bytes result sent to driver
[2021-05-16 22:34:31,924] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (5deb0a1bddc0, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,925] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 178 ms on 5deb0a1bddc0 (executor driver) (122/141)
[2021-05-16 22:34:31,926] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 174 ms on 5deb0a1bddc0 (executor driver) (123/141)
[2021-05-16 22:34:31,927] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1843 bytes result sent to driver
[2021-05-16 22:34:31,928] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-16 22:34:31,929] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (5deb0a1bddc0, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,931] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-16 22:34:31,932] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (5deb0a1bddc0, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,932] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 195 ms on 5deb0a1bddc0 (executor driver) (124/141)
[2021-05-16 22:34:31,933] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-16 22:34:31,941] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1843 bytes result sent to driver
[2021-05-16 22:34:31,942] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 202 ms on 5deb0a1bddc0 (executor driver) (125/141)
[2021-05-16 22:34:31,942] {docker.py:276} INFO - 21/05/17 01:34:31 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (5deb0a1bddc0, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:31,949] {docker.py:276} INFO - 21/05/17 01:34:31 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-16 22:34:32,116] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1886 bytes result sent to driver
[2021-05-16 22:34:32,117] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1886 bytes result sent to driver
[2021-05-16 22:34:32,118] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (5deb0a1bddc0, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,120] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-16 22:34:32,121] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (5deb0a1bddc0, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,123] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1886 bytes result sent to driver
21/05/17 01:34:32 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 201 ms on 5deb0a1bddc0 (executor driver) (126/141)
[2021-05-16 22:34:32,124] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1843 bytes result sent to driver
[2021-05-16 22:34:32,125] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-16 22:34:32,126] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (5deb0a1bddc0, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,127] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-16 22:34:32,128] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (5deb0a1bddc0, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,129] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-16 22:34:32,130] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 199 ms on 5deb0a1bddc0 (executor driver) (127/141)
[2021-05-16 22:34:32,131] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 201 ms on 5deb0a1bddc0 (executor driver) (128/141)
[2021-05-16 22:34:32,131] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 189 ms on 5deb0a1bddc0 (executor driver) (129/141)
[2021-05-16 22:34:32,301] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1843 bytes result sent to driver
[2021-05-16 22:34:32,302] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1843 bytes result sent to driver
[2021-05-16 22:34:32,304] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (5deb0a1bddc0, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,305] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 177 ms on 5deb0a1bddc0 (executor driver) (130/141)
[2021-05-16 22:34:32,306] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-16 22:34:32,308] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (5deb0a1bddc0, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,309] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 184 ms on 5deb0a1bddc0 (executor driver) (131/141)
[2021-05-16 22:34:32,310] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-16 22:34:32,311] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1843 bytes result sent to driver
[2021-05-16 22:34:32,312] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 194 ms on 5deb0a1bddc0 (executor driver) (132/141)
[2021-05-16 22:34:32,313] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1843 bytes result sent to driver
[2021-05-16 22:34:32,313] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (5deb0a1bddc0, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,314] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-16 22:34:32,315] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (5deb0a1bddc0, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,317] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 196 ms on 5deb0a1bddc0 (executor driver) (133/141)
[2021-05-16 22:34:32,317] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-16 22:34:32,488] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1843 bytes result sent to driver
[2021-05-16 22:34:32,490] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (5deb0a1bddc0, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,491] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1843 bytes result sent to driver
[2021-05-16 22:34:32,492] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 176 ms on 5deb0a1bddc0 (executor driver) (134/141)
21/05/17 01:34:32 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-16 22:34:32,494] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1843 bytes result sent to driver
[2021-05-16 22:34:32,495] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1843 bytes result sent to driver
21/05/17 01:34:32 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (5deb0a1bddc0, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,496] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 184 ms on 5deb0a1bddc0 (executor driver) (135/141)
[2021-05-16 22:34:32,498] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (5deb0a1bddc0, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,499] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-16 22:34:32,500] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-16 22:34:32,501] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 198 ms on 5deb0a1bddc0 (executor driver) (136/141)
[2021-05-16 22:34:32,503] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (5deb0a1bddc0, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,504] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 197 ms on 5deb0a1bddc0 (executor driver) (137/141)
[2021-05-16 22:34:32,504] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-16 22:34:32,672] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1886 bytes result sent to driver
[2021-05-16 22:34:32,681] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 193 ms on 5deb0a1bddc0 (executor driver) (138/141)
[2021-05-16 22:34:32,682] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1886 bytes result sent to driver
[2021-05-16 22:34:32,683] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 180 ms on 5deb0a1bddc0 (executor driver) (139/141)
[2021-05-16 22:34:32,684] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1886 bytes result sent to driver
[2021-05-16 22:34:32,685] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 192 ms on 5deb0a1bddc0 (executor driver) (140/141)
[2021-05-16 22:34:32,686] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1886 bytes result sent to driver
[2021-05-16 22:34:32,687] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 190 ms on 5deb0a1bddc0 (executor driver) (141/141)
[2021-05-16 22:34:32,690] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 7.462 s
[2021-05-16 22:34:32,691] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-16 22:34:32,699] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 22:34:32,701] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-16 22:34:32,705] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 7.615935 s
[2021-05-16 22:34:32,744] {docker.py:276} INFO - 21/05/17 01:34:32 INFO InMemoryFileIndex: It took 8236 ms to list leaf files for 141 paths.
[2021-05-16 22:34:32,881] {docker.py:276} INFO - 21/05/17 01:34:32 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621128813_to_1621130613.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621130613_to_1621132413.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621132413_to_1621134213.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621134213_to_1621136013.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621136013_to_1621137813.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621137813_to_1621139613.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621139613_to_1621141413.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621141413_to_1621143213.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621143213_to_1621145013.csv, s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621145013_to_1621146813.csv.
[2021-05-16 22:34:32,930] {docker.py:276} INFO - 21/05/17 01:34:32 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:32,933] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/17 01:34:32 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 22:34:32,933] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 22:34:32,934] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Missing parents: List()
[2021-05-16 22:34:32,935] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 22:34:32,959] {docker.py:276} INFO - 21/05/17 01:34:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-16 22:34:32,968] {docker.py:276} INFO - 21/05/17 01:34:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-16 22:34:32,969] {docker.py:276} INFO - 21/05/17 01:34:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5deb0a1bddc0:42667 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-16 22:34:32,975] {docker.py:276} INFO - 21/05/17 01:34:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-16 22:34:32,978] {docker.py:276} INFO - 21/05/17 01:34:32 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/17 01:34:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-16 22:34:32,979] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (5deb0a1bddc0, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,980] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (5deb0a1bddc0, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,981] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (5deb0a1bddc0, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,981] {docker.py:276} INFO - 21/05/17 01:34:32 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (5deb0a1bddc0, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:32,982] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-16 22:34:32,983] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-16 22:34:32,984] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-16 22:34:32,984] {docker.py:276} INFO - 21/05/17 01:34:32 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-16 22:34:33,067] {docker.py:276} INFO - 21/05/17 01:34:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 5deb0a1bddc0:42667 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-16 22:34:33,165] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-16 22:34:33,166] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (5deb0a1bddc0, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,167] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-16 22:34:33,167] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 187 ms on 5deb0a1bddc0 (executor driver) (1/141)
[2021-05-16 22:34:33,169] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-16 22:34:33,170] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (5deb0a1bddc0, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,171] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 192 ms on 5deb0a1bddc0 (executor driver) (2/141)
[2021-05-16 22:34:33,173] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-16 22:34:33,174] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (5deb0a1bddc0, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,174] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 195 ms on 5deb0a1bddc0 (executor driver) (3/141)
[2021-05-16 22:34:33,176] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-16 22:34:33,176] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 196 ms on 5deb0a1bddc0 (executor driver) (4/141)
[2021-05-16 22:34:33,177] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (5deb0a1bddc0, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,178] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-16 22:34:33,181] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-16 22:34:33,187] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-16 22:34:33,351] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-16 22:34:33,352] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 183 ms on 5deb0a1bddc0 (executor driver) (5/141)
[2021-05-16 22:34:33,353] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (5deb0a1bddc0, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,355] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-16 22:34:33,356] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
21/05/17 01:34:33 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-16 22:34:33,357] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (5deb0a1bddc0, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,358] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-16 22:34:33,359] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (5deb0a1bddc0, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,360] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 194 ms on 5deb0a1bddc0 (executor driver) (6/141)
[2021-05-16 22:34:33,360] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 184 ms on 5deb0a1bddc0 (executor driver) (7/141)
[2021-05-16 22:34:33,362] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-16 22:34:33,363] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-16 22:34:33,364] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (5deb0a1bddc0, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,365] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-16 22:34:33,366] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 192 ms on 5deb0a1bddc0 (executor driver) (8/141)
[2021-05-16 22:34:33,536] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
21/05/17 01:34:33 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-16 22:34:33,538] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (5deb0a1bddc0, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,540] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
21/05/17 01:34:33 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-16 22:34:33,541] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (5deb0a1bddc0, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,542] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 178 ms on 5deb0a1bddc0 (executor driver) (9/141)
[2021-05-16 22:34:33,543] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-16 22:34:33,544] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 185 ms on 5deb0a1bddc0 (executor driver) (10/141)
[2021-05-16 22:34:33,545] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (5deb0a1bddc0, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,546] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
[2021-05-16 22:34:33,546] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 193 ms on 5deb0a1bddc0 (executor driver) (11/141)
[2021-05-16 22:34:33,547] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
[2021-05-16 22:34:33,557] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (5deb0a1bddc0, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,558] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 202 ms on 5deb0a1bddc0 (executor driver) (12/141)
[2021-05-16 22:34:33,558] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-16 22:34:33,724] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1886 bytes result sent to driver
[2021-05-16 22:34:33,726] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (5deb0a1bddc0, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,728] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 183 ms on 5deb0a1bddc0 (executor driver) (13/141)
21/05/17 01:34:33 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
[2021-05-16 22:34:33,729] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1886 bytes result sent to driver
[2021-05-16 22:34:33,731] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (5deb0a1bddc0, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,733] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1843 bytes result sent to driver
[2021-05-16 22:34:33,734] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-16 22:34:33,739] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (5deb0a1bddc0, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 01:34:33 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1886 bytes result sent to driver
[2021-05-16 22:34:33,740] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 203 ms on 5deb0a1bddc0 (executor driver) (14/141)
[2021-05-16 22:34:33,741] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 184 ms on 5deb0a1bddc0 (executor driver) (15/141)
[2021-05-16 22:34:33,741] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-16 22:34:33,742] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (5deb0a1bddc0, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,743] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-16 22:34:33,744] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 203 ms on 5deb0a1bddc0 (executor driver) (16/141)
[2021-05-16 22:34:33,905] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1843 bytes result sent to driver
[2021-05-16 22:34:33,907] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (5deb0a1bddc0, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,908] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 183 ms on 5deb0a1bddc0 (executor driver) (17/141)
[2021-05-16 22:34:33,909] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1843 bytes result sent to driver
[2021-05-16 22:34:33,910] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
[2021-05-16 22:34:33,911] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-16 22:34:33,912] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (5deb0a1bddc0, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,913] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 182 ms on 5deb0a1bddc0 (executor driver) (18/141)
[2021-05-16 22:34:33,914] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 171 ms on 5deb0a1bddc0 (executor driver) (19/141)
[2021-05-16 22:34:33,914] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-16 22:34:33,915] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (5deb0a1bddc0, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,917] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-16 22:34:33,918] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1843 bytes result sent to driver
[2021-05-16 22:34:33,919] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (5deb0a1bddc0, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:33,920] {docker.py:276} INFO - 21/05/17 01:34:33 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 186 ms on 5deb0a1bddc0 (executor driver) (20/141)
[2021-05-16 22:34:33,922] {docker.py:276} INFO - 21/05/17 01:34:33 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-16 22:34:34,086] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-16 22:34:34,089] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
21/05/17 01:34:34 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (5deb0a1bddc0, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,090] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 179 ms on 5deb0a1bddc0 (executor driver) (21/141)
[2021-05-16 22:34:34,092] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
[2021-05-16 22:34:34,093] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (5deb0a1bddc0, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,094] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 188 ms on 5deb0a1bddc0 (executor driver) (22/141)
[2021-05-16 22:34:34,095] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-16 22:34:34,096] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-16 22:34:34,097] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-16 22:34:34,098] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (5deb0a1bddc0, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,100] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-16 22:34:34,100] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 185 ms on 5deb0a1bddc0 (executor driver) (23/141)
[2021-05-16 22:34:34,102] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (5deb0a1bddc0, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,102] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 184 ms on 5deb0a1bddc0 (executor driver) (24/141)
[2021-05-16 22:34:34,103] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-16 22:34:34,281] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1886 bytes result sent to driver
[2021-05-16 22:34:34,282] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1886 bytes result sent to driver
[2021-05-16 22:34:34,283] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (5deb0a1bddc0, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,284] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1886 bytes result sent to driver
[2021-05-16 22:34:34,286] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (5deb0a1bddc0, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,287] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-16 22:34:34,287] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 189 ms on 5deb0a1bddc0 (executor driver) (25/141)
[2021-05-16 22:34:34,288] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 197 ms on 5deb0a1bddc0 (executor driver) (26/141)
[2021-05-16 22:34:34,289] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 187 ms on 5deb0a1bddc0 (executor driver) (27/141)
[2021-05-16 22:34:34,290] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (5deb0a1bddc0, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,291] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
21/05/17 01:34:34 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-16 22:34:34,341] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1886 bytes result sent to driver
[2021-05-16 22:34:34,345] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (5deb0a1bddc0, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,348] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 260 ms on 5deb0a1bddc0 (executor driver) (28/141)
[2021-05-16 22:34:34,349] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-16 22:34:34,462] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-16 22:34:34,463] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
[2021-05-16 22:34:34,465] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (5deb0a1bddc0, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,467] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1843 bytes result sent to driver
[2021-05-16 22:34:34,468] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-16 22:34:34,469] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (5deb0a1bddc0, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,470] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-16 22:34:34,471] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (5deb0a1bddc0, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,472] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 190 ms on 5deb0a1bddc0 (executor driver) (29/141)
[2021-05-16 22:34:34,473] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 188 ms on 5deb0a1bddc0 (executor driver) (30/141)
[2021-05-16 22:34:34,474] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 184 ms on 5deb0a1bddc0 (executor driver) (31/141)
21/05/17 01:34:34 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-16 22:34:34,527] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
[2021-05-16 22:34:34,529] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (5deb0a1bddc0, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,531] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 188 ms on 5deb0a1bddc0 (executor driver) (32/141)
21/05/17 01:34:34 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-16 22:34:34,641] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1843 bytes result sent to driver
[2021-05-16 22:34:34,643] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (5deb0a1bddc0, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,644] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-16 22:34:34,645] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-16 22:34:34,647] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (5deb0a1bddc0, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,650] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1843 bytes result sent to driver
[2021-05-16 22:34:34,650] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 184 ms on 5deb0a1bddc0 (executor driver) (33/141)
[2021-05-16 22:34:34,651] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-16 22:34:34,651] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 181 ms on 5deb0a1bddc0 (executor driver) (34/141)
[2021-05-16 22:34:34,652] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 179 ms on 5deb0a1bddc0 (executor driver) (35/141)
[2021-05-16 22:34:34,652] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (5deb0a1bddc0, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,653] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-16 22:34:34,705] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1843 bytes result sent to driver
[2021-05-16 22:34:34,706] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (5deb0a1bddc0, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,707] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-16 22:34:34,707] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 178 ms on 5deb0a1bddc0 (executor driver) (36/141)
[2021-05-16 22:34:34,818] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1843 bytes result sent to driver
[2021-05-16 22:34:34,820] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (5deb0a1bddc0, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,821] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1843 bytes result sent to driver
[2021-05-16 22:34:34,822] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-16 22:34:34,823] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (5deb0a1bddc0, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,823] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 178 ms on 5deb0a1bddc0 (executor driver) (37/141)
[2021-05-16 22:34:34,824] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 173 ms on 5deb0a1bddc0 (executor driver) (38/141)
[2021-05-16 22:34:34,825] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-16 22:34:34,834] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1886 bytes result sent to driver
[2021-05-16 22:34:34,835] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (5deb0a1bddc0, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,836] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 194 ms on 5deb0a1bddc0 (executor driver) (39/141)
[2021-05-16 22:34:34,837] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-16 22:34:34,881] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1886 bytes result sent to driver
[2021-05-16 22:34:34,883] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (5deb0a1bddc0, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:34,884] {docker.py:276} INFO - 21/05/17 01:34:34 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-16 22:34:34,885] {docker.py:276} INFO - 21/05/17 01:34:34 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 180 ms on 5deb0a1bddc0 (executor driver) (40/141)
[2021-05-16 22:34:35,005] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1886 bytes result sent to driver
[2021-05-16 22:34:35,007] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (5deb0a1bddc0, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,008] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1843 bytes result sent to driver
[2021-05-16 22:34:35,009] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-16 22:34:35,010] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 190 ms on 5deb0a1bddc0 (executor driver) (41/141)
[2021-05-16 22:34:35,011] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (5deb0a1bddc0, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,012] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1886 bytes result sent to driver
[2021-05-16 22:34:35,012] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (5deb0a1bddc0, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,014] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
21/05/17 01:34:35 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 179 ms on 5deb0a1bddc0 (executor driver) (42/141)
[2021-05-16 22:34:35,014] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 192 ms on 5deb0a1bddc0 (executor driver) (43/141)
[2021-05-16 22:34:35,015] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-16 22:34:35,059] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1843 bytes result sent to driver
[2021-05-16 22:34:35,061] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (5deb0a1bddc0, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,062] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 179 ms on 5deb0a1bddc0 (executor driver) (44/141)
[2021-05-16 22:34:35,063] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-16 22:34:35,183] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1843 bytes result sent to driver
[2021-05-16 22:34:35,186] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (5deb0a1bddc0, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,187] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1843 bytes result sent to driver
[2021-05-16 22:34:35,188] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 178 ms on 5deb0a1bddc0 (executor driver) (45/141)
21/05/17 01:34:35 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
[2021-05-16 22:34:35,189] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1843 bytes result sent to driver
[2021-05-16 22:34:35,189] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (5deb0a1bddc0, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,191] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (5deb0a1bddc0, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 01:34:35 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-16 22:34:35,192] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-16 22:34:35,192] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 179 ms on 5deb0a1bddc0 (executor driver) (46/141)
[2021-05-16 22:34:35,193] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 188 ms on 5deb0a1bddc0 (executor driver) (47/141)
[2021-05-16 22:34:35,242] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1843 bytes result sent to driver
[2021-05-16 22:34:35,244] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (5deb0a1bddc0, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,245] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-16 22:34:35,246] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 185 ms on 5deb0a1bddc0 (executor driver) (48/141)
[2021-05-16 22:34:35,359] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
21/05/17 01:34:35 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
[2021-05-16 22:34:35,360] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (5deb0a1bddc0, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,361] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
[2021-05-16 22:34:35,362] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (5deb0a1bddc0, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,363] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
21/05/17 01:34:35 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 178 ms on 5deb0a1bddc0 (executor driver) (49/141)
[2021-05-16 22:34:35,364] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 176 ms on 5deb0a1bddc0 (executor driver) (50/141)
[2021-05-16 22:34:35,367] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
[2021-05-16 22:34:35,369] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (5deb0a1bddc0, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,370] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 180 ms on 5deb0a1bddc0 (executor driver) (51/141)
[2021-05-16 22:34:35,371] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-16 22:34:35,415] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1886 bytes result sent to driver
[2021-05-16 22:34:35,416] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (5deb0a1bddc0, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,417] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 174 ms on 5deb0a1bddc0 (executor driver) (52/141)
21/05/17 01:34:35 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
[2021-05-16 22:34:35,543] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1886 bytes result sent to driver
[2021-05-16 22:34:35,545] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (5deb0a1bddc0, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,546] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1886 bytes result sent to driver
[2021-05-16 22:34:35,547] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 186 ms on 5deb0a1bddc0 (executor driver) (53/141)
[2021-05-16 22:34:35,549] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1886 bytes result sent to driver
21/05/17 01:34:35 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
21/05/17 01:34:35 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 190 ms on 5deb0a1bddc0 (executor driver) (54/141)
[2021-05-16 22:34:35,550] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (5deb0a1bddc0, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,551] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-16 22:34:35,552] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (5deb0a1bddc0, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,553] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 186 ms on 5deb0a1bddc0 (executor driver) (55/141)
[2021-05-16 22:34:35,554] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-16 22:34:35,601] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1843 bytes result sent to driver
[2021-05-16 22:34:35,603] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (5deb0a1bddc0, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,604] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 188 ms on 5deb0a1bddc0 (executor driver) (56/141)
[2021-05-16 22:34:35,605] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-16 22:34:35,719] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1843 bytes result sent to driver
[2021-05-16 22:34:35,721] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1843 bytes result sent to driver
[2021-05-16 22:34:35,723] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (5deb0a1bddc0, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,724] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 180 ms on 5deb0a1bddc0 (executor driver) (57/141)
[2021-05-16 22:34:35,725] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 175 ms on 5deb0a1bddc0 (executor driver) (58/141)
[2021-05-16 22:34:35,726] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-16 22:34:35,726] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1843 bytes result sent to driver
[2021-05-16 22:34:35,727] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (5deb0a1bddc0, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,728] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
[2021-05-16 22:34:35,729] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (5deb0a1bddc0, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,730] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-16 22:34:35,731] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 179 ms on 5deb0a1bddc0 (executor driver) (59/141)
[2021-05-16 22:34:35,779] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1843 bytes result sent to driver
[2021-05-16 22:34:35,781] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (5deb0a1bddc0, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,782] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 180 ms on 5deb0a1bddc0 (executor driver) (60/141)
[2021-05-16 22:34:35,783] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-16 22:34:35,899] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1843 bytes result sent to driver
[2021-05-16 22:34:35,900] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1843 bytes result sent to driver
[2021-05-16 22:34:35,901] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (5deb0a1bddc0, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,902] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (5deb0a1bddc0, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,903] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1843 bytes result sent to driver
[2021-05-16 22:34:35,904] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 175 ms on 5deb0a1bddc0 (executor driver) (61/141)
21/05/17 01:34:35 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-16 22:34:35,905] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 178 ms on 5deb0a1bddc0 (executor driver) (62/141)
[2021-05-16 22:34:35,906] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-16 22:34:35,907] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (5deb0a1bddc0, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,908] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 187 ms on 5deb0a1bddc0 (executor driver) (63/141)
[2021-05-16 22:34:35,909] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-16 22:34:35,975] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1843 bytes result sent to driver
[2021-05-16 22:34:35,977] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (5deb0a1bddc0, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:35,977] {docker.py:276} INFO - 21/05/17 01:34:35 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 197 ms on 5deb0a1bddc0 (executor driver) (64/141)
[2021-05-16 22:34:35,978] {docker.py:276} INFO - 21/05/17 01:34:35 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-16 22:34:36,077] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1886 bytes result sent to driver
21/05/17 01:34:36 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1886 bytes result sent to driver
[2021-05-16 22:34:36,078] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (5deb0a1bddc0, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,079] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 172 ms on 5deb0a1bddc0 (executor driver) (65/141)
[2021-05-16 22:34:36,080] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1886 bytes result sent to driver
21/05/17 01:34:36 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-16 22:34:36,082] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (5deb0a1bddc0, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,083] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-16 22:34:36,084] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (5deb0a1bddc0, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,085] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 185 ms on 5deb0a1bddc0 (executor driver) (66/141)
[2021-05-16 22:34:36,085] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 184 ms on 5deb0a1bddc0 (executor driver) (67/141)
[2021-05-16 22:34:36,087] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-16 22:34:36,156] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1886 bytes result sent to driver
[2021-05-16 22:34:36,158] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (5deb0a1bddc0, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,159] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 182 ms on 5deb0a1bddc0 (executor driver) (68/141)
21/05/17 01:34:36 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-16 22:34:36,254] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
21/05/17 01:34:36 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
[2021-05-16 22:34:36,256] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (5deb0a1bddc0, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,257] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-16 22:34:36,258] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (5deb0a1bddc0, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,259] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 181 ms on 5deb0a1bddc0 (executor driver) (69/141)
[2021-05-16 22:34:36,260] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
[2021-05-16 22:34:36,260] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-16 22:34:36,261] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (5deb0a1bddc0, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,262] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 179 ms on 5deb0a1bddc0 (executor driver) (70/141)
[2021-05-16 22:34:36,263] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 181 ms on 5deb0a1bddc0 (executor driver) (71/141)
[2021-05-16 22:34:36,264] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
[2021-05-16 22:34:36,337] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1843 bytes result sent to driver
[2021-05-16 22:34:36,339] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (5deb0a1bddc0, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,340] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 182 ms on 5deb0a1bddc0 (executor driver) (72/141)
21/05/17 01:34:36 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-16 22:34:36,431] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1843 bytes result sent to driver
[2021-05-16 22:34:36,433] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (5deb0a1bddc0, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 01:34:36 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1843 bytes result sent to driver
[2021-05-16 22:34:36,435] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (5deb0a1bddc0, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,436] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-16 22:34:36,437] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-16 22:34:36,437] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 182 ms on 5deb0a1bddc0 (executor driver) (73/141)
[2021-05-16 22:34:36,438] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 181 ms on 5deb0a1bddc0 (executor driver) (74/141)
[2021-05-16 22:34:36,440] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1843 bytes result sent to driver
[2021-05-16 22:34:36,441] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (5deb0a1bddc0, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,442] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-16 22:34:36,443] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 182 ms on 5deb0a1bddc0 (executor driver) (75/141)
[2021-05-16 22:34:36,522] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1843 bytes result sent to driver
[2021-05-16 22:34:36,523] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (5deb0a1bddc0, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,525] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
21/05/17 01:34:36 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 187 ms on 5deb0a1bddc0 (executor driver) (76/141)
[2021-05-16 22:34:36,610] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1843 bytes result sent to driver
[2021-05-16 22:34:36,611] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (5deb0a1bddc0, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,612] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1843 bytes result sent to driver
21/05/17 01:34:36 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 171 ms on 5deb0a1bddc0 (executor driver) (77/141)
[2021-05-16 22:34:36,614] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
[2021-05-16 22:34:36,615] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (5deb0a1bddc0, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,616] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 183 ms on 5deb0a1bddc0 (executor driver) (78/141)
[2021-05-16 22:34:36,617] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
21/05/17 01:34:36 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1843 bytes result sent to driver
[2021-05-16 22:34:36,619] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (5deb0a1bddc0, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,620] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
21/05/17 01:34:36 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 186 ms on 5deb0a1bddc0 (executor driver) (79/141)
[2021-05-16 22:34:36,698] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1886 bytes result sent to driver
[2021-05-16 22:34:36,700] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (5deb0a1bddc0, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,701] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 178 ms on 5deb0a1bddc0 (executor driver) (80/141)
[2021-05-16 22:34:36,701] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-16 22:34:36,798] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1886 bytes result sent to driver
21/05/17 01:34:36 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1886 bytes result sent to driver
[2021-05-16 22:34:36,800] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (5deb0a1bddc0, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,801] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1886 bytes result sent to driver
[2021-05-16 22:34:36,802] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 188 ms on 5deb0a1bddc0 (executor driver) (81/141)
[2021-05-16 22:34:36,803] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-16 22:34:36,803] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (5deb0a1bddc0, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,805] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-16 22:34:36,806] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (5deb0a1bddc0, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,807] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 188 ms on 5deb0a1bddc0 (executor driver) (82/141)
[2021-05-16 22:34:36,808] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 197 ms on 5deb0a1bddc0 (executor driver) (83/141)
[2021-05-16 22:34:36,809] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-16 22:34:36,872] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1843 bytes result sent to driver
[2021-05-16 22:34:36,873] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (5deb0a1bddc0, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,873] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 174 ms on 5deb0a1bddc0 (executor driver) (84/141)
[2021-05-16 22:34:36,874] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-16 22:34:36,975] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1843 bytes result sent to driver
[2021-05-16 22:34:36,978] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (5deb0a1bddc0, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,978] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 175 ms on 5deb0a1bddc0 (executor driver) (85/141)
[2021-05-16 22:34:36,980] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-16 22:34:36,985] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1843 bytes result sent to driver
21/05/17 01:34:36 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (5deb0a1bddc0, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 01:34:36 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
21/05/17 01:34:36 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 185 ms on 5deb0a1bddc0 (executor driver) (86/141)
[2021-05-16 22:34:36,986] {docker.py:276} INFO - 21/05/17 01:34:36 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-16 22:34:36,988] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (5deb0a1bddc0, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:36,989] {docker.py:276} INFO - 21/05/17 01:34:36 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 184 ms on 5deb0a1bddc0 (executor driver) (87/141)
[2021-05-16 22:34:36,991] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-16 22:34:37,161] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1843 bytes result sent to driver
[2021-05-16 22:34:37,163] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1843 bytes result sent to driver
[2021-05-16 22:34:37,164] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1843 bytes result sent to driver
[2021-05-16 22:34:37,165] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (5deb0a1bddc0, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,166] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 179 ms on 5deb0a1bddc0 (executor driver) (88/141)
[2021-05-16 22:34:37,167] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-16 22:34:37,168] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (5deb0a1bddc0, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,169] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-16 22:34:37,170] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (5deb0a1bddc0, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,171] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 194 ms on 5deb0a1bddc0 (executor driver) (89/141)
[2021-05-16 22:34:37,172] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 189 ms on 5deb0a1bddc0 (executor driver) (90/141)
[2021-05-16 22:34:37,172] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-16 22:34:37,265] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1843 bytes result sent to driver
[2021-05-16 22:34:37,267] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (5deb0a1bddc0, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,268] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 396 ms on 5deb0a1bddc0 (executor driver) (91/141)
[2021-05-16 22:34:37,269] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-16 22:34:37,341] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1886 bytes result sent to driver
21/05/17 01:34:37 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1886 bytes result sent to driver
[2021-05-16 22:34:37,343] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (5deb0a1bddc0, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,344] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-16 22:34:37,345] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (5deb0a1bddc0, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,346] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 183 ms on 5deb0a1bddc0 (executor driver) (92/141)
[2021-05-16 22:34:37,347] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 178 ms on 5deb0a1bddc0 (executor driver) (93/141)
[2021-05-16 22:34:37,349] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-16 22:34:37,350] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1886 bytes result sent to driver
[2021-05-16 22:34:37,352] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (5deb0a1bddc0, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,353] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 186 ms on 5deb0a1bddc0 (executor driver) (94/141)
[2021-05-16 22:34:37,354] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-16 22:34:37,452] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1886 bytes result sent to driver
[2021-05-16 22:34:37,454] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (5deb0a1bddc0, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,455] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 189 ms on 5deb0a1bddc0 (executor driver) (95/141)
[2021-05-16 22:34:37,456] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-16 22:34:37,517] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1843 bytes result sent to driver
[2021-05-16 22:34:37,519] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (5deb0a1bddc0, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,520] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 178 ms on 5deb0a1bddc0 (executor driver) (96/141)
[2021-05-16 22:34:37,521] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-16 22:34:37,523] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1843 bytes result sent to driver
[2021-05-16 22:34:37,524] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (5deb0a1bddc0, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,525] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 173 ms on 5deb0a1bddc0 (executor driver) (97/141)
[2021-05-16 22:34:37,526] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-16 22:34:37,527] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1843 bytes result sent to driver
[2021-05-16 22:34:37,529] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (5deb0a1bddc0, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,530] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 186 ms on 5deb0a1bddc0 (executor driver) (98/141)
[2021-05-16 22:34:37,531] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
[2021-05-16 22:34:37,626] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1843 bytes result sent to driver
[2021-05-16 22:34:37,627] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (5deb0a1bddc0, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,628] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 175 ms on 5deb0a1bddc0 (executor driver) (99/141)
[2021-05-16 22:34:37,628] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-16 22:34:37,694] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1843 bytes result sent to driver
[2021-05-16 22:34:37,694] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1843 bytes result sent to driver
[2021-05-16 22:34:37,695] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (5deb0a1bddc0, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,695] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 167 ms on 5deb0a1bddc0 (executor driver) (100/141)
[2021-05-16 22:34:37,697] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
21/05/17 01:34:37 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (5deb0a1bddc0, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,698] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 174 ms on 5deb0a1bddc0 (executor driver) (101/141)
[2021-05-16 22:34:37,698] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-16 22:34:37,703] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1843 bytes result sent to driver
[2021-05-16 22:34:37,704] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (5deb0a1bddc0, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,705] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
21/05/17 01:34:37 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 188 ms on 5deb0a1bddc0 (executor driver) (102/141)
[2021-05-16 22:34:37,803] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
[2021-05-16 22:34:37,805] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (5deb0a1bddc0, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,806] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 180 ms on 5deb0a1bddc0 (executor driver) (103/141)
[2021-05-16 22:34:37,808] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-16 22:34:37,867] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1843 bytes result sent to driver
[2021-05-16 22:34:37,869] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (5deb0a1bddc0, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,870] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-16 22:34:37,871] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 176 ms on 5deb0a1bddc0 (executor driver) (104/141)
[2021-05-16 22:34:37,873] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1843 bytes result sent to driver
[2021-05-16 22:34:37,874] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (5deb0a1bddc0, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,875] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 171 ms on 5deb0a1bddc0 (executor driver) (105/141)
[2021-05-16 22:34:37,877] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-16 22:34:37,888] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1886 bytes result sent to driver
[2021-05-16 22:34:37,890] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (5deb0a1bddc0, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,891] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 194 ms on 5deb0a1bddc0 (executor driver) (106/141)
[2021-05-16 22:34:37,892] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-16 22:34:37,983] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1886 bytes result sent to driver
[2021-05-16 22:34:37,985] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (5deb0a1bddc0, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:37,986] {docker.py:276} INFO - 21/05/17 01:34:37 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
[2021-05-16 22:34:37,987] {docker.py:276} INFO - 21/05/17 01:34:37 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 183 ms on 5deb0a1bddc0 (executor driver) (107/141)
[2021-05-16 22:34:38,058] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1886 bytes result sent to driver
21/05/17 01:34:38 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1843 bytes result sent to driver
[2021-05-16 22:34:38,060] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (5deb0a1bddc0, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,062] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 187 ms on 5deb0a1bddc0 (executor driver) (108/141)
[2021-05-16 22:34:38,063] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (5deb0a1bddc0, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,065] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1886 bytes result sent to driver
[2021-05-16 22:34:38,066] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-16 22:34:38,067] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 177 ms on 5deb0a1bddc0 (executor driver) (109/141)
21/05/17 01:34:38 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-16 22:34:38,068] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 200 ms on 5deb0a1bddc0 (executor driver) (110/141)
[2021-05-16 22:34:38,070] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (5deb0a1bddc0, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,071] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
[2021-05-16 22:34:38,163] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1843 bytes result sent to driver
[2021-05-16 22:34:38,165] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (5deb0a1bddc0, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,166] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
21/05/17 01:34:38 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 182 ms on 5deb0a1bddc0 (executor driver) (111/141)
[2021-05-16 22:34:38,239] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1843 bytes result sent to driver
[2021-05-16 22:34:38,240] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1843 bytes result sent to driver
[2021-05-16 22:34:38,241] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (5deb0a1bddc0, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,243] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 184 ms on 5deb0a1bddc0 (executor driver) (112/141)
[2021-05-16 22:34:38,244] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
21/05/17 01:34:38 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1843 bytes result sent to driver
[2021-05-16 22:34:38,245] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (5deb0a1bddc0, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,247] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-16 22:34:38,248] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (5deb0a1bddc0, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,249] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 179 ms on 5deb0a1bddc0 (executor driver) (113/141)
[2021-05-16 22:34:38,249] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 186 ms on 5deb0a1bddc0 (executor driver) (114/141)
[2021-05-16 22:34:38,250] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-16 22:34:38,344] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1843 bytes result sent to driver
[2021-05-16 22:34:38,346] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (5deb0a1bddc0, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,347] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 183 ms on 5deb0a1bddc0 (executor driver) (115/141)
[2021-05-16 22:34:38,348] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-16 22:34:38,421] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1843 bytes result sent to driver
[2021-05-16 22:34:38,422] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-16 22:34:38,424] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (5deb0a1bddc0, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,426] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 181 ms on 5deb0a1bddc0 (executor driver) (116/141)
21/05/17 01:34:38 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-16 22:34:38,426] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1843 bytes result sent to driver
[2021-05-16 22:34:38,428] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (5deb0a1bddc0, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,429] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 181 ms on 5deb0a1bddc0 (executor driver) (117/141)
[2021-05-16 22:34:38,430] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 190 ms on 5deb0a1bddc0 (executor driver) (118/141)
[2021-05-16 22:34:38,431] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (5deb0a1bddc0, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 01:34:38 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-16 22:34:38,432] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-16 22:34:38,522] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1843 bytes result sent to driver
[2021-05-16 22:34:38,523] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (5deb0a1bddc0, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,524] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-16 22:34:38,525] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 178 ms on 5deb0a1bddc0 (executor driver) (119/141)
[2021-05-16 22:34:38,598] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1843 bytes result sent to driver
[2021-05-16 22:34:38,608] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (5deb0a1bddc0, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,609] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
21/05/17 01:34:38 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1886 bytes result sent to driver
21/05/17 01:34:38 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 182 ms on 5deb0a1bddc0 (executor driver) (120/141)
[2021-05-16 22:34:38,611] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (5deb0a1bddc0, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,611] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1886 bytes result sent to driver
[2021-05-16 22:34:38,612] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
[2021-05-16 22:34:38,613] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (5deb0a1bddc0, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,614] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-16 22:34:38,614] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 182 ms on 5deb0a1bddc0 (executor driver) (121/141)
[2021-05-16 22:34:38,615] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 190 ms on 5deb0a1bddc0 (executor driver) (122/141)
[2021-05-16 22:34:38,699] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1886 bytes result sent to driver
[2021-05-16 22:34:38,700] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (5deb0a1bddc0, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,702] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-16 22:34:38,703] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 179 ms on 5deb0a1bddc0 (executor driver) (123/141)
[2021-05-16 22:34:38,782] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1843 bytes result sent to driver
[2021-05-16 22:34:38,783] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1843 bytes result sent to driver
[2021-05-16 22:34:38,784] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (5deb0a1bddc0, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,786] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-16 22:34:38,786] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (5deb0a1bddc0, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,787] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
21/05/17 01:34:38 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 180 ms on 5deb0a1bddc0 (executor driver) (124/141)
[2021-05-16 22:34:38,788] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 176 ms on 5deb0a1bddc0 (executor driver) (125/141)
[2021-05-16 22:34:38,789] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1843 bytes result sent to driver
[2021-05-16 22:34:38,790] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (5deb0a1bddc0, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,791] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
21/05/17 01:34:38 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 181 ms on 5deb0a1bddc0 (executor driver) (126/141)
[2021-05-16 22:34:38,873] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1843 bytes result sent to driver
[2021-05-16 22:34:38,875] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (5deb0a1bddc0, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,876] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 177 ms on 5deb0a1bddc0 (executor driver) (127/141)
21/05/17 01:34:38 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-16 22:34:38,958] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1843 bytes result sent to driver
[2021-05-16 22:34:38,960] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1843 bytes result sent to driver
21/05/17 01:34:38 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (5deb0a1bddc0, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,962] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (5deb0a1bddc0, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,963] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 172 ms on 5deb0a1bddc0 (executor driver) (128/141)
[2021-05-16 22:34:38,963] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 177 ms on 5deb0a1bddc0 (executor driver) (129/141)
[2021-05-16 22:34:38,964] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
21/05/17 01:34:38 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-16 22:34:38,965] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1843 bytes result sent to driver
[2021-05-16 22:34:38,966] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (5deb0a1bddc0, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:38,967] {docker.py:276} INFO - 21/05/17 01:34:38 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-16 22:34:38,968] {docker.py:276} INFO - 21/05/17 01:34:38 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 185 ms on 5deb0a1bddc0 (executor driver) (130/141)
[2021-05-16 22:34:39,050] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1843 bytes result sent to driver
[2021-05-16 22:34:39,051] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (5deb0a1bddc0, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,052] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 178 ms on 5deb0a1bddc0 (executor driver) (131/141)
[2021-05-16 22:34:39,053] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
[2021-05-16 22:34:39,135] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1843 bytes result sent to driver
[2021-05-16 22:34:39,136] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1843 bytes result sent to driver
[2021-05-16 22:34:39,137] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1843 bytes result sent to driver
[2021-05-16 22:34:39,138] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (5deb0a1bddc0, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,139] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-16 22:34:39,140] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (5deb0a1bddc0, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,141] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-16 22:34:39,142] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (5deb0a1bddc0, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,143] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 184 ms on 5deb0a1bddc0 (executor driver) (132/141)
[2021-05-16 22:34:39,144] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 183 ms on 5deb0a1bddc0 (executor driver) (133/141)
[2021-05-16 22:34:39,145] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-16 22:34:39,145] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 178 ms on 5deb0a1bddc0 (executor driver) (134/141)
[2021-05-16 22:34:39,225] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1886 bytes result sent to driver
[2021-05-16 22:34:39,226] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (5deb0a1bddc0, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,226] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 176 ms on 5deb0a1bddc0 (executor driver) (135/141)
21/05/17 01:34:39 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-16 22:34:39,321] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1886 bytes result sent to driver
[2021-05-16 22:34:39,323] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (5deb0a1bddc0, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,324] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 187 ms on 5deb0a1bddc0 (executor driver) (136/141)
[2021-05-16 22:34:39,325] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-16 22:34:39,326] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1886 bytes result sent to driver
[2021-05-16 22:34:39,327] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1886 bytes result sent to driver
21/05/17 01:34:39 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (5deb0a1bddc0, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:39,328] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 187 ms on 5deb0a1bddc0 (executor driver) (137/141)
[2021-05-16 22:34:39,329] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-16 22:34:39,331] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 192 ms on 5deb0a1bddc0 (executor driver) (138/141)
[2021-05-16 22:34:39,402] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1843 bytes result sent to driver
[2021-05-16 22:34:39,403] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 177 ms on 5deb0a1bddc0 (executor driver) (139/141)
[2021-05-16 22:34:39,498] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-16 22:34:39,499] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 172 ms on 5deb0a1bddc0 (executor driver) (140/141)
[2021-05-16 22:34:39,505] {docker.py:276} INFO - 21/05/17 01:34:39 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1843 bytes result sent to driver
[2021-05-16 22:34:39,506] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 184 ms on 5deb0a1bddc0 (executor driver) (141/141)
[2021-05-16 22:34:39,507] {docker.py:276} INFO - 21/05/17 01:34:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-16 22:34:39,508] {docker.py:276} INFO - 21/05/17 01:34:39 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.573 s
[2021-05-16 22:34:39,509] {docker.py:276} INFO - 21/05/17 01:34:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 01:34:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-16 22:34:39,509] {docker.py:276} INFO - 21/05/17 01:34:39 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.586006 s
[2021-05-16 22:34:39,523] {docker.py:276} INFO - 21/05/17 01:34:39 INFO InMemoryFileIndex: It took 6652 ms to list leaf files for 141 paths.
[2021-05-16 22:34:41,736] {docker.py:276} INFO - 21/05/17 01:34:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5deb0a1bddc0:42667 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-16 22:34:42,223] {docker.py:276} INFO - 21/05/17 01:34:42 INFO FileSourceStrategy: Pushed Filters:
[2021-05-16 22:34:42,228] {docker.py:276} INFO - 21/05/17 01:34:42 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-16 22:34:42,233] {docker.py:276} INFO - 21/05/17 01:34:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-16 22:34:42,766] {docker.py:276} INFO - 21/05/17 01:34:42 INFO CodeGenerator: Code generated in 287.398 ms
[2021-05-16 22:34:42,779] {docker.py:276} INFO - 21/05/17 01:34:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-16 22:34:42,810] {docker.py:276} INFO - 21/05/17 01:34:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-16 22:34:42,811] {docker.py:276} INFO - 21/05/17 01:34:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5deb0a1bddc0:42667 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 22:34:42,812] {docker.py:276} INFO - 21/05/17 01:34:42 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:42,830] {docker.py:276} INFO - 21/05/17 01:34:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 22:34:42,949] {docker.py:276} INFO - 21/05/17 01:34:42 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:42,952] {docker.py:276} INFO - 21/05/17 01:34:42 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/17 01:34:42 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/17 01:34:42 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 22:34:42,952] {docker.py:276} INFO - 21/05/17 01:34:42 INFO DAGScheduler: Missing parents: List()
[2021-05-16 22:34:42,953] {docker.py:276} INFO - 21/05/17 01:34:42 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 22:34:42,979] {docker.py:276} INFO - 21/05/17 01:34:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-16 22:34:42,991] {docker.py:276} INFO - 21/05/17 01:34:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-16 22:34:42,992] {docker.py:276} INFO - 21/05/17 01:34:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5deb0a1bddc0:42667 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-16 22:34:42,992] {docker.py:276} INFO - 21/05/17 01:34:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-16 22:34:42,993] {docker.py:276} INFO - 21/05/17 01:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/17 01:34:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-16 22:34:42,997] {docker.py:276} INFO - 21/05/17 01:34:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (5deb0a1bddc0, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:42,998] {docker.py:276} INFO - 21/05/17 01:34:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-16 22:34:43,116] {docker.py:276} INFO - 21/05/17 01:34:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621128813_to_1621130613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:43,143] {docker.py:276} INFO - 21/05/17 01:34:43 INFO CodeGenerator: Code generated in 18.8679 ms
[2021-05-16 22:34:43,573] {docker.py:276} INFO - 21/05/17 01:34:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-16 22:34:43,575] {docker.py:276} INFO - 21/05/17 01:34:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 581 ms on 5deb0a1bddc0 (executor driver) (1/1)
[2021-05-16 22:34:43,575] {docker.py:276} INFO - 21/05/17 01:34:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-16 22:34:43,577] {docker.py:276} INFO - 21/05/17 01:34:43 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.620 s
[2021-05-16 22:34:43,577] {docker.py:276} INFO - 21/05/17 01:34:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 01:34:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-16 22:34:43,578] {docker.py:276} INFO - 21/05/17 01:34:43 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.629095 s
[2021-05-16 22:34:43,610] {docker.py:276} INFO - 21/05/17 01:34:43 INFO CodeGenerator: Code generated in 14.4014 ms
[2021-05-16 22:34:43,681] {docker.py:276} INFO - 21/05/17 01:34:43 INFO FileSourceStrategy: Pushed Filters:
[2021-05-16 22:34:43,681] {docker.py:276} INFO - 21/05/17 01:34:43 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-16 22:34:43,681] {docker.py:276} INFO - 21/05/17 01:34:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-16 22:34:43,687] {docker.py:276} INFO - 21/05/17 01:34:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-16 22:34:43,718] {docker.py:276} INFO - 21/05/17 01:34:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 5deb0a1bddc0:42667 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-16 22:34:43,720] {docker.py:276} INFO - 21/05/17 01:34:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-16 22:34:43,721] {docker.py:276} INFO - 21/05/17 01:34:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 5deb0a1bddc0:42667 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 22:34:43,722] {docker.py:276} INFO - 21/05/17 01:34:43 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:43,726] {docker.py:276} INFO - 21/05/17 01:34:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 22:34:44,176] {docker.py:276} INFO - 21/05/17 01:34:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 5deb0a1bddc0:42667 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 22:34:44,381] {docker.py:276} INFO - 21/05/17 01:34:44 INFO FileSourceStrategy: Pushed Filters: 
21/05/17 01:34:44 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-16 22:34:44,381] {docker.py:276} INFO - 21/05/17 01:34:44 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-16 22:34:45,008] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:34:45,008] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:34:45,012] {docker.py:276} INFO - 21/05/17 01:34:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:34:45,012] {docker.py:276} INFO - 21/05/17 01:34:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134448855989266649993838_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134448855989266649993838_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134448855989266649993838_0000}; taskId=attempt_202105170134448855989266649993838_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e831d51}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:34:45,014] {docker.py:276} INFO - 21/05/17 01:34:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:34:45,047] {docker.py:276} INFO - 21/05/17 01:34:45 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-16 22:34:45,140] {docker.py:276} INFO - 21/05/17 01:34:45 INFO CodeGenerator: Code generated in 58.9638 ms
[2021-05-16 22:34:45,143] {docker.py:276} INFO - 21/05/17 01:34:45 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-16 22:34:45,192] {docker.py:276} INFO - 21/05/17 01:34:45 INFO CodeGenerator: Code generated in 40.757 ms
[2021-05-16 22:34:45,196] {docker.py:276} INFO - 21/05/17 01:34:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 934.0 MiB)
[2021-05-16 22:34:45,237] {docker.py:276} INFO - 21/05/17 01:34:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-16 22:34:45,239] {docker.py:276} INFO - 21/05/17 01:34:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 5deb0a1bddc0:42667 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 22:34:45,241] {docker.py:276} INFO - 21/05/17 01:34:45 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:45,251] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 22:34:45,532] {docker.py:276} INFO - 21/05/17 01:34:45 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 22:34:45,537] {docker.py:276} INFO - 21/05/17 01:34:45 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-16 22:34:45,543] {docker.py:276} INFO - 21/05/17 01:34:45 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/17 01:34:45 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 22:34:45,543] {docker.py:276} INFO - 21/05/17 01:34:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-16 22:34:45,546] {docker.py:276} INFO - 21/05/17 01:34:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-16 22:34:45,548] {docker.py:276} INFO - 21/05/17 01:34:45 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 22:34:45,573] {docker.py:276} INFO - 21/05/17 01:34:45 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-16 22:34:45,612] {docker.py:276} INFO - 21/05/17 01:34:45 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-16 22:34:45,614] {docker.py:276} INFO - 21/05/17 01:34:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 5deb0a1bddc0:42667 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-16 22:34:45,615] {docker.py:276} INFO - 21/05/17 01:34:45 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-16 22:34:45,619] {docker.py:276} INFO - 21/05/17 01:34:45 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21/05/17 01:34:45 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-16 22:34:45,623] {docker.py:276} INFO - 21/05/17 01:34:45 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (5deb0a1bddc0, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:45,624] {docker.py:276} INFO - 21/05/17 01:34:45 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (5deb0a1bddc0, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:45,626] {docker.py:276} INFO - 21/05/17 01:34:45 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (5deb0a1bddc0, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:45,627] {docker.py:276} INFO - 21/05/17 01:34:45 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (5deb0a1bddc0, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:45,628] {docker.py:276} INFO - 21/05/17 01:34:45 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
[2021-05-16 22:34:45,634] {docker.py:276} INFO - 21/05/17 01:34:45 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-16 22:34:45,634] {docker.py:276} INFO - 21/05/17 01:34:45 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-16 22:34:45,635] {docker.py:276} INFO - 21/05/17 01:34:45 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-16 22:34:45,792] {docker.py:276} INFO - 21/05/17 01:34:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 5deb0a1bddc0:42667 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 22:34:45,811] {docker.py:276} INFO - 21/05/17 01:34:45 INFO CodeGenerator: Code generated in 68.203 ms
[2021-05-16 22:34:45,871] {docker.py:276} INFO - 21/05/17 01:34:45 INFO CodeGenerator: Code generated in 18.992 ms
[2021-05-16 22:34:45,920] {docker.py:276} INFO - 21/05/17 01:34:45 INFO CodeGenerator: Code generated in 34.5134 ms
[2021-05-16 22:34:45,952] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621128813_to_1621130613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:45,959] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621166613_to_1621168413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:45,960] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621186413_to_1621188213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:45,967] {docker.py:276} INFO - 21/05/17 01:34:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621155813_to_1621157613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:46,770] {docker.py:276} INFO - 21/05/17 01:34:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621130613_to_1621132413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:47,206] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621132413_to_1621134213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:47,267] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621168413_to_1621170213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:47,269] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621188213_to_1621190013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:47,291] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621157613_to_1621159413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:47,620] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621170213_to_1621172013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:47,625] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621190013_to_1621191813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:47,693] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621134213_to_1621136013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:47,793] {docker.py:276} INFO - 21/05/17 01:34:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621159413_to_1621161213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:48,016] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621191813_to_1621193613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:48,036] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621136013_to_1621137813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:48,098] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621172013_to_1621173813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:48,187] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621161213_to_1621163013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:48,372] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621193613_to_1621195413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:48,388] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621137813_to_1621139613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:48,453] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621173813_to_1621175613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:48,572] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621163013_to_1621164813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:48,735] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621139613_to_1621141413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:48,744] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621195413_to_1621197213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:48,800] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621175613_to_1621177413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:48,925] {docker.py:276} INFO - 21/05/17 01:34:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621164813_to_1621166613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:49,079] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621141413_to_1621143213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:49,156] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621177413_to_1621179213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:49,256] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621197213_to_1621199013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:49,294] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621166613_to_1621168413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:49,476] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621143213_to_1621145013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:49,496] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621179213_to_1621181013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:49,603] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621199013_to_1621200813.csv, range: 0-111709, partition values: [empty row]
[2021-05-16 22:34:49,646] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621168413_to_1621170213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:49,832] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621145013_to_1621146813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:49,834] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621181013_to_1621182813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:49,957] {docker.py:276} INFO - 21/05/17 01:34:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621202613_to_1621204413.csv, range: 0-111622, partition values: [empty row]
[2021-05-16 22:34:50,006] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621170213_to_1621172013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:50,185] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621146813_to_1621148613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:50,186] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621182813_to_1621184613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:50,300] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621200813_to_1621202613.csv, range: 0-111459, partition values: [empty row]
[2021-05-16 22:34:50,372] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621172013_to_1621173813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:50,543] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621184613_to_1621186413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:50,553] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621148613_to_1621150413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:50,651] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621204413_to_1621206213.csv, range: 0-111155, partition values: [empty row]
[2021-05-16 22:34:50,724] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621173813_to_1621175613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:50,888] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621186413_to_1621188213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:50,903] {docker.py:276} INFO - 21/05/17 01:34:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621150413_to_1621152213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:51,015] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621128813_to_1621130613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,071] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621175613_to_1621177413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,246] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621188213_to_1621190013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,249] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621152213_to_1621154013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:51,372] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621130613_to_1621132413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,598] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621190013_to_1621191813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,616] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621154013_to_1621155813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:51,721] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621132413_to_1621134213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,949] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621191813_to_1621193613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:51,955] {docker.py:276} INFO - 21/05/17 01:34:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621155813_to_1621157613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:52,065] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621134213_to_1621136013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,290] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621157613_to_1621159413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:52,362] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621193613_to_1621195413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,404] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621177413_to_1621179213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,405] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621136013_to_1621137813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,624] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621159413_to_1621161213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:52,744] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621195413_to_1621197213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,748] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621179213_to_1621181013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,893] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621137813_to_1621139613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:52,958] {docker.py:276} INFO - 21/05/17 01:34:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621161213_to_1621163013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:53,090] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621128813_to_1621130613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,101] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621181013_to_1621182813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,243] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621139613_to_1621141413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,299] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621163013_to_1621164813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:53,457] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621130613_to_1621132413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,474] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621182813_to_1621184613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,593] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621141413_to_1621143213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,650] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621164813_to_1621166613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:53,801] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621132413_to_1621134213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,842] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621184613_to_1621186413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,945] {docker.py:276} INFO - 21/05/17 01:34:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621143213_to_1621145013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:53,996] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621166613_to_1621168413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:54,138] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621134213_to_1621136013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,244] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621186413_to_1621188213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,303] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621145013_to_1621146813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,398] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621168413_to_1621170213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:54,504] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621136013_to_1621137813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,597] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621188213_to_1621190013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,636] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621146813_to_1621148613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,732] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621170213_to_1621172013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:54,847] {docker.py:276} INFO - 21/05/17 01:34:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621137813_to_1621139613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,979] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621190013_to_1621191813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:54,989] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621148613_to_1621150413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,071] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621172013_to_1621173813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:55,214] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621139613_to_1621141413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,330] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621150413_to_1621152213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,332] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621191813_to_1621193613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,401] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621173813_to_1621175613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:55,574] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621141413_to_1621143213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,677] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621152213_to_1621154013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,703] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621193613_to_1621195413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:55,742] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621175613_to_1621177413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:55,943] {docker.py:276} INFO - 21/05/17 01:34:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621143213_to_1621145013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,017] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621154013_to_1621155813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,095] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621177413_to_1621179213.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:56,120] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621195413_to_1621197213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,288] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621145013_to_1621146813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,361] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621155813_to_1621157613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,433] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621179213_to_1621181013.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:56,490] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621197213_to_1621199013.csv, range: 0-104483, partition values: [empty row]
[2021-05-16 22:34:56,631] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621146813_to_1621148613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,717] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621157613_to_1621159413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:56,767] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621181013_to_1621182813.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:56,896] {docker.py:276} INFO - 21/05/17 01:34:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621197213_to_1621199013.csv, range: 0-104457, partition values: [empty row]
[2021-05-16 22:34:56,978] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621148613_to_1621150413.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:57,064] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621159413_to_1621161213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:57,121] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621182813_to_1621184613.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:57,241] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621200813_to_1621202613.csv, range: 0-104301, partition values: [empty row]
[2021-05-16 22:34:57,317] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621150413_to_1621152213.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:57,399] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621161213_to_1621163013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:57,459] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621184613_to_1621186413.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 22:34:57,592] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621204413_to_1621206213.csv, range: 0-104273, partition values: [empty row]
[2021-05-16 22:34:57,686] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621152213_to_1621154013.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:57,738] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621163013_to_1621164813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:57,955] {docker.py:276} INFO - 21/05/17 01:34:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621211613_to_1621213413.csv, range: 0-104272, partition values: [empty row]
[2021-05-16 22:34:58,023] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621154013_to_1621155813.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:58,096] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621164813_to_1621166613.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 22:34:58,176] {docker.py:276} INFO - 21/05/17 01:34:58 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2722 bytes result sent to driver
[2021-05-16 22:34:58,177] {docker.py:276} INFO - 21/05/17 01:34:58 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (5deb0a1bddc0, executor driver, partition 4, PROCESS_LOCAL, 6214 bytes) taskResourceAssignments Map()
[2021-05-16 22:34:58,178] {docker.py:276} INFO - 21/05/17 01:34:58 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-16 22:34:58,179] {docker.py:276} INFO - 21/05/17 01:34:58 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 12573 ms on 5deb0a1bddc0 (executor driver) (1/5)
[2021-05-16 22:34:58,190] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621206213_to_1621208013.csv, range: 0-104132, partition values: [empty row]
[2021-05-16 22:34:58,305] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621209813_to_1621211613.csv, range: 0-104220, partition values: [empty row]
[2021-05-16 22:34:58,594] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621204413_to_1621206213.csv, range: 0-104012, partition values: [empty row]
[2021-05-16 22:34:58,672] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621209813_to_1621211613.csv, range: 0-104211, partition values: [empty row]
[2021-05-16 22:34:58,752] {docker.py:276} INFO - 21/05/17 01:34:58 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-16 22:34:58,754] {docker.py:276} INFO - 21/05/17 01:34:58 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 13110 ms on 5deb0a1bddc0 (executor driver) (2/5)
[2021-05-16 22:34:58,772] {docker.py:276} INFO - 21/05/17 01:34:58 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2679 bytes result sent to driver
[2021-05-16 22:34:58,773] {docker.py:276} INFO - 21/05/17 01:34:58 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 13130 ms on 5deb0a1bddc0 (executor driver) (3/5)
[2021-05-16 22:34:58,972] {docker.py:276} INFO - 21/05/17 01:34:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621206213_to_1621208013.csv, range: 0-104008, partition values: [empty row]
[2021-05-16 22:34:59,052] {docker.py:276} INFO - 21/05/17 01:34:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621202613_to_1621204413.csv, range: 0-104194, partition values: [empty row]
[2021-05-16 22:34:59,318] {docker.py:276} INFO - 21/05/17 01:34:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621200813_to_1621202613.csv, range: 0-104004, partition values: [empty row]
[2021-05-16 22:34:59,417] {docker.py:276} INFO - 21/05/17 01:34:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621211613_to_1621213413.csv, range: 0-104186, partition values: [empty row]
[2021-05-16 22:34:59,662] {docker.py:276} INFO - 21/05/17 01:34:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621209813_to_1621211613.csv, range: 0-103980, partition values: [empty row]
[2021-05-16 22:34:59,960] {docker.py:276} INFO - 21/05/17 01:34:59 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2679 bytes result sent to driver
[2021-05-16 22:34:59,961] {docker.py:276} INFO - 21/05/17 01:34:59 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 14316 ms on 5deb0a1bddc0 (executor driver) (4/5)
[2021-05-16 22:35:00,127] {docker.py:276} INFO - 21/05/17 01:35:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621208013_to_1621209813.csv, range: 0-103966, partition values: [empty row]
[2021-05-16 22:35:00,470] {docker.py:276} INFO - 21/05/17 01:35:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621199013_to_1621200813.csv, range: 0-103953, partition values: [empty row]
[2021-05-16 22:35:00,841] {docker.py:276} INFO - 21/05/17 01:35:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621208013_to_1621209813.csv, range: 0-103938, partition values: [empty row]
[2021-05-16 22:35:01,180] {docker.py:276} INFO - 21/05/17 01:35:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_22_33_33/from_1621206213_to_1621208013.csv, range: 0-103923, partition values: [empty row]
[2021-05-16 22:35:01,557] {docker.py:276} INFO - 21/05/17 01:35:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621208013_to_1621209813.csv, range: 0-103893, partition values: [empty row]
[2021-05-16 22:35:01,906] {docker.py:276} INFO - 21/05/17 01:35:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621202613_to_1621204413.csv, range: 0-103877, partition values: [empty row]
[2021-05-16 22:35:02,254] {docker.py:276} INFO - 21/05/17 01:35:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_22_33_33/from_1621211613_to_1621213413.csv, range: 0-103871, partition values: [empty row]
[2021-05-16 22:35:02,604] {docker.py:276} INFO - 21/05/17 01:35:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_22_33_33/from_1621199013_to_1621200813.csv, range: 0-103564, partition values: [empty row]
[2021-05-16 22:35:03,096] {docker.py:276} INFO - 21/05/17 01:35:03 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-16 22:35:03,097] {docker.py:276} INFO - 21/05/17 01:35:03 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 4891 ms on 5deb0a1bddc0 (executor driver) (5/5)
21/05/17 01:35:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-16 22:35:03,098] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 17.524 s
[2021-05-16 22:35:03,099] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: looking for newly runnable stages
[2021-05-16 22:35:03,100] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: running: Set()
[2021-05-16 22:35:03,101] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-16 22:35:03,102] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: failed: Set()
[2021-05-16 22:35:03,107] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 22:35:03,155] {docker.py:276} INFO - 21/05/17 01:35:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-16 22:35:03,162] {docker.py:276} INFO - 21/05/17 01:35:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-16 22:35:03,164] {docker.py:276} INFO - 21/05/17 01:35:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 5deb0a1bddc0:42667 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-16 22:35:03,165] {docker.py:276} INFO - 21/05/17 01:35:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-16 22:35:03,167] {docker.py:276} INFO - 21/05/17 01:35:03 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-16 22:35:03,168] {docker.py:276} INFO - 21/05/17 01:35:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-16 22:35:03,178] {docker.py:276} INFO - 21/05/17 01:35:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (5deb0a1bddc0, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:03,179] {docker.py:276} INFO - 21/05/17 01:35:03 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (5deb0a1bddc0, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:03,179] {docker.py:276} INFO - 21/05/17 01:35:03 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (5deb0a1bddc0, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:03,180] {docker.py:276} INFO - 21/05/17 01:35:03 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (5deb0a1bddc0, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:03,181] {docker.py:276} INFO - 21/05/17 01:35:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
21/05/17 01:35:03 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-16 22:35:03,182] {docker.py:276} INFO - 21/05/17 01:35:03 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-16 22:35:03,183] {docker.py:276} INFO - 21/05/17 01:35:03 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
[2021-05-16 22:35:03,290] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Getting 5 (20.8 KiB) non-empty blocks including 5 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:03,294] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2021-05-16 22:35:03,301] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:03,302] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
[2021-05-16 22:35:03,303] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:03,303] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
[2021-05-16 22:35:03,304] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:03,304] {docker.py:276} INFO - 21/05/17 01:35:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
[2021-05-16 22:35:03,344] {docker.py:276} INFO - 21/05/17 01:35:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:35:03,345] {docker.py:276} INFO - 21/05/17 01:35:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:35:03,345] {docker.py:276} INFO - 21/05/17 01:35:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:03,346] {docker.py:276} INFO - 21/05/17 01:35:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:03,347] {docker.py:276} INFO - 21/05/17 01:35:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:03,348] {docker.py:276} INFO - 21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:03,349] {docker.py:276} INFO - 21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457299941239516734916_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457299941239516734916_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457299941239516734916_0004}; taskId=attempt_202105170134457299941239516734916_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d159d25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445748428614459210238_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445748428614459210238_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445748428614459210238_0004}; taskId=attempt_20210517013445748428614459210238_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6116f3bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:03,353] {docker.py:276} INFO - 21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452122438486253245504_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452122438486253245504_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452122438486253245504_0004}; taskId=attempt_202105170134452122438486253245504_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14edcefd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:03,355] {docker.py:276} INFO - 21/05/17 01:35:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:03,356] {docker.py:276} INFO - 21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:03,357] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134457299941239516734916_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457299941239516734916_0004_m_000002_290
[2021-05-16 22:35:03,357] {docker.py:276} INFO - 21/05/17 01:35:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:03,358] {docker.py:276} INFO - 21/05/17 01:35:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458627741884934472517_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458627741884934472517_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458627741884934472517_0004}; taskId=attempt_202105170134458627741884934472517_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5777f82f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:03,359] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134452122438486253245504_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452122438486253245504_0004_m_000003_291
[2021-05-16 22:35:03,360] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Starting: Task committer attempt_20210517013445748428614459210238_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445748428614459210238_0004_m_000001_289
[2021-05-16 22:35:03,361] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134458627741884934472517_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458627741884934472517_0004_m_000000_288
[2021-05-16 22:35:03,397] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Task committer attempt_202105170134457299941239516734916_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457299941239516734916_0004_m_000002_290 : duration 0:00.042s
[2021-05-16 22:35:03,426] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Task committer attempt_20210517013445748428614459210238_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445748428614459210238_0004_m_000001_289 : duration 0:00.070s
[2021-05-16 22:35:03,444] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Task committer attempt_202105170134458627741884934472517_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458627741884934472517_0004_m_000000_288 : duration 0:00.088s
[2021-05-16 22:35:03,448] {docker.py:276} INFO - 21/05/17 01:35:03 INFO StagingCommitter: Task committer attempt_202105170134452122438486253245504_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452122438486253245504_0004_m_000003_291 : duration 0:00.092s
[2021-05-16 22:35:05,437] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134452122438486253245504_0004_m_000003_291: needsTaskCommit() Task attempt_202105170134452122438486253245504_0004_m_000003_291
[2021-05-16 22:35:05,438] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134458627741884934472517_0004_m_000000_288: needsTaskCommit() Task attempt_202105170134458627741884934472517_0004_m_000000_288
[2021-05-16 22:35:05,438] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134452122438486253245504_0004_m_000003_291: needsTaskCommit() Task attempt_202105170134452122438486253245504_0004_m_000003_291: duration 0:00.001s
21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134458627741884934472517_0004_m_000000_288: needsTaskCommit() Task attempt_202105170134458627741884934472517_0004_m_000000_288: duration 0:00.001s
[2021-05-16 22:35:05,440] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452122438486253245504_0004_m_000003_291
[2021-05-16 22:35:05,441] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458627741884934472517_0004_m_000000_288
[2021-05-16 22:35:05,447] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 4587 bytes result sent to driver
21/05/17 01:35:05 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
[2021-05-16 22:35:05,448] {docker.py:276} INFO - 21/05/17 01:35:05 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (5deb0a1bddc0, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:05,449] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-16 22:35:05,450] {docker.py:276} INFO - 21/05/17 01:35:05 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (5deb0a1bddc0, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:05,451] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-16 22:35:05,452] {docker.py:276} INFO - 21/05/17 01:35:05 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 2275 ms on 5deb0a1bddc0 (executor driver) (1/200)
[2021-05-16 22:35:05,476] {docker.py:276} INFO - 21/05/17 01:35:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 2303 ms on 5deb0a1bddc0 (executor driver) (2/200)
[2021-05-16 22:35:05,480] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:05,481] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_20210517013445748428614459210238_0004_m_000001_289: needsTaskCommit() Task attempt_20210517013445748428614459210238_0004_m_000001_289
[2021-05-16 22:35:05,482] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_20210517013445748428614459210238_0004_m_000001_289: needsTaskCommit() Task attempt_20210517013445748428614459210238_0004_m_000001_289: duration 0:00.002s
[2021-05-16 22:35:05,483] {docker.py:276} INFO - 21/05/17 01:35:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:05,483] {docker.py:276} INFO - 21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452268005940168285777_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452268005940168285777_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452268005940168285777_0004}; taskId=attempt_202105170134452268005940168285777_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bfcc945}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:05,484] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134452268005940168285777_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452268005940168285777_0004_m_000005_293
[2021-05-16 22:35:05,485] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445748428614459210238_0004_m_000001_289
[2021-05-16 22:35:05,485] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134457299941239516734916_0004_m_000002_290: needsTaskCommit() Task attempt_202105170134457299941239516734916_0004_m_000002_290
[2021-05-16 22:35:05,486] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134457299941239516734916_0004_m_000002_290: needsTaskCommit() Task attempt_202105170134457299941239516734916_0004_m_000002_290: duration 0:00.003s
[2021-05-16 22:35:05,486] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457299941239516734916_0004_m_000002_290
[2021-05-16 22:35:05,487] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-16 22:35:05,489] {docker.py:276} INFO - 21/05/17 01:35:05 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (5deb0a1bddc0, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:05,489] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
21/05/17 01:35:05 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 2313 ms on 5deb0a1bddc0 (executor driver) (3/200)
[2021-05-16 22:35:05,491] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 4587 bytes result sent to driver
[2021-05-16 22:35:05,494] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:05,495] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:05,495] {docker.py:276} INFO - 21/05/17 01:35:05 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (5deb0a1bddc0, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:05,496] {docker.py:276} INFO - 21/05/17 01:35:05 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
21/05/17 01:35:05 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 2321 ms on 5deb0a1bddc0 (executor driver) (4/200)
[2021-05-16 22:35:05,501] {docker.py:276} INFO - 21/05/17 01:35:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:05,502] {docker.py:276} INFO - 21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457120921730748282339_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457120921730748282339_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457120921730748282339_0004}; taskId=attempt_202105170134457120921730748282339_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49c1268c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:05,503] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:05,505] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134457120921730748282339_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457120921730748282339_0004_m_000004_292
[2021-05-16 22:35:05,510] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134452268005940168285777_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452268005940168285777_0004_m_000005_293 : duration 0:00.025s
[2021-05-16 22:35:05,513] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134457120921730748282339_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457120921730748282339_0004_m_000004_292 : duration 0:00.011s
[2021-05-16 22:35:05,514] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:05,514] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-16 22:35:05,519] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:05,520] {docker.py:276} INFO - 21/05/17 01:35:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:05,522] {docker.py:276} INFO - 21/05/17 01:35:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:05,522] {docker.py:276} INFO - 21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:05,523] {docker.py:276} INFO - 21/05/17 01:35:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452176379567196185787_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452176379567196185787_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452176379567196185787_0004}; taskId=attempt_202105170134452176379567196185787_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61211221}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:05,523] {docker.py:276} INFO - 21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:05,525] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:05,525] {docker.py:276} INFO - 21/05/17 01:35:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458510619396621642552_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458510619396621642552_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458510619396621642552_0004}; taskId=attempt_202105170134458510619396621642552_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71206ce7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:05,525] {docker.py:276} INFO - 21/05/17 01:35:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134458510619396621642552_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458510619396621642552_0004_m_000006_294
[2021-05-16 22:35:05,526] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134452176379567196185787_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452176379567196185787_0004_m_000007_295
[2021-05-16 22:35:05,541] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134452176379567196185787_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452176379567196185787_0004_m_000007_295 : duration 0:00.015s
[2021-05-16 22:35:05,545] {docker.py:276} INFO - 21/05/17 01:35:05 INFO StagingCommitter: Task committer attempt_202105170134458510619396621642552_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458510619396621642552_0004_m_000006_294 : duration 0:00.020s
[2021-05-16 22:35:08,185] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134457120921730748282339_0004_m_000004_292: needsTaskCommit() Task attempt_202105170134457120921730748282339_0004_m_000004_292
[2021-05-16 22:35:08,189] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134457120921730748282339_0004_m_000004_292: needsTaskCommit() Task attempt_202105170134457120921730748282339_0004_m_000004_292: duration 0:00.005s
21/05/17 01:35:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457120921730748282339_0004_m_000004_292
[2021-05-16 22:35:08,191] {docker.py:276} INFO - 21/05/17 01:35:08 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 4587 bytes result sent to driver
[2021-05-16 22:35:08,192] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (5deb0a1bddc0, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:08,194] {docker.py:276} INFO - 21/05/17 01:35:08 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-16 22:35:08,194] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 2749 ms on 5deb0a1bddc0 (executor driver) (5/200)
[2021-05-16 22:35:08,204] {docker.py:276} INFO - 21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:08,207] {docker.py:276} INFO - 21/05/17 01:35:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:08,207] {docker.py:276} INFO - 21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445480132409135468888_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445480132409135468888_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445480132409135468888_0004}; taskId=attempt_20210517013445480132409135468888_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@259436fb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:08,207] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_20210517013445480132409135468888_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445480132409135468888_0004_m_000008_296
[2021-05-16 22:35:08,210] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134452268005940168285777_0004_m_000005_293: needsTaskCommit() Task attempt_202105170134452268005940168285777_0004_m_000005_293
[2021-05-16 22:35:08,210] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134452268005940168285777_0004_m_000005_293: needsTaskCommit() Task attempt_202105170134452268005940168285777_0004_m_000005_293: duration 0:00.001s
[2021-05-16 22:35:08,211] {docker.py:276} INFO - 21/05/17 01:35:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452268005940168285777_0004_m_000005_293
[2021-05-16 22:35:08,212] {docker.py:276} INFO - 21/05/17 01:35:08 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 4587 bytes result sent to driver
[2021-05-16 22:35:08,213] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (5deb0a1bddc0, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:08,213] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 2767 ms on 5deb0a1bddc0 (executor driver) (6/200)
21/05/17 01:35:08 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-16 22:35:08,224] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_20210517013445480132409135468888_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445480132409135468888_0004_m_000008_296 : duration 0:00.017s
[2021-05-16 22:35:08,227] {docker.py:276} INFO - 21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:08,228] {docker.py:276} INFO - 21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:08,230] {docker.py:276} INFO - 21/05/17 01:35:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:08,231] {docker.py:276} INFO - 21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:08,231] {docker.py:276} INFO - 21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456053566390490539979_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456053566390490539979_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456053566390490539979_0004}; taskId=attempt_202105170134456053566390490539979_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@390e1843}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:08,231] {docker.py:276} INFO - 21/05/17 01:35:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134456053566390490539979_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456053566390490539979_0004_m_000009_297
[2021-05-16 22:35:08,236] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134456053566390490539979_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456053566390490539979_0004_m_000009_297 : duration 0:00.005s
[2021-05-16 22:35:08,365] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134452176379567196185787_0004_m_000007_295: needsTaskCommit() Task attempt_202105170134452176379567196185787_0004_m_000007_295
[2021-05-16 22:35:08,365] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134452176379567196185787_0004_m_000007_295: needsTaskCommit() Task attempt_202105170134452176379567196185787_0004_m_000007_295: duration 0:00.001s
[2021-05-16 22:35:08,366] {docker.py:276} INFO - 21/05/17 01:35:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452176379567196185787_0004_m_000007_295
[2021-05-16 22:35:08,366] {docker.py:276} INFO - 21/05/17 01:35:08 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-16 22:35:08,367] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (5deb0a1bddc0, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:08,368] {docker.py:276} INFO - 21/05/17 01:35:08 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
[2021-05-16 22:35:08,369] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 2878 ms on 5deb0a1bddc0 (executor driver) (7/200)
[2021-05-16 22:35:08,379] {docker.py:276} INFO - 21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:08,385] {docker.py:276} INFO - 21/05/17 01:35:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:08,385] {docker.py:276} INFO - 21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453056476173991640343_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453056476173991640343_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453056476173991640343_0004}; taskId=attempt_202105170134453056476173991640343_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52ecf1eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134453056476173991640343_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453056476173991640343_0004_m_000010_298
[2021-05-16 22:35:08,390] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134458510619396621642552_0004_m_000006_294: needsTaskCommit() Task attempt_202105170134458510619396621642552_0004_m_000006_294
[2021-05-16 22:35:08,391] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134458510619396621642552_0004_m_000006_294: needsTaskCommit() Task attempt_202105170134458510619396621642552_0004_m_000006_294: duration 0:00.001s
21/05/17 01:35:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458510619396621642552_0004_m_000006_294
[2021-05-16 22:35:08,392] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134453056476173991640343_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453056476173991640343_0004_m_000010_298 : duration 0:00.004s
21/05/17 01:35:08 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 4544 bytes result sent to driver
[2021-05-16 22:35:08,393] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (5deb0a1bddc0, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:08,394] {docker.py:276} INFO - 21/05/17 01:35:08 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 2909 ms on 5deb0a1bddc0 (executor driver) (8/200)
[2021-05-16 22:35:08,395] {docker.py:276} INFO - 21/05/17 01:35:08 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-16 22:35:08,403] {docker.py:276} INFO - 21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Getting 5 (21.4 KiB) non-empty blocks including 5 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:08,405] {docker.py:276} INFO - 21/05/17 01:35:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:08,405] {docker.py:276} INFO - 21/05/17 01:35:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451408568470221370023_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451408568470221370023_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451408568470221370023_0004}; taskId=attempt_202105170134451408568470221370023_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@256f52aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:08 INFO StagingCommitter: Starting: Task committer attempt_202105170134451408568470221370023_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451408568470221370023_0004_m_000011_299
[2021-05-16 22:35:08,409] {docker.py:276} INFO - 21/05/17 01:35:08 INFO StagingCommitter: Task committer attempt_202105170134451408568470221370023_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451408568470221370023_0004_m_000011_299 : duration 0:00.004s
[2021-05-16 22:35:10,866] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Starting: Task committer attempt_20210517013445480132409135468888_0004_m_000008_296: needsTaskCommit() Task attempt_20210517013445480132409135468888_0004_m_000008_296
[2021-05-16 22:35:10,867] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Task committer attempt_20210517013445480132409135468888_0004_m_000008_296: needsTaskCommit() Task attempt_20210517013445480132409135468888_0004_m_000008_296: duration 0:00.002s
21/05/17 01:35:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445480132409135468888_0004_m_000008_296
[2021-05-16 22:35:10,869] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134456053566390490539979_0004_m_000009_297: needsTaskCommit() Task attempt_202105170134456053566390490539979_0004_m_000009_297
[2021-05-16 22:35:10,870] {docker.py:276} INFO - 21/05/17 01:35:10 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 4587 bytes result sent to driver
[2021-05-16 22:35:10,870] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Task committer attempt_202105170134456053566390490539979_0004_m_000009_297: needsTaskCommit() Task attempt_202105170134456053566390490539979_0004_m_000009_297: duration 0:00.000s
21/05/17 01:35:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456053566390490539979_0004_m_000009_297
[2021-05-16 22:35:10,871] {docker.py:276} INFO - 21/05/17 01:35:10 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 4587 bytes result sent to driver
[2021-05-16 22:35:10,871] {docker.py:276} INFO - 21/05/17 01:35:10 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (5deb0a1bddc0, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:10,872] {docker.py:276} INFO - 21/05/17 01:35:10 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
21/05/17 01:35:10 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (5deb0a1bddc0, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:10,873] {docker.py:276} INFO - 21/05/17 01:35:10 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
[2021-05-16 22:35:10,874] {docker.py:276} INFO - 21/05/17 01:35:10 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 2664 ms on 5deb0a1bddc0 (executor driver) (9/200)
[2021-05-16 22:35:10,874] {docker.py:276} INFO - 21/05/17 01:35:10 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 2686 ms on 5deb0a1bddc0 (executor driver) (10/200)
[2021-05-16 22:35:10,883] {docker.py:276} INFO - 21/05/17 01:35:10 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:10,884] {docker.py:276} INFO - 21/05/17 01:35:10 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:10,886] {docker.py:276} INFO - 21/05/17 01:35:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:10,887] {docker.py:276} INFO - 21/05/17 01:35:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:35:10,887] {docker.py:276} INFO - 21/05/17 01:35:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452337991652012818229_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452337991652012818229_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452337991652012818229_0004}; taskId=attempt_202105170134452337991652012818229_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c3e4b14}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:10,888] {docker.py:276} INFO - 21/05/17 01:35:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457019628156991514765_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457019628156991514765_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457019628156991514765_0004}; taskId=attempt_202105170134457019628156991514765_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4983d045}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:10,888] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134457019628156991514765_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457019628156991514765_0004_m_000012_300
[2021-05-16 22:35:10,889] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134452337991652012818229_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452337991652012818229_0004_m_000013_301
[2021-05-16 22:35:10,895] {docker.py:276} INFO - 21/05/17 01:35:10 INFO StagingCommitter: Task committer attempt_202105170134457019628156991514765_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457019628156991514765_0004_m_000012_300 : duration 0:00.007s
21/05/17 01:35:10 INFO StagingCommitter: Task committer attempt_202105170134452337991652012818229_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452337991652012818229_0004_m_000013_301 : duration 0:00.008s
[2021-05-16 22:35:11,086] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Starting: Task committer attempt_202105170134453056476173991640343_0004_m_000010_298: needsTaskCommit() Task attempt_202105170134453056476173991640343_0004_m_000010_298
[2021-05-16 22:35:11,087] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Task committer attempt_202105170134453056476173991640343_0004_m_000010_298: needsTaskCommit() Task attempt_202105170134453056476173991640343_0004_m_000010_298: duration 0:00.000s
[2021-05-16 22:35:11,087] {docker.py:276} INFO - 21/05/17 01:35:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453056476173991640343_0004_m_000010_298
[2021-05-16 22:35:11,089] {docker.py:276} INFO - 21/05/17 01:35:11 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 4587 bytes result sent to driver
[2021-05-16 22:35:11,090] {docker.py:276} INFO - 21/05/17 01:35:11 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (5deb0a1bddc0, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:11,091] {docker.py:276} INFO - 21/05/17 01:35:11 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
21/05/17 01:35:11 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 2727 ms on 5deb0a1bddc0 (executor driver) (11/200)
[2021-05-16 22:35:11,101] {docker.py:276} INFO - 21/05/17 01:35:11 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:11,105] {docker.py:276} INFO - 21/05/17 01:35:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:11,105] {docker.py:276} INFO - 21/05/17 01:35:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051701344519363063924907413_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_2021051701344519363063924907413_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051701344519363063924907413_0004}; taskId=attempt_2021051701344519363063924907413_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34ccef29}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:11,105] {docker.py:276} INFO - 21/05/17 01:35:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:11,106] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Starting: Task committer attempt_2021051701344519363063924907413_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_2021051701344519363063924907413_0004_m_000014_302
[2021-05-16 22:35:11,109] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Task committer attempt_2021051701344519363063924907413_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_2021051701344519363063924907413_0004_m_000014_302 : duration 0:00.005s
[2021-05-16 22:35:11,244] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Starting: Task committer attempt_202105170134451408568470221370023_0004_m_000011_299: needsTaskCommit() Task attempt_202105170134451408568470221370023_0004_m_000011_299
[2021-05-16 22:35:11,245] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Task committer attempt_202105170134451408568470221370023_0004_m_000011_299: needsTaskCommit() Task attempt_202105170134451408568470221370023_0004_m_000011_299: duration 0:00.002s
[2021-05-16 22:35:11,245] {docker.py:276} INFO - 21/05/17 01:35:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451408568470221370023_0004_m_000011_299
[2021-05-16 22:35:11,248] {docker.py:276} INFO - 21/05/17 01:35:11 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 4587 bytes result sent to driver
[2021-05-16 22:35:11,249] {docker.py:276} INFO - 21/05/17 01:35:11 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (5deb0a1bddc0, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:11,250] {docker.py:276} INFO - 21/05/17 01:35:11 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 2860 ms on 5deb0a1bddc0 (executor driver) (12/200)
[2021-05-16 22:35:11,251] {docker.py:276} INFO - 21/05/17 01:35:11 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-16 22:35:11,263] {docker.py:276} INFO - 21/05/17 01:35:11 INFO ShuffleBlockFetcherIterator: Getting 5 (21.3 KiB) non-empty blocks including 5 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:11,265] {docker.py:276} INFO - 21/05/17 01:35:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:11,265] {docker.py:276} INFO - 21/05/17 01:35:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453353115555538672053_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453353115555538672053_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453353115555538672053_0004}; taskId=attempt_202105170134453353115555538672053_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36e99bb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:11 INFO StagingCommitter: Starting: Task committer attempt_202105170134453353115555538672053_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453353115555538672053_0004_m_000015_303
[2021-05-16 22:35:11,269] {docker.py:276} INFO - 21/05/17 01:35:11 INFO StagingCommitter: Task committer attempt_202105170134453353115555538672053_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453353115555538672053_0004_m_000015_303 : duration 0:00.004s
[2021-05-16 22:35:13,516] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134452337991652012818229_0004_m_000013_301: needsTaskCommit() Task attempt_202105170134452337991652012818229_0004_m_000013_301
[2021-05-16 22:35:13,517] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_202105170134452337991652012818229_0004_m_000013_301: needsTaskCommit() Task attempt_202105170134452337991652012818229_0004_m_000013_301: duration 0:00.002s
[2021-05-16 22:35:13,517] {docker.py:276} INFO - 21/05/17 01:35:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452337991652012818229_0004_m_000013_301
[2021-05-16 22:35:13,519] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-16 22:35:13,521] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (5deb0a1bddc0, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:13,522] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-16 22:35:13,522] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 2654 ms on 5deb0a1bddc0 (executor driver) (13/200)
[2021-05-16 22:35:13,533] {docker.py:276} INFO - 21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:13,535] {docker.py:276} INFO - 21/05/17 01:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:13,536] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,536] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452645123384802395809_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452645123384802395809_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452645123384802395809_0004}; taskId=attempt_202105170134452645123384802395809_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e4db513}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,536] {docker.py:276} INFO - 21/05/17 01:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:13,537] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134452645123384802395809_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452645123384802395809_0004_m_000016_304
[2021-05-16 22:35:13,540] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_202105170134452645123384802395809_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452645123384802395809_0004_m_000016_304 : duration 0:00.004s
[2021-05-16 22:35:13,573] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134457019628156991514765_0004_m_000012_300: needsTaskCommit() Task attempt_202105170134457019628156991514765_0004_m_000012_300
[2021-05-16 22:35:13,574] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_202105170134457019628156991514765_0004_m_000012_300: needsTaskCommit() Task attempt_202105170134457019628156991514765_0004_m_000012_300: duration 0:00.002s
[2021-05-16 22:35:13,574] {docker.py:276} INFO - 21/05/17 01:35:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457019628156991514765_0004_m_000012_300
[2021-05-16 22:35:13,575] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
[2021-05-16 22:35:13,577] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 2709 ms on 5deb0a1bddc0 (executor driver) (14/200)
[2021-05-16 22:35:13,578] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (5deb0a1bddc0, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:13,579] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
[2021-05-16 22:35:13,589] {docker.py:276} INFO - 21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:13,589] {docker.py:276} INFO - 21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:13,591] {docker.py:276} INFO - 21/05/17 01:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:13,591] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,592] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456399594388536068068_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456399594388536068068_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456399594388536068068_0004}; taskId=attempt_202105170134456399594388536068068_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12b793be}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,592] {docker.py:276} INFO - 21/05/17 01:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:13,593] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134456399594388536068068_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456399594388536068068_0004_m_000017_305
[2021-05-16 22:35:13,598] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_202105170134456399594388536068068_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456399594388536068068_0004_m_000017_305 : duration 0:00.006s
[2021-05-16 22:35:13,690] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_2021051701344519363063924907413_0004_m_000014_302: needsTaskCommit() Task attempt_2021051701344519363063924907413_0004_m_000014_302
[2021-05-16 22:35:13,691] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_2021051701344519363063924907413_0004_m_000014_302: needsTaskCommit() Task attempt_2021051701344519363063924907413_0004_m_000014_302: duration 0:00.002s
21/05/17 01:35:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051701344519363063924907413_0004_m_000014_302
[2021-05-16 22:35:13,693] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 4544 bytes result sent to driver
[2021-05-16 22:35:13,694] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (5deb0a1bddc0, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:13,694] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
[2021-05-16 22:35:13,695] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 2608 ms on 5deb0a1bddc0 (executor driver) (15/200)
[2021-05-16 22:35:13,705] {docker.py:276} INFO - 21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:13,707] {docker.py:276} INFO - 21/05/17 01:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,708] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445864833053963925557_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445864833053963925557_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445864833053963925557_0004}; taskId=attempt_20210517013445864833053963925557_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26521647}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_20210517013445864833053963925557_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445864833053963925557_0004_m_000018_306
[2021-05-16 22:35:13,712] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_20210517013445864833053963925557_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445864833053963925557_0004_m_000018_306 : duration 0:00.004s
[2021-05-16 22:35:13,769] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134453353115555538672053_0004_m_000015_303: needsTaskCommit() Task attempt_202105170134453353115555538672053_0004_m_000015_303
[2021-05-16 22:35:13,769] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_202105170134453353115555538672053_0004_m_000015_303: needsTaskCommit() Task attempt_202105170134453353115555538672053_0004_m_000015_303: duration 0:00.001s
21/05/17 01:35:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453353115555538672053_0004_m_000015_303
[2021-05-16 22:35:13,772] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 4587 bytes result sent to driver
[2021-05-16 22:35:13,773] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (5deb0a1bddc0, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:13,774] {docker.py:276} INFO - 21/05/17 01:35:13 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-16 22:35:13,775] {docker.py:276} INFO - 21/05/17 01:35:13 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 2531 ms on 5deb0a1bddc0 (executor driver) (16/200)
[2021-05-16 22:35:13,784] {docker.py:276} INFO - 21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:13,784] {docker.py:276} INFO - 21/05/17 01:35:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:13,786] {docker.py:276} INFO - 21/05/17 01:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:13,787] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,795] {docker.py:276} INFO - 21/05/17 01:35:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457383554338756599603_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457383554338756599603_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457383554338756599603_0004}; taskId=attempt_202105170134457383554338756599603_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73e4cae3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:13,795] {docker.py:276} INFO - 21/05/17 01:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134457383554338756599603_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457383554338756599603_0004_m_000019_307
[2021-05-16 22:35:13,799] {docker.py:276} INFO - 21/05/17 01:35:13 INFO StagingCommitter: Task committer attempt_202105170134457383554338756599603_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457383554338756599603_0004_m_000019_307 : duration 0:00.004s
[2021-05-16 22:35:16,408] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_20210517013445864833053963925557_0004_m_000018_306: needsTaskCommit() Task attempt_20210517013445864833053963925557_0004_m_000018_306
[2021-05-16 22:35:16,410] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_20210517013445864833053963925557_0004_m_000018_306: needsTaskCommit() Task attempt_20210517013445864833053963925557_0004_m_000018_306: duration 0:00.003s
21/05/17 01:35:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445864833053963925557_0004_m_000018_306
[2021-05-16 22:35:16,414] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134452645123384802395809_0004_m_000016_304: needsTaskCommit() Task attempt_202105170134452645123384802395809_0004_m_000016_304
21/05/17 01:35:16 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134456399594388536068068_0004_m_000017_305: needsTaskCommit() Task attempt_202105170134456399594388536068068_0004_m_000017_305
[2021-05-16 22:35:16,414] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134452645123384802395809_0004_m_000016_304: needsTaskCommit() Task attempt_202105170134452645123384802395809_0004_m_000016_304: duration 0:00.001s
21/05/17 01:35:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452645123384802395809_0004_m_000016_304
[2021-05-16 22:35:16,415] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134456399594388536068068_0004_m_000017_305: needsTaskCommit() Task attempt_202105170134456399594388536068068_0004_m_000017_305: duration 0:00.001s
21/05/17 01:35:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456399594388536068068_0004_m_000017_305
21/05/17 01:35:16 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (5deb0a1bddc0, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:35:16 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
21/05/17 01:35:16 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-16 22:35:16,416] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 2724 ms on 5deb0a1bddc0 (executor driver) (17/200)
[2021-05-16 22:35:16,416] {docker.py:276} INFO - 21/05/17 01:35:16 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-16 22:35:16,417] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (5deb0a1bddc0, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:16,417] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (5deb0a1bddc0, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:16,418] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 2843 ms on 5deb0a1bddc0 (executor driver) (18/200)
[2021-05-16 22:35:16,418] {docker.py:276} INFO - 21/05/17 01:35:16 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-16 22:35:16,419] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 2902 ms on 5deb0a1bddc0 (executor driver) (19/200)
[2021-05-16 22:35:16,423] {docker.py:276} INFO - 21/05/17 01:35:16 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-16 22:35:16,429] {docker.py:276} INFO - 21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:16,430] {docker.py:276} INFO - 21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:16,430] {docker.py:276} INFO - 21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 22:35:16,431] {docker.py:276} INFO - 21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 01:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:16,432] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:16,432] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454000674863733618824_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454000674863733618824_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454000674863733618824_0004}; taskId=attempt_202105170134454000674863733618824_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b7a810c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134454000674863733618824_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454000674863733618824_0004_m_000020_308
[2021-05-16 22:35:16,435] {docker.py:276} INFO - 21/05/17 01:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:16,436] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:16,438] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134454000674863733618824_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454000674863733618824_0004_m_000020_308 : duration 0:00.006s
[2021-05-16 22:35:16,439] {docker.py:276} INFO - 21/05/17 01:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:16,439] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:16,440] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452431236101168546087_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452431236101168546087_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452431236101168546087_0004}; taskId=attempt_202105170134452431236101168546087_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3745d651}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:16,441] {docker.py:276} INFO - 21/05/17 01:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:16,441] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134452431236101168546087_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452431236101168546087_0004_m_000021_309
[2021-05-16 22:35:16,446] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452333600718824779462_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452333600718824779462_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452333600718824779462_0004}; taskId=attempt_202105170134452333600718824779462_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11c92ca7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134452333600718824779462_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452333600718824779462_0004_m_000022_310
[2021-05-16 22:35:16,449] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134452431236101168546087_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452431236101168546087_0004_m_000021_309 : duration 0:00.007s
[2021-05-16 22:35:16,458] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134452333600718824779462_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452333600718824779462_0004_m_000022_310 : duration 0:00.012s
[2021-05-16 22:35:16,460] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134457383554338756599603_0004_m_000019_307: needsTaskCommit() Task attempt_202105170134457383554338756599603_0004_m_000019_307
[2021-05-16 22:35:16,460] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134457383554338756599603_0004_m_000019_307: needsTaskCommit() Task attempt_202105170134457383554338756599603_0004_m_000019_307: duration 0:00.000s
[2021-05-16 22:35:16,461] {docker.py:276} INFO - 21/05/17 01:35:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457383554338756599603_0004_m_000019_307
[2021-05-16 22:35:16,461] {docker.py:276} INFO - 21/05/17 01:35:16 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-16 22:35:16,462] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (5deb0a1bddc0, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:16,463] {docker.py:276} INFO - 21/05/17 01:35:16 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 2693 ms on 5deb0a1bddc0 (executor driver) (20/200)
[2021-05-16 22:35:16,464] {docker.py:276} INFO - 21/05/17 01:35:16 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-16 22:35:16,471] {docker.py:276} INFO - 21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:16,480] {docker.py:276} INFO - 21/05/17 01:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:16,480] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:16,481] {docker.py:276} INFO - 21/05/17 01:35:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454834144542416977963_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454834144542416977963_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454834144542416977963_0004}; taskId=attempt_202105170134454834144542416977963_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@311ceee0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:16,481] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134454834144542416977963_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454834144542416977963_0004_m_000023_311
[2021-05-16 22:35:16,485] {docker.py:276} INFO - 21/05/17 01:35:16 INFO StagingCommitter: Task committer attempt_202105170134454834144542416977963_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454834144542416977963_0004_m_000023_311 : duration 0:00.004s
[2021-05-16 22:35:19,094] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134452333600718824779462_0004_m_000022_310: needsTaskCommit() Task attempt_202105170134452333600718824779462_0004_m_000022_310
21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134454000674863733618824_0004_m_000020_308: needsTaskCommit() Task attempt_202105170134454000674863733618824_0004_m_000020_308
[2021-05-16 22:35:19,095] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134452333600718824779462_0004_m_000022_310: needsTaskCommit() Task attempt_202105170134452333600718824779462_0004_m_000022_310: duration 0:00.001s
21/05/17 01:35:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452333600718824779462_0004_m_000022_310
[2021-05-16 22:35:19,096] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134454000674863733618824_0004_m_000020_308: needsTaskCommit() Task attempt_202105170134454000674863733618824_0004_m_000020_308: duration 0:00.002s
21/05/17 01:35:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454000674863733618824_0004_m_000020_308
[2021-05-16 22:35:19,097] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 4544 bytes result sent to driver
[2021-05-16 22:35:19,098] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (5deb0a1bddc0, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:19,098] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 4544 bytes result sent to driver
[2021-05-16 22:35:19,099] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (5deb0a1bddc0, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:19,100] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-16 22:35:19,101] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
21/05/17 01:35:19 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 2685 ms on 5deb0a1bddc0 (executor driver) (21/200)
[2021-05-16 22:35:19,102] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 2693 ms on 5deb0a1bddc0 (executor driver) (22/200)
[2021-05-16 22:35:19,110] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134452431236101168546087_0004_m_000021_309: needsTaskCommit() Task attempt_202105170134452431236101168546087_0004_m_000021_309
[2021-05-16 22:35:19,111] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134452431236101168546087_0004_m_000021_309: needsTaskCommit() Task attempt_202105170134452431236101168546087_0004_m_000021_309: duration 0:00.001s
[2021-05-16 22:35:19,112] {docker.py:276} INFO - 21/05/17 01:35:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452431236101168546087_0004_m_000021_309
[2021-05-16 22:35:19,113] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:19,113] {docker.py:276} INFO - 21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:19,114] {docker.py:276} INFO - 21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-16 22:35:19,114] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (5deb0a1bddc0, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:19,115] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 2704 ms on 5deb0a1bddc0 (executor driver) (23/200)
[2021-05-16 22:35:19,116] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-16 22:35:19,116] {docker.py:276} INFO - 21/05/17 01:35:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:19,117] {docker.py:276} INFO - 21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456815783297231662260_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456815783297231662260_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456815783297231662260_0004}; taskId=attempt_202105170134456815783297231662260_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f25ac9c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134456815783297231662260_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456815783297231662260_0004_m_000024_312
[2021-05-16 22:35:19,124] {docker.py:276} INFO - 21/05/17 01:35:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:19,124] {docker.py:276} INFO - 21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453582935069154725263_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453582935069154725263_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453582935069154725263_0004}; taskId=attempt_202105170134453582935069154725263_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2031550e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:19,127] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134456815783297231662260_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456815783297231662260_0004_m_000024_312 : duration 0:00.010s
[2021-05-16 22:35:19,129] {docker.py:276} INFO - 21/05/17 01:35:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134453582935069154725263_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453582935069154725263_0004_m_000025_313
[2021-05-16 22:35:19,132] {docker.py:276} INFO - 21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:19,134] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134453582935069154725263_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453582935069154725263_0004_m_000025_313 : duration 0:00.005s
[2021-05-16 22:35:19,141] {docker.py:276} INFO - 21/05/17 01:35:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:19,141] {docker.py:276} INFO - 21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457640998568403316436_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457640998568403316436_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457640998568403316436_0004}; taskId=attempt_202105170134457640998568403316436_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@213a56c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:19,142] {docker.py:276} INFO - 21/05/17 01:35:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:19,144] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134457640998568403316436_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457640998568403316436_0004_m_000026_314
[2021-05-16 22:35:19,149] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134457640998568403316436_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457640998568403316436_0004_m_000026_314 : duration 0:00.008s
[2021-05-16 22:35:19,279] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134454834144542416977963_0004_m_000023_311: needsTaskCommit() Task attempt_202105170134454834144542416977963_0004_m_000023_311
[2021-05-16 22:35:19,279] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134454834144542416977963_0004_m_000023_311: needsTaskCommit() Task attempt_202105170134454834144542416977963_0004_m_000023_311: duration 0:00.001s
[2021-05-16 22:35:19,280] {docker.py:276} INFO - 21/05/17 01:35:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454834144542416977963_0004_m_000023_311
[2021-05-16 22:35:19,280] {docker.py:276} INFO - 21/05/17 01:35:19 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 4544 bytes result sent to driver
[2021-05-16 22:35:19,281] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (5deb0a1bddc0, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:19,282] {docker.py:276} INFO - 21/05/17 01:35:19 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 2823 ms on 5deb0a1bddc0 (executor driver) (24/200)
21/05/17 01:35:19 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
[2021-05-16 22:35:19,291] {docker.py:276} INFO - 21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:19,293] {docker.py:276} INFO - 21/05/17 01:35:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458401859314462330547_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458401859314462330547_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458401859314462330547_0004}; taskId=attempt_202105170134458401859314462330547_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17609323}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:19 INFO StagingCommitter: Starting: Task committer attempt_202105170134458401859314462330547_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458401859314462330547_0004_m_000027_315
[2021-05-16 22:35:19,296] {docker.py:276} INFO - 21/05/17 01:35:19 INFO StagingCommitter: Task committer attempt_202105170134458401859314462330547_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458401859314462330547_0004_m_000027_315 : duration 0:00.003s
[2021-05-16 22:35:21,718] {docker.py:276} INFO - 21/05/17 01:35:21 INFO StagingCommitter: Starting: Task committer attempt_202105170134456815783297231662260_0004_m_000024_312: needsTaskCommit() Task attempt_202105170134456815783297231662260_0004_m_000024_312
21/05/17 01:35:21 INFO StagingCommitter: Task committer attempt_202105170134456815783297231662260_0004_m_000024_312: needsTaskCommit() Task attempt_202105170134456815783297231662260_0004_m_000024_312: duration 0:00.001s
21/05/17 01:35:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456815783297231662260_0004_m_000024_312
[2021-05-16 22:35:21,719] {docker.py:276} INFO - 21/05/17 01:35:21 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 4587 bytes result sent to driver
[2021-05-16 22:35:21,720] {docker.py:276} INFO - 21/05/17 01:35:21 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (5deb0a1bddc0, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:21,721] {docker.py:276} INFO - 21/05/17 01:35:21 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 2627 ms on 5deb0a1bddc0 (executor driver) (25/200)
[2021-05-16 22:35:21,721] {docker.py:276} INFO - 21/05/17 01:35:21 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-16 22:35:21,729] {docker.py:276} INFO - 21/05/17 01:35:21 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:21,731] {docker.py:276} INFO - 21/05/17 01:35:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455339580171273777744_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455339580171273777744_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455339580171273777744_0004}; taskId=attempt_202105170134455339580171273777744_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@500021df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:21 INFO StagingCommitter: Starting: Task committer attempt_202105170134455339580171273777744_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455339580171273777744_0004_m_000028_316
[2021-05-16 22:35:21,734] {docker.py:276} INFO - 21/05/17 01:35:21 INFO StagingCommitter: Task committer attempt_202105170134455339580171273777744_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455339580171273777744_0004_m_000028_316 : duration 0:00.002s
[2021-05-16 22:35:21,786] {docker.py:276} INFO - 21/05/17 01:35:21 INFO StagingCommitter: Starting: Task committer attempt_202105170134457640998568403316436_0004_m_000026_314: needsTaskCommit() Task attempt_202105170134457640998568403316436_0004_m_000026_314
[2021-05-16 22:35:21,786] {docker.py:276} INFO - 21/05/17 01:35:21 INFO StagingCommitter: Task committer attempt_202105170134457640998568403316436_0004_m_000026_314: needsTaskCommit() Task attempt_202105170134457640998568403316436_0004_m_000026_314: duration 0:00.001s
[2021-05-16 22:35:21,787] {docker.py:276} INFO - 21/05/17 01:35:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457640998568403316436_0004_m_000026_314
[2021-05-16 22:35:21,787] {docker.py:276} INFO - 21/05/17 01:35:21 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 4587 bytes result sent to driver
[2021-05-16 22:35:21,789] {docker.py:276} INFO - 21/05/17 01:35:21 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (5deb0a1bddc0, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:21,789] {docker.py:276} INFO - 21/05/17 01:35:21 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 2679 ms on 5deb0a1bddc0 (executor driver) (26/200)
[2021-05-16 22:35:21,791] {docker.py:276} INFO - 21/05/17 01:35:21 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-16 22:35:21,800] {docker.py:276} INFO - 21/05/17 01:35:21 INFO ShuffleBlockFetcherIterator: Getting 5 (21.2 KiB) non-empty blocks including 5 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:21,802] {docker.py:276} INFO - 21/05/17 01:35:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:21,803] {docker.py:276} INFO - 21/05/17 01:35:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:21,803] {docker.py:276} INFO - 21/05/17 01:35:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454226744602975356564_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454226744602975356564_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454226744602975356564_0004}; taskId=attempt_202105170134454226744602975356564_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1af4d19d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:21,803] {docker.py:276} INFO - 21/05/17 01:35:21 INFO StagingCommitter: Starting: Task committer attempt_202105170134454226744602975356564_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454226744602975356564_0004_m_000029_317
[2021-05-16 22:35:21,806] {docker.py:276} INFO - 21/05/17 01:35:21 INFO StagingCommitter: Task committer attempt_202105170134454226744602975356564_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454226744602975356564_0004_m_000029_317 : duration 0:00.003s
[2021-05-16 22:35:22,016] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Starting: Task committer attempt_202105170134453582935069154725263_0004_m_000025_313: needsTaskCommit() Task attempt_202105170134453582935069154725263_0004_m_000025_313
[2021-05-16 22:35:22,016] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Task committer attempt_202105170134453582935069154725263_0004_m_000025_313: needsTaskCommit() Task attempt_202105170134453582935069154725263_0004_m_000025_313: duration 0:00.001s
[2021-05-16 22:35:22,017] {docker.py:276} INFO - 21/05/17 01:35:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453582935069154725263_0004_m_000025_313
[2021-05-16 22:35:22,017] {docker.py:276} INFO - 21/05/17 01:35:22 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 4587 bytes result sent to driver
[2021-05-16 22:35:22,018] {docker.py:276} INFO - 21/05/17 01:35:22 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (5deb0a1bddc0, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:22,019] {docker.py:276} INFO - 21/05/17 01:35:22 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 2924 ms on 5deb0a1bddc0 (executor driver) (27/200)
[2021-05-16 22:35:22,020] {docker.py:276} INFO - 21/05/17 01:35:22 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
[2021-05-16 22:35:22,027] {docker.py:276} INFO - 21/05/17 01:35:22 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:22,029] {docker.py:276} INFO - 21/05/17 01:35:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453804539081078110447_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453804539081078110447_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453804539081078110447_0004}; taskId=attempt_202105170134453804539081078110447_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@dc5d257}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:22,030] {docker.py:276} INFO - 21/05/17 01:35:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:22,030] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Starting: Task committer attempt_202105170134453804539081078110447_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453804539081078110447_0004_m_000030_318
[2021-05-16 22:35:22,033] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Task committer attempt_202105170134453804539081078110447_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453804539081078110447_0004_m_000030_318 : duration 0:00.004s
[2021-05-16 22:35:22,131] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Starting: Task committer attempt_202105170134458401859314462330547_0004_m_000027_315: needsTaskCommit() Task attempt_202105170134458401859314462330547_0004_m_000027_315
[2021-05-16 22:35:22,132] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Task committer attempt_202105170134458401859314462330547_0004_m_000027_315: needsTaskCommit() Task attempt_202105170134458401859314462330547_0004_m_000027_315: duration 0:00.002s
21/05/17 01:35:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458401859314462330547_0004_m_000027_315
[2021-05-16 22:35:22,134] {docker.py:276} INFO - 21/05/17 01:35:22 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 4587 bytes result sent to driver
[2021-05-16 22:35:22,134] {docker.py:276} INFO - 21/05/17 01:35:22 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (5deb0a1bddc0, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:22,135] {docker.py:276} INFO - 21/05/17 01:35:22 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
21/05/17 01:35:22 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 2858 ms on 5deb0a1bddc0 (executor driver) (28/200)
[2021-05-16 22:35:22,143] {docker.py:276} INFO - 21/05/17 01:35:22 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:22,149] {docker.py:276} INFO - 21/05/17 01:35:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:22,150] {docker.py:276} INFO - 21/05/17 01:35:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:22,151] {docker.py:276} INFO - 21/05/17 01:35:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445639783236675756598_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445639783236675756598_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445639783236675756598_0004}; taskId=attempt_20210517013445639783236675756598_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cf64e3a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:22 INFO StagingCommitter: Starting: Task committer attempt_20210517013445639783236675756598_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445639783236675756598_0004_m_000031_319
[2021-05-16 22:35:22,154] {docker.py:276} INFO - 21/05/17 01:35:22 INFO StagingCommitter: Task committer attempt_20210517013445639783236675756598_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445639783236675756598_0004_m_000031_319 : duration 0:00.003s
[2021-05-16 22:35:24,447] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134455339580171273777744_0004_m_000028_316: needsTaskCommit() Task attempt_202105170134455339580171273777744_0004_m_000028_316
[2021-05-16 22:35:24,448] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134455339580171273777744_0004_m_000028_316: needsTaskCommit() Task attempt_202105170134455339580171273777744_0004_m_000028_316: duration 0:00.001s
[2021-05-16 22:35:24,448] {docker.py:276} INFO - 21/05/17 01:35:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455339580171273777744_0004_m_000028_316
[2021-05-16 22:35:24,450] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 4544 bytes result sent to driver
[2021-05-16 22:35:24,452] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 2735 ms on 5deb0a1bddc0 (executor driver) (29/200)
[2021-05-16 22:35:24,453] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (5deb0a1bddc0, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:24,453] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-16 22:35:24,464] {docker.py:276} INFO - 21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:24,465] {docker.py:276} INFO - 21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:24,468] {docker.py:276} INFO - 21/05/17 01:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:24,469] {docker.py:276} INFO - 21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:24,469] {docker.py:276} INFO - 21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456749811439635344672_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456749811439635344672_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456749811439635344672_0004}; taskId=attempt_202105170134456749811439635344672_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e7dcec1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:24,470] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134456749811439635344672_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456749811439635344672_0004_m_000032_320
[2021-05-16 22:35:24,475] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134456749811439635344672_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456749811439635344672_0004_m_000032_320 : duration 0:00.005s
[2021-05-16 22:35:24,540] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134454226744602975356564_0004_m_000029_317: needsTaskCommit() Task attempt_202105170134454226744602975356564_0004_m_000029_317
[2021-05-16 22:35:24,540] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134454226744602975356564_0004_m_000029_317: needsTaskCommit() Task attempt_202105170134454226744602975356564_0004_m_000029_317: duration 0:00.002s
21/05/17 01:35:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454226744602975356564_0004_m_000029_317
[2021-05-16 22:35:24,542] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 4544 bytes result sent to driver
[2021-05-16 22:35:24,543] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (5deb0a1bddc0, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:24,545] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 2759 ms on 5deb0a1bddc0 (executor driver) (30/200)
21/05/17 01:35:24 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-16 22:35:24,553] {docker.py:276} INFO - 21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:24,555] {docker.py:276} INFO - 21/05/17 01:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454074855928279765841_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454074855928279765841_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454074855928279765841_0004}; taskId=attempt_202105170134454074855928279765841_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26c9f066}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134454074855928279765841_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454074855928279765841_0004_m_000033_321
[2021-05-16 22:35:24,558] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134454074855928279765841_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454074855928279765841_0004_m_000033_321 : duration 0:00.003s
[2021-05-16 22:35:24,803] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_20210517013445639783236675756598_0004_m_000031_319: needsTaskCommit() Task attempt_20210517013445639783236675756598_0004_m_000031_319
[2021-05-16 22:35:24,803] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_20210517013445639783236675756598_0004_m_000031_319: needsTaskCommit() Task attempt_20210517013445639783236675756598_0004_m_000031_319: duration 0:00.002s
[2021-05-16 22:35:24,804] {docker.py:276} INFO - 21/05/17 01:35:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445639783236675756598_0004_m_000031_319
[2021-05-16 22:35:24,807] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 4544 bytes result sent to driver
[2021-05-16 22:35:24,808] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (5deb0a1bddc0, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:24,809] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
21/05/17 01:35:24 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 2678 ms on 5deb0a1bddc0 (executor driver) (31/200)
[2021-05-16 22:35:24,820] {docker.py:276} INFO - 21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:24,822] {docker.py:276} INFO - 21/05/17 01:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454861878986073759404_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454861878986073759404_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454861878986073759404_0004}; taskId=attempt_202105170134454861878986073759404_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b1fa1b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134454861878986073759404_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454861878986073759404_0004_m_000034_322
[2021-05-16 22:35:24,825] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134454861878986073759404_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454861878986073759404_0004_m_000034_322 : duration 0:00.003s
[2021-05-16 22:35:24,834] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134453804539081078110447_0004_m_000030_318: needsTaskCommit() Task attempt_202105170134453804539081078110447_0004_m_000030_318
[2021-05-16 22:35:24,834] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134453804539081078110447_0004_m_000030_318: needsTaskCommit() Task attempt_202105170134453804539081078110447_0004_m_000030_318: duration 0:00.002s
21/05/17 01:35:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453804539081078110447_0004_m_000030_318
[2021-05-16 22:35:24,836] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 4544 bytes result sent to driver
[2021-05-16 22:35:24,837] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (5deb0a1bddc0, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:24,838] {docker.py:276} INFO - 21/05/17 01:35:24 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 2823 ms on 5deb0a1bddc0 (executor driver) (32/200)
[2021-05-16 22:35:24,839] {docker.py:276} INFO - 21/05/17 01:35:24 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
[2021-05-16 22:35:24,848] {docker.py:276} INFO - 21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:24,850] {docker.py:276} INFO - 21/05/17 01:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455269134092007442823_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455269134092007442823_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455269134092007442823_0004}; taskId=attempt_202105170134455269134092007442823_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3db24e07}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:24,851] {docker.py:276} INFO - 21/05/17 01:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134455269134092007442823_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455269134092007442823_0004_m_000035_323
[2021-05-16 22:35:24,853] {docker.py:276} INFO - 21/05/17 01:35:24 INFO StagingCommitter: Task committer attempt_202105170134455269134092007442823_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455269134092007442823_0004_m_000035_323 : duration 0:00.003s
[2021-05-16 22:35:25,489] {docker.py:276} INFO - 21/05/17 01:35:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 5deb0a1bddc0:42667 in memory (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-16 22:35:27,210] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134454074855928279765841_0004_m_000033_321: needsTaskCommit() Task attempt_202105170134454074855928279765841_0004_m_000033_321
[2021-05-16 22:35:27,211] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134454074855928279765841_0004_m_000033_321: needsTaskCommit() Task attempt_202105170134454074855928279765841_0004_m_000033_321: duration 0:00.000s
21/05/17 01:35:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454074855928279765841_0004_m_000033_321
[2021-05-16 22:35:27,212] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 4587 bytes result sent to driver
[2021-05-16 22:35:27,214] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (5deb0a1bddc0, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:27,215] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 2675 ms on 5deb0a1bddc0 (executor driver) (33/200)
[2021-05-16 22:35:27,216] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-16 22:35:27,226] {docker.py:276} INFO - 21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:27,229] {docker.py:276} INFO - 21/05/17 01:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:27,229] {docker.py:276} INFO - 21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456167895882635107244_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456167895882635107244_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456167895882635107244_0004}; taskId=attempt_202105170134456167895882635107244_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69c66bd0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:27,230] {docker.py:276} INFO - 21/05/17 01:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:27,230] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134456167895882635107244_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456167895882635107244_0004_m_000036_324
[2021-05-16 22:35:27,233] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134456167895882635107244_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456167895882635107244_0004_m_000036_324 : duration 0:00.003s
[2021-05-16 22:35:27,337] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134456749811439635344672_0004_m_000032_320: needsTaskCommit() Task attempt_202105170134456749811439635344672_0004_m_000032_320
[2021-05-16 22:35:27,338] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134456749811439635344672_0004_m_000032_320: needsTaskCommit() Task attempt_202105170134456749811439635344672_0004_m_000032_320: duration 0:00.002s
21/05/17 01:35:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456749811439635344672_0004_m_000032_320
[2021-05-16 22:35:27,340] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 4587 bytes result sent to driver
[2021-05-16 22:35:27,341] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (5deb0a1bddc0, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:27,342] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
[2021-05-16 22:35:27,343] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 2894 ms on 5deb0a1bddc0 (executor driver) (34/200)
[2021-05-16 22:35:27,353] {docker.py:276} INFO - 21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Getting 5 (21.1 KiB) non-empty blocks including 5 (21.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:27,355] {docker.py:276} INFO - 21/05/17 01:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455070439180748525793_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455070439180748525793_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455070439180748525793_0004}; taskId=attempt_202105170134455070439180748525793_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79c3d473}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:27,355] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134455070439180748525793_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455070439180748525793_0004_m_000037_325
[2021-05-16 22:35:27,359] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134455070439180748525793_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455070439180748525793_0004_m_000037_325 : duration 0:00.004s
[2021-05-16 22:35:27,458] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134454861878986073759404_0004_m_000034_322: needsTaskCommit() Task attempt_202105170134454861878986073759404_0004_m_000034_322
[2021-05-16 22:35:27,459] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134454861878986073759404_0004_m_000034_322: needsTaskCommit() Task attempt_202105170134454861878986073759404_0004_m_000034_322: duration 0:00.002s
[2021-05-16 22:35:27,460] {docker.py:276} INFO - 21/05/17 01:35:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454861878986073759404_0004_m_000034_322
[2021-05-16 22:35:27,461] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 4587 bytes result sent to driver
[2021-05-16 22:35:27,462] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (5deb0a1bddc0, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:27,464] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 2658 ms on 5deb0a1bddc0 (executor driver) (35/200)
[2021-05-16 22:35:27,465] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
[2021-05-16 22:35:27,475] {docker.py:276} INFO - 21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Getting 5 (22.3 KiB) non-empty blocks including 5 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:27,476] {docker.py:276} INFO - 21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:27,478] {docker.py:276} INFO - 21/05/17 01:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:27,479] {docker.py:276} INFO - 21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445412603220076320508_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445412603220076320508_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445412603220076320508_0004}; taskId=attempt_20210517013445412603220076320508_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a2c3c49}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_20210517013445412603220076320508_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445412603220076320508_0004_m_000038_326
[2021-05-16 22:35:27,481] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_20210517013445412603220076320508_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445412603220076320508_0004_m_000038_326 : duration 0:00.002s
[2021-05-16 22:35:27,720] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134455269134092007442823_0004_m_000035_323: needsTaskCommit() Task attempt_202105170134455269134092007442823_0004_m_000035_323
[2021-05-16 22:35:27,721] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134455269134092007442823_0004_m_000035_323: needsTaskCommit() Task attempt_202105170134455269134092007442823_0004_m_000035_323: duration 0:00.001s
21/05/17 01:35:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455269134092007442823_0004_m_000035_323
[2021-05-16 22:35:27,722] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 4587 bytes result sent to driver
[2021-05-16 22:35:27,724] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (5deb0a1bddc0, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:27,725] {docker.py:276} INFO - 21/05/17 01:35:27 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 2891 ms on 5deb0a1bddc0 (executor driver) (36/200)
[2021-05-16 22:35:27,726] {docker.py:276} INFO - 21/05/17 01:35:27 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-16 22:35:27,735] {docker.py:276} INFO - 21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:27,736] {docker.py:276} INFO - 21/05/17 01:35:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:27,738] {docker.py:276} INFO - 21/05/17 01:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:27,738] {docker.py:276} INFO - 21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:27,739] {docker.py:276} INFO - 21/05/17 01:35:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457640135285354537477_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457640135285354537477_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457640135285354537477_0004}; taskId=attempt_202105170134457640135285354537477_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@236e9e27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:27,739] {docker.py:276} INFO - 21/05/17 01:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:27 INFO StagingCommitter: Starting: Task committer attempt_202105170134457640135285354537477_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457640135285354537477_0004_m_000039_327
[2021-05-16 22:35:27,743] {docker.py:276} INFO - 21/05/17 01:35:27 INFO StagingCommitter: Task committer attempt_202105170134457640135285354537477_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457640135285354537477_0004_m_000039_327 : duration 0:00.004s
[2021-05-16 22:35:30,026] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_202105170134455070439180748525793_0004_m_000037_325: needsTaskCommit() Task attempt_202105170134455070439180748525793_0004_m_000037_325
[2021-05-16 22:35:30,027] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_202105170134455070439180748525793_0004_m_000037_325: needsTaskCommit() Task attempt_202105170134455070439180748525793_0004_m_000037_325: duration 0:00.001s
21/05/17 01:35:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455070439180748525793_0004_m_000037_325
[2021-05-16 22:35:30,029] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
[2021-05-16 22:35:30,030] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (5deb0a1bddc0, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:30,031] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 2658 ms on 5deb0a1bddc0 (executor driver) (37/200)
[2021-05-16 22:35:30,033] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_202105170134456167895882635107244_0004_m_000036_324: needsTaskCommit() Task attempt_202105170134456167895882635107244_0004_m_000036_324
21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_202105170134456167895882635107244_0004_m_000036_324: needsTaskCommit() Task attempt_202105170134456167895882635107244_0004_m_000036_324: duration 0:00.001s
21/05/17 01:35:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456167895882635107244_0004_m_000036_324
[2021-05-16 22:35:30,034] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-16 22:35:30,035] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (5deb0a1bddc0, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:30,036] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 2791 ms on 5deb0a1bddc0 (executor driver) (38/200)
[2021-05-16 22:35:30,036] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-16 22:35:30,047] {docker.py:276} INFO - 21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:30,049] {docker.py:276} INFO - 21/05/17 01:35:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:30,050] {docker.py:276} INFO - 21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:30,051] {docker.py:276} INFO - 21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453166373407947724942_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453166373407947724942_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453166373407947724942_0004}; taskId=attempt_202105170134453166373407947724942_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5cd0c1ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:30,051] {docker.py:276} INFO - 21/05/17 01:35:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:30,051] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_202105170134453166373407947724942_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453166373407947724942_0004_m_000040_328
[2021-05-16 22:35:30,052] {docker.py:276} INFO - 21/05/17 01:35:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:30,052] {docker.py:276} INFO - 21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:30,052] {docker.py:276} INFO - 21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445823483659114717625_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445823483659114717625_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445823483659114717625_0004}; taskId=attempt_20210517013445823483659114717625_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13781ddf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_20210517013445823483659114717625_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445823483659114717625_0004_m_000041_329
[2021-05-16 22:35:30,054] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_20210517013445823483659114717625_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445823483659114717625_0004_m_000041_329 : duration 0:00.003s
[2021-05-16 22:35:30,054] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_202105170134453166373407947724942_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453166373407947724942_0004_m_000040_328 : duration 0:00.005s
[2021-05-16 22:35:30,119] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_20210517013445412603220076320508_0004_m_000038_326: needsTaskCommit() Task attempt_20210517013445412603220076320508_0004_m_000038_326
[2021-05-16 22:35:30,119] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_20210517013445412603220076320508_0004_m_000038_326: needsTaskCommit() Task attempt_20210517013445412603220076320508_0004_m_000038_326: duration 0:00.000s
21/05/17 01:35:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445412603220076320508_0004_m_000038_326
[2021-05-16 22:35:30,121] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 4544 bytes result sent to driver
[2021-05-16 22:35:30,122] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (5deb0a1bddc0, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:30,123] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 2629 ms on 5deb0a1bddc0 (executor driver) (39/200)
[2021-05-16 22:35:30,123] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
[2021-05-16 22:35:30,132] {docker.py:276} INFO - 21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:30,133] {docker.py:276} INFO - 21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:30,136] {docker.py:276} INFO - 21/05/17 01:35:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:35:30,137] {docker.py:276} INFO - 21/05/17 01:35:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:30,137] {docker.py:276} INFO - 21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:30,138] {docker.py:276} INFO - 21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453304745181707827773_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453304745181707827773_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453304745181707827773_0004}; taskId=attempt_202105170134453304745181707827773_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f44704d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:30,138] {docker.py:276} INFO - 21/05/17 01:35:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:30,138] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_202105170134453304745181707827773_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453304745181707827773_0004_m_000042_330
[2021-05-16 22:35:30,142] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_202105170134453304745181707827773_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453304745181707827773_0004_m_000042_330 : duration 0:00.004s
[2021-05-16 22:35:30,382] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_202105170134457640135285354537477_0004_m_000039_327: needsTaskCommit() Task attempt_202105170134457640135285354537477_0004_m_000039_327
[2021-05-16 22:35:30,383] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_202105170134457640135285354537477_0004_m_000039_327: needsTaskCommit() Task attempt_202105170134457640135285354537477_0004_m_000039_327: duration 0:00.002s
21/05/17 01:35:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457640135285354537477_0004_m_000039_327
[2021-05-16 22:35:30,385] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 4544 bytes result sent to driver
[2021-05-16 22:35:30,386] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (5deb0a1bddc0, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:30,387] {docker.py:276} INFO - 21/05/17 01:35:30 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
[2021-05-16 22:35:30,388] {docker.py:276} INFO - 21/05/17 01:35:30 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 2632 ms on 5deb0a1bddc0 (executor driver) (40/200)
[2021-05-16 22:35:30,398] {docker.py:276} INFO - 21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:30,400] {docker.py:276} INFO - 21/05/17 01:35:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445759413492674511381_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445759413492674511381_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445759413492674511381_0004}; taskId=attempt_20210517013445759413492674511381_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2192b122}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:30,401] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Starting: Task committer attempt_20210517013445759413492674511381_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445759413492674511381_0004_m_000043_331
[2021-05-16 22:35:30,403] {docker.py:276} INFO - 21/05/17 01:35:30 INFO StagingCommitter: Task committer attempt_20210517013445759413492674511381_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445759413492674511381_0004_m_000043_331 : duration 0:00.003s
[2021-05-16 22:35:32,688] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134453166373407947724942_0004_m_000040_328: needsTaskCommit() Task attempt_202105170134453166373407947724942_0004_m_000040_328
[2021-05-16 22:35:32,689] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_202105170134453166373407947724942_0004_m_000040_328: needsTaskCommit() Task attempt_202105170134453166373407947724942_0004_m_000040_328: duration 0:00.001s
21/05/17 01:35:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453166373407947724942_0004_m_000040_328
[2021-05-16 22:35:32,692] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 4587 bytes result sent to driver
[2021-05-16 22:35:32,693] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (5deb0a1bddc0, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:32,694] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-16 22:35:32,695] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 2668 ms on 5deb0a1bddc0 (executor driver) (41/200)
[2021-05-16 22:35:32,704] {docker.py:276} INFO - 21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:32,707] {docker.py:276} INFO - 21/05/17 01:35:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455098531782498826725_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455098531782498826725_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455098531782498826725_0004}; taskId=attempt_202105170134455098531782498826725_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2145164e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134455098531782498826725_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455098531782498826725_0004_m_000044_332
[2021-05-16 22:35:32,712] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_202105170134455098531782498826725_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455098531782498826725_0004_m_000044_332 : duration 0:00.004s
[2021-05-16 22:35:32,762] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_20210517013445759413492674511381_0004_m_000043_331: needsTaskCommit() Task attempt_20210517013445759413492674511381_0004_m_000043_331
[2021-05-16 22:35:32,763] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_20210517013445759413492674511381_0004_m_000043_331: needsTaskCommit() Task attempt_20210517013445759413492674511381_0004_m_000043_331: duration 0:00.000s
21/05/17 01:35:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445759413492674511381_0004_m_000043_331
[2021-05-16 22:35:32,768] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 4587 bytes result sent to driver
[2021-05-16 22:35:32,771] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134453304745181707827773_0004_m_000042_330: needsTaskCommit() Task attempt_202105170134453304745181707827773_0004_m_000042_330
[2021-05-16 22:35:32,772] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_202105170134453304745181707827773_0004_m_000042_330: needsTaskCommit() Task attempt_202105170134453304745181707827773_0004_m_000042_330: duration 0:00.001s
21/05/17 01:35:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453304745181707827773_0004_m_000042_330
[2021-05-16 22:35:32,772] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (5deb0a1bddc0, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:32,773] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 2388 ms on 5deb0a1bddc0 (executor driver) (42/200)
[2021-05-16 22:35:32,774] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-16 22:35:32,774] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 4587 bytes result sent to driver
[2021-05-16 22:35:32,775] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 2656 ms on 5deb0a1bddc0 (executor driver) (43/200)
[2021-05-16 22:35:32,776] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (5deb0a1bddc0, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:32,776] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-16 22:35:32,785] {docker.py:276} INFO - 21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:32,788] {docker.py:276} INFO - 21/05/17 01:35:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:35:32,788] {docker.py:276} INFO - 21/05/17 01:35:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:32,788] {docker.py:276} INFO - 21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:32,789] {docker.py:276} INFO - 21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445680935569127777218_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445680935569127777218_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445680935569127777218_0004}; taskId=attempt_20210517013445680935569127777218_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f16e418}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:32,789] {docker.py:276} INFO - 21/05/17 01:35:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_20210517013445680935569127777218_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445680935569127777218_0004_m_000045_333
[2021-05-16 22:35:32,790] {docker.py:276} INFO - 21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:32,790] {docker.py:276} INFO - 21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 22:35:32,792] {docker.py:276} INFO - 21/05/17 01:35:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:32,792] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_20210517013445680935569127777218_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445680935569127777218_0004_m_000045_333 : duration 0:00.004s
21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456449753103825427388_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456449753103825427388_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456449753103825427388_0004}; taskId=attempt_202105170134456449753103825427388_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd9bd50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:32,793] {docker.py:276} INFO - 21/05/17 01:35:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:32,793] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134456449753103825427388_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456449753103825427388_0004_m_000046_334
[2021-05-16 22:35:32,796] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_202105170134456449753103825427388_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456449753103825427388_0004_m_000046_334 : duration 0:00.003s
[2021-05-16 22:35:32,827] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_20210517013445823483659114717625_0004_m_000041_329: needsTaskCommit() Task attempt_20210517013445823483659114717625_0004_m_000041_329
[2021-05-16 22:35:32,828] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_20210517013445823483659114717625_0004_m_000041_329: needsTaskCommit() Task attempt_20210517013445823483659114717625_0004_m_000041_329: duration 0:00.002s
[2021-05-16 22:35:32,829] {docker.py:276} INFO - 21/05/17 01:35:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445823483659114717625_0004_m_000041_329
[2021-05-16 22:35:32,831] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 4587 bytes result sent to driver
[2021-05-16 22:35:32,832] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (5deb0a1bddc0, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:32,833] {docker.py:276} INFO - 21/05/17 01:35:32 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 2802 ms on 5deb0a1bddc0 (executor driver) (44/200)
[2021-05-16 22:35:32,834] {docker.py:276} INFO - 21/05/17 01:35:32 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-16 22:35:32,844] {docker.py:276} INFO - 21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:32,846] {docker.py:276} INFO - 21/05/17 01:35:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454716049383362315978_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454716049383362315978_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454716049383362315978_0004}; taskId=attempt_202105170134454716049383362315978_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6efaf981}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:32,847] {docker.py:276} INFO - 21/05/17 01:35:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134454716049383362315978_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454716049383362315978_0004_m_000047_335
[2021-05-16 22:35:32,850] {docker.py:276} INFO - 21/05/17 01:35:32 INFO StagingCommitter: Task committer attempt_202105170134454716049383362315978_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454716049383362315978_0004_m_000047_335 : duration 0:00.003s
[2021-05-16 22:35:35,325] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134455098531782498826725_0004_m_000044_332: needsTaskCommit() Task attempt_202105170134455098531782498826725_0004_m_000044_332
[2021-05-16 22:35:35,325] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134455098531782498826725_0004_m_000044_332: needsTaskCommit() Task attempt_202105170134455098531782498826725_0004_m_000044_332: duration 0:00.001s
21/05/17 01:35:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455098531782498826725_0004_m_000044_332
[2021-05-16 22:35:35,326] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 4544 bytes result sent to driver
[2021-05-16 22:35:35,328] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (5deb0a1bddc0, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:35,328] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 2639 ms on 5deb0a1bddc0 (executor driver) (45/200)
[2021-05-16 22:35:35,329] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
[2021-05-16 22:35:35,338] {docker.py:276} INFO - 21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:35,340] {docker.py:276} INFO - 21/05/17 01:35:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453497102227639643329_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453497102227639643329_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453497102227639643329_0004}; taskId=attempt_202105170134453497102227639643329_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e863f77}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:35,340] {docker.py:276} INFO - 21/05/17 01:35:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134453497102227639643329_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453497102227639643329_0004_m_000048_336
[2021-05-16 22:35:35,342] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134453497102227639643329_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453497102227639643329_0004_m_000048_336 : duration 0:00.002s
[2021-05-16 22:35:35,412] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134456449753103825427388_0004_m_000046_334: needsTaskCommit() Task attempt_202105170134456449753103825427388_0004_m_000046_334
[2021-05-16 22:35:35,413] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134456449753103825427388_0004_m_000046_334: needsTaskCommit() Task attempt_202105170134456449753103825427388_0004_m_000046_334: duration 0:00.001s
21/05/17 01:35:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456449753103825427388_0004_m_000046_334
[2021-05-16 22:35:35,414] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 4544 bytes result sent to driver
[2021-05-16 22:35:35,415] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (5deb0a1bddc0, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:35,416] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-16 22:35:35,417] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 2645 ms on 5deb0a1bddc0 (executor driver) (46/200)
[2021-05-16 22:35:35,422] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_20210517013445680935569127777218_0004_m_000045_333: needsTaskCommit() Task attempt_20210517013445680935569127777218_0004_m_000045_333
[2021-05-16 22:35:35,423] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_20210517013445680935569127777218_0004_m_000045_333: needsTaskCommit() Task attempt_20210517013445680935569127777218_0004_m_000045_333: duration 0:00.000s
[2021-05-16 22:35:35,423] {docker.py:276} INFO - 21/05/17 01:35:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445680935569127777218_0004_m_000045_333
[2021-05-16 22:35:35,424] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 4544 bytes result sent to driver
[2021-05-16 22:35:35,425] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (5deb0a1bddc0, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:35,426] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 2659 ms on 5deb0a1bddc0 (executor driver) (47/200)
[2021-05-16 22:35:35,427] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-16 22:35:35,430] {docker.py:276} INFO - 21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:35,432] {docker.py:276} INFO - 21/05/17 01:35:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:35,432] {docker.py:276} INFO - 21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:35,433] {docker.py:276} INFO - 21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451505857596239822250_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451505857596239822250_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451505857596239822250_0004}; taskId=attempt_202105170134451505857596239822250_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5db6d529}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:35,433] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134451505857596239822250_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451505857596239822250_0004_m_000049_337
[2021-05-16 22:35:35,436] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134451505857596239822250_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451505857596239822250_0004_m_000049_337 : duration 0:00.003s
[2021-05-16 22:35:35,436] {docker.py:276} INFO - 21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:35,437] {docker.py:276} INFO - 21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:35,438] {docker.py:276} INFO - 21/05/17 01:35:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:35,439] {docker.py:276} INFO - 21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:35,439] {docker.py:276} INFO - 21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451607284719061487441_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451607284719061487441_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451607284719061487441_0004}; taskId=attempt_202105170134451607284719061487441_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@765b283d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:35,440] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134451607284719061487441_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451607284719061487441_0004_m_000050_338
[2021-05-16 22:35:35,442] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134451607284719061487441_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451607284719061487441_0004_m_000050_338 : duration 0:00.003s
[2021-05-16 22:35:35,648] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134454716049383362315978_0004_m_000047_335: needsTaskCommit() Task attempt_202105170134454716049383362315978_0004_m_000047_335
[2021-05-16 22:35:35,649] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134454716049383362315978_0004_m_000047_335: needsTaskCommit() Task attempt_202105170134454716049383362315978_0004_m_000047_335: duration 0:00.001s
21/05/17 01:35:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454716049383362315978_0004_m_000047_335
[2021-05-16 22:35:35,650] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 4544 bytes result sent to driver
[2021-05-16 22:35:35,652] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (5deb0a1bddc0, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:35,653] {docker.py:276} INFO - 21/05/17 01:35:35 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-16 22:35:35,653] {docker.py:276} INFO - 21/05/17 01:35:35 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 2824 ms on 5deb0a1bddc0 (executor driver) (48/200)
[2021-05-16 22:35:35,664] {docker.py:276} INFO - 21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:35,666] {docker.py:276} INFO - 21/05/17 01:35:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453146728885727781720_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453146728885727781720_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453146728885727781720_0004}; taskId=attempt_202105170134453146728885727781720_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f7b3ec5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:35 INFO StagingCommitter: Starting: Task committer attempt_202105170134453146728885727781720_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453146728885727781720_0004_m_000051_339
[2021-05-16 22:35:35,669] {docker.py:276} INFO - 21/05/17 01:35:35 INFO StagingCommitter: Task committer attempt_202105170134453146728885727781720_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453146728885727781720_0004_m_000051_339 : duration 0:00.003s
[2021-05-16 22:35:37,633] {docker.py:276} INFO - 21/05/17 01:35:37 INFO StagingCommitter: Starting: Task committer attempt_202105170134453497102227639643329_0004_m_000048_336: needsTaskCommit() Task attempt_202105170134453497102227639643329_0004_m_000048_336
21/05/17 01:35:37 INFO StagingCommitter: Task committer attempt_202105170134453497102227639643329_0004_m_000048_336: needsTaskCommit() Task attempt_202105170134453497102227639643329_0004_m_000048_336: duration 0:00.001s
[2021-05-16 22:35:37,634] {docker.py:276} INFO - 21/05/17 01:35:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453497102227639643329_0004_m_000048_336
[2021-05-16 22:35:37,636] {docker.py:276} INFO - 21/05/17 01:35:37 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 4544 bytes result sent to driver
[2021-05-16 22:35:37,636] {docker.py:276} INFO - 21/05/17 01:35:37 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (5deb0a1bddc0, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:37,638] {docker.py:276} INFO - 21/05/17 01:35:37 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
[2021-05-16 22:35:37,639] {docker.py:276} INFO - 21/05/17 01:35:37 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 2313 ms on 5deb0a1bddc0 (executor driver) (49/200)
[2021-05-16 22:35:37,648] {docker.py:276} INFO - 21/05/17 01:35:37 INFO ShuffleBlockFetcherIterator: Getting 5 (20.9 KiB) non-empty blocks including 5 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:37,651] {docker.py:276} INFO - 21/05/17 01:35:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455837424685446971116_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455837424685446971116_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455837424685446971116_0004}; taskId=attempt_202105170134455837424685446971116_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39608ec2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:37 INFO StagingCommitter: Starting: Task committer attempt_202105170134455837424685446971116_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455837424685446971116_0004_m_000052_340
[2021-05-16 22:35:37,654] {docker.py:276} INFO - 21/05/17 01:35:37 INFO StagingCommitter: Task committer attempt_202105170134455837424685446971116_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455837424685446971116_0004_m_000052_340 : duration 0:00.004s
[2021-05-16 22:35:38,032] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134451607284719061487441_0004_m_000050_338: needsTaskCommit() Task attempt_202105170134451607284719061487441_0004_m_000050_338
[2021-05-16 22:35:38,032] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Task committer attempt_202105170134451607284719061487441_0004_m_000050_338: needsTaskCommit() Task attempt_202105170134451607284719061487441_0004_m_000050_338: duration 0:00.000s
[2021-05-16 22:35:38,033] {docker.py:276} INFO - 21/05/17 01:35:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451607284719061487441_0004_m_000050_338
[2021-05-16 22:35:38,034] {docker.py:276} INFO - 21/05/17 01:35:38 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-16 22:35:38,035] {docker.py:276} INFO - 21/05/17 01:35:38 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (5deb0a1bddc0, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:38,036] {docker.py:276} INFO - 21/05/17 01:35:38 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 2615 ms on 5deb0a1bddc0 (executor driver) (50/200)
21/05/17 01:35:38 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-16 22:35:38,054] {docker.py:276} INFO - 21/05/17 01:35:38 INFO ShuffleBlockFetcherIterator: Getting 5 (24.7 KiB) non-empty blocks including 5 (24.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:38,056] {docker.py:276} INFO - 21/05/17 01:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:38,057] {docker.py:276} INFO - 21/05/17 01:35:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:38,058] {docker.py:276} INFO - 21/05/17 01:35:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458316305449153273113_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458316305449153273113_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458316305449153273113_0004}; taskId=attempt_202105170134458316305449153273113_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@412e3891}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:38,058] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134458316305449153273113_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458316305449153273113_0004_m_000053_341
[2021-05-16 22:35:38,062] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Task committer attempt_202105170134458316305449153273113_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458316305449153273113_0004_m_000053_341 : duration 0:00.004s
[2021-05-16 22:35:38,261] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134451505857596239822250_0004_m_000049_337: needsTaskCommit() Task attempt_202105170134451505857596239822250_0004_m_000049_337
[2021-05-16 22:35:38,262] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Task committer attempt_202105170134451505857596239822250_0004_m_000049_337: needsTaskCommit() Task attempt_202105170134451505857596239822250_0004_m_000049_337: duration 0:00.000s
21/05/17 01:35:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451505857596239822250_0004_m_000049_337
[2021-05-16 22:35:38,264] {docker.py:276} INFO - 21/05/17 01:35:38 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 4587 bytes result sent to driver
[2021-05-16 22:35:38,265] {docker.py:276} INFO - 21/05/17 01:35:38 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (5deb0a1bddc0, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:38,267] {docker.py:276} INFO - 21/05/17 01:35:38 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
21/05/17 01:35:38 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 2856 ms on 5deb0a1bddc0 (executor driver) (51/200)
[2021-05-16 22:35:38,277] {docker.py:276} INFO - 21/05/17 01:35:38 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:38,279] {docker.py:276} INFO - 21/05/17 01:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:38,279] {docker.py:276} INFO - 21/05/17 01:35:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458975362697916852867_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458975362697916852867_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458975362697916852867_0004}; taskId=attempt_202105170134458975362697916852867_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b835850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:38,280] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134458975362697916852867_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458975362697916852867_0004_m_000054_342
[2021-05-16 22:35:38,282] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Task committer attempt_202105170134458975362697916852867_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458975362697916852867_0004_m_000054_342 : duration 0:00.003s
[2021-05-16 22:35:38,594] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134453146728885727781720_0004_m_000051_339: needsTaskCommit() Task attempt_202105170134453146728885727781720_0004_m_000051_339
[2021-05-16 22:35:38,594] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Task committer attempt_202105170134453146728885727781720_0004_m_000051_339: needsTaskCommit() Task attempt_202105170134453146728885727781720_0004_m_000051_339: duration 0:00.001s
21/05/17 01:35:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453146728885727781720_0004_m_000051_339
[2021-05-16 22:35:38,595] {docker.py:276} INFO - 21/05/17 01:35:38 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 4587 bytes result sent to driver
[2021-05-16 22:35:38,596] {docker.py:276} INFO - 21/05/17 01:35:38 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (5deb0a1bddc0, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:38,597] {docker.py:276} INFO - 21/05/17 01:35:38 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
21/05/17 01:35:38 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 2949 ms on 5deb0a1bddc0 (executor driver) (52/200)
[2021-05-16 22:35:38,605] {docker.py:276} INFO - 21/05/17 01:35:38 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:38,608] {docker.py:276} INFO - 21/05/17 01:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:38,609] {docker.py:276} INFO - 21/05/17 01:35:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:38,609] {docker.py:276} INFO - 21/05/17 01:35:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454689664322700222897_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454689664322700222897_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454689664322700222897_0004}; taskId=attempt_202105170134454689664322700222897_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40331c3e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:38,610] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134454689664322700222897_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454689664322700222897_0004_m_000055_343
[2021-05-16 22:35:38,613] {docker.py:276} INFO - 21/05/17 01:35:38 INFO StagingCommitter: Task committer attempt_202105170134454689664322700222897_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454689664322700222897_0004_m_000055_343 : duration 0:00.003s
[2021-05-16 22:35:40,204] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134455837424685446971116_0004_m_000052_340: needsTaskCommit() Task attempt_202105170134455837424685446971116_0004_m_000052_340
[2021-05-16 22:35:40,204] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134455837424685446971116_0004_m_000052_340: needsTaskCommit() Task attempt_202105170134455837424685446971116_0004_m_000052_340: duration 0:00.000s
21/05/17 01:35:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455837424685446971116_0004_m_000052_340
[2021-05-16 22:35:40,206] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 4587 bytes result sent to driver
[2021-05-16 22:35:40,207] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (5deb0a1bddc0, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:40,208] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
[2021-05-16 22:35:40,208] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 2575 ms on 5deb0a1bddc0 (executor driver) (53/200)
[2021-05-16 22:35:40,216] {docker.py:276} INFO - 21/05/17 01:35:40 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:40,219] {docker.py:276} INFO - 21/05/17 01:35:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:40,219] {docker.py:276} INFO - 21/05/17 01:35:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455509191967904888670_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455509191967904888670_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455509191967904888670_0004}; taskId=attempt_202105170134455509191967904888670_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d1971bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:40,220] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134455509191967904888670_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455509191967904888670_0004_m_000056_344
[2021-05-16 22:35:40,222] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134455509191967904888670_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455509191967904888670_0004_m_000056_344 : duration 0:00.004s
[2021-05-16 22:35:40,761] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134458316305449153273113_0004_m_000053_341: needsTaskCommit() Task attempt_202105170134458316305449153273113_0004_m_000053_341
21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134458316305449153273113_0004_m_000053_341: needsTaskCommit() Task attempt_202105170134458316305449153273113_0004_m_000053_341: duration 0:00.000s
21/05/17 01:35:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458316305449153273113_0004_m_000053_341
[2021-05-16 22:35:40,763] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 4587 bytes result sent to driver
[2021-05-16 22:35:40,764] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (5deb0a1bddc0, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:40,764] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
[2021-05-16 22:35:40,765] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 2733 ms on 5deb0a1bddc0 (executor driver) (54/200)
[2021-05-16 22:35:40,774] {docker.py:276} INFO - 21/05/17 01:35:40 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:40,774] {docker.py:276} INFO - 21/05/17 01:35:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:40,776] {docker.py:276} INFO - 21/05/17 01:35:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:40,777] {docker.py:276} INFO - 21/05/17 01:35:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:40,777] {docker.py:276} INFO - 21/05/17 01:35:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457091235925484041476_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457091235925484041476_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457091235925484041476_0004}; taskId=attempt_202105170134457091235925484041476_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52631d3e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:40,777] {docker.py:276} INFO - 21/05/17 01:35:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:40,778] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134457091235925484041476_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457091235925484041476_0004_m_000057_345
[2021-05-16 22:35:40,781] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134457091235925484041476_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457091235925484041476_0004_m_000057_345 : duration 0:00.003s
[2021-05-16 22:35:40,922] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134458975362697916852867_0004_m_000054_342: needsTaskCommit() Task attempt_202105170134458975362697916852867_0004_m_000054_342
[2021-05-16 22:35:40,924] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134458975362697916852867_0004_m_000054_342: needsTaskCommit() Task attempt_202105170134458975362697916852867_0004_m_000054_342: duration 0:00.002s
21/05/17 01:35:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458975362697916852867_0004_m_000054_342
[2021-05-16 22:35:40,926] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-16 22:35:40,927] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (5deb0a1bddc0, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:40,928] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 2666 ms on 5deb0a1bddc0 (executor driver) (55/200)
[2021-05-16 22:35:40,929] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-16 22:35:40,938] {docker.py:276} INFO - 21/05/17 01:35:40 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:40,941] {docker.py:276} INFO - 21/05/17 01:35:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:40,941] {docker.py:276} INFO - 21/05/17 01:35:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:40,942] {docker.py:276} INFO - 21/05/17 01:35:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458794770408897105260_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458794770408897105260_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458794770408897105260_0004}; taskId=attempt_202105170134458794770408897105260_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@161d7ac2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:40,943] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134458794770408897105260_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458794770408897105260_0004_m_000058_346
[2021-05-16 22:35:40,947] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134458794770408897105260_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458794770408897105260_0004_m_000058_346 : duration 0:00.005s
[2021-05-16 22:35:40,980] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134454689664322700222897_0004_m_000055_343: needsTaskCommit() Task attempt_202105170134454689664322700222897_0004_m_000055_343
[2021-05-16 22:35:40,982] {docker.py:276} INFO - 21/05/17 01:35:40 INFO StagingCommitter: Task committer attempt_202105170134454689664322700222897_0004_m_000055_343: needsTaskCommit() Task attempt_202105170134454689664322700222897_0004_m_000055_343: duration 0:00.001s
21/05/17 01:35:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454689664322700222897_0004_m_000055_343
[2021-05-16 22:35:40,983] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-16 22:35:40,984] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (5deb0a1bddc0, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:40,985] {docker.py:276} INFO - 21/05/17 01:35:40 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-16 22:35:40,986] {docker.py:276} INFO - 21/05/17 01:35:40 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 2392 ms on 5deb0a1bddc0 (executor driver) (56/200)
[2021-05-16 22:35:40,996] {docker.py:276} INFO - 21/05/17 01:35:41 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:40,998] {docker.py:276} INFO - 21/05/17 01:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:40,998] {docker.py:276} INFO - 21/05/17 01:35:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:40,999] {docker.py:276} INFO - 21/05/17 01:35:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452480656863281373779_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452480656863281373779_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452480656863281373779_0004}; taskId=attempt_202105170134452480656863281373779_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14e65be2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:40,999] {docker.py:276} INFO - 21/05/17 01:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:40,999] {docker.py:276} INFO - 21/05/17 01:35:41 INFO StagingCommitter: Starting: Task committer attempt_202105170134452480656863281373779_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452480656863281373779_0004_m_000059_347
[2021-05-16 22:35:41,002] {docker.py:276} INFO - 21/05/17 01:35:41 INFO StagingCommitter: Task committer attempt_202105170134452480656863281373779_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452480656863281373779_0004_m_000059_347 : duration 0:00.003s
[2021-05-16 22:35:42,847] {docker.py:276} INFO - 21/05/17 01:35:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134455509191967904888670_0004_m_000056_344: needsTaskCommit() Task attempt_202105170134455509191967904888670_0004_m_000056_344
[2021-05-16 22:35:42,848] {docker.py:276} INFO - 21/05/17 01:35:42 INFO StagingCommitter: Task committer attempt_202105170134455509191967904888670_0004_m_000056_344: needsTaskCommit() Task attempt_202105170134455509191967904888670_0004_m_000056_344: duration 0:00.001s
21/05/17 01:35:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455509191967904888670_0004_m_000056_344
[2021-05-16 22:35:42,851] {docker.py:276} INFO - 21/05/17 01:35:42 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 4544 bytes result sent to driver
[2021-05-16 22:35:42,852] {docker.py:276} INFO - 21/05/17 01:35:42 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (5deb0a1bddc0, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:42,853] {docker.py:276} INFO - 21/05/17 01:35:42 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 2649 ms on 5deb0a1bddc0 (executor driver) (57/200)
[2021-05-16 22:35:42,854] {docker.py:276} INFO - 21/05/17 01:35:42 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
[2021-05-16 22:35:42,865] {docker.py:276} INFO - 21/05/17 01:35:42 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:42,867] {docker.py:276} INFO - 21/05/17 01:35:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457148200468037364273_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457148200468037364273_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457148200468037364273_0004}; taskId=attempt_202105170134457148200468037364273_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e908637}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:42,868] {docker.py:276} INFO - 21/05/17 01:35:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134457148200468037364273_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457148200468037364273_0004_m_000060_348
[2021-05-16 22:35:42,870] {docker.py:276} INFO - 21/05/17 01:35:42 INFO StagingCommitter: Task committer attempt_202105170134457148200468037364273_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457148200468037364273_0004_m_000060_348 : duration 0:00.003s
[2021-05-16 22:35:43,028] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Starting: Task committer attempt_202105170134458794770408897105260_0004_m_000058_346: needsTaskCommit() Task attempt_202105170134458794770408897105260_0004_m_000058_346
[2021-05-16 22:35:43,029] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Task committer attempt_202105170134458794770408897105260_0004_m_000058_346: needsTaskCommit() Task attempt_202105170134458794770408897105260_0004_m_000058_346: duration 0:00.000s
21/05/17 01:35:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458794770408897105260_0004_m_000058_346
[2021-05-16 22:35:43,030] {docker.py:276} INFO - 21/05/17 01:35:43 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 4544 bytes result sent to driver
[2021-05-16 22:35:43,032] {docker.py:276} INFO - 21/05/17 01:35:43 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (5deb0a1bddc0, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:43,032] {docker.py:276} INFO - 21/05/17 01:35:43 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-16 22:35:43,033] {docker.py:276} INFO - 21/05/17 01:35:43 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 2110 ms on 5deb0a1bddc0 (executor driver) (58/200)
[2021-05-16 22:35:43,042] {docker.py:276} INFO - 21/05/17 01:35:43 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:43,043] {docker.py:276} INFO - 21/05/17 01:35:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:43,044] {docker.py:276} INFO - 21/05/17 01:35:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454666183358586432250_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454666183358586432250_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454666183358586432250_0004}; taskId=attempt_202105170134454666183358586432250_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31a34ffe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:43 INFO StagingCommitter: Starting: Task committer attempt_202105170134454666183358586432250_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454666183358586432250_0004_m_000061_349
[2021-05-16 22:35:43,046] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Task committer attempt_202105170134454666183358586432250_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454666183358586432250_0004_m_000061_349 : duration 0:00.002s
[2021-05-16 22:35:43,359] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Starting: Task committer attempt_202105170134457091235925484041476_0004_m_000057_345: needsTaskCommit() Task attempt_202105170134457091235925484041476_0004_m_000057_345
[2021-05-16 22:35:43,360] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Task committer attempt_202105170134457091235925484041476_0004_m_000057_345: needsTaskCommit() Task attempt_202105170134457091235925484041476_0004_m_000057_345: duration 0:00.001s
21/05/17 01:35:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457091235925484041476_0004_m_000057_345
[2021-05-16 22:35:43,363] {docker.py:276} INFO - 21/05/17 01:35:43 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 4544 bytes result sent to driver
[2021-05-16 22:35:43,364] {docker.py:276} INFO - 21/05/17 01:35:43 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (5deb0a1bddc0, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:43,365] {docker.py:276} INFO - 21/05/17 01:35:43 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-16 22:35:43,365] {docker.py:276} INFO - 21/05/17 01:35:43 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 2605 ms on 5deb0a1bddc0 (executor driver) (59/200)
[2021-05-16 22:35:43,385] {docker.py:276} INFO - 21/05/17 01:35:43 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:35:43,385] {docker.py:276} INFO - 21/05/17 01:35:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:43,388] {docker.py:276} INFO - 21/05/17 01:35:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:43,388] {docker.py:276} INFO - 21/05/17 01:35:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445327950499004155104_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445327950499004155104_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445327950499004155104_0004}; taskId=attempt_20210517013445327950499004155104_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3901ae04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:43,389] {docker.py:276} INFO - 21/05/17 01:35:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:43,389] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Starting: Task committer attempt_20210517013445327950499004155104_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445327950499004155104_0004_m_000062_350
[2021-05-16 22:35:43,393] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Task committer attempt_20210517013445327950499004155104_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445327950499004155104_0004_m_000062_350 : duration 0:00.004s
[2021-05-16 22:35:43,578] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Starting: Task committer attempt_202105170134452480656863281373779_0004_m_000059_347: needsTaskCommit() Task attempt_202105170134452480656863281373779_0004_m_000059_347
[2021-05-16 22:35:43,579] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Task committer attempt_202105170134452480656863281373779_0004_m_000059_347: needsTaskCommit() Task attempt_202105170134452480656863281373779_0004_m_000059_347: duration 0:00.002s
21/05/17 01:35:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452480656863281373779_0004_m_000059_347
[2021-05-16 22:35:43,581] {docker.py:276} INFO - 21/05/17 01:35:43 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-16 22:35:43,583] {docker.py:276} INFO - 21/05/17 01:35:43 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (5deb0a1bddc0, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:43,585] {docker.py:276} INFO - 21/05/17 01:35:43 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 2604 ms on 5deb0a1bddc0 (executor driver) (60/200)
[2021-05-16 22:35:43,585] {docker.py:276} INFO - 21/05/17 01:35:43 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
[2021-05-16 22:35:43,595] {docker.py:276} INFO - 21/05/17 01:35:43 INFO ShuffleBlockFetcherIterator: Getting 5 (22.3 KiB) non-empty blocks including 5 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:43,598] {docker.py:276} INFO - 21/05/17 01:35:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:43,598] {docker.py:276} INFO - 21/05/17 01:35:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458151375804891015808_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458151375804891015808_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458151375804891015808_0004}; taskId=attempt_202105170134458151375804891015808_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48e66858}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:43 INFO StagingCommitter: Starting: Task committer attempt_202105170134458151375804891015808_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458151375804891015808_0004_m_000063_351
[2021-05-16 22:35:43,602] {docker.py:276} INFO - 21/05/17 01:35:43 INFO StagingCommitter: Task committer attempt_202105170134458151375804891015808_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458151375804891015808_0004_m_000063_351 : duration 0:00.004s
[2021-05-16 22:35:45,568] {docker.py:276} INFO - 21/05/17 01:35:45 INFO StagingCommitter: Starting: Task committer attempt_202105170134454666183358586432250_0004_m_000061_349: needsTaskCommit() Task attempt_202105170134454666183358586432250_0004_m_000061_349
[2021-05-16 22:35:45,569] {docker.py:276} INFO - 21/05/17 01:35:45 INFO StagingCommitter: Task committer attempt_202105170134454666183358586432250_0004_m_000061_349: needsTaskCommit() Task attempt_202105170134454666183358586432250_0004_m_000061_349: duration 0:00.001s
21/05/17 01:35:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454666183358586432250_0004_m_000061_349
[2021-05-16 22:35:45,570] {docker.py:276} INFO - 21/05/17 01:35:45 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 4587 bytes result sent to driver
[2021-05-16 22:35:45,572] {docker.py:276} INFO - 21/05/17 01:35:45 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (5deb0a1bddc0, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:45,572] {docker.py:276} INFO - 21/05/17 01:35:45 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 2544 ms on 5deb0a1bddc0 (executor driver) (61/200)
[2021-05-16 22:35:45,574] {docker.py:276} INFO - 21/05/17 01:35:45 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-16 22:35:45,585] {docker.py:276} INFO - 21/05/17 01:35:45 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:45,587] {docker.py:276} INFO - 21/05/17 01:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:45,588] {docker.py:276} INFO - 21/05/17 01:35:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451228782282327801224_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451228782282327801224_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451228782282327801224_0004}; taskId=attempt_202105170134451228782282327801224_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3bf74b3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:45,588] {docker.py:276} INFO - 21/05/17 01:35:45 INFO StagingCommitter: Starting: Task committer attempt_202105170134451228782282327801224_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451228782282327801224_0004_m_000064_352
[2021-05-16 22:35:45,590] {docker.py:276} INFO - 21/05/17 01:35:45 INFO StagingCommitter: Task committer attempt_202105170134451228782282327801224_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451228782282327801224_0004_m_000064_352 : duration 0:00.003s
[2021-05-16 22:35:45,988] {docker.py:276} INFO - 21/05/17 01:35:46 INFO StagingCommitter: Starting: Task committer attempt_202105170134457148200468037364273_0004_m_000060_348: needsTaskCommit() Task attempt_202105170134457148200468037364273_0004_m_000060_348
[2021-05-16 22:35:45,988] {docker.py:276} INFO - 21/05/17 01:35:46 INFO StagingCommitter: Task committer attempt_202105170134457148200468037364273_0004_m_000060_348: needsTaskCommit() Task attempt_202105170134457148200468037364273_0004_m_000060_348: duration 0:00.000s
21/05/17 01:35:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457148200468037364273_0004_m_000060_348
[2021-05-16 22:35:45,989] {docker.py:276} INFO - 21/05/17 01:35:46 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 4587 bytes result sent to driver
[2021-05-16 22:35:45,990] {docker.py:276} INFO - 21/05/17 01:35:46 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (5deb0a1bddc0, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:35:46 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 3143 ms on 5deb0a1bddc0 (executor driver) (62/200)
[2021-05-16 22:35:45,992] {docker.py:276} INFO - 21/05/17 01:35:46 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-16 22:35:46,002] {docker.py:276} INFO - 21/05/17 01:35:46 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:46,004] {docker.py:276} INFO - 21/05/17 01:35:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445561881785706178083_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445561881785706178083_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445561881785706178083_0004}; taskId=attempt_20210517013445561881785706178083_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a007f65}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:46 INFO StagingCommitter: Starting: Task committer attempt_20210517013445561881785706178083_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445561881785706178083_0004_m_000065_353
[2021-05-16 22:35:46,008] {docker.py:276} INFO - 21/05/17 01:35:46 INFO StagingCommitter: Task committer attempt_20210517013445561881785706178083_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445561881785706178083_0004_m_000065_353 : duration 0:00.003s
[2021-05-16 22:35:46,178] {docker.py:276} INFO - 21/05/17 01:35:46 INFO StagingCommitter: Starting: Task committer attempt_20210517013445327950499004155104_0004_m_000062_350: needsTaskCommit() Task attempt_20210517013445327950499004155104_0004_m_000062_350
21/05/17 01:35:46 INFO StagingCommitter: Starting: Task committer attempt_202105170134458151375804891015808_0004_m_000063_351: needsTaskCommit() Task attempt_202105170134458151375804891015808_0004_m_000063_351
21/05/17 01:35:46 INFO StagingCommitter: Task committer attempt_20210517013445327950499004155104_0004_m_000062_350: needsTaskCommit() Task attempt_20210517013445327950499004155104_0004_m_000062_350: duration 0:00.000s
21/05/17 01:35:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445327950499004155104_0004_m_000062_350
21/05/17 01:35:46 INFO StagingCommitter: Task committer attempt_202105170134458151375804891015808_0004_m_000063_351: needsTaskCommit() Task attempt_202105170134458151375804891015808_0004_m_000063_351: duration 0:00.000s
21/05/17 01:35:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458151375804891015808_0004_m_000063_351
[2021-05-16 22:35:46,179] {docker.py:276} INFO - 21/05/17 01:35:46 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 4587 bytes result sent to driver
21/05/17 01:35:46 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-16 22:35:46,180] {docker.py:276} INFO - 21/05/17 01:35:46 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (5deb0a1bddc0, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:46,181] {docker.py:276} INFO - 21/05/17 01:35:46 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
21/05/17 01:35:46 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (5deb0a1bddc0, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:46,182] {docker.py:276} INFO - 21/05/17 01:35:46 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-16 22:35:46,184] {docker.py:276} INFO - 21/05/17 01:35:46 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 2823 ms on 5deb0a1bddc0 (executor driver) (63/200)
21/05/17 01:35:46 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 2604 ms on 5deb0a1bddc0 (executor driver) (64/200)
[2021-05-16 22:35:46,189] {docker.py:276} INFO - 21/05/17 01:35:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.6 KiB) non-empty blocks including 5 (23.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:46,192] {docker.py:276} INFO - 21/05/17 01:35:46 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 01:35:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:46,193] {docker.py:276} INFO - 21/05/17 01:35:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452314108056741558244_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452314108056741558244_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452314108056741558244_0004}; taskId=attempt_202105170134452314108056741558244_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c3d7328}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:46 INFO StagingCommitter: Starting: Task committer attempt_202105170134452314108056741558244_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452314108056741558244_0004_m_000066_354
[2021-05-16 22:35:46,193] {docker.py:276} INFO - 21/05/17 01:35:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:46,194] {docker.py:276} INFO - 21/05/17 01:35:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455189505134273991337_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455189505134273991337_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455189505134273991337_0004}; taskId=attempt_202105170134455189505134273991337_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3da8671c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:46 INFO StagingCommitter: Starting: Task committer attempt_202105170134455189505134273991337_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455189505134273991337_0004_m_000067_355
[2021-05-16 22:35:46,195] {docker.py:276} INFO - 21/05/17 01:35:46 INFO StagingCommitter: Task committer attempt_202105170134452314108056741558244_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452314108056741558244_0004_m_000066_354 : duration 0:00.003s
[2021-05-16 22:35:46,196] {docker.py:276} INFO - 21/05/17 01:35:46 INFO StagingCommitter: Task committer attempt_202105170134455189505134273991337_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455189505134273991337_0004_m_000067_355 : duration 0:00.003s
[2021-05-16 22:35:48,261] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134451228782282327801224_0004_m_000064_352: needsTaskCommit() Task attempt_202105170134451228782282327801224_0004_m_000064_352
[2021-05-16 22:35:48,262] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_202105170134451228782282327801224_0004_m_000064_352: needsTaskCommit() Task attempt_202105170134451228782282327801224_0004_m_000064_352: duration 0:00.000s
21/05/17 01:35:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451228782282327801224_0004_m_000064_352
[2021-05-16 22:35:48,263] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-16 22:35:48,264] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (5deb0a1bddc0, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:48,265] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 2697 ms on 5deb0a1bddc0 (executor driver) (65/200)
[2021-05-16 22:35:48,266] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-16 22:35:48,275] {docker.py:276} INFO - 21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:48,277] {docker.py:276} INFO - 21/05/17 01:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454703937207286488694_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454703937207286488694_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454703937207286488694_0004}; taskId=attempt_202105170134454703937207286488694_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@312b2a80}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:48,278] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134454703937207286488694_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454703937207286488694_0004_m_000068_356
[2021-05-16 22:35:48,280] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_202105170134454703937207286488694_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454703937207286488694_0004_m_000068_356 : duration 0:00.003s
[2021-05-16 22:35:48,487] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134452314108056741558244_0004_m_000066_354: needsTaskCommit() Task attempt_202105170134452314108056741558244_0004_m_000066_354
[2021-05-16 22:35:48,488] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_202105170134452314108056741558244_0004_m_000066_354: needsTaskCommit() Task attempt_202105170134452314108056741558244_0004_m_000066_354: duration 0:00.001s
[2021-05-16 22:35:48,490] {docker.py:276} INFO - 21/05/17 01:35:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452314108056741558244_0004_m_000066_354
[2021-05-16 22:35:48,490] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-16 22:35:48,491] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (5deb0a1bddc0, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:48,493] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 2316 ms on 5deb0a1bddc0 (executor driver) (66/200)
[2021-05-16 22:35:48,494] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-16 22:35:48,502] {docker.py:276} INFO - 21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:48,505] {docker.py:276} INFO - 21/05/17 01:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452671122977477520875_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452671122977477520875_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452671122977477520875_0004}; taskId=attempt_202105170134452671122977477520875_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c35c31a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:48,505] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134452671122977477520875_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452671122977477520875_0004_m_000069_357
[2021-05-16 22:35:48,508] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_202105170134452671122977477520875_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452671122977477520875_0004_m_000069_357 : duration 0:00.003s
[2021-05-16 22:35:48,513] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_20210517013445561881785706178083_0004_m_000065_353: needsTaskCommit() Task attempt_20210517013445561881785706178083_0004_m_000065_353
21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_20210517013445561881785706178083_0004_m_000065_353: needsTaskCommit() Task attempt_20210517013445561881785706178083_0004_m_000065_353: duration 0:00.000s
21/05/17 01:35:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445561881785706178083_0004_m_000065_353
[2021-05-16 22:35:48,514] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-16 22:35:48,515] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (5deb0a1bddc0, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:48,516] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 2529 ms on 5deb0a1bddc0 (executor driver) (67/200)
[2021-05-16 22:35:48,516] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-16 22:35:48,524] {docker.py:276} INFO - 21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:48,525] {docker.py:276} INFO - 21/05/17 01:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451007518966648779106_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451007518966648779106_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451007518966648779106_0004}; taskId=attempt_202105170134451007518966648779106_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c1c439e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:48,525] {docker.py:276} INFO - 21/05/17 01:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134451007518966648779106_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451007518966648779106_0004_m_000070_358
[2021-05-16 22:35:48,527] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_202105170134451007518966648779106_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451007518966648779106_0004_m_000070_358 : duration 0:00.003s
[2021-05-16 22:35:48,825] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134455189505134273991337_0004_m_000067_355: needsTaskCommit() Task attempt_202105170134455189505134273991337_0004_m_000067_355
[2021-05-16 22:35:48,826] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_202105170134455189505134273991337_0004_m_000067_355: needsTaskCommit() Task attempt_202105170134455189505134273991337_0004_m_000067_355: duration 0:00.001s
21/05/17 01:35:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455189505134273991337_0004_m_000067_355
[2021-05-16 22:35:48,828] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 4544 bytes result sent to driver
[2021-05-16 22:35:48,829] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (5deb0a1bddc0, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:48,830] {docker.py:276} INFO - 21/05/17 01:35:48 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
[2021-05-16 22:35:48,831] {docker.py:276} INFO - 21/05/17 01:35:48 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 2653 ms on 5deb0a1bddc0 (executor driver) (68/200)
[2021-05-16 22:35:48,842] {docker.py:276} INFO - 21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:48,843] {docker.py:276} INFO - 21/05/17 01:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445317105149686979269_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445317105149686979269_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445317105149686979269_0004}; taskId=attempt_20210517013445317105149686979269_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c9059a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:48 INFO StagingCommitter: Starting: Task committer attempt_20210517013445317105149686979269_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445317105149686979269_0004_m_000071_359
[2021-05-16 22:35:48,846] {docker.py:276} INFO - 21/05/17 01:35:48 INFO StagingCommitter: Task committer attempt_20210517013445317105149686979269_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445317105149686979269_0004_m_000071_359 : duration 0:00.003s
[2021-05-16 22:35:50,912] {docker.py:276} INFO - 21/05/17 01:35:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134454703937207286488694_0004_m_000068_356: needsTaskCommit() Task attempt_202105170134454703937207286488694_0004_m_000068_356
21/05/17 01:35:50 INFO StagingCommitter: Task committer attempt_202105170134454703937207286488694_0004_m_000068_356: needsTaskCommit() Task attempt_202105170134454703937207286488694_0004_m_000068_356: duration 0:00.000s
21/05/17 01:35:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454703937207286488694_0004_m_000068_356
[2021-05-16 22:35:50,913] {docker.py:276} INFO - 21/05/17 01:35:50 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 4587 bytes result sent to driver
[2021-05-16 22:35:50,914] {docker.py:276} INFO - 21/05/17 01:35:50 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (5deb0a1bddc0, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:50,915] {docker.py:276} INFO - 21/05/17 01:35:50 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-16 22:35:50,916] {docker.py:276} INFO - 21/05/17 01:35:50 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 2655 ms on 5deb0a1bddc0 (executor driver) (69/200)
[2021-05-16 22:35:50,926] {docker.py:276} INFO - 21/05/17 01:35:50 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:50,928] {docker.py:276} INFO - 21/05/17 01:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455120024544461641397_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455120024544461641397_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455120024544461641397_0004}; taskId=attempt_202105170134455120024544461641397_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@333b4b6b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134455120024544461641397_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455120024544461641397_0004_m_000072_360
[2021-05-16 22:35:50,931] {docker.py:276} INFO - 21/05/17 01:35:50 INFO StagingCommitter: Task committer attempt_202105170134455120024544461641397_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455120024544461641397_0004_m_000072_360 : duration 0:00.003s
[2021-05-16 22:35:51,117] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Starting: Task committer attempt_202105170134451007518966648779106_0004_m_000070_358: needsTaskCommit() Task attempt_202105170134451007518966648779106_0004_m_000070_358
[2021-05-16 22:35:51,117] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Task committer attempt_202105170134451007518966648779106_0004_m_000070_358: needsTaskCommit() Task attempt_202105170134451007518966648779106_0004_m_000070_358: duration 0:00.000s
21/05/17 01:35:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451007518966648779106_0004_m_000070_358
[2021-05-16 22:35:51,119] {docker.py:276} INFO - 21/05/17 01:35:51 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 4587 bytes result sent to driver
[2021-05-16 22:35:51,121] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Starting: Task committer attempt_202105170134452671122977477520875_0004_m_000069_357: needsTaskCommit() Task attempt_202105170134452671122977477520875_0004_m_000069_357
21/05/17 01:35:51 INFO StagingCommitter: Task committer attempt_202105170134452671122977477520875_0004_m_000069_357: needsTaskCommit() Task attempt_202105170134452671122977477520875_0004_m_000069_357: duration 0:00.001s
21/05/17 01:35:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452671122977477520875_0004_m_000069_357
[2021-05-16 22:35:51,122] {docker.py:276} INFO - 21/05/17 01:35:51 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (5deb0a1bddc0, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:35:51 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 4587 bytes result sent to driver
[2021-05-16 22:35:51,123] {docker.py:276} INFO - 21/05/17 01:35:51 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-16 22:35:51,125] {docker.py:276} INFO - 21/05/17 01:35:51 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (5deb0a1bddc0, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:35:51 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-16 22:35:51,126] {docker.py:276} INFO - 21/05/17 01:35:51 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 2614 ms on 5deb0a1bddc0 (executor driver) (70/200)
[2021-05-16 22:35:51,128] {docker.py:276} INFO - 21/05/17 01:35:51 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 2640 ms on 5deb0a1bddc0 (executor driver) (71/200)
[2021-05-16 22:35:51,134] {docker.py:276} INFO - 21/05/17 01:35:51 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:51,135] {docker.py:276} INFO - 21/05/17 01:35:51 INFO ShuffleBlockFetcherIterator: Getting 5 (21.5 KiB) non-empty blocks including 5 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:51,136] {docker.py:276} INFO - 21/05/17 01:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:51,137] {docker.py:276} INFO - 21/05/17 01:35:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452261116681152800330_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452261116681152800330_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452261116681152800330_0004}; taskId=attempt_202105170134452261116681152800330_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55d32fc3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:51,137] {docker.py:276} INFO - 21/05/17 01:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:51 INFO StagingCommitter: Starting: Task committer attempt_202105170134452261116681152800330_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452261116681152800330_0004_m_000073_361
[2021-05-16 22:35:51,138] {docker.py:276} INFO - 21/05/17 01:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:51,139] {docker.py:276} INFO - 21/05/17 01:35:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457915796168124486825_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457915796168124486825_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457915796168124486825_0004}; taskId=attempt_202105170134457915796168124486825_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@138060b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:51,140] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Starting: Task committer attempt_202105170134457915796168124486825_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457915796168124486825_0004_m_000074_362
[2021-05-16 22:35:51,141] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Task committer attempt_202105170134452261116681152800330_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452261116681152800330_0004_m_000073_361 : duration 0:00.004s
[2021-05-16 22:35:51,145] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Task committer attempt_202105170134457915796168124486825_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457915796168124486825_0004_m_000074_362 : duration 0:00.006s
[2021-05-16 22:35:51,473] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Starting: Task committer attempt_20210517013445317105149686979269_0004_m_000071_359: needsTaskCommit() Task attempt_20210517013445317105149686979269_0004_m_000071_359
[2021-05-16 22:35:51,473] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Task committer attempt_20210517013445317105149686979269_0004_m_000071_359: needsTaskCommit() Task attempt_20210517013445317105149686979269_0004_m_000071_359: duration 0:00.001s
21/05/17 01:35:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445317105149686979269_0004_m_000071_359
[2021-05-16 22:35:51,476] {docker.py:276} INFO - 21/05/17 01:35:51 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 4587 bytes result sent to driver
[2021-05-16 22:35:51,477] {docker.py:276} INFO - 21/05/17 01:35:51 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (5deb0a1bddc0, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:51,478] {docker.py:276} INFO - 21/05/17 01:35:51 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-16 22:35:51,479] {docker.py:276} INFO - 21/05/17 01:35:51 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 2652 ms on 5deb0a1bddc0 (executor driver) (72/200)
[2021-05-16 22:35:51,488] {docker.py:276} INFO - 21/05/17 01:35:51 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:51,490] {docker.py:276} INFO - 21/05/17 01:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455200024089488287989_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455200024089488287989_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455200024089488287989_0004}; taskId=attempt_202105170134455200024089488287989_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29c36a6a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:51 INFO StagingCommitter: Starting: Task committer attempt_202105170134455200024089488287989_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455200024089488287989_0004_m_000075_363
[2021-05-16 22:35:51,493] {docker.py:276} INFO - 21/05/17 01:35:51 INFO StagingCommitter: Task committer attempt_202105170134455200024089488287989_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455200024089488287989_0004_m_000075_363 : duration 0:00.003s
[2021-05-16 22:35:53,545] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134455120024544461641397_0004_m_000072_360: needsTaskCommit() Task attempt_202105170134455120024544461641397_0004_m_000072_360
[2021-05-16 22:35:53,546] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Task committer attempt_202105170134455120024544461641397_0004_m_000072_360: needsTaskCommit() Task attempt_202105170134455120024544461641397_0004_m_000072_360: duration 0:00.002s
21/05/17 01:35:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455120024544461641397_0004_m_000072_360
[2021-05-16 22:35:53,549] {docker.py:276} INFO - 21/05/17 01:35:53 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 4544 bytes result sent to driver
[2021-05-16 22:35:53,550] {docker.py:276} INFO - 21/05/17 01:35:53 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (5deb0a1bddc0, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:53,551] {docker.py:276} INFO - 21/05/17 01:35:53 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
21/05/17 01:35:53 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 2641 ms on 5deb0a1bddc0 (executor driver) (73/200)
[2021-05-16 22:35:53,561] {docker.py:276} INFO - 21/05/17 01:35:53 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:53,563] {docker.py:276} INFO - 21/05/17 01:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457521548542771722404_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457521548542771722404_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457521548542771722404_0004}; taskId=attempt_202105170134457521548542771722404_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@354c91a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:53,564] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134457521548542771722404_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457521548542771722404_0004_m_000076_364
[2021-05-16 22:35:53,567] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Task committer attempt_202105170134457521548542771722404_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457521548542771722404_0004_m_000076_364 : duration 0:00.003s
[2021-05-16 22:35:53,775] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134452261116681152800330_0004_m_000073_361: needsTaskCommit() Task attempt_202105170134452261116681152800330_0004_m_000073_361
[2021-05-16 22:35:53,776] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Task committer attempt_202105170134452261116681152800330_0004_m_000073_361: needsTaskCommit() Task attempt_202105170134452261116681152800330_0004_m_000073_361: duration 0:00.001s
21/05/17 01:35:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452261116681152800330_0004_m_000073_361
[2021-05-16 22:35:53,778] {docker.py:276} INFO - 21/05/17 01:35:53 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 4544 bytes result sent to driver
[2021-05-16 22:35:53,779] {docker.py:276} INFO - 21/05/17 01:35:53 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (5deb0a1bddc0, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:53,780] {docker.py:276} INFO - 21/05/17 01:35:53 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
[2021-05-16 22:35:53,781] {docker.py:276} INFO - 21/05/17 01:35:53 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 2664 ms on 5deb0a1bddc0 (executor driver) (74/200)
[2021-05-16 22:35:53,791] {docker.py:276} INFO - 21/05/17 01:35:53 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:53,793] {docker.py:276} INFO - 21/05/17 01:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134459204968384615310440_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459204968384615310440_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134459204968384615310440_0004}; taskId=attempt_202105170134459204968384615310440_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@631d87}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:53,794] {docker.py:276} INFO - 21/05/17 01:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:53,794] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134459204968384615310440_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459204968384615310440_0004_m_000077_365
[2021-05-16 22:35:53,796] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Task committer attempt_202105170134459204968384615310440_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459204968384615310440_0004_m_000077_365 : duration 0:00.002s
[2021-05-16 22:35:53,797] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134457915796168124486825_0004_m_000074_362: needsTaskCommit() Task attempt_202105170134457915796168124486825_0004_m_000074_362
[2021-05-16 22:35:53,798] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Task committer attempt_202105170134457915796168124486825_0004_m_000074_362: needsTaskCommit() Task attempt_202105170134457915796168124486825_0004_m_000074_362: duration 0:00.000s
21/05/17 01:35:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457915796168124486825_0004_m_000074_362
[2021-05-16 22:35:53,799] {docker.py:276} INFO - 21/05/17 01:35:53 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 4544 bytes result sent to driver
[2021-05-16 22:35:53,800] {docker.py:276} INFO - 21/05/17 01:35:53 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (5deb0a1bddc0, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:53,801] {docker.py:276} INFO - 21/05/17 01:35:53 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 2679 ms on 5deb0a1bddc0 (executor driver) (75/200)
[2021-05-16 22:35:53,801] {docker.py:276} INFO - 21/05/17 01:35:53 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-16 22:35:53,808] {docker.py:276} INFO - 21/05/17 01:35:53 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:53,810] {docker.py:276} INFO - 21/05/17 01:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458221888278513750076_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458221888278513750076_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458221888278513750076_0004}; taskId=attempt_202105170134458221888278513750076_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45512781}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134458221888278513750076_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458221888278513750076_0004_m_000078_366
[2021-05-16 22:35:53,812] {docker.py:276} INFO - 21/05/17 01:35:53 INFO StagingCommitter: Task committer attempt_202105170134458221888278513750076_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458221888278513750076_0004_m_000078_366 : duration 0:00.003s
[2021-05-16 22:35:54,100] {docker.py:276} INFO - 21/05/17 01:35:54 INFO StagingCommitter: Starting: Task committer attempt_202105170134455200024089488287989_0004_m_000075_363: needsTaskCommit() Task attempt_202105170134455200024089488287989_0004_m_000075_363
21/05/17 01:35:54 INFO StagingCommitter: Task committer attempt_202105170134455200024089488287989_0004_m_000075_363: needsTaskCommit() Task attempt_202105170134455200024089488287989_0004_m_000075_363: duration 0:00.001s
21/05/17 01:35:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455200024089488287989_0004_m_000075_363
[2021-05-16 22:35:54,102] {docker.py:276} INFO - 21/05/17 01:35:54 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 4544 bytes result sent to driver
[2021-05-16 22:35:54,103] {docker.py:276} INFO - 21/05/17 01:35:54 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (5deb0a1bddc0, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:54,104] {docker.py:276} INFO - 21/05/17 01:35:54 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 2632 ms on 5deb0a1bddc0 (executor driver) (76/200)
[2021-05-16 22:35:54,105] {docker.py:276} INFO - 21/05/17 01:35:54 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
[2021-05-16 22:35:54,114] {docker.py:276} INFO - 21/05/17 01:35:54 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:54,116] {docker.py:276} INFO - 21/05/17 01:35:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455266138751343498289_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455266138751343498289_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455266138751343498289_0004}; taskId=attempt_202105170134455266138751343498289_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b5ed21b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:54,116] {docker.py:276} INFO - 21/05/17 01:35:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:54,116] {docker.py:276} INFO - 21/05/17 01:35:54 INFO StagingCommitter: Starting: Task committer attempt_202105170134455266138751343498289_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455266138751343498289_0004_m_000079_367
[2021-05-16 22:35:54,119] {docker.py:276} INFO - 21/05/17 01:35:54 INFO StagingCommitter: Task committer attempt_202105170134455266138751343498289_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455266138751343498289_0004_m_000079_367 : duration 0:00.003s
[2021-05-16 22:35:56,348] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134459204968384615310440_0004_m_000077_365: needsTaskCommit() Task attempt_202105170134459204968384615310440_0004_m_000077_365
[2021-05-16 22:35:56,349] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134459204968384615310440_0004_m_000077_365: needsTaskCommit() Task attempt_202105170134459204968384615310440_0004_m_000077_365: duration 0:00.001s
21/05/17 01:35:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134459204968384615310440_0004_m_000077_365
[2021-05-16 22:35:56,350] {docker.py:276} INFO - 21/05/17 01:35:56 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 4544 bytes result sent to driver
[2021-05-16 22:35:56,351] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (5deb0a1bddc0, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:56,353] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 2575 ms on 5deb0a1bddc0 (executor driver) (77/200)
21/05/17 01:35:56 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-16 22:35:56,371] {docker.py:276} INFO - 21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:56,373] {docker.py:276} INFO - 21/05/17 01:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:56,374] {docker.py:276} INFO - 21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454454232712241345662_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454454232712241345662_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454454232712241345662_0004}; taskId=attempt_202105170134454454232712241345662_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6411e780}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:56,374] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134454454232712241345662_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454454232712241345662_0004_m_000080_368
[2021-05-16 22:35:56,376] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134454454232712241345662_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454454232712241345662_0004_m_000080_368 : duration 0:00.003s
[2021-05-16 22:35:56,421] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134457521548542771722404_0004_m_000076_364: needsTaskCommit() Task attempt_202105170134457521548542771722404_0004_m_000076_364
[2021-05-16 22:35:56,422] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134457521548542771722404_0004_m_000076_364: needsTaskCommit() Task attempt_202105170134457521548542771722404_0004_m_000076_364: duration 0:00.000s
21/05/17 01:35:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457521548542771722404_0004_m_000076_364
[2021-05-16 22:35:56,423] {docker.py:276} INFO - 21/05/17 01:35:56 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 4587 bytes result sent to driver
[2021-05-16 22:35:56,425] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (5deb0a1bddc0, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:56,426] {docker.py:276} INFO - 21/05/17 01:35:56 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-16 22:35:56,426] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 2880 ms on 5deb0a1bddc0 (executor driver) (78/200)
[2021-05-16 22:35:56,442] {docker.py:276} INFO - 21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Getting 5 (23.8 KiB) non-empty blocks including 5 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:56,444] {docker.py:276} INFO - 21/05/17 01:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457932805139466110031_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457932805139466110031_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457932805139466110031_0004}; taskId=attempt_202105170134457932805139466110031_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ecb8f9e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134457932805139466110031_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457932805139466110031_0004_m_000081_369
[2021-05-16 22:35:56,446] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134457932805139466110031_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457932805139466110031_0004_m_000081_369 : duration 0:00.003s
[2021-05-16 22:35:56,556] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134458221888278513750076_0004_m_000078_366: needsTaskCommit() Task attempt_202105170134458221888278513750076_0004_m_000078_366
[2021-05-16 22:35:56,557] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134458221888278513750076_0004_m_000078_366: needsTaskCommit() Task attempt_202105170134458221888278513750076_0004_m_000078_366: duration 0:00.001s
21/05/17 01:35:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458221888278513750076_0004_m_000078_366
[2021-05-16 22:35:56,559] {docker.py:276} INFO - 21/05/17 01:35:56 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 4587 bytes result sent to driver
[2021-05-16 22:35:56,561] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (5deb0a1bddc0, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:56,562] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 2764 ms on 5deb0a1bddc0 (executor driver) (79/200)
[2021-05-16 22:35:56,564] {docker.py:276} INFO - 21/05/17 01:35:56 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
[2021-05-16 22:35:56,574] {docker.py:276} INFO - 21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:56,576] {docker.py:276} INFO - 21/05/17 01:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445125866966221812539_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445125866966221812539_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445125866966221812539_0004}; taskId=attempt_20210517013445125866966221812539_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c961b18}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:56,577] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_20210517013445125866966221812539_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445125866966221812539_0004_m_000082_370
[2021-05-16 22:35:56,579] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_20210517013445125866966221812539_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445125866966221812539_0004_m_000082_370 : duration 0:00.003s
[2021-05-16 22:35:56,807] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134455266138751343498289_0004_m_000079_367: needsTaskCommit() Task attempt_202105170134455266138751343498289_0004_m_000079_367
[2021-05-16 22:35:56,808] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134455266138751343498289_0004_m_000079_367: needsTaskCommit() Task attempt_202105170134455266138751343498289_0004_m_000079_367: duration 0:00.001s
21/05/17 01:35:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455266138751343498289_0004_m_000079_367
[2021-05-16 22:35:56,809] {docker.py:276} INFO - 21/05/17 01:35:56 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 4587 bytes result sent to driver
[2021-05-16 22:35:56,810] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (5deb0a1bddc0, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:56,811] {docker.py:276} INFO - 21/05/17 01:35:56 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 2711 ms on 5deb0a1bddc0 (executor driver) (80/200)
21/05/17 01:35:56 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-16 22:35:56,818] {docker.py:276} INFO - 21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:56,820] {docker.py:276} INFO - 21/05/17 01:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458876866349211123713_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458876866349211123713_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458876866349211123713_0004}; taskId=attempt_202105170134458876866349211123713_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12dd01c1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:56 INFO StagingCommitter: Starting: Task committer attempt_202105170134458876866349211123713_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458876866349211123713_0004_m_000083_371
[2021-05-16 22:35:56,823] {docker.py:276} INFO - 21/05/17 01:35:56 INFO StagingCommitter: Task committer attempt_202105170134458876866349211123713_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458876866349211123713_0004_m_000083_371 : duration 0:00.003s
[2021-05-16 22:35:58,931] {docker.py:276} INFO - 21/05/17 01:35:58 INFO StagingCommitter: Starting: Task committer attempt_202105170134454454232712241345662_0004_m_000080_368: needsTaskCommit() Task attempt_202105170134454454232712241345662_0004_m_000080_368
[2021-05-16 22:35:58,932] {docker.py:276} INFO - 21/05/17 01:35:58 INFO StagingCommitter: Task committer attempt_202105170134454454232712241345662_0004_m_000080_368: needsTaskCommit() Task attempt_202105170134454454232712241345662_0004_m_000080_368: duration 0:00.001s
21/05/17 01:35:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454454232712241345662_0004_m_000080_368
[2021-05-16 22:35:58,934] {docker.py:276} INFO - 21/05/17 01:35:58 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 4587 bytes result sent to driver
[2021-05-16 22:35:58,936] {docker.py:276} INFO - 21/05/17 01:35:58 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (5deb0a1bddc0, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:58,937] {docker.py:276} INFO - 21/05/17 01:35:58 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 2554 ms on 5deb0a1bddc0 (executor driver) (81/200)
[2021-05-16 22:35:58,937] {docker.py:276} INFO - 21/05/17 01:35:58 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
[2021-05-16 22:35:58,949] {docker.py:276} INFO - 21/05/17 01:35:58 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:58,953] {docker.py:276} INFO - 21/05/17 01:35:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455227535784394908005_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455227535784394908005_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455227535784394908005_0004}; taskId=attempt_202105170134455227535784394908005_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31fbf36a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:35:58,953] {docker.py:276} INFO - 21/05/17 01:35:58 INFO StagingCommitter: Starting: Task committer attempt_202105170134455227535784394908005_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455227535784394908005_0004_m_000084_372
[2021-05-16 22:35:58,956] {docker.py:276} INFO - 21/05/17 01:35:58 INFO StagingCommitter: Task committer attempt_202105170134455227535784394908005_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455227535784394908005_0004_m_000084_372 : duration 0:00.003s
[2021-05-16 22:35:59,124] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134457932805139466110031_0004_m_000081_369: needsTaskCommit() Task attempt_202105170134457932805139466110031_0004_m_000081_369
[2021-05-16 22:35:59,125] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Task committer attempt_202105170134457932805139466110031_0004_m_000081_369: needsTaskCommit() Task attempt_202105170134457932805139466110031_0004_m_000081_369: duration 0:00.000s
21/05/17 01:35:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457932805139466110031_0004_m_000081_369
[2021-05-16 22:35:59,127] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Starting: Task committer attempt_20210517013445125866966221812539_0004_m_000082_370: needsTaskCommit() Task attempt_20210517013445125866966221812539_0004_m_000082_370
[2021-05-16 22:35:59,128] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Task committer attempt_20210517013445125866966221812539_0004_m_000082_370: needsTaskCommit() Task attempt_20210517013445125866966221812539_0004_m_000082_370: duration 0:00.001s
21/05/17 01:35:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445125866966221812539_0004_m_000082_370
21/05/17 01:35:59 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 4587 bytes result sent to driver
[2021-05-16 22:35:59,129] {docker.py:276} INFO - 21/05/17 01:35:59 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-16 22:35:59,129] {docker.py:276} INFO - 21/05/17 01:35:59 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (5deb0a1bddc0, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:59,130] {docker.py:276} INFO - 21/05/17 01:35:59 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-16 22:35:59,130] {docker.py:276} INFO - 21/05/17 01:35:59 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 2674 ms on 5deb0a1bddc0 (executor driver) (82/200)
[2021-05-16 22:35:59,131] {docker.py:276} INFO - 21/05/17 01:35:59 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (5deb0a1bddc0, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:59,132] {docker.py:276} INFO - 21/05/17 01:35:59 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 2541 ms on 5deb0a1bddc0 (executor driver) (83/200)
21/05/17 01:35:59 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-16 22:35:59,140] {docker.py:276} INFO - 21/05/17 01:35:59 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:59,141] {docker.py:276} INFO - 21/05/17 01:35:59 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:59,141] {docker.py:276} INFO - 21/05/17 01:35:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:35:59,142] {docker.py:276} INFO - 21/05/17 01:35:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453789548113391673506_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453789548113391673506_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453789548113391673506_0004}; taskId=attempt_202105170134453789548113391673506_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c1d0667}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134453789548113391673506_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453789548113391673506_0004_m_000085_373
[2021-05-16 22:35:59,143] {docker.py:276} INFO - 21/05/17 01:35:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452981157521442085732_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452981157521442085732_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452981157521442085732_0004}; taskId=attempt_202105170134452981157521442085732_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bb7c6e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:35:59,143] {docker.py:276} INFO - 21/05/17 01:35:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134452981157521442085732_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452981157521442085732_0004_m_000086_374
[2021-05-16 22:35:59,145] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Task committer attempt_202105170134453789548113391673506_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453789548113391673506_0004_m_000085_373 : duration 0:00.003s
[2021-05-16 22:35:59,147] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Task committer attempt_202105170134452981157521442085732_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452981157521442085732_0004_m_000086_374 : duration 0:00.003s
[2021-05-16 22:35:59,627] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134458876866349211123713_0004_m_000083_371: needsTaskCommit() Task attempt_202105170134458876866349211123713_0004_m_000083_371
21/05/17 01:35:59 INFO StagingCommitter: Task committer attempt_202105170134458876866349211123713_0004_m_000083_371: needsTaskCommit() Task attempt_202105170134458876866349211123713_0004_m_000083_371: duration 0:00.001s
21/05/17 01:35:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458876866349211123713_0004_m_000083_371
[2021-05-16 22:35:59,628] {docker.py:276} INFO - 21/05/17 01:35:59 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 4544 bytes result sent to driver
[2021-05-16 22:35:59,630] {docker.py:276} INFO - 21/05/17 01:35:59 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (5deb0a1bddc0, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:35:59,630] {docker.py:276} INFO - 21/05/17 01:35:59 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-16 22:35:59,632] {docker.py:276} INFO - 21/05/17 01:35:59 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 2790 ms on 5deb0a1bddc0 (executor driver) (84/200)
[2021-05-16 22:35:59,641] {docker.py:276} INFO - 21/05/17 01:35:59 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:35:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:35:59,643] {docker.py:276} INFO - 21/05/17 01:35:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:35:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:35:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:35:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456868268235791070382_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456868268235791070382_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456868268235791070382_0004}; taskId=attempt_202105170134456868268235791070382_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d21eadf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:35:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:35:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134456868268235791070382_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456868268235791070382_0004_m_000087_375
[2021-05-16 22:35:59,646] {docker.py:276} INFO - 21/05/17 01:35:59 INFO StagingCommitter: Task committer attempt_202105170134456868268235791070382_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456868268235791070382_0004_m_000087_375 : duration 0:00.003s
[2021-05-16 22:36:01,649] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134455227535784394908005_0004_m_000084_372: needsTaskCommit() Task attempt_202105170134455227535784394908005_0004_m_000084_372
[2021-05-16 22:36:01,651] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Task committer attempt_202105170134455227535784394908005_0004_m_000084_372: needsTaskCommit() Task attempt_202105170134455227535784394908005_0004_m_000084_372: duration 0:00.001s
21/05/17 01:36:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455227535784394908005_0004_m_000084_372
[2021-05-16 22:36:01,652] {docker.py:276} INFO - 21/05/17 01:36:01 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 4544 bytes result sent to driver
[2021-05-16 22:36:01,653] {docker.py:276} INFO - 21/05/17 01:36:01 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (5deb0a1bddc0, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:01,655] {docker.py:276} INFO - 21/05/17 01:36:01 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 2723 ms on 5deb0a1bddc0 (executor driver) (85/200)
21/05/17 01:36:01 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-16 22:36:01,664] {docker.py:276} INFO - 21/05/17 01:36:01 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:01,666] {docker.py:276} INFO - 21/05/17 01:36:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456890119285699953656_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456890119285699953656_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456890119285699953656_0004}; taskId=attempt_202105170134456890119285699953656_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17890bf1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134456890119285699953656_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456890119285699953656_0004_m_000088_376
[2021-05-16 22:36:01,669] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Task committer attempt_202105170134456890119285699953656_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456890119285699953656_0004_m_000088_376 : duration 0:00.003s
[2021-05-16 22:36:01,826] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134453789548113391673506_0004_m_000085_373: needsTaskCommit() Task attempt_202105170134453789548113391673506_0004_m_000085_373
[2021-05-16 22:36:01,827] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Task committer attempt_202105170134453789548113391673506_0004_m_000085_373: needsTaskCommit() Task attempt_202105170134453789548113391673506_0004_m_000085_373: duration 0:00.000s
21/05/17 01:36:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453789548113391673506_0004_m_000085_373
[2021-05-16 22:36:01,829] {docker.py:276} INFO - 21/05/17 01:36:01 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 4544 bytes result sent to driver
[2021-05-16 22:36:01,830] {docker.py:276} INFO - 21/05/17 01:36:01 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (5deb0a1bddc0, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:01,831] {docker.py:276} INFO - 21/05/17 01:36:01 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
[2021-05-16 22:36:01,832] {docker.py:276} INFO - 21/05/17 01:36:01 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 2707 ms on 5deb0a1bddc0 (executor driver) (86/200)
[2021-05-16 22:36:01,848] {docker.py:276} INFO - 21/05/17 01:36:01 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:01,850] {docker.py:276} INFO - 21/05/17 01:36:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:01,850] {docker.py:276} INFO - 21/05/17 01:36:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452304881424965575765_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452304881424965575765_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452304881424965575765_0004}; taskId=attempt_202105170134452304881424965575765_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76f35ead}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:01,850] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134452304881424965575765_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452304881424965575765_0004_m_000089_377
[2021-05-16 22:36:01,853] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Task committer attempt_202105170134452304881424965575765_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452304881424965575765_0004_m_000089_377 : duration 0:00.003s
[2021-05-16 22:36:01,924] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134452981157521442085732_0004_m_000086_374: needsTaskCommit() Task attempt_202105170134452981157521442085732_0004_m_000086_374
[2021-05-16 22:36:01,924] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Task committer attempt_202105170134452981157521442085732_0004_m_000086_374: needsTaskCommit() Task attempt_202105170134452981157521442085732_0004_m_000086_374: duration 0:00.001s
21/05/17 01:36:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452981157521442085732_0004_m_000086_374
[2021-05-16 22:36:01,926] {docker.py:276} INFO - 21/05/17 01:36:01 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 4544 bytes result sent to driver
[2021-05-16 22:36:01,927] {docker.py:276} INFO - 21/05/17 01:36:01 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (5deb0a1bddc0, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:01,928] {docker.py:276} INFO - 21/05/17 01:36:01 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-16 22:36:01,929] {docker.py:276} INFO - 21/05/17 01:36:01 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 2802 ms on 5deb0a1bddc0 (executor driver) (87/200)
[2021-05-16 22:36:01,938] {docker.py:276} INFO - 21/05/17 01:36:01 INFO ShuffleBlockFetcherIterator: Getting 5 (23.8 KiB) non-empty blocks including 5 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:01,940] {docker.py:276} INFO - 21/05/17 01:36:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:01,941] {docker.py:276} INFO - 21/05/17 01:36:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455729782246483197892_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455729782246483197892_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455729782246483197892_0004}; taskId=attempt_202105170134455729782246483197892_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ee8c238}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134455729782246483197892_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455729782246483197892_0004_m_000090_378
[2021-05-16 22:36:01,946] {docker.py:276} INFO - 21/05/17 01:36:01 INFO StagingCommitter: Task committer attempt_202105170134455729782246483197892_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455729782246483197892_0004_m_000090_378 : duration 0:00.005s
[2021-05-16 22:36:02,426] {docker.py:276} INFO - 21/05/17 01:36:02 INFO StagingCommitter: Starting: Task committer attempt_202105170134456868268235791070382_0004_m_000087_375: needsTaskCommit() Task attempt_202105170134456868268235791070382_0004_m_000087_375
[2021-05-16 22:36:02,427] {docker.py:276} INFO - 21/05/17 01:36:02 INFO StagingCommitter: Task committer attempt_202105170134456868268235791070382_0004_m_000087_375: needsTaskCommit() Task attempt_202105170134456868268235791070382_0004_m_000087_375: duration 0:00.002s
21/05/17 01:36:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456868268235791070382_0004_m_000087_375
[2021-05-16 22:36:02,431] {docker.py:276} INFO - 21/05/17 01:36:02 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
21/05/17 01:36:02 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (5deb0a1bddc0, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:02,432] {docker.py:276} INFO - 21/05/17 01:36:02 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-16 22:36:02,433] {docker.py:276} INFO - 21/05/17 01:36:02 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 2806 ms on 5deb0a1bddc0 (executor driver) (88/200)
[2021-05-16 22:36:02,450] {docker.py:276} INFO - 21/05/17 01:36:02 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:02,452] {docker.py:276} INFO - 21/05/17 01:36:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051701344514813210030754940_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_2021051701344514813210030754940_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051701344514813210030754940_0004}; taskId=attempt_2021051701344514813210030754940_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b51a799}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:02,452] {docker.py:276} INFO - 21/05/17 01:36:02 INFO StagingCommitter: Starting: Task committer attempt_2021051701344514813210030754940_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_2021051701344514813210030754940_0004_m_000091_379
[2021-05-16 22:36:02,454] {docker.py:276} INFO - 21/05/17 01:36:02 INFO StagingCommitter: Task committer attempt_2021051701344514813210030754940_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_2021051701344514813210030754940_0004_m_000091_379 : duration 0:00.002s
[2021-05-16 22:36:04,248] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134456890119285699953656_0004_m_000088_376: needsTaskCommit() Task attempt_202105170134456890119285699953656_0004_m_000088_376
[2021-05-16 22:36:04,249] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Task committer attempt_202105170134456890119285699953656_0004_m_000088_376: needsTaskCommit() Task attempt_202105170134456890119285699953656_0004_m_000088_376: duration 0:00.000s
21/05/17 01:36:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456890119285699953656_0004_m_000088_376
[2021-05-16 22:36:04,250] {docker.py:276} INFO - 21/05/17 01:36:04 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 4587 bytes result sent to driver
[2021-05-16 22:36:04,251] {docker.py:276} INFO - 21/05/17 01:36:04 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (5deb0a1bddc0, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:04,251] {docker.py:276} INFO - 21/05/17 01:36:04 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-16 22:36:04,252] {docker.py:276} INFO - 21/05/17 01:36:04 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 2603 ms on 5deb0a1bddc0 (executor driver) (89/200)
[2021-05-16 22:36:04,259] {docker.py:276} INFO - 21/05/17 01:36:04 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:04,262] {docker.py:276} INFO - 21/05/17 01:36:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:04,262] {docker.py:276} INFO - 21/05/17 01:36:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:04,263] {docker.py:276} INFO - 21/05/17 01:36:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451084630349125631534_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451084630349125631534_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451084630349125631534_0004}; taskId=attempt_202105170134451084630349125631534_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4080af54}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:04,263] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134451084630349125631534_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451084630349125631534_0004_m_000092_380
[2021-05-16 22:36:04,266] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Task committer attempt_202105170134451084630349125631534_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451084630349125631534_0004_m_000092_380 : duration 0:00.003s
[2021-05-16 22:36:04,557] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134455729782246483197892_0004_m_000090_378: needsTaskCommit() Task attempt_202105170134455729782246483197892_0004_m_000090_378
[2021-05-16 22:36:04,558] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Task committer attempt_202105170134455729782246483197892_0004_m_000090_378: needsTaskCommit() Task attempt_202105170134455729782246483197892_0004_m_000090_378: duration 0:00.001s
21/05/17 01:36:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455729782246483197892_0004_m_000090_378
[2021-05-16 22:36:04,561] {docker.py:276} INFO - 21/05/17 01:36:04 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 4587 bytes result sent to driver
[2021-05-16 22:36:04,562] {docker.py:276} INFO - 21/05/17 01:36:04 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (5deb0a1bddc0, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:04,563] {docker.py:276} INFO - 21/05/17 01:36:04 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
[2021-05-16 22:36:04,563] {docker.py:276} INFO - 21/05/17 01:36:04 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 2639 ms on 5deb0a1bddc0 (executor driver) (90/200)
[2021-05-16 22:36:04,572] {docker.py:276} INFO - 21/05/17 01:36:04 INFO ShuffleBlockFetcherIterator: Getting 5 (21.7 KiB) non-empty blocks including 5 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:04,574] {docker.py:276} INFO - 21/05/17 01:36:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:04,574] {docker.py:276} INFO - 21/05/17 01:36:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456773944355455533073_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456773944355455533073_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456773944355455533073_0004}; taskId=attempt_202105170134456773944355455533073_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39b1e38}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134456773944355455533073_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456773944355455533073_0004_m_000093_381
[2021-05-16 22:36:04,577] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Task committer attempt_202105170134456773944355455533073_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456773944355455533073_0004_m_000093_381 : duration 0:00.003s
[2021-05-16 22:36:04,652] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134452304881424965575765_0004_m_000089_377: needsTaskCommit() Task attempt_202105170134452304881424965575765_0004_m_000089_377
[2021-05-16 22:36:04,653] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Task committer attempt_202105170134452304881424965575765_0004_m_000089_377: needsTaskCommit() Task attempt_202105170134452304881424965575765_0004_m_000089_377: duration 0:00.001s
21/05/17 01:36:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452304881424965575765_0004_m_000089_377
[2021-05-16 22:36:04,656] {docker.py:276} INFO - 21/05/17 01:36:04 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 4587 bytes result sent to driver
[2021-05-16 22:36:04,658] {docker.py:276} INFO - 21/05/17 01:36:04 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (5deb0a1bddc0, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:04,660] {docker.py:276} INFO - 21/05/17 01:36:04 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-16 22:36:04,661] {docker.py:276} INFO - 21/05/17 01:36:04 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 2833 ms on 5deb0a1bddc0 (executor driver) (91/200)
[2021-05-16 22:36:04,670] {docker.py:276} INFO - 21/05/17 01:36:04 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:04,671] {docker.py:276} INFO - 21/05/17 01:36:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457039963530384257616_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457039963530384257616_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457039963530384257616_0004}; taskId=attempt_202105170134457039963530384257616_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@361a2a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134457039963530384257616_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457039963530384257616_0004_m_000094_382
[2021-05-16 22:36:04,674] {docker.py:276} INFO - 21/05/17 01:36:04 INFO StagingCommitter: Task committer attempt_202105170134457039963530384257616_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457039963530384257616_0004_m_000094_382 : duration 0:00.002s
[2021-05-16 22:36:05,274] {docker.py:276} INFO - 21/05/17 01:36:05 INFO StagingCommitter: Starting: Task committer attempt_2021051701344514813210030754940_0004_m_000091_379: needsTaskCommit() Task attempt_2021051701344514813210030754940_0004_m_000091_379
[2021-05-16 22:36:05,275] {docker.py:276} INFO - 21/05/17 01:36:05 INFO StagingCommitter: Task committer attempt_2021051701344514813210030754940_0004_m_000091_379: needsTaskCommit() Task attempt_2021051701344514813210030754940_0004_m_000091_379: duration 0:00.001s
21/05/17 01:36:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051701344514813210030754940_0004_m_000091_379
[2021-05-16 22:36:05,278] {docker.py:276} INFO - 21/05/17 01:36:05 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 4587 bytes result sent to driver
[2021-05-16 22:36:05,280] {docker.py:276} INFO - 21/05/17 01:36:05 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (5deb0a1bddc0, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:05,281] {docker.py:276} INFO - 21/05/17 01:36:05 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 2854 ms on 5deb0a1bddc0 (executor driver) (92/200)
[2021-05-16 22:36:05,282] {docker.py:276} INFO - 21/05/17 01:36:05 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-16 22:36:05,291] {docker.py:276} INFO - 21/05/17 01:36:05 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:05,293] {docker.py:276} INFO - 21/05/17 01:36:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456401266219412292400_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456401266219412292400_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456401266219412292400_0004}; taskId=attempt_202105170134456401266219412292400_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68fb2e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:05,294] {docker.py:276} INFO - 21/05/17 01:36:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:05,294] {docker.py:276} INFO - 21/05/17 01:36:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134456401266219412292400_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456401266219412292400_0004_m_000095_383
[2021-05-16 22:36:05,296] {docker.py:276} INFO - 21/05/17 01:36:05 INFO StagingCommitter: Task committer attempt_202105170134456401266219412292400_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456401266219412292400_0004_m_000095_383 : duration 0:00.003s
[2021-05-16 22:36:06,997] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134451084630349125631534_0004_m_000092_380: needsTaskCommit() Task attempt_202105170134451084630349125631534_0004_m_000092_380
[2021-05-16 22:36:06,998] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134451084630349125631534_0004_m_000092_380: needsTaskCommit() Task attempt_202105170134451084630349125631534_0004_m_000092_380: duration 0:00.001s
21/05/17 01:36:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451084630349125631534_0004_m_000092_380
[2021-05-16 22:36:06,999] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 4544 bytes result sent to driver
[2021-05-16 22:36:07,000] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (5deb0a1bddc0, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:07,002] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
21/05/17 01:36:07 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 2753 ms on 5deb0a1bddc0 (executor driver) (93/200)
[2021-05-16 22:36:07,013] {docker.py:276} INFO - 21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Getting 5 (23.8 KiB) non-empty blocks including 5 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:07,015] {docker.py:276} INFO - 21/05/17 01:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457548002049020771061_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457548002049020771061_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457548002049020771061_0004}; taskId=attempt_202105170134457548002049020771061_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a3711a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:07,016] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134457548002049020771061_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457548002049020771061_0004_m_000096_384
[2021-05-16 22:36:07,018] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134457548002049020771061_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457548002049020771061_0004_m_000096_384 : duration 0:00.003s
[2021-05-16 22:36:07,343] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134456773944355455533073_0004_m_000093_381: needsTaskCommit() Task attempt_202105170134456773944355455533073_0004_m_000093_381
21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134456773944355455533073_0004_m_000093_381: needsTaskCommit() Task attempt_202105170134456773944355455533073_0004_m_000093_381: duration 0:00.000s
[2021-05-16 22:36:07,344] {docker.py:276} INFO - 21/05/17 01:36:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456773944355455533073_0004_m_000093_381
[2021-05-16 22:36:07,345] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 4544 bytes result sent to driver
[2021-05-16 22:36:07,347] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (5deb0a1bddc0, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:07,348] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
[2021-05-16 22:36:07,349] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 2790 ms on 5deb0a1bddc0 (executor driver) (94/200)
[2021-05-16 22:36:07,358] {docker.py:276} INFO - 21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:07,360] {docker.py:276} INFO - 21/05/17 01:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:07,360] {docker.py:276} INFO - 21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455774091823049155593_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455774091823049155593_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455774091823049155593_0004}; taskId=attempt_202105170134455774091823049155593_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5871405a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:07,360] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134455774091823049155593_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455774091823049155593_0004_m_000097_385
[2021-05-16 22:36:07,363] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134455774091823049155593_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455774091823049155593_0004_m_000097_385 : duration 0:00.003s
[2021-05-16 22:36:07,636] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134457039963530384257616_0004_m_000094_382: needsTaskCommit() Task attempt_202105170134457039963530384257616_0004_m_000094_382
[2021-05-16 22:36:07,636] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134457039963530384257616_0004_m_000094_382: needsTaskCommit() Task attempt_202105170134457039963530384257616_0004_m_000094_382: duration 0:00.001s
21/05/17 01:36:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457039963530384257616_0004_m_000094_382
[2021-05-16 22:36:07,637] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 4544 bytes result sent to driver
[2021-05-16 22:36:07,638] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (5deb0a1bddc0, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:07,639] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-16 22:36:07,640] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 2985 ms on 5deb0a1bddc0 (executor driver) (95/200)
[2021-05-16 22:36:07,648] {docker.py:276} INFO - 21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:07,651] {docker.py:276} INFO - 21/05/17 01:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455642888788227799980_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455642888788227799980_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455642888788227799980_0004}; taskId=attempt_202105170134455642888788227799980_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29121dd8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:07,651] {docker.py:276} INFO - 21/05/17 01:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134455642888788227799980_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455642888788227799980_0004_m_000098_386
[2021-05-16 22:36:07,654] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134455642888788227799980_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455642888788227799980_0004_m_000098_386 : duration 0:00.004s
[2021-05-16 22:36:07,823] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134456401266219412292400_0004_m_000095_383: needsTaskCommit() Task attempt_202105170134456401266219412292400_0004_m_000095_383
[2021-05-16 22:36:07,824] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134456401266219412292400_0004_m_000095_383: needsTaskCommit() Task attempt_202105170134456401266219412292400_0004_m_000095_383: duration 0:00.001s
21/05/17 01:36:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456401266219412292400_0004_m_000095_383
[2021-05-16 22:36:07,826] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 4544 bytes result sent to driver
[2021-05-16 22:36:07,828] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (5deb0a1bddc0, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:07,829] {docker.py:276} INFO - 21/05/17 01:36:07 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 2552 ms on 5deb0a1bddc0 (executor driver) (96/200)
[2021-05-16 22:36:07,830] {docker.py:276} INFO - 21/05/17 01:36:07 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-16 22:36:07,839] {docker.py:276} INFO - 21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:07,840] {docker.py:276} INFO - 21/05/17 01:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:07,841] {docker.py:276} INFO - 21/05/17 01:36:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458012471690464433449_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458012471690464433449_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458012471690464433449_0004}; taskId=attempt_202105170134458012471690464433449_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72d7c385}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134458012471690464433449_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458012471690464433449_0004_m_000099_387
[2021-05-16 22:36:07,843] {docker.py:276} INFO - 21/05/17 01:36:07 INFO StagingCommitter: Task committer attempt_202105170134458012471690464433449_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458012471690464433449_0004_m_000099_387 : duration 0:00.003s
[2021-05-16 22:36:09,643] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134457548002049020771061_0004_m_000096_384: needsTaskCommit() Task attempt_202105170134457548002049020771061_0004_m_000096_384
[2021-05-16 22:36:09,644] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Task committer attempt_202105170134457548002049020771061_0004_m_000096_384: needsTaskCommit() Task attempt_202105170134457548002049020771061_0004_m_000096_384: duration 0:00.001s
21/05/17 01:36:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457548002049020771061_0004_m_000096_384
[2021-05-16 22:36:09,647] {docker.py:276} INFO - 21/05/17 01:36:09 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 4544 bytes result sent to driver
[2021-05-16 22:36:09,648] {docker.py:276} INFO - 21/05/17 01:36:09 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (5deb0a1bddc0, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:09,650] {docker.py:276} INFO - 21/05/17 01:36:09 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 2653 ms on 5deb0a1bddc0 (executor driver) (97/200)
[2021-05-16 22:36:09,651] {docker.py:276} INFO - 21/05/17 01:36:09 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
[2021-05-16 22:36:09,661] {docker.py:276} INFO - 21/05/17 01:36:09 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:09,661] {docker.py:276} INFO - 21/05/17 01:36:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:09,665] {docker.py:276} INFO - 21/05/17 01:36:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:09,666] {docker.py:276} INFO - 21/05/17 01:36:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457544883856414470220_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457544883856414470220_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457544883856414470220_0004}; taskId=attempt_202105170134457544883856414470220_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd7d8d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:09,666] {docker.py:276} INFO - 21/05/17 01:36:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:09,666] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134457544883856414470220_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457544883856414470220_0004_m_000100_388
[2021-05-16 22:36:09,670] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Task committer attempt_202105170134457544883856414470220_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457544883856414470220_0004_m_000100_388 : duration 0:00.004s
[2021-05-16 22:36:09,825] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134455642888788227799980_0004_m_000098_386: needsTaskCommit() Task attempt_202105170134455642888788227799980_0004_m_000098_386
[2021-05-16 22:36:09,826] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Task committer attempt_202105170134455642888788227799980_0004_m_000098_386: needsTaskCommit() Task attempt_202105170134455642888788227799980_0004_m_000098_386: duration 0:00.001s
21/05/17 01:36:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455642888788227799980_0004_m_000098_386
[2021-05-16 22:36:09,827] {docker.py:276} INFO - 21/05/17 01:36:09 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 4587 bytes result sent to driver
[2021-05-16 22:36:09,829] {docker.py:276} INFO - 21/05/17 01:36:09 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (5deb0a1bddc0, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:09,830] {docker.py:276} INFO - 21/05/17 01:36:09 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-16 22:36:09,831] {docker.py:276} INFO - 21/05/17 01:36:09 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 2195 ms on 5deb0a1bddc0 (executor driver) (98/200)
[2021-05-16 22:36:09,841] {docker.py:276} INFO - 21/05/17 01:36:09 INFO ShuffleBlockFetcherIterator: Getting 5 (21.3 KiB) non-empty blocks including 5 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:09,841] {docker.py:276} INFO - 21/05/17 01:36:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:09,843] {docker.py:276} INFO - 21/05/17 01:36:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:36:09,844] {docker.py:276} INFO - 21/05/17 01:36:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:09,844] {docker.py:276} INFO - 21/05/17 01:36:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:09,844] {docker.py:276} INFO - 21/05/17 01:36:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451921788584599089531_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451921788584599089531_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451921788584599089531_0004}; taskId=attempt_202105170134451921788584599089531_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40f5e65b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:09,845] {docker.py:276} INFO - 21/05/17 01:36:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:09,845] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134451921788584599089531_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451921788584599089531_0004_m_000101_389
[2021-05-16 22:36:09,848] {docker.py:276} INFO - 21/05/17 01:36:09 INFO StagingCommitter: Task committer attempt_202105170134451921788584599089531_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451921788584599089531_0004_m_000101_389 : duration 0:00.004s
[2021-05-16 22:36:10,121] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134455774091823049155593_0004_m_000097_385: needsTaskCommit() Task attempt_202105170134455774091823049155593_0004_m_000097_385
[2021-05-16 22:36:10,122] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Task committer attempt_202105170134455774091823049155593_0004_m_000097_385: needsTaskCommit() Task attempt_202105170134455774091823049155593_0004_m_000097_385: duration 0:00.000s
21/05/17 01:36:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455774091823049155593_0004_m_000097_385
[2021-05-16 22:36:10,125] {docker.py:276} INFO - 21/05/17 01:36:10 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 4587 bytes result sent to driver
[2021-05-16 22:36:10,127] {docker.py:276} INFO - 21/05/17 01:36:10 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (5deb0a1bddc0, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:10,127] {docker.py:276} INFO - 21/05/17 01:36:10 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-16 22:36:10,128] {docker.py:276} INFO - 21/05/17 01:36:10 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 2785 ms on 5deb0a1bddc0 (executor driver) (99/200)
[2021-05-16 22:36:10,137] {docker.py:276} INFO - 21/05/17 01:36:10 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:10,139] {docker.py:276} INFO - 21/05/17 01:36:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:10,140] {docker.py:276} INFO - 21/05/17 01:36:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454113749324509708345_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454113749324509708345_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454113749324509708345_0004}; taskId=attempt_202105170134454113749324509708345_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@371afa98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:10,140] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134454113749324509708345_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454113749324509708345_0004_m_000102_390
[2021-05-16 22:36:10,143] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Task committer attempt_202105170134454113749324509708345_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454113749324509708345_0004_m_000102_390 : duration 0:00.003s
[2021-05-16 22:36:10,553] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134458012471690464433449_0004_m_000099_387: needsTaskCommit() Task attempt_202105170134458012471690464433449_0004_m_000099_387
[2021-05-16 22:36:10,555] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Task committer attempt_202105170134458012471690464433449_0004_m_000099_387: needsTaskCommit() Task attempt_202105170134458012471690464433449_0004_m_000099_387: duration 0:00.003s
21/05/17 01:36:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458012471690464433449_0004_m_000099_387
[2021-05-16 22:36:10,557] {docker.py:276} INFO - 21/05/17 01:36:10 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 4587 bytes result sent to driver
[2021-05-16 22:36:10,558] {docker.py:276} INFO - 21/05/17 01:36:10 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (5deb0a1bddc0, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:10,559] {docker.py:276} INFO - 21/05/17 01:36:10 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 2733 ms on 5deb0a1bddc0 (executor driver) (100/200)
[2021-05-16 22:36:10,559] {docker.py:276} INFO - 21/05/17 01:36:10 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-16 22:36:10,569] {docker.py:276} INFO - 21/05/17 01:36:10 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:10,571] {docker.py:276} INFO - 21/05/17 01:36:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134459109133273920899356_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459109133273920899356_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134459109133273920899356_0004}; taskId=attempt_202105170134459109133273920899356_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e1e0e2a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:10,571] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134459109133273920899356_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459109133273920899356_0004_m_000103_391
[2021-05-16 22:36:10,574] {docker.py:276} INFO - 21/05/17 01:36:10 INFO StagingCommitter: Task committer attempt_202105170134459109133273920899356_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459109133273920899356_0004_m_000103_391 : duration 0:00.003s
[2021-05-16 22:36:12,350] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134451921788584599089531_0004_m_000101_389: needsTaskCommit() Task attempt_202105170134451921788584599089531_0004_m_000101_389
[2021-05-16 22:36:12,350] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Task committer attempt_202105170134451921788584599089531_0004_m_000101_389: needsTaskCommit() Task attempt_202105170134451921788584599089531_0004_m_000101_389: duration 0:00.001s
21/05/17 01:36:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451921788584599089531_0004_m_000101_389
[2021-05-16 22:36:12,352] {docker.py:276} INFO - 21/05/17 01:36:12 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 4544 bytes result sent to driver
[2021-05-16 22:36:12,354] {docker.py:276} INFO - 21/05/17 01:36:12 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (5deb0a1bddc0, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:12,355] {docker.py:276} INFO - 21/05/17 01:36:12 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 2530 ms on 5deb0a1bddc0 (executor driver) (101/200)
[2021-05-16 22:36:12,356] {docker.py:276} INFO - 21/05/17 01:36:12 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
[2021-05-16 22:36:12,365] {docker.py:276} INFO - 21/05/17 01:36:12 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:12,367] {docker.py:276} INFO - 21/05/17 01:36:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134459049489077627862877_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459049489077627862877_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134459049489077627862877_0004}; taskId=attempt_202105170134459049489077627862877_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40d6245e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134459049489077627862877_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459049489077627862877_0004_m_000104_392
[2021-05-16 22:36:12,370] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Task committer attempt_202105170134459049489077627862877_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459049489077627862877_0004_m_000104_392 : duration 0:00.003s
[2021-05-16 22:36:12,793] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134454113749324509708345_0004_m_000102_390: needsTaskCommit() Task attempt_202105170134454113749324509708345_0004_m_000102_390
[2021-05-16 22:36:12,794] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Task committer attempt_202105170134454113749324509708345_0004_m_000102_390: needsTaskCommit() Task attempt_202105170134454113749324509708345_0004_m_000102_390: duration 0:00.001s
[2021-05-16 22:36:12,795] {docker.py:276} INFO - 21/05/17 01:36:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454113749324509708345_0004_m_000102_390
[2021-05-16 22:36:12,797] {docker.py:276} INFO - 21/05/17 01:36:12 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 4544 bytes result sent to driver
[2021-05-16 22:36:12,797] {docker.py:276} INFO - 21/05/17 01:36:12 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (5deb0a1bddc0, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:12,798] {docker.py:276} INFO - 21/05/17 01:36:12 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 2677 ms on 5deb0a1bddc0 (executor driver) (102/200)
[2021-05-16 22:36:12,799] {docker.py:276} INFO - 21/05/17 01:36:12 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-16 22:36:12,808] {docker.py:276} INFO - 21/05/17 01:36:12 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:12,809] {docker.py:276} INFO - 21/05/17 01:36:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451345232410118540309_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451345232410118540309_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451345232410118540309_0004}; taskId=attempt_202105170134451345232410118540309_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14b43e20}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:12,810] {docker.py:276} INFO - 21/05/17 01:36:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:12,810] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134451345232410118540309_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451345232410118540309_0004_m_000105_393
[2021-05-16 22:36:12,812] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Task committer attempt_202105170134451345232410118540309_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451345232410118540309_0004_m_000105_393 : duration 0:00.003s
[2021-05-16 22:36:12,877] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134457544883856414470220_0004_m_000100_388: needsTaskCommit() Task attempt_202105170134457544883856414470220_0004_m_000100_388
[2021-05-16 22:36:12,879] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Task committer attempt_202105170134457544883856414470220_0004_m_000100_388: needsTaskCommit() Task attempt_202105170134457544883856414470220_0004_m_000100_388: duration 0:00.001s
[2021-05-16 22:36:12,879] {docker.py:276} INFO - 21/05/17 01:36:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457544883856414470220_0004_m_000100_388
[2021-05-16 22:36:12,881] {docker.py:276} INFO - 21/05/17 01:36:12 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 4587 bytes result sent to driver
[2021-05-16 22:36:12,882] {docker.py:276} INFO - 21/05/17 01:36:12 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (5deb0a1bddc0, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:12,883] {docker.py:276} INFO - 21/05/17 01:36:12 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 3239 ms on 5deb0a1bddc0 (executor driver) (103/200)
21/05/17 01:36:12 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-16 22:36:12,893] {docker.py:276} INFO - 21/05/17 01:36:12 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:12,895] {docker.py:276} INFO - 21/05/17 01:36:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457504609216681081747_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457504609216681081747_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457504609216681081747_0004}; taskId=attempt_202105170134457504609216681081747_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70b38371}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:12,895] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134457504609216681081747_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457504609216681081747_0004_m_000106_394
[2021-05-16 22:36:12,898] {docker.py:276} INFO - 21/05/17 01:36:12 INFO StagingCommitter: Task committer attempt_202105170134457504609216681081747_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457504609216681081747_0004_m_000106_394 : duration 0:00.002s
[2021-05-16 22:36:13,375] {docker.py:276} INFO - 21/05/17 01:36:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134459109133273920899356_0004_m_000103_391: needsTaskCommit() Task attempt_202105170134459109133273920899356_0004_m_000103_391
[2021-05-16 22:36:13,376] {docker.py:276} INFO - 21/05/17 01:36:13 INFO StagingCommitter: Task committer attempt_202105170134459109133273920899356_0004_m_000103_391: needsTaskCommit() Task attempt_202105170134459109133273920899356_0004_m_000103_391: duration 0:00.000s
21/05/17 01:36:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134459109133273920899356_0004_m_000103_391
[2021-05-16 22:36:13,377] {docker.py:276} INFO - 21/05/17 01:36:13 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 4544 bytes result sent to driver
[2021-05-16 22:36:13,379] {docker.py:276} INFO - 21/05/17 01:36:13 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (5deb0a1bddc0, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:13,380] {docker.py:276} INFO - 21/05/17 01:36:13 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 2826 ms on 5deb0a1bddc0 (executor driver) (104/200)
21/05/17 01:36:13 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
[2021-05-16 22:36:13,390] {docker.py:276} INFO - 21/05/17 01:36:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:13,391] {docker.py:276} INFO - 21/05/17 01:36:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451457184688094805112_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451457184688094805112_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451457184688094805112_0004}; taskId=attempt_202105170134451457184688094805112_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e9187e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134451457184688094805112_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451457184688094805112_0004_m_000107_395
[2021-05-16 22:36:13,395] {docker.py:276} INFO - 21/05/17 01:36:13 INFO StagingCommitter: Task committer attempt_202105170134451457184688094805112_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451457184688094805112_0004_m_000107_395 : duration 0:00.003s
[2021-05-16 22:36:15,150] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134459049489077627862877_0004_m_000104_392: needsTaskCommit() Task attempt_202105170134459049489077627862877_0004_m_000104_392
[2021-05-16 22:36:15,151] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134459049489077627862877_0004_m_000104_392: needsTaskCommit() Task attempt_202105170134459049489077627862877_0004_m_000104_392: duration 0:00.002s
21/05/17 01:36:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134459049489077627862877_0004_m_000104_392
[2021-05-16 22:36:15,153] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 4544 bytes result sent to driver
[2021-05-16 22:36:15,156] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (5deb0a1bddc0, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:36:15 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 2805 ms on 5deb0a1bddc0 (executor driver) (105/200)
21/05/17 01:36:15 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
[2021-05-16 22:36:15,165] {docker.py:276} INFO - 21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:15,166] {docker.py:276} INFO - 21/05/17 01:36:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:15,167] {docker.py:276} INFO - 21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453139281247239585647_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453139281247239585647_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453139281247239585647_0004}; taskId=attempt_202105170134453139281247239585647_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55ff6915}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134453139281247239585647_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453139281247239585647_0004_m_000108_396
[2021-05-16 22:36:15,169] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134453139281247239585647_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453139281247239585647_0004_m_000108_396 : duration 0:00.002s
[2021-05-16 22:36:15,441] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134451345232410118540309_0004_m_000105_393: needsTaskCommit() Task attempt_202105170134451345232410118540309_0004_m_000105_393
[2021-05-16 22:36:15,442] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134451345232410118540309_0004_m_000105_393: needsTaskCommit() Task attempt_202105170134451345232410118540309_0004_m_000105_393: duration 0:00.001s
21/05/17 01:36:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451345232410118540309_0004_m_000105_393
[2021-05-16 22:36:15,443] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 4544 bytes result sent to driver
[2021-05-16 22:36:15,444] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (5deb0a1bddc0, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:15,446] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 2651 ms on 5deb0a1bddc0 (executor driver) (106/200)
[2021-05-16 22:36:15,447] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-16 22:36:15,456] {docker.py:276} INFO - 21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:15,458] {docker.py:276} INFO - 21/05/17 01:36:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455672610624164300380_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455672610624164300380_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455672610624164300380_0004}; taskId=attempt_202105170134455672610624164300380_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@786865f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:15,459] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134455672610624164300380_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455672610624164300380_0004_m_000109_397
[2021-05-16 22:36:15,462] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134455672610624164300380_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455672610624164300380_0004_m_000109_397 : duration 0:00.004s
[2021-05-16 22:36:15,618] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134451457184688094805112_0004_m_000107_395: needsTaskCommit() Task attempt_202105170134451457184688094805112_0004_m_000107_395
[2021-05-16 22:36:15,619] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134451457184688094805112_0004_m_000107_395: needsTaskCommit() Task attempt_202105170134451457184688094805112_0004_m_000107_395: duration 0:00.002s
[2021-05-16 22:36:15,620] {docker.py:276} INFO - 21/05/17 01:36:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451457184688094805112_0004_m_000107_395
[2021-05-16 22:36:15,621] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 4544 bytes result sent to driver
[2021-05-16 22:36:15,623] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (5deb0a1bddc0, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:15,624] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-16 22:36:15,624] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 2248 ms on 5deb0a1bddc0 (executor driver) (107/200)
[2021-05-16 22:36:15,649] {docker.py:276} INFO - 21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:15,651] {docker.py:276} INFO - 21/05/17 01:36:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455308435033115857900_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455308435033115857900_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455308435033115857900_0004}; taskId=attempt_202105170134455308435033115857900_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34991c14}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134455308435033115857900_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455308435033115857900_0004_m_000110_398
[2021-05-16 22:36:15,654] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134455308435033115857900_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455308435033115857900_0004_m_000110_398 : duration 0:00.003s
[2021-05-16 22:36:15,725] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134457504609216681081747_0004_m_000106_394: needsTaskCommit() Task attempt_202105170134457504609216681081747_0004_m_000106_394
[2021-05-16 22:36:15,725] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_202105170134457504609216681081747_0004_m_000106_394: needsTaskCommit() Task attempt_202105170134457504609216681081747_0004_m_000106_394: duration 0:00.001s
[2021-05-16 22:36:15,726] {docker.py:276} INFO - 21/05/17 01:36:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457504609216681081747_0004_m_000106_394
[2021-05-16 22:36:15,726] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 4587 bytes result sent to driver
[2021-05-16 22:36:15,727] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (5deb0a1bddc0, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:15,728] {docker.py:276} INFO - 21/05/17 01:36:15 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-16 22:36:15,729] {docker.py:276} INFO - 21/05/17 01:36:15 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 2851 ms on 5deb0a1bddc0 (executor driver) (108/200)
[2021-05-16 22:36:15,737] {docker.py:276} INFO - 21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:15,737] {docker.py:276} INFO - 21/05/17 01:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:15,739] {docker.py:276} INFO - 21/05/17 01:36:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:36:15,739] {docker.py:276} INFO - 21/05/17 01:36:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:15,740] {docker.py:276} INFO - 21/05/17 01:36:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445566952452904988954_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445566952452904988954_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445566952452904988954_0004}; taskId=attempt_20210517013445566952452904988954_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fa1fccd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:15,740] {docker.py:276} INFO - 21/05/17 01:36:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:15,740] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Starting: Task committer attempt_20210517013445566952452904988954_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445566952452904988954_0004_m_000111_399
[2021-05-16 22:36:15,743] {docker.py:276} INFO - 21/05/17 01:36:15 INFO StagingCommitter: Task committer attempt_20210517013445566952452904988954_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445566952452904988954_0004_m_000111_399 : duration 0:00.003s
[2021-05-16 22:36:17,740] {docker.py:276} INFO - 21/05/17 01:36:17 INFO StagingCommitter: Starting: Task committer attempt_202105170134453139281247239585647_0004_m_000108_396: needsTaskCommit() Task attempt_202105170134453139281247239585647_0004_m_000108_396
[2021-05-16 22:36:17,741] {docker.py:276} INFO - 21/05/17 01:36:17 INFO StagingCommitter: Task committer attempt_202105170134453139281247239585647_0004_m_000108_396: needsTaskCommit() Task attempt_202105170134453139281247239585647_0004_m_000108_396: duration 0:00.001s
21/05/17 01:36:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453139281247239585647_0004_m_000108_396
[2021-05-16 22:36:17,741] {docker.py:276} INFO - 21/05/17 01:36:17 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 4587 bytes result sent to driver
[2021-05-16 22:36:17,743] {docker.py:276} INFO - 21/05/17 01:36:17 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (5deb0a1bddc0, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:17,744] {docker.py:276} INFO - 21/05/17 01:36:17 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-16 22:36:17,745] {docker.py:276} INFO - 21/05/17 01:36:17 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 2594 ms on 5deb0a1bddc0 (executor driver) (109/200)
[2021-05-16 22:36:17,753] {docker.py:276} INFO - 21/05/17 01:36:17 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:17,755] {docker.py:276} INFO - 21/05/17 01:36:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:17,756] {docker.py:276} INFO - 21/05/17 01:36:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454540632989370043817_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454540632989370043817_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454540632989370043817_0004}; taskId=attempt_202105170134454540632989370043817_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e6e47c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:17 INFO StagingCommitter: Starting: Task committer attempt_202105170134454540632989370043817_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454540632989370043817_0004_m_000112_400
[2021-05-16 22:36:17,759] {docker.py:276} INFO - 21/05/17 01:36:17 INFO StagingCommitter: Task committer attempt_202105170134454540632989370043817_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454540632989370043817_0004_m_000112_400 : duration 0:00.003s
[2021-05-16 22:36:18,278] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Starting: Task committer attempt_202105170134455308435033115857900_0004_m_000110_398: needsTaskCommit() Task attempt_202105170134455308435033115857900_0004_m_000110_398
[2021-05-16 22:36:18,279] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Task committer attempt_202105170134455308435033115857900_0004_m_000110_398: needsTaskCommit() Task attempt_202105170134455308435033115857900_0004_m_000110_398: duration 0:00.000s
21/05/17 01:36:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455308435033115857900_0004_m_000110_398
[2021-05-16 22:36:18,279] {docker.py:276} INFO - 21/05/17 01:36:18 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 4587 bytes result sent to driver
[2021-05-16 22:36:18,280] {docker.py:276} INFO - 21/05/17 01:36:18 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (5deb0a1bddc0, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:18,281] {docker.py:276} INFO - 21/05/17 01:36:18 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-16 22:36:18,281] {docker.py:276} INFO - 21/05/17 01:36:18 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 2662 ms on 5deb0a1bddc0 (executor driver) (110/200)
[2021-05-16 22:36:18,292] {docker.py:276} INFO - 21/05/17 01:36:18 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:18,295] {docker.py:276} INFO - 21/05/17 01:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:18,296] {docker.py:276} INFO - 21/05/17 01:36:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453969818975158877166_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453969818975158877166_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453969818975158877166_0004}; taskId=attempt_202105170134453969818975158877166_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7df9ef4e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:18 INFO StagingCommitter: Starting: Task committer attempt_202105170134453969818975158877166_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453969818975158877166_0004_m_000113_401
[2021-05-16 22:36:18,301] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Task committer attempt_202105170134453969818975158877166_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453969818975158877166_0004_m_000113_401 : duration 0:00.005s
[2021-05-16 22:36:18,440] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Starting: Task committer attempt_20210517013445566952452904988954_0004_m_000111_399: needsTaskCommit() Task attempt_20210517013445566952452904988954_0004_m_000111_399
[2021-05-16 22:36:18,441] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Task committer attempt_20210517013445566952452904988954_0004_m_000111_399: needsTaskCommit() Task attempt_20210517013445566952452904988954_0004_m_000111_399: duration 0:00.000s
21/05/17 01:36:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445566952452904988954_0004_m_000111_399
[2021-05-16 22:36:18,442] {docker.py:276} INFO - 21/05/17 01:36:18 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 4544 bytes result sent to driver
[2021-05-16 22:36:18,443] {docker.py:276} INFO - 21/05/17 01:36:18 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (5deb0a1bddc0, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:18,444] {docker.py:276} INFO - 21/05/17 01:36:18 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-16 22:36:18,444] {docker.py:276} INFO - 21/05/17 01:36:18 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 2720 ms on 5deb0a1bddc0 (executor driver) (111/200)
[2021-05-16 22:36:18,455] {docker.py:276} INFO - 21/05/17 01:36:18 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:18,457] {docker.py:276} INFO - 21/05/17 01:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:36:18,458] {docker.py:276} INFO - 21/05/17 01:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:18,458] {docker.py:276} INFO - 21/05/17 01:36:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:18,459] {docker.py:276} INFO - 21/05/17 01:36:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452454126442617180591_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452454126442617180591_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452454126442617180591_0004}; taskId=attempt_202105170134452454126442617180591_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@715d4feb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:18,459] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Starting: Task committer attempt_202105170134452454126442617180591_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452454126442617180591_0004_m_000114_402
[2021-05-16 22:36:18,462] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Task committer attempt_202105170134452454126442617180591_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452454126442617180591_0004_m_000114_402 : duration 0:00.005s
[2021-05-16 22:36:18,634] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Starting: Task committer attempt_202105170134455672610624164300380_0004_m_000109_397: needsTaskCommit() Task attempt_202105170134455672610624164300380_0004_m_000109_397
[2021-05-16 22:36:18,634] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Task committer attempt_202105170134455672610624164300380_0004_m_000109_397: needsTaskCommit() Task attempt_202105170134455672610624164300380_0004_m_000109_397: duration 0:00.001s
21/05/17 01:36:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455672610624164300380_0004_m_000109_397
[2021-05-16 22:36:18,636] {docker.py:276} INFO - 21/05/17 01:36:18 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 4587 bytes result sent to driver
[2021-05-16 22:36:18,637] {docker.py:276} INFO - 21/05/17 01:36:18 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (5deb0a1bddc0, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:18,638] {docker.py:276} INFO - 21/05/17 01:36:18 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
21/05/17 01:36:18 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 3198 ms on 5deb0a1bddc0 (executor driver) (112/200)
[2021-05-16 22:36:18,648] {docker.py:276} INFO - 21/05/17 01:36:18 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:18,650] {docker.py:276} INFO - 21/05/17 01:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445211545695592390296_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445211545695592390296_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445211545695592390296_0004}; taskId=attempt_20210517013445211545695592390296_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6691ff4a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:18,650] {docker.py:276} INFO - 21/05/17 01:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:18 INFO StagingCommitter: Starting: Task committer attempt_20210517013445211545695592390296_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445211545695592390296_0004_m_000115_403
[2021-05-16 22:36:18,653] {docker.py:276} INFO - 21/05/17 01:36:18 INFO StagingCommitter: Task committer attempt_20210517013445211545695592390296_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445211545695592390296_0004_m_000115_403 : duration 0:00.002s
[2021-05-16 22:36:20,377] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Starting: Task committer attempt_202105170134454540632989370043817_0004_m_000112_400: needsTaskCommit() Task attempt_202105170134454540632989370043817_0004_m_000112_400
[2021-05-16 22:36:20,377] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Task committer attempt_202105170134454540632989370043817_0004_m_000112_400: needsTaskCommit() Task attempt_202105170134454540632989370043817_0004_m_000112_400: duration 0:00.001s
21/05/17 01:36:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454540632989370043817_0004_m_000112_400
[2021-05-16 22:36:20,378] {docker.py:276} INFO - 21/05/17 01:36:20 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 4544 bytes result sent to driver
[2021-05-16 22:36:20,379] {docker.py:276} INFO - 21/05/17 01:36:20 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (5deb0a1bddc0, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:20,381] {docker.py:276} INFO - 21/05/17 01:36:20 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
21/05/17 01:36:20 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 2641 ms on 5deb0a1bddc0 (executor driver) (113/200)
[2021-05-16 22:36:20,390] {docker.py:276} INFO - 21/05/17 01:36:20 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:20,392] {docker.py:276} INFO - 21/05/17 01:36:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454950266506575198641_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454950266506575198641_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454950266506575198641_0004}; taskId=attempt_202105170134454950266506575198641_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4492cae0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:20,392] {docker.py:276} INFO - 21/05/17 01:36:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:20 INFO StagingCommitter: Starting: Task committer attempt_202105170134454950266506575198641_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454950266506575198641_0004_m_000116_404
[2021-05-16 22:36:20,395] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Task committer attempt_202105170134454950266506575198641_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454950266506575198641_0004_m_000116_404 : duration 0:00.003s
[2021-05-16 22:36:20,539] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Starting: Task committer attempt_202105170134452454126442617180591_0004_m_000114_402: needsTaskCommit() Task attempt_202105170134452454126442617180591_0004_m_000114_402
[2021-05-16 22:36:20,539] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Task committer attempt_202105170134452454126442617180591_0004_m_000114_402: needsTaskCommit() Task attempt_202105170134452454126442617180591_0004_m_000114_402: duration 0:00.000s
[2021-05-16 22:36:20,540] {docker.py:276} INFO - 21/05/17 01:36:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452454126442617180591_0004_m_000114_402
[2021-05-16 22:36:20,542] {docker.py:276} INFO - 21/05/17 01:36:20 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 4544 bytes result sent to driver
[2021-05-16 22:36:20,543] {docker.py:276} INFO - 21/05/17 01:36:20 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (5deb0a1bddc0, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:20,544] {docker.py:276} INFO - 21/05/17 01:36:20 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
[2021-05-16 22:36:20,544] {docker.py:276} INFO - 21/05/17 01:36:20 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 2104 ms on 5deb0a1bddc0 (executor driver) (114/200)
[2021-05-16 22:36:20,553] {docker.py:276} INFO - 21/05/17 01:36:20 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:20,556] {docker.py:276} INFO - 21/05/17 01:36:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:20,557] {docker.py:276} INFO - 21/05/17 01:36:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451921777812056000585_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451921777812056000585_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451921777812056000585_0004}; taskId=attempt_202105170134451921777812056000585_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c2c1eff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:20,558] {docker.py:276} INFO - 21/05/17 01:36:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:20 INFO StagingCommitter: Starting: Task committer attempt_202105170134451921777812056000585_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451921777812056000585_0004_m_000117_405
[2021-05-16 22:36:20,560] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Task committer attempt_202105170134451921777812056000585_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451921777812056000585_0004_m_000117_405 : duration 0:00.003s
[2021-05-16 22:36:20,872] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Starting: Task committer attempt_202105170134453969818975158877166_0004_m_000113_401: needsTaskCommit() Task attempt_202105170134453969818975158877166_0004_m_000113_401
[2021-05-16 22:36:20,873] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Task committer attempt_202105170134453969818975158877166_0004_m_000113_401: needsTaskCommit() Task attempt_202105170134453969818975158877166_0004_m_000113_401: duration 0:00.001s
21/05/17 01:36:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453969818975158877166_0004_m_000113_401
[2021-05-16 22:36:20,875] {docker.py:276} INFO - 21/05/17 01:36:20 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 4544 bytes result sent to driver
[2021-05-16 22:36:20,875] {docker.py:276} INFO - 21/05/17 01:36:20 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (5deb0a1bddc0, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:20,876] {docker.py:276} INFO - 21/05/17 01:36:20 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 2599 ms on 5deb0a1bddc0 (executor driver) (115/200)
21/05/17 01:36:20 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-16 22:36:20,887] {docker.py:276} INFO - 21/05/17 01:36:20 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:20,891] {docker.py:276} INFO - 21/05/17 01:36:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:20,891] {docker.py:276} INFO - 21/05/17 01:36:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445691153532859254938_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445691153532859254938_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445691153532859254938_0004}; taskId=attempt_20210517013445691153532859254938_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a42e6be}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:20 INFO StagingCommitter: Starting: Task committer attempt_20210517013445691153532859254938_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445691153532859254938_0004_m_000118_406
[2021-05-16 22:36:20,894] {docker.py:276} INFO - 21/05/17 01:36:20 INFO StagingCommitter: Task committer attempt_20210517013445691153532859254938_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445691153532859254938_0004_m_000118_406 : duration 0:00.003s
[2021-05-16 22:36:21,337] {docker.py:276} INFO - 21/05/17 01:36:21 INFO StagingCommitter: Starting: Task committer attempt_20210517013445211545695592390296_0004_m_000115_403: needsTaskCommit() Task attempt_20210517013445211545695592390296_0004_m_000115_403
21/05/17 01:36:21 INFO StagingCommitter: Task committer attempt_20210517013445211545695592390296_0004_m_000115_403: needsTaskCommit() Task attempt_20210517013445211545695592390296_0004_m_000115_403: duration 0:00.000s
21/05/17 01:36:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445211545695592390296_0004_m_000115_403
[2021-05-16 22:36:21,340] {docker.py:276} INFO - 21/05/17 01:36:21 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 4544 bytes result sent to driver
[2021-05-16 22:36:21,341] {docker.py:276} INFO - 21/05/17 01:36:21 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (5deb0a1bddc0, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:21,341] {docker.py:276} INFO - 21/05/17 01:36:21 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
[2021-05-16 22:36:21,342] {docker.py:276} INFO - 21/05/17 01:36:21 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 2708 ms on 5deb0a1bddc0 (executor driver) (116/200)
[2021-05-16 22:36:21,351] {docker.py:276} INFO - 21/05/17 01:36:21 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:21,353] {docker.py:276} INFO - 21/05/17 01:36:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451203914937528679200_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451203914937528679200_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451203914937528679200_0004}; taskId=attempt_202105170134451203914937528679200_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f77478b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:21,353] {docker.py:276} INFO - 21/05/17 01:36:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:21 INFO StagingCommitter: Starting: Task committer attempt_202105170134451203914937528679200_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451203914937528679200_0004_m_000119_407
[2021-05-16 22:36:21,356] {docker.py:276} INFO - 21/05/17 01:36:21 INFO StagingCommitter: Task committer attempt_202105170134451203914937528679200_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451203914937528679200_0004_m_000119_407 : duration 0:00.004s
[2021-05-16 22:36:23,143] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Starting: Task committer attempt_202105170134451921777812056000585_0004_m_000117_405: needsTaskCommit() Task attempt_202105170134451921777812056000585_0004_m_000117_405
[2021-05-16 22:36:23,144] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Task committer attempt_202105170134451921777812056000585_0004_m_000117_405: needsTaskCommit() Task attempt_202105170134451921777812056000585_0004_m_000117_405: duration 0:00.001s
21/05/17 01:36:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451921777812056000585_0004_m_000117_405
[2021-05-16 22:36:23,145] {docker.py:276} INFO - 21/05/17 01:36:23 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 4587 bytes result sent to driver
[2021-05-16 22:36:23,146] {docker.py:276} INFO - 21/05/17 01:36:23 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (5deb0a1bddc0, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:23,147] {docker.py:276} INFO - 21/05/17 01:36:23 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 2607 ms on 5deb0a1bddc0 (executor driver) (117/200)
[2021-05-16 22:36:23,148] {docker.py:276} INFO - 21/05/17 01:36:23 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-16 22:36:23,165] {docker.py:276} INFO - 21/05/17 01:36:23 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:23,167] {docker.py:276} INFO - 21/05/17 01:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451777369946846903010_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451777369946846903010_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451777369946846903010_0004}; taskId=attempt_202105170134451777369946846903010_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d47398d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:23 INFO StagingCommitter: Starting: Task committer attempt_202105170134451777369946846903010_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451777369946846903010_0004_m_000120_408
[2021-05-16 22:36:23,171] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Task committer attempt_202105170134451777369946846903010_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451777369946846903010_0004_m_000120_408 : duration 0:00.003s
[2021-05-16 22:36:23,544] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Starting: Task committer attempt_20210517013445691153532859254938_0004_m_000118_406: needsTaskCommit() Task attempt_20210517013445691153532859254938_0004_m_000118_406
[2021-05-16 22:36:23,545] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Task committer attempt_20210517013445691153532859254938_0004_m_000118_406: needsTaskCommit() Task attempt_20210517013445691153532859254938_0004_m_000118_406: duration 0:00.001s
[2021-05-16 22:36:23,545] {docker.py:276} INFO - 21/05/17 01:36:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445691153532859254938_0004_m_000118_406
[2021-05-16 22:36:23,547] {docker.py:276} INFO - 21/05/17 01:36:23 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 4587 bytes result sent to driver
[2021-05-16 22:36:23,548] {docker.py:276} INFO - 21/05/17 01:36:23 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (5deb0a1bddc0, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:23,548] {docker.py:276} INFO - 21/05/17 01:36:23 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
21/05/17 01:36:23 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 2678 ms on 5deb0a1bddc0 (executor driver) (118/200)
[2021-05-16 22:36:23,557] {docker.py:276} INFO - 21/05/17 01:36:23 INFO ShuffleBlockFetcherIterator: Getting 5 (25.0 KiB) non-empty blocks including 5 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:23,558] {docker.py:276} INFO - 21/05/17 01:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454951631262370283649_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454951631262370283649_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454951631262370283649_0004}; taskId=attempt_202105170134454951631262370283649_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33d94f96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:23,559] {docker.py:276} INFO - 21/05/17 01:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:23 INFO StagingCommitter: Starting: Task committer attempt_202105170134454951631262370283649_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454951631262370283649_0004_m_000121_409
[2021-05-16 22:36:23,562] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Task committer attempt_202105170134454951631262370283649_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454951631262370283649_0004_m_000121_409 : duration 0:00.002s
[2021-05-16 22:36:23,703] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Starting: Task committer attempt_202105170134454950266506575198641_0004_m_000116_404: needsTaskCommit() Task attempt_202105170134454950266506575198641_0004_m_000116_404
[2021-05-16 22:36:23,704] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Task committer attempt_202105170134454950266506575198641_0004_m_000116_404: needsTaskCommit() Task attempt_202105170134454950266506575198641_0004_m_000116_404: duration 0:00.000s
21/05/17 01:36:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454950266506575198641_0004_m_000116_404
[2021-05-16 22:36:23,705] {docker.py:276} INFO - 21/05/17 01:36:23 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 4587 bytes result sent to driver
[2021-05-16 22:36:23,706] {docker.py:276} INFO - 21/05/17 01:36:23 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (5deb0a1bddc0, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:23,707] {docker.py:276} INFO - 21/05/17 01:36:23 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 3332 ms on 5deb0a1bddc0 (executor driver) (119/200)
21/05/17 01:36:23 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
[2021-05-16 22:36:23,716] {docker.py:276} INFO - 21/05/17 01:36:23 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:23,718] {docker.py:276} INFO - 21/05/17 01:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:23,718] {docker.py:276} INFO - 21/05/17 01:36:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453768386893649697156_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453768386893649697156_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453768386893649697156_0004}; taskId=attempt_202105170134453768386893649697156_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c972ab9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:23,718] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Starting: Task committer attempt_202105170134453768386893649697156_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453768386893649697156_0004_m_000122_410
[2021-05-16 22:36:23,721] {docker.py:276} INFO - 21/05/17 01:36:23 INFO StagingCommitter: Task committer attempt_202105170134453768386893649697156_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453768386893649697156_0004_m_000122_410 : duration 0:00.003s
[2021-05-16 22:36:24,124] {docker.py:276} INFO - 21/05/17 01:36:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134451203914937528679200_0004_m_000119_407: needsTaskCommit() Task attempt_202105170134451203914937528679200_0004_m_000119_407
[2021-05-16 22:36:24,125] {docker.py:276} INFO - 21/05/17 01:36:24 INFO StagingCommitter: Task committer attempt_202105170134451203914937528679200_0004_m_000119_407: needsTaskCommit() Task attempt_202105170134451203914937528679200_0004_m_000119_407: duration 0:00.001s
[2021-05-16 22:36:24,126] {docker.py:276} INFO - 21/05/17 01:36:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451203914937528679200_0004_m_000119_407
[2021-05-16 22:36:24,128] {docker.py:276} INFO - 21/05/17 01:36:24 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 4587 bytes result sent to driver
[2021-05-16 22:36:24,129] {docker.py:276} INFO - 21/05/17 01:36:24 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (5deb0a1bddc0, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:24,131] {docker.py:276} INFO - 21/05/17 01:36:24 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 2794 ms on 5deb0a1bddc0 (executor driver) (120/200)
[2021-05-16 22:36:24,131] {docker.py:276} INFO - 21/05/17 01:36:24 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
[2021-05-16 22:36:24,140] {docker.py:276} INFO - 21/05/17 01:36:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:24,142] {docker.py:276} INFO - 21/05/17 01:36:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:24,142] {docker.py:276} INFO - 21/05/17 01:36:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452284985982737940856_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452284985982737940856_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452284985982737940856_0004}; taskId=attempt_202105170134452284985982737940856_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fc9026e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:24,142] {docker.py:276} INFO - 21/05/17 01:36:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:24 INFO StagingCommitter: Starting: Task committer attempt_202105170134452284985982737940856_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452284985982737940856_0004_m_000123_411
[2021-05-16 22:36:24,145] {docker.py:276} INFO - 21/05/17 01:36:24 INFO StagingCommitter: Task committer attempt_202105170134452284985982737940856_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452284985982737940856_0004_m_000123_411 : duration 0:00.003s
[2021-05-16 22:36:25,980] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134451777369946846903010_0004_m_000120_408: needsTaskCommit() Task attempt_202105170134451777369946846903010_0004_m_000120_408
[2021-05-16 22:36:25,981] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134451777369946846903010_0004_m_000120_408: needsTaskCommit() Task attempt_202105170134451777369946846903010_0004_m_000120_408: duration 0:00.001s
21/05/17 01:36:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451777369946846903010_0004_m_000120_408
[2021-05-16 22:36:25,983] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 4587 bytes result sent to driver
[2021-05-16 22:36:25,984] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (5deb0a1bddc0, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:25,986] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 2842 ms on 5deb0a1bddc0 (executor driver) (121/200)
[2021-05-16 22:36:25,986] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-16 22:36:25,994] {docker.py:276} INFO - 21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:25,996] {docker.py:276} INFO - 21/05/17 01:36:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:25,996] {docker.py:276} INFO - 21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:25,996] {docker.py:276} INFO - 21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456023006693681709031_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456023006693681709031_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456023006693681709031_0004}; taskId=attempt_202105170134456023006693681709031_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f0fb0cd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134456023006693681709031_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456023006693681709031_0004_m_000124_412
[2021-05-16 22:36:25,999] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134456023006693681709031_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456023006693681709031_0004_m_000124_412 : duration 0:00.003s
[2021-05-16 22:36:26,323] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134454951631262370283649_0004_m_000121_409: needsTaskCommit() Task attempt_202105170134454951631262370283649_0004_m_000121_409
21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134454951631262370283649_0004_m_000121_409: needsTaskCommit() Task attempt_202105170134454951631262370283649_0004_m_000121_409: duration 0:00.000s
21/05/17 01:36:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454951631262370283649_0004_m_000121_409
21/05/17 01:36:26 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 4544 bytes result sent to driver
[2021-05-16 22:36:26,325] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (5deb0a1bddc0, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:26,326] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-16 22:36:26,326] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 2780 ms on 5deb0a1bddc0 (executor driver) (122/200)
[2021-05-16 22:36:26,336] {docker.py:276} INFO - 21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:26,338] {docker.py:276} INFO - 21/05/17 01:36:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453101980358038144910_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453101980358038144910_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453101980358038144910_0004}; taskId=attempt_202105170134453101980358038144910_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b15921a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:26,338] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134453101980358038144910_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453101980358038144910_0004_m_000125_413
[2021-05-16 22:36:26,341] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134453101980358038144910_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453101980358038144910_0004_m_000125_413 : duration 0:00.003s
[2021-05-16 22:36:26,358] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134453768386893649697156_0004_m_000122_410: needsTaskCommit() Task attempt_202105170134453768386893649697156_0004_m_000122_410
[2021-05-16 22:36:26,359] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134453768386893649697156_0004_m_000122_410: needsTaskCommit() Task attempt_202105170134453768386893649697156_0004_m_000122_410: duration 0:00.001s
21/05/17 01:36:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453768386893649697156_0004_m_000122_410
[2021-05-16 22:36:26,359] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 4544 bytes result sent to driver
[2021-05-16 22:36:26,360] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (5deb0a1bddc0, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:26,360] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
[2021-05-16 22:36:26,361] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 2658 ms on 5deb0a1bddc0 (executor driver) (123/200)
[2021-05-16 22:36:26,367] {docker.py:276} INFO - 21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:26,369] {docker.py:276} INFO - 21/05/17 01:36:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451733013713418550208_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451733013713418550208_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451733013713418550208_0004}; taskId=attempt_202105170134451733013713418550208_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34769335}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:26,370] {docker.py:276} INFO - 21/05/17 01:36:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:26,370] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134451733013713418550208_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451733013713418550208_0004_m_000126_414
[2021-05-16 22:36:26,373] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134451733013713418550208_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451733013713418550208_0004_m_000126_414 : duration 0:00.003s
[2021-05-16 22:36:26,749] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134452284985982737940856_0004_m_000123_411: needsTaskCommit() Task attempt_202105170134452284985982737940856_0004_m_000123_411
21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134452284985982737940856_0004_m_000123_411: needsTaskCommit() Task attempt_202105170134452284985982737940856_0004_m_000123_411: duration 0:00.001s
21/05/17 01:36:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452284985982737940856_0004_m_000123_411
[2021-05-16 22:36:26,752] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 4544 bytes result sent to driver
[2021-05-16 22:36:26,753] {docker.py:276} INFO - 21/05/17 01:36:26 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (5deb0a1bddc0, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:36:26 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 2627 ms on 5deb0a1bddc0 (executor driver) (124/200)
[2021-05-16 22:36:26,754] {docker.py:276} INFO - 21/05/17 01:36:26 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-16 22:36:26,768] {docker.py:276} INFO - 21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:26,769] {docker.py:276} INFO - 21/05/17 01:36:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-16 22:36:26,772] {docker.py:276} INFO - 21/05/17 01:36:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:26,772] {docker.py:276} INFO - 21/05/17 01:36:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452832792010721155323_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452832792010721155323_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452832792010721155323_0004}; taskId=attempt_202105170134452832792010721155323_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f4b2886}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:26 INFO StagingCommitter: Starting: Task committer attempt_202105170134452832792010721155323_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452832792010721155323_0004_m_000127_415
[2021-05-16 22:36:26,775] {docker.py:276} INFO - 21/05/17 01:36:26 INFO StagingCommitter: Task committer attempt_202105170134452832792010721155323_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452832792010721155323_0004_m_000127_415 : duration 0:00.003s
[2021-05-16 22:36:28,613] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Starting: Task committer attempt_202105170134456023006693681709031_0004_m_000124_412: needsTaskCommit() Task attempt_202105170134456023006693681709031_0004_m_000124_412
[2021-05-16 22:36:28,613] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Task committer attempt_202105170134456023006693681709031_0004_m_000124_412: needsTaskCommit() Task attempt_202105170134456023006693681709031_0004_m_000124_412: duration 0:00.000s
21/05/17 01:36:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456023006693681709031_0004_m_000124_412
[2021-05-16 22:36:28,614] {docker.py:276} INFO - 21/05/17 01:36:28 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 4544 bytes result sent to driver
[2021-05-16 22:36:28,615] {docker.py:276} INFO - 21/05/17 01:36:28 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (5deb0a1bddc0, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:28,616] {docker.py:276} INFO - 21/05/17 01:36:28 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
21/05/17 01:36:28 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 2602 ms on 5deb0a1bddc0 (executor driver) (125/200)
[2021-05-16 22:36:28,624] {docker.py:276} INFO - 21/05/17 01:36:28 INFO ShuffleBlockFetcherIterator: Getting 5 (21.2 KiB) non-empty blocks including 5 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:28,625] {docker.py:276} INFO - 21/05/17 01:36:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457909714917714146970_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457909714917714146970_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457909714917714146970_0004}; taskId=attempt_202105170134457909714917714146970_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c2059af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:28,626] {docker.py:276} INFO - 21/05/17 01:36:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:28 INFO StagingCommitter: Starting: Task committer attempt_202105170134457909714917714146970_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457909714917714146970_0004_m_000128_416
[2021-05-16 22:36:28,628] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Task committer attempt_202105170134457909714917714146970_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457909714917714146970_0004_m_000128_416 : duration 0:00.002s
[2021-05-16 22:36:28,941] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Starting: Task committer attempt_202105170134453101980358038144910_0004_m_000125_413: needsTaskCommit() Task attempt_202105170134453101980358038144910_0004_m_000125_413
[2021-05-16 22:36:28,942] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Task committer attempt_202105170134453101980358038144910_0004_m_000125_413: needsTaskCommit() Task attempt_202105170134453101980358038144910_0004_m_000125_413: duration 0:00.001s
21/05/17 01:36:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453101980358038144910_0004_m_000125_413
[2021-05-16 22:36:28,943] {docker.py:276} INFO - 21/05/17 01:36:28 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 4544 bytes result sent to driver
[2021-05-16 22:36:28,944] {docker.py:276} INFO - 21/05/17 01:36:28 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (5deb0a1bddc0, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:28,946] {docker.py:276} INFO - 21/05/17 01:36:28 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 2591 ms on 5deb0a1bddc0 (executor driver) (126/200)
[2021-05-16 22:36:28,947] {docker.py:276} INFO - 21/05/17 01:36:28 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-16 22:36:28,956] {docker.py:276} INFO - 21/05/17 01:36:28 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:28,957] {docker.py:276} INFO - 21/05/17 01:36:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452678419850083881843_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452678419850083881843_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452678419850083881843_0004}; taskId=attempt_202105170134452678419850083881843_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48d3bcbc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:28,958] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Starting: Task committer attempt_202105170134452678419850083881843_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452678419850083881843_0004_m_000129_417
[2021-05-16 22:36:28,961] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Task committer attempt_202105170134452678419850083881843_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452678419850083881843_0004_m_000129_417 : duration 0:00.003s
[2021-05-16 22:36:28,985] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Starting: Task committer attempt_202105170134451733013713418550208_0004_m_000126_414: needsTaskCommit() Task attempt_202105170134451733013713418550208_0004_m_000126_414
[2021-05-16 22:36:28,986] {docker.py:276} INFO - 21/05/17 01:36:28 INFO StagingCommitter: Task committer attempt_202105170134451733013713418550208_0004_m_000126_414: needsTaskCommit() Task attempt_202105170134451733013713418550208_0004_m_000126_414: duration 0:00.000s
21/05/17 01:36:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451733013713418550208_0004_m_000126_414
[2021-05-16 22:36:28,986] {docker.py:276} INFO - 21/05/17 01:36:28 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 4544 bytes result sent to driver
[2021-05-16 22:36:28,988] {docker.py:276} INFO - 21/05/17 01:36:28 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (5deb0a1bddc0, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:28,988] {docker.py:276} INFO - 21/05/17 01:36:28 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 2597 ms on 5deb0a1bddc0 (executor driver) (127/200)
[2021-05-16 22:36:28,990] {docker.py:276} INFO - 21/05/17 01:36:28 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-16 22:36:29,006] {docker.py:276} INFO - 21/05/17 01:36:29 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:29,007] {docker.py:276} INFO - 21/05/17 01:36:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:29,008] {docker.py:276} INFO - 21/05/17 01:36:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:29,008] {docker.py:276} INFO - 21/05/17 01:36:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454369614118958959211_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454369614118958959211_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454369614118958959211_0004}; taskId=attempt_202105170134454369614118958959211_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32a868d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:29,008] {docker.py:276} INFO - 21/05/17 01:36:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:29 INFO StagingCommitter: Starting: Task committer attempt_202105170134454369614118958959211_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454369614118958959211_0004_m_000130_418
[2021-05-16 22:36:29,011] {docker.py:276} INFO - 21/05/17 01:36:29 INFO StagingCommitter: Task committer attempt_202105170134454369614118958959211_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454369614118958959211_0004_m_000130_418 : duration 0:00.002s
[2021-05-16 22:36:29,452] {docker.py:276} INFO - 21/05/17 01:36:29 INFO StagingCommitter: Starting: Task committer attempt_202105170134452832792010721155323_0004_m_000127_415: needsTaskCommit() Task attempt_202105170134452832792010721155323_0004_m_000127_415
[2021-05-16 22:36:29,453] {docker.py:276} INFO - 21/05/17 01:36:29 INFO StagingCommitter: Task committer attempt_202105170134452832792010721155323_0004_m_000127_415: needsTaskCommit() Task attempt_202105170134452832792010721155323_0004_m_000127_415: duration 0:00.000s
21/05/17 01:36:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452832792010721155323_0004_m_000127_415
[2021-05-16 22:36:29,454] {docker.py:276} INFO - 21/05/17 01:36:29 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 4587 bytes result sent to driver
[2021-05-16 22:36:29,455] {docker.py:276} INFO - 21/05/17 01:36:29 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (5deb0a1bddc0, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:29,457] {docker.py:276} INFO - 21/05/17 01:36:29 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 2674 ms on 5deb0a1bddc0 (executor driver) (128/200)
[2021-05-16 22:36:29,458] {docker.py:276} INFO - 21/05/17 01:36:29 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-16 22:36:29,468] {docker.py:276} INFO - 21/05/17 01:36:29 INFO ShuffleBlockFetcherIterator: Getting 5 (21.1 KiB) non-empty blocks including 5 (21.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:29,470] {docker.py:276} INFO - 21/05/17 01:36:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:29,470] {docker.py:276} INFO - 21/05/17 01:36:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458725081791750931655_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458725081791750931655_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458725081791750931655_0004}; taskId=attempt_202105170134458725081791750931655_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4117ccd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:29 INFO StagingCommitter: Starting: Task committer attempt_202105170134458725081791750931655_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458725081791750931655_0004_m_000131_419
[2021-05-16 22:36:29,474] {docker.py:276} INFO - 21/05/17 01:36:29 INFO StagingCommitter: Task committer attempt_202105170134458725081791750931655_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458725081791750931655_0004_m_000131_419 : duration 0:00.003s
[2021-05-16 22:36:31,361] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Starting: Task committer attempt_202105170134457909714917714146970_0004_m_000128_416: needsTaskCommit() Task attempt_202105170134457909714917714146970_0004_m_000128_416
[2021-05-16 22:36:31,361] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Task committer attempt_202105170134457909714917714146970_0004_m_000128_416: needsTaskCommit() Task attempt_202105170134457909714917714146970_0004_m_000128_416: duration 0:00.001s
21/05/17 01:36:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457909714917714146970_0004_m_000128_416
[2021-05-16 22:36:31,362] {docker.py:276} INFO - 21/05/17 01:36:31 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 4587 bytes result sent to driver
[2021-05-16 22:36:31,363] {docker.py:276} INFO - 21/05/17 01:36:31 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (5deb0a1bddc0, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:31,364] {docker.py:276} INFO - 21/05/17 01:36:31 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-16 22:36:31,364] {docker.py:276} INFO - 21/05/17 01:36:31 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 2752 ms on 5deb0a1bddc0 (executor driver) (129/200)
[2021-05-16 22:36:31,372] {docker.py:276} INFO - 21/05/17 01:36:31 INFO ShuffleBlockFetcherIterator: Getting 5 (21.7 KiB) non-empty blocks including 5 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:31,373] {docker.py:276} INFO - 21/05/17 01:36:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445258198698740738194_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445258198698740738194_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445258198698740738194_0004}; taskId=attempt_20210517013445258198698740738194_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5139a003}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:31,374] {docker.py:276} INFO - 21/05/17 01:36:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:31,374] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Starting: Task committer attempt_20210517013445258198698740738194_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445258198698740738194_0004_m_000132_420
[2021-05-16 22:36:31,377] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Task committer attempt_20210517013445258198698740738194_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445258198698740738194_0004_m_000132_420 : duration 0:00.003s
[2021-05-16 22:36:31,721] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Starting: Task committer attempt_202105170134454369614118958959211_0004_m_000130_418: needsTaskCommit() Task attempt_202105170134454369614118958959211_0004_m_000130_418
[2021-05-16 22:36:31,722] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Task committer attempt_202105170134454369614118958959211_0004_m_000130_418: needsTaskCommit() Task attempt_202105170134454369614118958959211_0004_m_000130_418: duration 0:00.001s
21/05/17 01:36:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454369614118958959211_0004_m_000130_418
[2021-05-16 22:36:31,723] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Starting: Task committer attempt_202105170134452678419850083881843_0004_m_000129_417: needsTaskCommit() Task attempt_202105170134452678419850083881843_0004_m_000129_417
21/05/17 01:36:31 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 4587 bytes result sent to driver
[2021-05-16 22:36:31,724] {docker.py:276} INFO - 21/05/17 01:36:31 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (5deb0a1bddc0, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:36:31 INFO StagingCommitter: Task committer attempt_202105170134452678419850083881843_0004_m_000129_417: needsTaskCommit() Task attempt_202105170134452678419850083881843_0004_m_000129_417: duration 0:00.001s
21/05/17 01:36:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452678419850083881843_0004_m_000129_417
[2021-05-16 22:36:31,725] {docker.py:276} INFO - 21/05/17 01:36:31 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 2741 ms on 5deb0a1bddc0 (executor driver) (130/200)
[2021-05-16 22:36:31,726] {docker.py:276} INFO - 21/05/17 01:36:31 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-16 22:36:31,727] {docker.py:276} INFO - 21/05/17 01:36:31 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 4587 bytes result sent to driver
[2021-05-16 22:36:31,729] {docker.py:276} INFO - 21/05/17 01:36:31 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (5deb0a1bddc0, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:31,730] {docker.py:276} INFO - 21/05/17 01:36:31 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
21/05/17 01:36:31 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 2789 ms on 5deb0a1bddc0 (executor driver) (131/200)
[2021-05-16 22:36:31,739] {docker.py:276} INFO - 21/05/17 01:36:31 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:31,739] {docker.py:276} INFO - 21/05/17 01:36:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 01:36:31 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:31,740] {docker.py:276} INFO - 21/05/17 01:36:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:31,742] {docker.py:276} INFO - 21/05/17 01:36:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457722101767401858753_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457722101767401858753_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457722101767401858753_0004}; taskId=attempt_202105170134457722101767401858753_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fa1b368}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:31 INFO StagingCommitter: Starting: Task committer attempt_202105170134457722101767401858753_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457722101767401858753_0004_m_000134_422
[2021-05-16 22:36:31,742] {docker.py:276} INFO - 21/05/17 01:36:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:36:31,743] {docker.py:276} INFO - 21/05/17 01:36:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:31,744] {docker.py:276} INFO - 21/05/17 01:36:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:31,744] {docker.py:276} INFO - 21/05/17 01:36:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453690972397321333714_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453690972397321333714_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453690972397321333714_0004}; taskId=attempt_202105170134453690972397321333714_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22f0d61f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:31,745] {docker.py:276} INFO - 21/05/17 01:36:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:31,745] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Starting: Task committer attempt_202105170134453690972397321333714_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453690972397321333714_0004_m_000133_421
[2021-05-16 22:36:31,747] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Task committer attempt_202105170134457722101767401858753_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457722101767401858753_0004_m_000134_422 : duration 0:00.006s
[2021-05-16 22:36:31,753] {docker.py:276} INFO - 21/05/17 01:36:31 INFO StagingCommitter: Task committer attempt_202105170134453690972397321333714_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453690972397321333714_0004_m_000133_421 : duration 0:00.009s
[2021-05-16 22:36:32,172] {docker.py:276} INFO - 21/05/17 01:36:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134458725081791750931655_0004_m_000131_419: needsTaskCommit() Task attempt_202105170134458725081791750931655_0004_m_000131_419
[2021-05-16 22:36:32,172] {docker.py:276} INFO - 21/05/17 01:36:32 INFO StagingCommitter: Task committer attempt_202105170134458725081791750931655_0004_m_000131_419: needsTaskCommit() Task attempt_202105170134458725081791750931655_0004_m_000131_419: duration 0:00.000s
21/05/17 01:36:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458725081791750931655_0004_m_000131_419
[2021-05-16 22:36:32,176] {docker.py:276} INFO - 21/05/17 01:36:32 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 4544 bytes result sent to driver
21/05/17 01:36:32 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (5deb0a1bddc0, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:32,177] {docker.py:276} INFO - 21/05/17 01:36:32 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-16 22:36:32,178] {docker.py:276} INFO - 21/05/17 01:36:32 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 2725 ms on 5deb0a1bddc0 (executor driver) (132/200)
[2021-05-16 22:36:32,187] {docker.py:276} INFO - 21/05/17 01:36:32 INFO ShuffleBlockFetcherIterator: Getting 5 (21.3 KiB) non-empty blocks including 5 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:32,189] {docker.py:276} INFO - 21/05/17 01:36:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456327450252416065049_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456327450252416065049_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456327450252416065049_0004}; taskId=attempt_202105170134456327450252416065049_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd716e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:32,190] {docker.py:276} INFO - 21/05/17 01:36:32 INFO StagingCommitter: Starting: Task committer attempt_202105170134456327450252416065049_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456327450252416065049_0004_m_000135_423
[2021-05-16 22:36:32,193] {docker.py:276} INFO - 21/05/17 01:36:32 INFO StagingCommitter: Task committer attempt_202105170134456327450252416065049_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456327450252416065049_0004_m_000135_423 : duration 0:00.003s
[2021-05-16 22:36:33,515] {docker.py:276} INFO - 21/05/17 01:36:33 INFO StagingCommitter: Starting: Task committer attempt_20210517013445258198698740738194_0004_m_000132_420: needsTaskCommit() Task attempt_20210517013445258198698740738194_0004_m_000132_420
[2021-05-16 22:36:33,516] {docker.py:276} INFO - 21/05/17 01:36:33 INFO StagingCommitter: Task committer attempt_20210517013445258198698740738194_0004_m_000132_420: needsTaskCommit() Task attempt_20210517013445258198698740738194_0004_m_000132_420: duration 0:00.002s
[2021-05-16 22:36:33,517] {docker.py:276} INFO - 21/05/17 01:36:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445258198698740738194_0004_m_000132_420
[2021-05-16 22:36:33,518] {docker.py:276} INFO - 21/05/17 01:36:33 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 4544 bytes result sent to driver
[2021-05-16 22:36:33,520] {docker.py:276} INFO - 21/05/17 01:36:33 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (5deb0a1bddc0, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:33,521] {docker.py:276} INFO - 21/05/17 01:36:33 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 2161 ms on 5deb0a1bddc0 (executor driver) (133/200)
[2021-05-16 22:36:33,521] {docker.py:276} INFO - 21/05/17 01:36:33 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
[2021-05-16 22:36:33,530] {docker.py:276} INFO - 21/05/17 01:36:33 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:33,531] {docker.py:276} INFO - 21/05/17 01:36:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:33,532] {docker.py:276} INFO - 21/05/17 01:36:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445654152587883597169_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445654152587883597169_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445654152587883597169_0004}; taskId=attempt_20210517013445654152587883597169_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67782b73}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:33,532] {docker.py:276} INFO - 21/05/17 01:36:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:33,532] {docker.py:276} INFO - 21/05/17 01:36:33 INFO StagingCommitter: Starting: Task committer attempt_20210517013445654152587883597169_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445654152587883597169_0004_m_000136_424
[2021-05-16 22:36:33,535] {docker.py:276} INFO - 21/05/17 01:36:33 INFO StagingCommitter: Task committer attempt_20210517013445654152587883597169_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445654152587883597169_0004_m_000136_424 : duration 0:00.003s
[2021-05-16 22:36:34,326] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Starting: Task committer attempt_202105170134457722101767401858753_0004_m_000134_422: needsTaskCommit() Task attempt_202105170134457722101767401858753_0004_m_000134_422
[2021-05-16 22:36:34,327] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Task committer attempt_202105170134457722101767401858753_0004_m_000134_422: needsTaskCommit() Task attempt_202105170134457722101767401858753_0004_m_000134_422: duration 0:00.001s
[2021-05-16 22:36:34,327] {docker.py:276} INFO - 21/05/17 01:36:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457722101767401858753_0004_m_000134_422
[2021-05-16 22:36:34,329] {docker.py:276} INFO - 21/05/17 01:36:34 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 4544 bytes result sent to driver
[2021-05-16 22:36:34,331] {docker.py:276} INFO - 21/05/17 01:36:34 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (5deb0a1bddc0, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:34,331] {docker.py:276} INFO - 21/05/17 01:36:34 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
[2021-05-16 22:36:34,332] {docker.py:276} INFO - 21/05/17 01:36:34 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 2606 ms on 5deb0a1bddc0 (executor driver) (134/200)
[2021-05-16 22:36:34,337] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Starting: Task committer attempt_202105170134453690972397321333714_0004_m_000133_421: needsTaskCommit() Task attempt_202105170134453690972397321333714_0004_m_000133_421
[2021-05-16 22:36:34,338] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Task committer attempt_202105170134453690972397321333714_0004_m_000133_421: needsTaskCommit() Task attempt_202105170134453690972397321333714_0004_m_000133_421: duration 0:00.001s
21/05/17 01:36:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453690972397321333714_0004_m_000133_421
[2021-05-16 22:36:34,339] {docker.py:276} INFO - 21/05/17 01:36:34 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 4544 bytes result sent to driver
[2021-05-16 22:36:34,340] {docker.py:276} INFO - 21/05/17 01:36:34 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (5deb0a1bddc0, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:34,340] {docker.py:276} INFO - 21/05/17 01:36:34 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 2620 ms on 5deb0a1bddc0 (executor driver) (135/200)
[2021-05-16 22:36:34,341] {docker.py:276} INFO - 21/05/17 01:36:34 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
[2021-05-16 22:36:34,344] {docker.py:276} INFO - 21/05/17 01:36:34 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:34,345] {docker.py:276} INFO - 21/05/17 01:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:34,346] {docker.py:276} INFO - 21/05/17 01:36:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457911583456510214289_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457911583456510214289_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457911583456510214289_0004}; taskId=attempt_202105170134457911583456510214289_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@770afc91}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:34,347] {docker.py:276} INFO - 21/05/17 01:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:34 INFO StagingCommitter: Starting: Task committer attempt_202105170134457911583456510214289_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457911583456510214289_0004_m_000137_425
[2021-05-16 22:36:34,350] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Task committer attempt_202105170134457911583456510214289_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457911583456510214289_0004_m_000137_425 : duration 0:00.004s
[2021-05-16 22:36:34,351] {docker.py:276} INFO - 21/05/17 01:36:34 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:34,353] {docker.py:276} INFO - 21/05/17 01:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:34,353] {docker.py:276} INFO - 21/05/17 01:36:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454517945499784856630_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454517945499784856630_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454517945499784856630_0004}; taskId=attempt_202105170134454517945499784856630_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22fb6696}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:34,353] {docker.py:276} INFO - 21/05/17 01:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:34,354] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Starting: Task committer attempt_202105170134454517945499784856630_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454517945499784856630_0004_m_000138_426
[2021-05-16 22:36:34,356] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Task committer attempt_202105170134454517945499784856630_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454517945499784856630_0004_m_000138_426 : duration 0:00.003s
[2021-05-16 22:36:34,752] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Starting: Task committer attempt_202105170134456327450252416065049_0004_m_000135_423: needsTaskCommit() Task attempt_202105170134456327450252416065049_0004_m_000135_423
[2021-05-16 22:36:34,753] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Task committer attempt_202105170134456327450252416065049_0004_m_000135_423: needsTaskCommit() Task attempt_202105170134456327450252416065049_0004_m_000135_423: duration 0:00.001s
21/05/17 01:36:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456327450252416065049_0004_m_000135_423
[2021-05-16 22:36:34,754] {docker.py:276} INFO - 21/05/17 01:36:34 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 4544 bytes result sent to driver
[2021-05-16 22:36:34,755] {docker.py:276} INFO - 21/05/17 01:36:34 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (5deb0a1bddc0, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:34,755] {docker.py:276} INFO - 21/05/17 01:36:34 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
21/05/17 01:36:34 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 2584 ms on 5deb0a1bddc0 (executor driver) (136/200)
[2021-05-16 22:36:34,763] {docker.py:276} INFO - 21/05/17 01:36:34 INFO ShuffleBlockFetcherIterator: Getting 5 (21.1 KiB) non-empty blocks including 5 (21.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:34,764] {docker.py:276} INFO - 21/05/17 01:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457170266572157922829_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457170266572157922829_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457170266572157922829_0004}; taskId=attempt_202105170134457170266572157922829_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23d597cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:34 INFO StagingCommitter: Starting: Task committer attempt_202105170134457170266572157922829_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457170266572157922829_0004_m_000139_427
[2021-05-16 22:36:34,767] {docker.py:276} INFO - 21/05/17 01:36:34 INFO StagingCommitter: Task committer attempt_202105170134457170266572157922829_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457170266572157922829_0004_m_000139_427 : duration 0:00.003s
[2021-05-16 22:36:36,187] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Starting: Task committer attempt_20210517013445654152587883597169_0004_m_000136_424: needsTaskCommit() Task attempt_20210517013445654152587883597169_0004_m_000136_424
[2021-05-16 22:36:36,188] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Task committer attempt_20210517013445654152587883597169_0004_m_000136_424: needsTaskCommit() Task attempt_20210517013445654152587883597169_0004_m_000136_424: duration 0:00.001s
21/05/17 01:36:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445654152587883597169_0004_m_000136_424
[2021-05-16 22:36:36,190] {docker.py:276} INFO - 21/05/17 01:36:36 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 4587 bytes result sent to driver
[2021-05-16 22:36:36,191] {docker.py:276} INFO - 21/05/17 01:36:36 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (5deb0a1bddc0, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:36,191] {docker.py:276} INFO - 21/05/17 01:36:36 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 2675 ms on 5deb0a1bddc0 (executor driver) (137/200)
[2021-05-16 22:36:36,192] {docker.py:276} INFO - 21/05/17 01:36:36 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-16 22:36:36,201] {docker.py:276} INFO - 21/05/17 01:36:36 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:36,203] {docker.py:276} INFO - 21/05/17 01:36:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:36,203] {docker.py:276} INFO - 21/05/17 01:36:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458377055405239924830_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458377055405239924830_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458377055405239924830_0004}; taskId=attempt_202105170134458377055405239924830_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40270398}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:36 INFO StagingCommitter: Starting: Task committer attempt_202105170134458377055405239924830_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458377055405239924830_0004_m_000140_428
[2021-05-16 22:36:36,206] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Task committer attempt_202105170134458377055405239924830_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458377055405239924830_0004_m_000140_428 : duration 0:00.003s
[2021-05-16 22:36:36,987] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Starting: Task committer attempt_202105170134457911583456510214289_0004_m_000137_425: needsTaskCommit() Task attempt_202105170134457911583456510214289_0004_m_000137_425
[2021-05-16 22:36:36,988] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Starting: Task committer attempt_202105170134454517945499784856630_0004_m_000138_426: needsTaskCommit() Task attempt_202105170134454517945499784856630_0004_m_000138_426
[2021-05-16 22:36:36,989] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Task committer attempt_202105170134454517945499784856630_0004_m_000138_426: needsTaskCommit() Task attempt_202105170134454517945499784856630_0004_m_000138_426: duration 0:00.002s
21/05/17 01:36:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454517945499784856630_0004_m_000138_426
[2021-05-16 22:36:36,990] {docker.py:276} INFO - 21/05/17 01:36:36 INFO StagingCommitter: Task committer attempt_202105170134457911583456510214289_0004_m_000137_425: needsTaskCommit() Task attempt_202105170134457911583456510214289_0004_m_000137_425: duration 0:00.002s
21/05/17 01:36:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457911583456510214289_0004_m_000137_425
[2021-05-16 22:36:36,991] {docker.py:276} INFO - 21/05/17 01:36:36 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 4587 bytes result sent to driver
21/05/17 01:36:36 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 4587 bytes result sent to driver
[2021-05-16 22:36:36,992] {docker.py:276} INFO - 21/05/17 01:36:37 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (5deb0a1bddc0, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:36,993] {docker.py:276} INFO - 21/05/17 01:36:37 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (5deb0a1bddc0, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:36,993] {docker.py:276} INFO - 21/05/17 01:36:37 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 2655 ms on 5deb0a1bddc0 (executor driver) (138/200)
[2021-05-16 22:36:36,993] {docker.py:276} INFO - 21/05/17 01:36:37 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-16 22:36:36,994] {docker.py:276} INFO - 21/05/17 01:36:37 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 2667 ms on 5deb0a1bddc0 (executor driver) (139/200)
[2021-05-16 22:36:36,996] {docker.py:276} INFO - 21/05/17 01:36:37 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
[2021-05-16 22:36:37,004] {docker.py:276} INFO - 21/05/17 01:36:37 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:37,004] {docker.py:276} INFO - 21/05/17 01:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:37,005] {docker.py:276} INFO - 21/05/17 01:36:37 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:37,005] {docker.py:276} INFO - 21/05/17 01:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:37,006] {docker.py:276} INFO - 21/05/17 01:36:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:37,007] {docker.py:276} INFO - 21/05/17 01:36:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:37,008] {docker.py:276} INFO - 21/05/17 01:36:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458779291036017329078_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458779291036017329078_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458779291036017329078_0004}; taskId=attempt_202105170134458779291036017329078_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dac9ab1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:37,008] {docker.py:276} INFO - 21/05/17 01:36:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:37,009] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Starting: Task committer attempt_202105170134458779291036017329078_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458779291036017329078_0004_m_000142_430
[2021-05-16 22:36:37,009] {docker.py:276} INFO - 21/05/17 01:36:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:37,009] {docker.py:276} INFO - 21/05/17 01:36:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:37,009] {docker.py:276} INFO - 21/05/17 01:36:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451412568570400178804_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451412568570400178804_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451412568570400178804_0004}; taskId=attempt_202105170134451412568570400178804_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e4d6c2f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:37,010] {docker.py:276} INFO - 21/05/17 01:36:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:37,010] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Starting: Task committer attempt_202105170134451412568570400178804_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451412568570400178804_0004_m_000141_429
[2021-05-16 22:36:37,012] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Task committer attempt_202105170134458779291036017329078_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458779291036017329078_0004_m_000142_430 : duration 0:00.005s
[2021-05-16 22:36:37,013] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Task committer attempt_202105170134451412568570400178804_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451412568570400178804_0004_m_000141_429 : duration 0:00.004s
[2021-05-16 22:36:37,425] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Starting: Task committer attempt_202105170134457170266572157922829_0004_m_000139_427: needsTaskCommit() Task attempt_202105170134457170266572157922829_0004_m_000139_427
[2021-05-16 22:36:37,426] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Task committer attempt_202105170134457170266572157922829_0004_m_000139_427: needsTaskCommit() Task attempt_202105170134457170266572157922829_0004_m_000139_427: duration 0:00.001s
[2021-05-16 22:36:37,426] {docker.py:276} INFO - 21/05/17 01:36:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457170266572157922829_0004_m_000139_427
[2021-05-16 22:36:37,428] {docker.py:276} INFO - 21/05/17 01:36:37 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 4587 bytes result sent to driver
[2021-05-16 22:36:37,429] {docker.py:276} INFO - 21/05/17 01:36:37 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (5deb0a1bddc0, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:37,430] {docker.py:276} INFO - 21/05/17 01:36:37 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
[2021-05-16 22:36:37,430] {docker.py:276} INFO - 21/05/17 01:36:37 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 2678 ms on 5deb0a1bddc0 (executor driver) (140/200)
[2021-05-16 22:36:37,440] {docker.py:276} INFO - 21/05/17 01:36:37 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:37,441] {docker.py:276} INFO - 21/05/17 01:36:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452171894652970257814_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452171894652970257814_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452171894652970257814_0004}; taskId=attempt_202105170134452171894652970257814_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@204c1ebf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:37,442] {docker.py:276} INFO - 21/05/17 01:36:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:37 INFO StagingCommitter: Starting: Task committer attempt_202105170134452171894652970257814_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452171894652970257814_0004_m_000143_431
[2021-05-16 22:36:37,445] {docker.py:276} INFO - 21/05/17 01:36:37 INFO StagingCommitter: Task committer attempt_202105170134452171894652970257814_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452171894652970257814_0004_m_000143_431 : duration 0:00.003s
[2021-05-16 22:36:38,937] {docker.py:276} INFO - 21/05/17 01:36:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134458377055405239924830_0004_m_000140_428: needsTaskCommit() Task attempt_202105170134458377055405239924830_0004_m_000140_428
[2021-05-16 22:36:38,938] {docker.py:276} INFO - 21/05/17 01:36:38 INFO StagingCommitter: Task committer attempt_202105170134458377055405239924830_0004_m_000140_428: needsTaskCommit() Task attempt_202105170134458377055405239924830_0004_m_000140_428: duration 0:00.000s
21/05/17 01:36:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458377055405239924830_0004_m_000140_428
[2021-05-16 22:36:38,939] {docker.py:276} INFO - 21/05/17 01:36:38 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 4544 bytes result sent to driver
[2021-05-16 22:36:38,941] {docker.py:276} INFO - 21/05/17 01:36:38 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (5deb0a1bddc0, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:38,941] {docker.py:276} INFO - 21/05/17 01:36:38 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 2754 ms on 5deb0a1bddc0 (executor driver) (141/200)
[2021-05-16 22:36:38,942] {docker.py:276} INFO - 21/05/17 01:36:38 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-16 22:36:38,950] {docker.py:276} INFO - 21/05/17 01:36:38 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:38,952] {docker.py:276} INFO - 21/05/17 01:36:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:38,952] {docker.py:276} INFO - 21/05/17 01:36:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458519974208990787572_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458519974208990787572_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458519974208990787572_0004}; taskId=attempt_202105170134458519974208990787572_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53d92434}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:38 INFO StagingCommitter: Starting: Task committer attempt_202105170134458519974208990787572_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458519974208990787572_0004_m_000144_432
[2021-05-16 22:36:38,955] {docker.py:276} INFO - 21/05/17 01:36:38 INFO StagingCommitter: Task committer attempt_202105170134458519974208990787572_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458519974208990787572_0004_m_000144_432 : duration 0:00.003s
[2021-05-16 22:36:39,170] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Starting: Task committer attempt_202105170134458779291036017329078_0004_m_000142_430: needsTaskCommit() Task attempt_202105170134458779291036017329078_0004_m_000142_430
[2021-05-16 22:36:39,171] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Task committer attempt_202105170134458779291036017329078_0004_m_000142_430: needsTaskCommit() Task attempt_202105170134458779291036017329078_0004_m_000142_430: duration 0:00.000s
21/05/17 01:36:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458779291036017329078_0004_m_000142_430
[2021-05-16 22:36:39,172] {docker.py:276} INFO - 21/05/17 01:36:39 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 4544 bytes result sent to driver
[2021-05-16 22:36:39,173] {docker.py:276} INFO - 21/05/17 01:36:39 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (5deb0a1bddc0, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:39,175] {docker.py:276} INFO - 21/05/17 01:36:39 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 2186 ms on 5deb0a1bddc0 (executor driver) (142/200)
[2021-05-16 22:36:39,175] {docker.py:276} INFO - 21/05/17 01:36:39 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
[2021-05-16 22:36:39,185] {docker.py:276} INFO - 21/05/17 01:36:39 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:39,187] {docker.py:276} INFO - 21/05/17 01:36:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134459078577303640201932_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459078577303640201932_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134459078577303640201932_0004}; taskId=attempt_202105170134459078577303640201932_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@485eeef6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:39,188] {docker.py:276} INFO - 21/05/17 01:36:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:39 INFO StagingCommitter: Starting: Task committer attempt_202105170134459078577303640201932_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459078577303640201932_0004_m_000145_433
[2021-05-16 22:36:39,190] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Task committer attempt_202105170134459078577303640201932_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459078577303640201932_0004_m_000145_433 : duration 0:00.002s
[2021-05-16 22:36:39,610] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Starting: Task committer attempt_202105170134451412568570400178804_0004_m_000141_429: needsTaskCommit() Task attempt_202105170134451412568570400178804_0004_m_000141_429
[2021-05-16 22:36:39,611] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Task committer attempt_202105170134451412568570400178804_0004_m_000141_429: needsTaskCommit() Task attempt_202105170134451412568570400178804_0004_m_000141_429: duration 0:00.001s
21/05/17 01:36:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451412568570400178804_0004_m_000141_429
[2021-05-16 22:36:39,613] {docker.py:276} INFO - 21/05/17 01:36:39 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 4544 bytes result sent to driver
[2021-05-16 22:36:39,614] {docker.py:276} INFO - 21/05/17 01:36:39 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (5deb0a1bddc0, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:39,615] {docker.py:276} INFO - 21/05/17 01:36:39 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
[2021-05-16 22:36:39,616] {docker.py:276} INFO - 21/05/17 01:36:39 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 2628 ms on 5deb0a1bddc0 (executor driver) (143/200)
[2021-05-16 22:36:39,625] {docker.py:276} INFO - 21/05/17 01:36:39 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:39,626] {docker.py:276} INFO - 21/05/17 01:36:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:39,627] {docker.py:276} INFO - 21/05/17 01:36:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:36:39,628] {docker.py:276} INFO - 21/05/17 01:36:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:39,628] {docker.py:276} INFO - 21/05/17 01:36:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:39,628] {docker.py:276} INFO - 21/05/17 01:36:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456304218453442154060_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456304218453442154060_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456304218453442154060_0004}; taskId=attempt_202105170134456304218453442154060_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c09937e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:39,629] {docker.py:276} INFO - 21/05/17 01:36:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:39,629] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Starting: Task committer attempt_202105170134456304218453442154060_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456304218453442154060_0004_m_000146_434
[2021-05-16 22:36:39,632] {docker.py:276} INFO - 21/05/17 01:36:39 INFO StagingCommitter: Task committer attempt_202105170134456304218453442154060_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456304218453442154060_0004_m_000146_434 : duration 0:00.003s
[2021-05-16 22:36:40,174] {docker.py:276} INFO - 21/05/17 01:36:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134452171894652970257814_0004_m_000143_431: needsTaskCommit() Task attempt_202105170134452171894652970257814_0004_m_000143_431
[2021-05-16 22:36:40,175] {docker.py:276} INFO - 21/05/17 01:36:40 INFO StagingCommitter: Task committer attempt_202105170134452171894652970257814_0004_m_000143_431: needsTaskCommit() Task attempt_202105170134452171894652970257814_0004_m_000143_431: duration 0:00.000s
21/05/17 01:36:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452171894652970257814_0004_m_000143_431
[2021-05-16 22:36:40,177] {docker.py:276} INFO - 21/05/17 01:36:40 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 4544 bytes result sent to driver
21/05/17 01:36:40 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (5deb0a1bddc0, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:40,178] {docker.py:276} INFO - 21/05/17 01:36:40 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 2753 ms on 5deb0a1bddc0 (executor driver) (144/200)
21/05/17 01:36:40 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-16 22:36:40,187] {docker.py:276} INFO - 21/05/17 01:36:40 INFO ShuffleBlockFetcherIterator: Getting 5 (20.7 KiB) non-empty blocks including 5 (20.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:40,188] {docker.py:276} INFO - 21/05/17 01:36:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455857819529362951213_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455857819529362951213_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455857819529362951213_0004}; taskId=attempt_202105170134455857819529362951213_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54283770}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:40 INFO StagingCommitter: Starting: Task committer attempt_202105170134455857819529362951213_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455857819529362951213_0004_m_000147_435
[2021-05-16 22:36:40,192] {docker.py:276} INFO - 21/05/17 01:36:40 INFO StagingCommitter: Task committer attempt_202105170134455857819529362951213_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455857819529362951213_0004_m_000147_435 : duration 0:00.003s
[2021-05-16 22:36:41,885] {docker.py:276} INFO - 21/05/17 01:36:41 INFO StagingCommitter: Starting: Task committer attempt_202105170134459078577303640201932_0004_m_000145_433: needsTaskCommit() Task attempt_202105170134459078577303640201932_0004_m_000145_433
[2021-05-16 22:36:41,886] {docker.py:276} INFO - 21/05/17 01:36:41 INFO StagingCommitter: Task committer attempt_202105170134459078577303640201932_0004_m_000145_433: needsTaskCommit() Task attempt_202105170134459078577303640201932_0004_m_000145_433: duration 0:00.001s
21/05/17 01:36:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134459078577303640201932_0004_m_000145_433
[2021-05-16 22:36:41,886] {docker.py:276} INFO - 21/05/17 01:36:41 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 4544 bytes result sent to driver
[2021-05-16 22:36:41,887] {docker.py:276} INFO - 21/05/17 01:36:41 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (5deb0a1bddc0, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:41,887] {docker.py:276} INFO - 21/05/17 01:36:41 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 2718 ms on 5deb0a1bddc0 (executor driver) (145/200)
[2021-05-16 22:36:41,888] {docker.py:276} INFO - 21/05/17 01:36:41 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
[2021-05-16 22:36:41,894] {docker.py:276} INFO - 21/05/17 01:36:41 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:41,896] {docker.py:276} INFO - 21/05/17 01:36:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453242540313897721456_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453242540313897721456_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453242540313897721456_0004}; taskId=attempt_202105170134453242540313897721456_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49532735}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:41,897] {docker.py:276} INFO - 21/05/17 01:36:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:41 INFO StagingCommitter: Starting: Task committer attempt_202105170134453242540313897721456_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453242540313897721456_0004_m_000148_436
[2021-05-16 22:36:41,899] {docker.py:276} INFO - 21/05/17 01:36:41 INFO StagingCommitter: Task committer attempt_202105170134453242540313897721456_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453242540313897721456_0004_m_000148_436 : duration 0:00.003s
[2021-05-16 22:36:42,194] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134458519974208990787572_0004_m_000144_432: needsTaskCommit() Task attempt_202105170134458519974208990787572_0004_m_000144_432
[2021-05-16 22:36:42,196] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Task committer attempt_202105170134458519974208990787572_0004_m_000144_432: needsTaskCommit() Task attempt_202105170134458519974208990787572_0004_m_000144_432: duration 0:00.002s
[2021-05-16 22:36:42,196] {docker.py:276} INFO - 21/05/17 01:36:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458519974208990787572_0004_m_000144_432
[2021-05-16 22:36:42,198] {docker.py:276} INFO - 21/05/17 01:36:42 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 4544 bytes result sent to driver
[2021-05-16 22:36:42,199] {docker.py:276} INFO - 21/05/17 01:36:42 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (5deb0a1bddc0, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:42,200] {docker.py:276} INFO - 21/05/17 01:36:42 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
21/05/17 01:36:42 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 3264 ms on 5deb0a1bddc0 (executor driver) (146/200)
[2021-05-16 22:36:42,221] {docker.py:276} INFO - 21/05/17 01:36:42 INFO ShuffleBlockFetcherIterator: Getting 5 (20.4 KiB) non-empty blocks including 5 (20.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:42,222] {docker.py:276} INFO - 21/05/17 01:36:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:42,223] {docker.py:276} INFO - 21/05/17 01:36:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455364672269392558053_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455364672269392558053_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455364672269392558053_0004}; taskId=attempt_202105170134455364672269392558053_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e5e2f3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:42,223] {docker.py:276} INFO - 21/05/17 01:36:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134455364672269392558053_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455364672269392558053_0004_m_000149_437
[2021-05-16 22:36:42,226] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Task committer attempt_202105170134455364672269392558053_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455364672269392558053_0004_m_000149_437 : duration 0:00.003s
[2021-05-16 22:36:42,261] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134456304218453442154060_0004_m_000146_434: needsTaskCommit() Task attempt_202105170134456304218453442154060_0004_m_000146_434
[2021-05-16 22:36:42,261] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Task committer attempt_202105170134456304218453442154060_0004_m_000146_434: needsTaskCommit() Task attempt_202105170134456304218453442154060_0004_m_000146_434: duration 0:00.000s
21/05/17 01:36:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456304218453442154060_0004_m_000146_434
[2021-05-16 22:36:42,262] {docker.py:276} INFO - 21/05/17 01:36:42 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 4587 bytes result sent to driver
[2021-05-16 22:36:42,263] {docker.py:276} INFO - 21/05/17 01:36:42 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (5deb0a1bddc0, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:42,264] {docker.py:276} INFO - 21/05/17 01:36:42 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-16 22:36:42,264] {docker.py:276} INFO - 21/05/17 01:36:42 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 2654 ms on 5deb0a1bddc0 (executor driver) (147/200)
[2021-05-16 22:36:42,272] {docker.py:276} INFO - 21/05/17 01:36:42 INFO ShuffleBlockFetcherIterator: Getting 5 (21.4 KiB) non-empty blocks including 5 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:42,274] {docker.py:276} INFO - 21/05/17 01:36:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:42,274] {docker.py:276} INFO - 21/05/17 01:36:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134459093787502776364321_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459093787502776364321_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134459093787502776364321_0004}; taskId=attempt_202105170134459093787502776364321_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69193b02}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:42,274] {docker.py:276} INFO - 21/05/17 01:36:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134459093787502776364321_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459093787502776364321_0004_m_000150_438
[2021-05-16 22:36:42,277] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Task committer attempt_202105170134459093787502776364321_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459093787502776364321_0004_m_000150_438 : duration 0:00.002s
[2021-05-16 22:36:42,377] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134455857819529362951213_0004_m_000147_435: needsTaskCommit() Task attempt_202105170134455857819529362951213_0004_m_000147_435
[2021-05-16 22:36:42,378] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Task committer attempt_202105170134455857819529362951213_0004_m_000147_435: needsTaskCommit() Task attempt_202105170134455857819529362951213_0004_m_000147_435: duration 0:00.001s
21/05/17 01:36:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455857819529362951213_0004_m_000147_435
[2021-05-16 22:36:42,380] {docker.py:276} INFO - 21/05/17 01:36:42 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 4587 bytes result sent to driver
[2021-05-16 22:36:42,381] {docker.py:276} INFO - 21/05/17 01:36:42 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (5deb0a1bddc0, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:42,382] {docker.py:276} INFO - 21/05/17 01:36:42 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-16 22:36:42,383] {docker.py:276} INFO - 21/05/17 01:36:42 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 2209 ms on 5deb0a1bddc0 (executor driver) (148/200)
[2021-05-16 22:36:42,392] {docker.py:276} INFO - 21/05/17 01:36:42 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:42,394] {docker.py:276} INFO - 21/05/17 01:36:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452288941445984968200_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452288941445984968200_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452288941445984968200_0004}; taskId=attempt_202105170134452288941445984968200_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@672007c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:42,395] {docker.py:276} INFO - 21/05/17 01:36:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:42 INFO StagingCommitter: Starting: Task committer attempt_202105170134452288941445984968200_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452288941445984968200_0004_m_000151_439
[2021-05-16 22:36:42,397] {docker.py:276} INFO - 21/05/17 01:36:42 INFO StagingCommitter: Task committer attempt_202105170134452288941445984968200_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452288941445984968200_0004_m_000151_439 : duration 0:00.003s
[2021-05-16 22:36:44,532] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Starting: Task committer attempt_202105170134453242540313897721456_0004_m_000148_436: needsTaskCommit() Task attempt_202105170134453242540313897721456_0004_m_000148_436
[2021-05-16 22:36:44,533] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Task committer attempt_202105170134453242540313897721456_0004_m_000148_436: needsTaskCommit() Task attempt_202105170134453242540313897721456_0004_m_000148_436: duration 0:00.000s
21/05/17 01:36:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453242540313897721456_0004_m_000148_436
[2021-05-16 22:36:44,536] {docker.py:276} INFO - 21/05/17 01:36:44 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 4587 bytes result sent to driver
[2021-05-16 22:36:44,537] {docker.py:276} INFO - 21/05/17 01:36:44 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (5deb0a1bddc0, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:44,537] {docker.py:276} INFO - 21/05/17 01:36:44 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
[2021-05-16 22:36:44,538] {docker.py:276} INFO - 21/05/17 01:36:44 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 2654 ms on 5deb0a1bddc0 (executor driver) (149/200)
[2021-05-16 22:36:44,549] {docker.py:276} INFO - 21/05/17 01:36:44 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:44,553] {docker.py:276} INFO - 21/05/17 01:36:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456429377694931202928_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456429377694931202928_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456429377694931202928_0004}; taskId=attempt_202105170134456429377694931202928_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6eaa77ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:44,553] {docker.py:276} INFO - 21/05/17 01:36:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:44 INFO StagingCommitter: Starting: Task committer attempt_202105170134456429377694931202928_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456429377694931202928_0004_m_000152_440
[2021-05-16 22:36:44,556] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Task committer attempt_202105170134456429377694931202928_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456429377694931202928_0004_m_000152_440 : duration 0:00.002s
[2021-05-16 22:36:44,838] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Starting: Task committer attempt_202105170134452288941445984968200_0004_m_000151_439: needsTaskCommit() Task attempt_202105170134452288941445984968200_0004_m_000151_439
[2021-05-16 22:36:44,839] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Task committer attempt_202105170134452288941445984968200_0004_m_000151_439: needsTaskCommit() Task attempt_202105170134452288941445984968200_0004_m_000151_439: duration 0:00.001s
21/05/17 01:36:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452288941445984968200_0004_m_000151_439
[2021-05-16 22:36:44,840] {docker.py:276} INFO - 21/05/17 01:36:44 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 4544 bytes result sent to driver
[2021-05-16 22:36:44,841] {docker.py:276} INFO - 21/05/17 01:36:44 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (5deb0a1bddc0, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:44,842] {docker.py:276} INFO - 21/05/17 01:36:44 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 2464 ms on 5deb0a1bddc0 (executor driver) (150/200)
21/05/17 01:36:44 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-16 22:36:44,851] {docker.py:276} INFO - 21/05/17 01:36:44 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:44,853] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Starting: Task committer attempt_202105170134459093787502776364321_0004_m_000150_438: needsTaskCommit() Task attempt_202105170134459093787502776364321_0004_m_000150_438
21/05/17 01:36:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:44,853] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Task committer attempt_202105170134459093787502776364321_0004_m_000150_438: needsTaskCommit() Task attempt_202105170134459093787502776364321_0004_m_000150_438: duration 0:00.000s
[2021-05-16 22:36:44,854] {docker.py:276} INFO - 21/05/17 01:36:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134459093787502776364321_0004_m_000150_438
[2021-05-16 22:36:44,854] {docker.py:276} INFO - 21/05/17 01:36:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452541468467436676926_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452541468467436676926_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452541468467436676926_0004}; taskId=attempt_202105170134452541468467436676926_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@598b751}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:44 INFO StagingCommitter: Starting: Task committer attempt_202105170134452541468467436676926_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452541468467436676926_0004_m_000153_441
[2021-05-16 22:36:44,855] {docker.py:276} INFO - 21/05/17 01:36:44 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 4544 bytes result sent to driver
[2021-05-16 22:36:44,855] {docker.py:276} INFO - 21/05/17 01:36:44 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (5deb0a1bddc0, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:44,856] {docker.py:276} INFO - 21/05/17 01:36:44 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 2596 ms on 5deb0a1bddc0 (executor driver) (151/200)
[2021-05-16 22:36:44,857] {docker.py:276} INFO - 21/05/17 01:36:44 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-16 22:36:44,860] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Task committer attempt_202105170134452541468467436676926_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452541468467436676926_0004_m_000153_441 : duration 0:00.006s
[2021-05-16 22:36:44,864] {docker.py:276} INFO - 21/05/17 01:36:44 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:44,864] {docker.py:276} INFO - 21/05/17 01:36:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:44,865] {docker.py:276} INFO - 21/05/17 01:36:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:44,866] {docker.py:276} INFO - 21/05/17 01:36:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445344010948681585787_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445344010948681585787_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445344010948681585787_0004}; taskId=attempt_20210517013445344010948681585787_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@468d0623}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:44 INFO StagingCommitter: Starting: Task committer attempt_20210517013445344010948681585787_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445344010948681585787_0004_m_000154_442
[2021-05-16 22:36:44,868] {docker.py:276} INFO - 21/05/17 01:36:44 INFO StagingCommitter: Task committer attempt_20210517013445344010948681585787_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445344010948681585787_0004_m_000154_442 : duration 0:00.002s
[2021-05-16 22:36:45,439] {docker.py:276} INFO - 21/05/17 01:36:45 INFO StagingCommitter: Starting: Task committer attempt_202105170134455364672269392558053_0004_m_000149_437: needsTaskCommit() Task attempt_202105170134455364672269392558053_0004_m_000149_437
[2021-05-16 22:36:45,440] {docker.py:276} INFO - 21/05/17 01:36:45 INFO StagingCommitter: Task committer attempt_202105170134455364672269392558053_0004_m_000149_437: needsTaskCommit() Task attempt_202105170134455364672269392558053_0004_m_000149_437: duration 0:00.001s
21/05/17 01:36:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455364672269392558053_0004_m_000149_437
[2021-05-16 22:36:45,444] {docker.py:276} INFO - 21/05/17 01:36:45 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 4587 bytes result sent to driver
[2021-05-16 22:36:45,445] {docker.py:276} INFO - 21/05/17 01:36:45 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (5deb0a1bddc0, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:45,446] {docker.py:276} INFO - 21/05/17 01:36:45 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 3252 ms on 5deb0a1bddc0 (executor driver) (152/200)
[2021-05-16 22:36:45,447] {docker.py:276} INFO - 21/05/17 01:36:45 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
[2021-05-16 22:36:45,456] {docker.py:276} INFO - 21/05/17 01:36:45 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:45,457] {docker.py:276} INFO - 21/05/17 01:36:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451938310576976101278_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451938310576976101278_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451938310576976101278_0004}; taskId=attempt_202105170134451938310576976101278_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2709879e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:45,458] {docker.py:276} INFO - 21/05/17 01:36:45 INFO StagingCommitter: Starting: Task committer attempt_202105170134451938310576976101278_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451938310576976101278_0004_m_000155_443
[2021-05-16 22:36:45,461] {docker.py:276} INFO - 21/05/17 01:36:45 INFO StagingCommitter: Task committer attempt_202105170134451938310576976101278_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451938310576976101278_0004_m_000155_443 : duration 0:00.004s
[2021-05-16 22:36:47,131] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Starting: Task committer attempt_202105170134456429377694931202928_0004_m_000152_440: needsTaskCommit() Task attempt_202105170134456429377694931202928_0004_m_000152_440
[2021-05-16 22:36:47,140] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Task committer attempt_202105170134456429377694931202928_0004_m_000152_440: needsTaskCommit() Task attempt_202105170134456429377694931202928_0004_m_000152_440: duration 0:00.001s
21/05/17 01:36:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456429377694931202928_0004_m_000152_440
[2021-05-16 22:36:47,140] {docker.py:276} INFO - 21/05/17 01:36:47 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 4544 bytes result sent to driver
[2021-05-16 22:36:47,140] {docker.py:276} INFO - 21/05/17 01:36:47 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (5deb0a1bddc0, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:47,141] {docker.py:276} INFO - 21/05/17 01:36:47 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
[2021-05-16 22:36:47,141] {docker.py:276} INFO - 21/05/17 01:36:47 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 2603 ms on 5deb0a1bddc0 (executor driver) (153/200)
[2021-05-16 22:36:47,144] {docker.py:276} INFO - 21/05/17 01:36:47 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:47,164] {docker.py:276} INFO - 21/05/17 01:36:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455131549335209728612_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455131549335209728612_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455131549335209728612_0004}; taskId=attempt_202105170134455131549335209728612_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@432d8d58}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:47,165] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Starting: Task committer attempt_202105170134455131549335209728612_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455131549335209728612_0004_m_000156_444
[2021-05-16 22:36:47,166] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Task committer attempt_202105170134455131549335209728612_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455131549335209728612_0004_m_000156_444 : duration 0:00.003s
[2021-05-16 22:36:47,453] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Starting: Task committer attempt_202105170134452541468467436676926_0004_m_000153_441: needsTaskCommit() Task attempt_202105170134452541468467436676926_0004_m_000153_441
21/05/17 01:36:47 INFO StagingCommitter: Starting: Task committer attempt_20210517013445344010948681585787_0004_m_000154_442: needsTaskCommit() Task attempt_20210517013445344010948681585787_0004_m_000154_442
[2021-05-16 22:36:47,454] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Task committer attempt_20210517013445344010948681585787_0004_m_000154_442: needsTaskCommit() Task attempt_20210517013445344010948681585787_0004_m_000154_442: duration 0:00.000s
21/05/17 01:36:47 INFO StagingCommitter: Task committer attempt_202105170134452541468467436676926_0004_m_000153_441: needsTaskCommit() Task attempt_202105170134452541468467436676926_0004_m_000153_441: duration 0:00.001s
21/05/17 01:36:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445344010948681585787_0004_m_000154_442
21/05/17 01:36:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452541468467436676926_0004_m_000153_441
[2021-05-16 22:36:47,455] {docker.py:276} INFO - 21/05/17 01:36:47 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 4544 bytes result sent to driver
[2021-05-16 22:36:47,456] {docker.py:276} INFO - 21/05/17 01:36:47 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (5deb0a1bddc0, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:36:47 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 4587 bytes result sent to driver
[2021-05-16 22:36:47,458] {docker.py:276} INFO - 21/05/17 01:36:47 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (5deb0a1bddc0, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:47,459] {docker.py:276} INFO - 21/05/17 01:36:47 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
21/05/17 01:36:47 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 2606 ms on 5deb0a1bddc0 (executor driver) (154/200)
21/05/17 01:36:47 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
21/05/17 01:36:47 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 2621 ms on 5deb0a1bddc0 (executor driver) (155/200)
[2021-05-16 22:36:47,469] {docker.py:276} INFO - 21/05/17 01:36:47 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:47,469] {docker.py:276} INFO - 21/05/17 01:36:47 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:47,470] {docker.py:276} INFO - 21/05/17 01:36:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:47,471] {docker.py:276} INFO - 21/05/17 01:36:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:47,471] {docker.py:276} INFO - 21/05/17 01:36:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456598786614956124496_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456598786614956124496_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456598786614956124496_0004}; taskId=attempt_202105170134456598786614956124496_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ecb38a2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:47,472] {docker.py:276} INFO - 21/05/17 01:36:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:47,472] {docker.py:276} INFO - 21/05/17 01:36:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455969103715117274077_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455969103715117274077_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455969103715117274077_0004}; taskId=attempt_202105170134455969103715117274077_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ff070e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:47,473] {docker.py:276} INFO - 21/05/17 01:36:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:47,473] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Starting: Task committer attempt_202105170134455969103715117274077_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455969103715117274077_0004_m_000157_445
[2021-05-16 22:36:47,473] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Starting: Task committer attempt_202105170134456598786614956124496_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456598786614956124496_0004_m_000158_446
[2021-05-16 22:36:47,475] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Task committer attempt_202105170134455969103715117274077_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455969103715117274077_0004_m_000157_445 : duration 0:00.004s
[2021-05-16 22:36:47,476] {docker.py:276} INFO - 21/05/17 01:36:47 INFO StagingCommitter: Task committer attempt_202105170134456598786614956124496_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456598786614956124496_0004_m_000158_446 : duration 0:00.005s
[2021-05-16 22:36:48,076] {docker.py:276} INFO - 21/05/17 01:36:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134451938310576976101278_0004_m_000155_443: needsTaskCommit() Task attempt_202105170134451938310576976101278_0004_m_000155_443
21/05/17 01:36:48 INFO StagingCommitter: Task committer attempt_202105170134451938310576976101278_0004_m_000155_443: needsTaskCommit() Task attempt_202105170134451938310576976101278_0004_m_000155_443: duration 0:00.001s
21/05/17 01:36:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451938310576976101278_0004_m_000155_443
[2021-05-16 22:36:48,077] {docker.py:276} INFO - 21/05/17 01:36:48 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 4587 bytes result sent to driver
[2021-05-16 22:36:48,078] {docker.py:276} INFO - 21/05/17 01:36:48 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (5deb0a1bddc0, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:48,079] {docker.py:276} INFO - 21/05/17 01:36:48 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
[2021-05-16 22:36:48,080] {docker.py:276} INFO - 21/05/17 01:36:48 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 2638 ms on 5deb0a1bddc0 (executor driver) (156/200)
[2021-05-16 22:36:48,096] {docker.py:276} INFO - 21/05/17 01:36:48 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:48,097] {docker.py:276} INFO - 21/05/17 01:36:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453408710932969995456_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453408710932969995456_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453408710932969995456_0004}; taskId=attempt_202105170134453408710932969995456_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7676764c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:48,098] {docker.py:276} INFO - 21/05/17 01:36:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:48 INFO StagingCommitter: Starting: Task committer attempt_202105170134453408710932969995456_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453408710932969995456_0004_m_000159_447
[2021-05-16 22:36:48,101] {docker.py:276} INFO - 21/05/17 01:36:48 INFO StagingCommitter: Task committer attempt_202105170134453408710932969995456_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453408710932969995456_0004_m_000159_447 : duration 0:00.003s
[2021-05-16 22:36:49,750] {docker.py:276} INFO - 21/05/17 01:36:49 INFO StagingCommitter: Starting: Task committer attempt_202105170134455131549335209728612_0004_m_000156_444: needsTaskCommit() Task attempt_202105170134455131549335209728612_0004_m_000156_444
[2021-05-16 22:36:49,750] {docker.py:276} INFO - 21/05/17 01:36:49 INFO StagingCommitter: Task committer attempt_202105170134455131549335209728612_0004_m_000156_444: needsTaskCommit() Task attempt_202105170134455131549335209728612_0004_m_000156_444: duration 0:00.001s
21/05/17 01:36:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455131549335209728612_0004_m_000156_444
[2021-05-16 22:36:49,752] {docker.py:276} INFO - 21/05/17 01:36:49 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 4587 bytes result sent to driver
[2021-05-16 22:36:49,754] {docker.py:276} INFO - 21/05/17 01:36:49 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (5deb0a1bddc0, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:49,755] {docker.py:276} INFO - 21/05/17 01:36:49 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 2624 ms on 5deb0a1bddc0 (executor driver) (157/200)
[2021-05-16 22:36:49,756] {docker.py:276} INFO - 21/05/17 01:36:49 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
[2021-05-16 22:36:49,765] {docker.py:276} INFO - 21/05/17 01:36:49 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:49,767] {docker.py:276} INFO - 21/05/17 01:36:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:49,767] {docker.py:276} INFO - 21/05/17 01:36:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455625078606066165532_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455625078606066165532_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455625078606066165532_0004}; taskId=attempt_202105170134455625078606066165532_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c6166cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:49 INFO StagingCommitter: Starting: Task committer attempt_202105170134455625078606066165532_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455625078606066165532_0004_m_000160_448
[2021-05-16 22:36:49,770] {docker.py:276} INFO - 21/05/17 01:36:49 INFO StagingCommitter: Task committer attempt_202105170134455625078606066165532_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455625078606066165532_0004_m_000160_448 : duration 0:00.003s
[2021-05-16 22:36:50,100] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134456598786614956124496_0004_m_000158_446: needsTaskCommit() Task attempt_202105170134456598786614956124496_0004_m_000158_446
[2021-05-16 22:36:50,101] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Task committer attempt_202105170134456598786614956124496_0004_m_000158_446: needsTaskCommit() Task attempt_202105170134456598786614956124496_0004_m_000158_446: duration 0:00.001s
21/05/17 01:36:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456598786614956124496_0004_m_000158_446
[2021-05-16 22:36:50,103] {docker.py:276} INFO - 21/05/17 01:36:50 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 4587 bytes result sent to driver
[2021-05-16 22:36:50,104] {docker.py:276} INFO - 21/05/17 01:36:50 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (5deb0a1bddc0, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:50,105] {docker.py:276} INFO - 21/05/17 01:36:50 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 2650 ms on 5deb0a1bddc0 (executor driver) (158/200)
[2021-05-16 22:36:50,106] {docker.py:276} INFO - 21/05/17 01:36:50 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
[2021-05-16 22:36:50,116] {docker.py:276} INFO - 21/05/17 01:36:50 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:50,118] {docker.py:276} INFO - 21/05/17 01:36:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:50,118] {docker.py:276} INFO - 21/05/17 01:36:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452881264762087034788_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452881264762087034788_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452881264762087034788_0004}; taskId=attempt_202105170134452881264762087034788_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fd2bc69}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134452881264762087034788_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452881264762087034788_0004_m_000161_449
[2021-05-16 22:36:50,121] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Task committer attempt_202105170134452881264762087034788_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452881264762087034788_0004_m_000161_449 : duration 0:00.003s
[2021-05-16 22:36:50,240] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134455969103715117274077_0004_m_000157_445: needsTaskCommit() Task attempt_202105170134455969103715117274077_0004_m_000157_445
[2021-05-16 22:36:50,241] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Task committer attempt_202105170134455969103715117274077_0004_m_000157_445: needsTaskCommit() Task attempt_202105170134455969103715117274077_0004_m_000157_445: duration 0:00.000s
21/05/17 01:36:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455969103715117274077_0004_m_000157_445
[2021-05-16 22:36:50,242] {docker.py:276} INFO - 21/05/17 01:36:50 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 4587 bytes result sent to driver
[2021-05-16 22:36:50,243] {docker.py:276} INFO - 21/05/17 01:36:50 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (5deb0a1bddc0, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:50,245] {docker.py:276} INFO - 21/05/17 01:36:50 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 2793 ms on 5deb0a1bddc0 (executor driver) (159/200)
[2021-05-16 22:36:50,245] {docker.py:276} INFO - 21/05/17 01:36:50 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-16 22:36:50,255] {docker.py:276} INFO - 21/05/17 01:36:50 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:50,257] {docker.py:276} INFO - 21/05/17 01:36:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453023151119417064989_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453023151119417064989_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453023151119417064989_0004}; taskId=attempt_202105170134453023151119417064989_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4901872d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:50,257] {docker.py:276} INFO - 21/05/17 01:36:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134453023151119417064989_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453023151119417064989_0004_m_000162_450
[2021-05-16 22:36:50,260] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Task committer attempt_202105170134453023151119417064989_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453023151119417064989_0004_m_000162_450 : duration 0:00.003s
[2021-05-16 22:36:50,277] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Starting: Task committer attempt_202105170134453408710932969995456_0004_m_000159_447: needsTaskCommit() Task attempt_202105170134453408710932969995456_0004_m_000159_447
[2021-05-16 22:36:50,278] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Task committer attempt_202105170134453408710932969995456_0004_m_000159_447: needsTaskCommit() Task attempt_202105170134453408710932969995456_0004_m_000159_447: duration 0:00.001s
[2021-05-16 22:36:50,278] {docker.py:276} INFO - 21/05/17 01:36:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453408710932969995456_0004_m_000159_447
[2021-05-16 22:36:50,279] {docker.py:276} INFO - 21/05/17 01:36:50 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 4587 bytes result sent to driver
[2021-05-16 22:36:50,280] {docker.py:276} INFO - 21/05/17 01:36:50 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (5deb0a1bddc0, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:50,281] {docker.py:276} INFO - 21/05/17 01:36:50 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-16 22:36:50,281] {docker.py:276} INFO - 21/05/17 01:36:50 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 2207 ms on 5deb0a1bddc0 (executor driver) (160/200)
[2021-05-16 22:36:50,289] {docker.py:276} INFO - 21/05/17 01:36:50 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:50,290] {docker.py:276} INFO - 21/05/17 01:36:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:50,291] {docker.py:276} INFO - 21/05/17 01:36:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:50,291] {docker.py:276} INFO - 21/05/17 01:36:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445835673352005118654_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445835673352005118654_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445835673352005118654_0004}; taskId=attempt_20210517013445835673352005118654_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23d852c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:50,291] {docker.py:276} INFO - 21/05/17 01:36:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:50,292] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Starting: Task committer attempt_20210517013445835673352005118654_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445835673352005118654_0004_m_000163_451
[2021-05-16 22:36:50,294] {docker.py:276} INFO - 21/05/17 01:36:50 INFO StagingCommitter: Task committer attempt_20210517013445835673352005118654_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445835673352005118654_0004_m_000163_451 : duration 0:00.003s
[2021-05-16 22:36:52,513] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Starting: Task committer attempt_202105170134455625078606066165532_0004_m_000160_448: needsTaskCommit() Task attempt_202105170134455625078606066165532_0004_m_000160_448
21/05/17 01:36:52 INFO StagingCommitter: Task committer attempt_202105170134455625078606066165532_0004_m_000160_448: needsTaskCommit() Task attempt_202105170134455625078606066165532_0004_m_000160_448: duration 0:00.001s
21/05/17 01:36:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455625078606066165532_0004_m_000160_448
[2021-05-16 22:36:52,515] {docker.py:276} INFO - 21/05/17 01:36:52 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 4544 bytes result sent to driver
[2021-05-16 22:36:52,516] {docker.py:276} INFO - 21/05/17 01:36:52 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (5deb0a1bddc0, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:52,517] {docker.py:276} INFO - 21/05/17 01:36:52 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
21/05/17 01:36:52 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 2767 ms on 5deb0a1bddc0 (executor driver) (161/200)
[2021-05-16 22:36:52,529] {docker.py:276} INFO - 21/05/17 01:36:52 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-16 22:36:52,531] {docker.py:276} INFO - 21/05/17 01:36:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:52,532] {docker.py:276} INFO - 21/05/17 01:36:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454872149867874032156_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454872149867874032156_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454872149867874032156_0004}; taskId=attempt_202105170134454872149867874032156_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57ad3377}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:52,532] {docker.py:276} INFO - 21/05/17 01:36:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:52 INFO StagingCommitter: Starting: Task committer attempt_202105170134454872149867874032156_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454872149867874032156_0004_m_000164_452
[2021-05-16 22:36:52,535] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Task committer attempt_202105170134454872149867874032156_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454872149867874032156_0004_m_000164_452 : duration 0:00.003s
[2021-05-16 22:36:52,724] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Starting: Task committer attempt_202105170134453023151119417064989_0004_m_000162_450: needsTaskCommit() Task attempt_202105170134453023151119417064989_0004_m_000162_450
21/05/17 01:36:52 INFO StagingCommitter: Task committer attempt_202105170134453023151119417064989_0004_m_000162_450: needsTaskCommit() Task attempt_202105170134453023151119417064989_0004_m_000162_450: duration 0:00.001s
[2021-05-16 22:36:52,724] {docker.py:276} INFO - 21/05/17 01:36:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453023151119417064989_0004_m_000162_450
[2021-05-16 22:36:52,727] {docker.py:276} INFO - 21/05/17 01:36:52 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 4544 bytes result sent to driver
[2021-05-16 22:36:52,728] {docker.py:276} INFO - 21/05/17 01:36:52 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (5deb0a1bddc0, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:52,729] {docker.py:276} INFO - 21/05/17 01:36:52 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
[2021-05-16 22:36:52,729] {docker.py:276} INFO - 21/05/17 01:36:52 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 2489 ms on 5deb0a1bddc0 (executor driver) (162/200)
[2021-05-16 22:36:52,739] {docker.py:276} INFO - 21/05/17 01:36:52 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:52,740] {docker.py:276} INFO - 21/05/17 01:36:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457111987948622703918_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457111987948622703918_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457111987948622703918_0004}; taskId=attempt_202105170134457111987948622703918_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60ad46a6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:52 INFO StagingCommitter: Starting: Task committer attempt_202105170134457111987948622703918_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457111987948622703918_0004_m_000165_453
[2021-05-16 22:36:52,743] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Task committer attempt_202105170134457111987948622703918_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457111987948622703918_0004_m_000165_453 : duration 0:00.002s
[2021-05-16 22:36:52,859] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Starting: Task committer attempt_20210517013445835673352005118654_0004_m_000163_451: needsTaskCommit() Task attempt_20210517013445835673352005118654_0004_m_000163_451
[2021-05-16 22:36:52,860] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Task committer attempt_20210517013445835673352005118654_0004_m_000163_451: needsTaskCommit() Task attempt_20210517013445835673352005118654_0004_m_000163_451: duration 0:00.000s
21/05/17 01:36:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445835673352005118654_0004_m_000163_451
[2021-05-16 22:36:52,862] {docker.py:276} INFO - 21/05/17 01:36:52 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 4544 bytes result sent to driver
[2021-05-16 22:36:52,863] {docker.py:276} INFO - 21/05/17 01:36:52 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (5deb0a1bddc0, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:52,864] {docker.py:276} INFO - 21/05/17 01:36:52 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 2587 ms on 5deb0a1bddc0 (executor driver) (163/200)
[2021-05-16 22:36:52,864] {docker.py:276} INFO - 21/05/17 01:36:52 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
[2021-05-16 22:36:52,875] {docker.py:276} INFO - 21/05/17 01:36:52 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:52,875] {docker.py:276} INFO - 21/05/17 01:36:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:52,876] {docker.py:276} INFO - 21/05/17 01:36:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:52,877] {docker.py:276} INFO - 21/05/17 01:36:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452377031614093651404_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452377031614093651404_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452377031614093651404_0004}; taskId=attempt_202105170134452377031614093651404_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31e2220a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:52 INFO StagingCommitter: Starting: Task committer attempt_202105170134452377031614093651404_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452377031614093651404_0004_m_000166_454
[2021-05-16 22:36:52,880] {docker.py:276} INFO - 21/05/17 01:36:52 INFO StagingCommitter: Task committer attempt_202105170134452377031614093651404_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452377031614093651404_0004_m_000166_454 : duration 0:00.003s
[2021-05-16 22:36:53,320] {docker.py:276} INFO - 21/05/17 01:36:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134452881264762087034788_0004_m_000161_449: needsTaskCommit() Task attempt_202105170134452881264762087034788_0004_m_000161_449
[2021-05-16 22:36:53,321] {docker.py:276} INFO - 21/05/17 01:36:53 INFO StagingCommitter: Task committer attempt_202105170134452881264762087034788_0004_m_000161_449: needsTaskCommit() Task attempt_202105170134452881264762087034788_0004_m_000161_449: duration 0:00.001s
21/05/17 01:36:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452881264762087034788_0004_m_000161_449
[2021-05-16 22:36:53,324] {docker.py:276} INFO - 21/05/17 01:36:53 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 4544 bytes result sent to driver
[2021-05-16 22:36:53,325] {docker.py:276} INFO - 21/05/17 01:36:53 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (5deb0a1bddc0, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:53,326] {docker.py:276} INFO - 21/05/17 01:36:53 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 3226 ms on 5deb0a1bddc0 (executor driver) (164/200)
21/05/17 01:36:53 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-16 22:36:53,336] {docker.py:276} INFO - 21/05/17 01:36:53 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:53,338] {docker.py:276} INFO - 21/05/17 01:36:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452161060409760349220_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452161060409760349220_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452161060409760349220_0004}; taskId=attempt_202105170134452161060409760349220_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@287ddeca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:53 INFO StagingCommitter: Starting: Task committer attempt_202105170134452161060409760349220_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452161060409760349220_0004_m_000167_455
[2021-05-16 22:36:53,341] {docker.py:276} INFO - 21/05/17 01:36:53 INFO StagingCommitter: Task committer attempt_202105170134452161060409760349220_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452161060409760349220_0004_m_000167_455 : duration 0:00.004s
[2021-05-16 22:36:55,400] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134457111987948622703918_0004_m_000165_453: needsTaskCommit() Task attempt_202105170134457111987948622703918_0004_m_000165_453
21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134457111987948622703918_0004_m_000165_453: needsTaskCommit() Task attempt_202105170134457111987948622703918_0004_m_000165_453: duration 0:00.001s
21/05/17 01:36:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457111987948622703918_0004_m_000165_453
[2021-05-16 22:36:55,402] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 4544 bytes result sent to driver
[2021-05-16 22:36:55,403] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (5deb0a1bddc0, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:55,404] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
21/05/17 01:36:55 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 2679 ms on 5deb0a1bddc0 (executor driver) (165/200)
[2021-05-16 22:36:55,408] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134454872149867874032156_0004_m_000164_452: needsTaskCommit() Task attempt_202105170134454872149867874032156_0004_m_000164_452
[2021-05-16 22:36:55,409] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134454872149867874032156_0004_m_000164_452: needsTaskCommit() Task attempt_202105170134454872149867874032156_0004_m_000164_452: duration 0:00.000s
[2021-05-16 22:36:55,409] {docker.py:276} INFO - 21/05/17 01:36:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454872149867874032156_0004_m_000164_452
[2021-05-16 22:36:55,411] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 4544 bytes result sent to driver
[2021-05-16 22:36:55,412] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (5deb0a1bddc0, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:55,412] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-16 22:36:55,413] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 2901 ms on 5deb0a1bddc0 (executor driver) (166/200)
[2021-05-16 22:36:55,418] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134452377031614093651404_0004_m_000166_454: needsTaskCommit() Task attempt_202105170134452377031614093651404_0004_m_000166_454
[2021-05-16 22:36:55,419] {docker.py:276} INFO - 21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 22:36:55,419] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134452377031614093651404_0004_m_000166_454: needsTaskCommit() Task attempt_202105170134452377031614093651404_0004_m_000166_454: duration 0:00.001s
21/05/17 01:36:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452377031614093651404_0004_m_000166_454
[2021-05-16 22:36:55,420] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 4544 bytes result sent to driver
[2021-05-16 22:36:55,429] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (5deb0a1bddc0, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:55,429] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 2571 ms on 5deb0a1bddc0 (executor driver) (167/200)
[2021-05-16 22:36:55,430] {docker.py:276} INFO - 21/05/17 01:36:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:55,431] {docker.py:276} INFO - 21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:55,431] {docker.py:276} INFO - 21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445503367678383246671_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445503367678383246671_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445503367678383246671_0004}; taskId=attempt_20210517013445503367678383246671_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3beb33a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:55,432] {docker.py:276} INFO - 21/05/17 01:36:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:55,432] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_20210517013445503367678383246671_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445503367678383246671_0004_m_000168_456
[2021-05-16 22:36:55,432] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
[2021-05-16 22:36:55,434] {docker.py:276} INFO - 21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:55,436] {docker.py:276} INFO - 21/05/17 01:36:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455868928251175883905_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455868928251175883905_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455868928251175883905_0004}; taskId=attempt_202105170134455868928251175883905_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32da684b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134455868928251175883905_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455868928251175883905_0004_m_000169_457
[2021-05-16 22:36:55,439] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_20210517013445503367678383246671_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445503367678383246671_0004_m_000168_456 : duration 0:00.007s
[2021-05-16 22:36:55,439] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134455868928251175883905_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455868928251175883905_0004_m_000169_457 : duration 0:00.003s
[2021-05-16 22:36:55,441] {docker.py:276} INFO - 21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:36:55,441] {docker.py:276} INFO - 21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:55,443] {docker.py:276} INFO - 21/05/17 01:36:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:36:55,443] {docker.py:276} INFO - 21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:55,443] {docker.py:276} INFO - 21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452735608687401669378_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452735608687401669378_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452735608687401669378_0004}; taskId=attempt_202105170134452735608687401669378_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66cb3d99}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:55,444] {docker.py:276} INFO - 21/05/17 01:36:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:36:55,444] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134452735608687401669378_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452735608687401669378_0004_m_000170_458
[2021-05-16 22:36:55,447] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134452735608687401669378_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452735608687401669378_0004_m_000170_458 : duration 0:00.003s
[2021-05-16 22:36:55,907] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134452161060409760349220_0004_m_000167_455: needsTaskCommit() Task attempt_202105170134452161060409760349220_0004_m_000167_455
[2021-05-16 22:36:55,907] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134452161060409760349220_0004_m_000167_455: needsTaskCommit() Task attempt_202105170134452161060409760349220_0004_m_000167_455: duration 0:00.000s
21/05/17 01:36:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452161060409760349220_0004_m_000167_455
21/05/17 01:36:55 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 4587 bytes result sent to driver
[2021-05-16 22:36:55,910] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (5deb0a1bddc0, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:55,911] {docker.py:276} INFO - 21/05/17 01:36:55 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 2588 ms on 5deb0a1bddc0 (executor driver) (168/200)
[2021-05-16 22:36:55,911] {docker.py:276} INFO - 21/05/17 01:36:55 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
[2021-05-16 22:36:55,919] {docker.py:276} INFO - 21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:55,921] {docker.py:276} INFO - 21/05/17 01:36:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456520245523469384508_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456520245523469384508_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456520245523469384508_0004}; taskId=attempt_202105170134456520245523469384508_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7883dda7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:55 INFO StagingCommitter: Starting: Task committer attempt_202105170134456520245523469384508_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456520245523469384508_0004_m_000171_459
[2021-05-16 22:36:55,923] {docker.py:276} INFO - 21/05/17 01:36:55 INFO StagingCommitter: Task committer attempt_202105170134456520245523469384508_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456520245523469384508_0004_m_000171_459 : duration 0:00.002s
[2021-05-16 22:36:57,543] {docker.py:276} INFO - 21/05/17 01:36:57 INFO StagingCommitter: Starting: Task committer attempt_202105170134455868928251175883905_0004_m_000169_457: needsTaskCommit() Task attempt_202105170134455868928251175883905_0004_m_000169_457
[2021-05-16 22:36:57,543] {docker.py:276} INFO - 21/05/17 01:36:57 INFO StagingCommitter: Task committer attempt_202105170134455868928251175883905_0004_m_000169_457: needsTaskCommit() Task attempt_202105170134455868928251175883905_0004_m_000169_457: duration 0:00.000s
21/05/17 01:36:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455868928251175883905_0004_m_000169_457
[2021-05-16 22:36:57,544] {docker.py:276} INFO - 21/05/17 01:36:57 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 4587 bytes result sent to driver
[2021-05-16 22:36:57,545] {docker.py:276} INFO - 21/05/17 01:36:57 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (5deb0a1bddc0, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:57,546] {docker.py:276} INFO - 21/05/17 01:36:57 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
[2021-05-16 22:36:57,547] {docker.py:276} INFO - 21/05/17 01:36:57 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 2137 ms on 5deb0a1bddc0 (executor driver) (169/200)
[2021-05-16 22:36:57,554] {docker.py:276} INFO - 21/05/17 01:36:57 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:57,556] {docker.py:276} INFO - 21/05/17 01:36:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445662224579372251031_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445662224579372251031_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445662224579372251031_0004}; taskId=attempt_20210517013445662224579372251031_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60a84519}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:57,556] {docker.py:276} INFO - 21/05/17 01:36:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:57 INFO StagingCommitter: Starting: Task committer attempt_20210517013445662224579372251031_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445662224579372251031_0004_m_000172_460
[2021-05-16 22:36:57,559] {docker.py:276} INFO - 21/05/17 01:36:57 INFO StagingCommitter: Task committer attempt_20210517013445662224579372251031_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445662224579372251031_0004_m_000172_460 : duration 0:00.003s
[2021-05-16 22:36:57,739] {docker.py:276} INFO - 21/05/17 01:36:57 INFO StagingCommitter: Starting: Task committer attempt_20210517013445503367678383246671_0004_m_000168_456: needsTaskCommit() Task attempt_20210517013445503367678383246671_0004_m_000168_456
21/05/17 01:36:57 INFO StagingCommitter: Task committer attempt_20210517013445503367678383246671_0004_m_000168_456: needsTaskCommit() Task attempt_20210517013445503367678383246671_0004_m_000168_456: duration 0:00.000s
21/05/17 01:36:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445503367678383246671_0004_m_000168_456
[2021-05-16 22:36:57,741] {docker.py:276} INFO - 21/05/17 01:36:57 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 4587 bytes result sent to driver
[2021-05-16 22:36:57,742] {docker.py:276} INFO - 21/05/17 01:36:57 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (5deb0a1bddc0, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:57,743] {docker.py:276} INFO - 21/05/17 01:36:57 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 2343 ms on 5deb0a1bddc0 (executor driver) (170/200)
[2021-05-16 22:36:57,743] {docker.py:276} INFO - 21/05/17 01:36:57 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
[2021-05-16 22:36:57,752] {docker.py:276} INFO - 21/05/17 01:36:57 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:57,754] {docker.py:276} INFO - 21/05/17 01:36:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:36:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457958633723129553630_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457958633723129553630_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457958633723129553630_0004}; taskId=attempt_202105170134457958633723129553630_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5163109d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:57 INFO StagingCommitter: Starting: Task committer attempt_202105170134457958633723129553630_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457958633723129553630_0004_m_000173_461
[2021-05-16 22:36:57,758] {docker.py:276} INFO - 21/05/17 01:36:57 INFO StagingCommitter: Task committer attempt_202105170134457958633723129553630_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457958633723129553630_0004_m_000173_461 : duration 0:00.004s
[2021-05-16 22:36:58,067] {docker.py:276} INFO - 21/05/17 01:36:58 INFO StagingCommitter: Starting: Task committer attempt_202105170134452735608687401669378_0004_m_000170_458: needsTaskCommit() Task attempt_202105170134452735608687401669378_0004_m_000170_458
[2021-05-16 22:36:58,068] {docker.py:276} INFO - 21/05/17 01:36:58 INFO StagingCommitter: Task committer attempt_202105170134452735608687401669378_0004_m_000170_458: needsTaskCommit() Task attempt_202105170134452735608687401669378_0004_m_000170_458: duration 0:00.002s
21/05/17 01:36:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452735608687401669378_0004_m_000170_458
[2021-05-16 22:36:58,070] {docker.py:276} INFO - 21/05/17 01:36:58 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 4544 bytes result sent to driver
[2021-05-16 22:36:58,071] {docker.py:276} INFO - 21/05/17 01:36:58 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (5deb0a1bddc0, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:58,072] {docker.py:276} INFO - 21/05/17 01:36:58 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 2645 ms on 5deb0a1bddc0 (executor driver) (171/200)
[2021-05-16 22:36:58,072] {docker.py:276} INFO - 21/05/17 01:36:58 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-16 22:36:58,081] {docker.py:276} INFO - 21/05/17 01:36:58 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:58,083] {docker.py:276} INFO - 21/05/17 01:36:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:58,084] {docker.py:276} INFO - 21/05/17 01:36:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454879432828027488868_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454879432828027488868_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454879432828027488868_0004}; taskId=attempt_202105170134454879432828027488868_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cbd3f55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:58 INFO StagingCommitter: Starting: Task committer attempt_202105170134454879432828027488868_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454879432828027488868_0004_m_000174_462
[2021-05-16 22:36:58,086] {docker.py:276} INFO - 21/05/17 01:36:58 INFO StagingCommitter: Task committer attempt_202105170134454879432828027488868_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454879432828027488868_0004_m_000174_462 : duration 0:00.003s
[2021-05-16 22:36:58,495] {docker.py:276} INFO - 21/05/17 01:36:58 INFO StagingCommitter: Starting: Task committer attempt_202105170134456520245523469384508_0004_m_000171_459: needsTaskCommit() Task attempt_202105170134456520245523469384508_0004_m_000171_459
[2021-05-16 22:36:58,496] {docker.py:276} INFO - 21/05/17 01:36:58 INFO StagingCommitter: Task committer attempt_202105170134456520245523469384508_0004_m_000171_459: needsTaskCommit() Task attempt_202105170134456520245523469384508_0004_m_000171_459: duration 0:00.000s
21/05/17 01:36:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456520245523469384508_0004_m_000171_459
[2021-05-16 22:36:58,496] {docker.py:276} INFO - 21/05/17 01:36:58 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 4544 bytes result sent to driver
[2021-05-16 22:36:58,497] {docker.py:276} INFO - 21/05/17 01:36:58 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (5deb0a1bddc0, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:58,497] {docker.py:276} INFO - 21/05/17 01:36:58 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-16 22:36:58,498] {docker.py:276} INFO - 21/05/17 01:36:58 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 2593 ms on 5deb0a1bddc0 (executor driver) (172/200)
[2021-05-16 22:36:58,505] {docker.py:276} INFO - 21/05/17 01:36:58 INFO ShuffleBlockFetcherIterator: Getting 5 (21.2 KiB) non-empty blocks including 5 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:58,506] {docker.py:276} INFO - 21/05/17 01:36:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:58,507] {docker.py:276} INFO - 21/05/17 01:36:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456285942437197362304_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456285942437197362304_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456285942437197362304_0004}; taskId=attempt_202105170134456285942437197362304_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@753b77cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:58 INFO StagingCommitter: Starting: Task committer attempt_202105170134456285942437197362304_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456285942437197362304_0004_m_000175_463
[2021-05-16 22:36:58,509] {docker.py:276} INFO - 21/05/17 01:36:58 INFO StagingCommitter: Task committer attempt_202105170134456285942437197362304_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456285942437197362304_0004_m_000175_463 : duration 0:00.003s
[2021-05-16 22:36:59,943] {docker.py:276} INFO - 21/05/17 01:36:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134457958633723129553630_0004_m_000173_461: needsTaskCommit() Task attempt_202105170134457958633723129553630_0004_m_000173_461
[2021-05-16 22:36:59,944] {docker.py:276} INFO - 21/05/17 01:36:59 INFO StagingCommitter: Task committer attempt_202105170134457958633723129553630_0004_m_000173_461: needsTaskCommit() Task attempt_202105170134457958633723129553630_0004_m_000173_461: duration 0:00.000s
21/05/17 01:36:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457958633723129553630_0004_m_000173_461
[2021-05-16 22:36:59,945] {docker.py:276} INFO - 21/05/17 01:36:59 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 4544 bytes result sent to driver
[2021-05-16 22:36:59,947] {docker.py:276} INFO - 21/05/17 01:36:59 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (5deb0a1bddc0, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:36:59,948] {docker.py:276} INFO - 21/05/17 01:36:59 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 2174 ms on 5deb0a1bddc0 (executor driver) (173/200)
21/05/17 01:36:59 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-16 22:36:59,958] {docker.py:276} INFO - 21/05/17 01:36:59 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:36:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:36:59,959] {docker.py:276} INFO - 21/05/17 01:36:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:36:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:36:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:36:59,960] {docker.py:276} INFO - 21/05/17 01:36:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134456231377195312502620_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456231377195312502620_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134456231377195312502620_0004}; taskId=attempt_202105170134456231377195312502620_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47d5aa79}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:36:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:36:59 INFO StagingCommitter: Starting: Task committer attempt_202105170134456231377195312502620_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456231377195312502620_0004_m_000176_464
[2021-05-16 22:36:59,963] {docker.py:276} INFO - 21/05/17 01:36:59 INFO StagingCommitter: Task committer attempt_202105170134456231377195312502620_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134456231377195312502620_0004_m_000176_464 : duration 0:00.002s
[2021-05-16 22:37:00,679] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Starting: Task committer attempt_202105170134454879432828027488868_0004_m_000174_462: needsTaskCommit() Task attempt_202105170134454879432828027488868_0004_m_000174_462
[2021-05-16 22:37:00,679] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Task committer attempt_202105170134454879432828027488868_0004_m_000174_462: needsTaskCommit() Task attempt_202105170134454879432828027488868_0004_m_000174_462: duration 0:00.001s
21/05/17 01:37:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454879432828027488868_0004_m_000174_462
[2021-05-16 22:37:00,681] {docker.py:276} INFO - 21/05/17 01:37:00 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 4544 bytes result sent to driver
[2021-05-16 22:37:00,683] {docker.py:276} INFO - 21/05/17 01:37:00 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (5deb0a1bddc0, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:00,683] {docker.py:276} INFO - 21/05/17 01:37:00 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
[2021-05-16 22:37:00,684] {docker.py:276} INFO - 21/05/17 01:37:00 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 2583 ms on 5deb0a1bddc0 (executor driver) (174/200)
[2021-05-16 22:37:00,705] {docker.py:276} INFO - 21/05/17 01:37:00 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:00,705] {docker.py:276} INFO - 21/05/17 01:37:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:00,706] {docker.py:276} INFO - 21/05/17 01:37:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:00,706] {docker.py:276} INFO - 21/05/17 01:37:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452057934374216626189_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452057934374216626189_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452057934374216626189_0004}; taskId=attempt_202105170134452057934374216626189_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6231ad0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:00,706] {docker.py:276} INFO - 21/05/17 01:37:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:00,707] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Starting: Task committer attempt_202105170134452057934374216626189_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452057934374216626189_0004_m_000177_465
[2021-05-16 22:37:00,707] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Starting: Task committer attempt_20210517013445662224579372251031_0004_m_000172_460: needsTaskCommit() Task attempt_20210517013445662224579372251031_0004_m_000172_460
[2021-05-16 22:37:00,707] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Task committer attempt_20210517013445662224579372251031_0004_m_000172_460: needsTaskCommit() Task attempt_20210517013445662224579372251031_0004_m_000172_460: duration 0:00.001s
[2021-05-16 22:37:00,707] {docker.py:276} INFO - 21/05/17 01:37:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445662224579372251031_0004_m_000172_460
[2021-05-16 22:37:00,708] {docker.py:276} INFO - 21/05/17 01:37:00 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 4544 bytes result sent to driver
[2021-05-16 22:37:00,708] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Task committer attempt_202105170134452057934374216626189_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452057934374216626189_0004_m_000177_465 : duration 0:00.004s
[2021-05-16 22:37:00,708] {docker.py:276} INFO - 21/05/17 01:37:00 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (5deb0a1bddc0, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:00,708] {docker.py:276} INFO - 21/05/17 01:37:00 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 3125 ms on 5deb0a1bddc0 (executor driver) (175/200)
[2021-05-16 22:37:00,709] {docker.py:276} INFO - 21/05/17 01:37:00 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-16 22:37:00,709] {docker.py:276} INFO - 21/05/17 01:37:00 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:37:00,709] {docker.py:276} INFO - 21/05/17 01:37:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:00,710] {docker.py:276} INFO - 21/05/17 01:37:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:00,711] {docker.py:276} INFO - 21/05/17 01:37:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134451361316948831786159_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451361316948831786159_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134451361316948831786159_0004}; taskId=attempt_202105170134451361316948831786159_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e5003f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:00 INFO StagingCommitter: Starting: Task committer attempt_202105170134451361316948831786159_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451361316948831786159_0004_m_000178_466
[2021-05-16 22:37:00,713] {docker.py:276} INFO - 21/05/17 01:37:00 INFO StagingCommitter: Task committer attempt_202105170134451361316948831786159_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134451361316948831786159_0004_m_000178_466 : duration 0:00.002s
[2021-05-16 22:37:01,333] {docker.py:276} INFO - 21/05/17 01:37:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134456285942437197362304_0004_m_000175_463: needsTaskCommit() Task attempt_202105170134456285942437197362304_0004_m_000175_463
[2021-05-16 22:37:01,334] {docker.py:276} INFO - 21/05/17 01:37:01 INFO StagingCommitter: Task committer attempt_202105170134456285942437197362304_0004_m_000175_463: needsTaskCommit() Task attempt_202105170134456285942437197362304_0004_m_000175_463: duration 0:00.001s
21/05/17 01:37:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456285942437197362304_0004_m_000175_463
[2021-05-16 22:37:01,335] {docker.py:276} INFO - 21/05/17 01:37:01 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 4544 bytes result sent to driver
[2021-05-16 22:37:01,336] {docker.py:276} INFO - 21/05/17 01:37:01 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (5deb0a1bddc0, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:01,337] {docker.py:276} INFO - 21/05/17 01:37:01 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
[2021-05-16 22:37:01,338] {docker.py:276} INFO - 21/05/17 01:37:01 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 2809 ms on 5deb0a1bddc0 (executor driver) (176/200)
[2021-05-16 22:37:01,356] {docker.py:276} INFO - 21/05/17 01:37:01 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:01,357] {docker.py:276} INFO - 21/05/17 01:37:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455501346972636668801_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455501346972636668801_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455501346972636668801_0004}; taskId=attempt_202105170134455501346972636668801_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bc0f0a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:01,357] {docker.py:276} INFO - 21/05/17 01:37:01 INFO StagingCommitter: Starting: Task committer attempt_202105170134455501346972636668801_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455501346972636668801_0004_m_000179_467
[2021-05-16 22:37:01,360] {docker.py:276} INFO - 21/05/17 01:37:01 INFO StagingCommitter: Task committer attempt_202105170134455501346972636668801_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455501346972636668801_0004_m_000179_467 : duration 0:00.003s
[2021-05-16 22:37:02,569] {docker.py:276} INFO - 21/05/17 01:37:02 INFO StagingCommitter: Starting: Task committer attempt_202105170134456231377195312502620_0004_m_000176_464: needsTaskCommit() Task attempt_202105170134456231377195312502620_0004_m_000176_464
[2021-05-16 22:37:02,571] {docker.py:276} INFO - 21/05/17 01:37:02 INFO StagingCommitter: Task committer attempt_202105170134456231377195312502620_0004_m_000176_464: needsTaskCommit() Task attempt_202105170134456231377195312502620_0004_m_000176_464: duration 0:00.000s
21/05/17 01:37:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134456231377195312502620_0004_m_000176_464
[2021-05-16 22:37:02,572] {docker.py:276} INFO - 21/05/17 01:37:02 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 4587 bytes result sent to driver
[2021-05-16 22:37:02,573] {docker.py:276} INFO - 21/05/17 01:37:02 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (5deb0a1bddc0, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:02,574] {docker.py:276} INFO - 21/05/17 01:37:02 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
[2021-05-16 22:37:02,575] {docker.py:276} INFO - 21/05/17 01:37:02 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 2632 ms on 5deb0a1bddc0 (executor driver) (177/200)
[2021-05-16 22:37:02,583] {docker.py:276} INFO - 21/05/17 01:37:02 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:02,585] {docker.py:276} INFO - 21/05/17 01:37:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457155303182522010940_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457155303182522010940_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457155303182522010940_0004}; taskId=attempt_202105170134457155303182522010940_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9967314}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:02,585] {docker.py:276} INFO - 21/05/17 01:37:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:02 INFO StagingCommitter: Starting: Task committer attempt_202105170134457155303182522010940_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457155303182522010940_0004_m_000180_468
[2021-05-16 22:37:02,588] {docker.py:276} INFO - 21/05/17 01:37:02 INFO StagingCommitter: Task committer attempt_202105170134457155303182522010940_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457155303182522010940_0004_m_000180_468 : duration 0:00.003s
[2021-05-16 22:37:03,324] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134451361316948831786159_0004_m_000178_466: needsTaskCommit() Task attempt_202105170134451361316948831786159_0004_m_000178_466
21/05/17 01:37:03 INFO StagingCommitter: Task committer attempt_202105170134451361316948831786159_0004_m_000178_466: needsTaskCommit() Task attempt_202105170134451361316948831786159_0004_m_000178_466: duration 0:00.001s
[2021-05-16 22:37:03,325] {docker.py:276} INFO - 21/05/17 01:37:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134451361316948831786159_0004_m_000178_466
[2021-05-16 22:37:03,327] {docker.py:276} INFO - 21/05/17 01:37:03 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 4587 bytes result sent to driver
[2021-05-16 22:37:03,329] {docker.py:276} INFO - 21/05/17 01:37:03 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (5deb0a1bddc0, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:03,330] {docker.py:276} INFO - 21/05/17 01:37:03 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
[2021-05-16 22:37:03,330] {docker.py:276} INFO - 21/05/17 01:37:03 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 2631 ms on 5deb0a1bddc0 (executor driver) (178/200)
[2021-05-16 22:37:03,340] {docker.py:276} INFO - 21/05/17 01:37:03 INFO ShuffleBlockFetcherIterator: Getting 5 (24.3 KiB) non-empty blocks including 5 (24.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:37:03,341] {docker.py:276} INFO - 21/05/17 01:37:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:03,342] {docker.py:276} INFO - 21/05/17 01:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:37:03,343] {docker.py:276} INFO - 21/05/17 01:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:03,343] {docker.py:276} INFO - 21/05/17 01:37:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:03,344] {docker.py:276} INFO - 21/05/17 01:37:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457994830624914345747_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457994830624914345747_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457994830624914345747_0004}; taskId=attempt_202105170134457994830624914345747_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58fe3d03}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:03,344] {docker.py:276} INFO - 21/05/17 01:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:03,344] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134457994830624914345747_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457994830624914345747_0004_m_000181_469
[2021-05-16 22:37:03,348] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Task committer attempt_202105170134457994830624914345747_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457994830624914345747_0004_m_000181_469 : duration 0:00.003s
[2021-05-16 22:37:03,359] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134452057934374216626189_0004_m_000177_465: needsTaskCommit() Task attempt_202105170134452057934374216626189_0004_m_000177_465
[2021-05-16 22:37:03,360] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Task committer attempt_202105170134452057934374216626189_0004_m_000177_465: needsTaskCommit() Task attempt_202105170134452057934374216626189_0004_m_000177_465: duration 0:00.000s
21/05/17 01:37:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452057934374216626189_0004_m_000177_465
[2021-05-16 22:37:03,361] {docker.py:276} INFO - 21/05/17 01:37:03 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 4587 bytes result sent to driver
[2021-05-16 22:37:03,362] {docker.py:276} INFO - 21/05/17 01:37:03 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (5deb0a1bddc0, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:03,363] {docker.py:276} INFO - 21/05/17 01:37:03 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 2685 ms on 5deb0a1bddc0 (executor driver) (179/200)
[2021-05-16 22:37:03,363] {docker.py:276} INFO - 21/05/17 01:37:03 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
[2021-05-16 22:37:03,371] {docker.py:276} INFO - 21/05/17 01:37:03 INFO ShuffleBlockFetcherIterator: Getting 5 (21.7 KiB) non-empty blocks including 5 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:37:03,371] {docker.py:276} INFO - 21/05/17 01:37:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:03,373] {docker.py:276} INFO - 21/05/17 01:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:37:03,373] {docker.py:276} INFO - 21/05/17 01:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:03,373] {docker.py:276} INFO - 21/05/17 01:37:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:03,374] {docker.py:276} INFO - 21/05/17 01:37:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457488606979056111627_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457488606979056111627_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457488606979056111627_0004}; taskId=attempt_202105170134457488606979056111627_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@337c9c78}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:03,374] {docker.py:276} INFO - 21/05/17 01:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:03,374] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134457488606979056111627_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457488606979056111627_0004_m_000182_470
[2021-05-16 22:37:03,378] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Task committer attempt_202105170134457488606979056111627_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457488606979056111627_0004_m_000182_470 : duration 0:00.003s
[2021-05-16 22:37:03,525] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Starting: Task committer attempt_202105170134455501346972636668801_0004_m_000179_467: needsTaskCommit() Task attempt_202105170134455501346972636668801_0004_m_000179_467
[2021-05-16 22:37:03,526] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Task committer attempt_202105170134455501346972636668801_0004_m_000179_467: needsTaskCommit() Task attempt_202105170134455501346972636668801_0004_m_000179_467: duration 0:00.001s
21/05/17 01:37:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455501346972636668801_0004_m_000179_467
[2021-05-16 22:37:03,529] {docker.py:276} INFO - 21/05/17 01:37:03 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 4587 bytes result sent to driver
[2021-05-16 22:37:03,530] {docker.py:276} INFO - 21/05/17 01:37:03 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (5deb0a1bddc0, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:03,531] {docker.py:276} INFO - 21/05/17 01:37:03 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-16 22:37:03,532] {docker.py:276} INFO - 21/05/17 01:37:03 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 2198 ms on 5deb0a1bddc0 (executor driver) (180/200)
[2021-05-16 22:37:03,542] {docker.py:276} INFO - 21/05/17 01:37:03 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:03,544] {docker.py:276} INFO - 21/05/17 01:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445828235116905647077_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445828235116905647077_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445828235116905647077_0004}; taskId=attempt_20210517013445828235116905647077_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63d737b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:03,544] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Starting: Task committer attempt_20210517013445828235116905647077_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445828235116905647077_0004_m_000183_471
[2021-05-16 22:37:03,547] {docker.py:276} INFO - 21/05/17 01:37:03 INFO StagingCommitter: Task committer attempt_20210517013445828235116905647077_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445828235116905647077_0004_m_000183_471 : duration 0:00.003s
[2021-05-16 22:37:04,731] {docker.py:276} INFO - 21/05/17 01:37:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134457155303182522010940_0004_m_000180_468: needsTaskCommit() Task attempt_202105170134457155303182522010940_0004_m_000180_468
21/05/17 01:37:04 INFO StagingCommitter: Task committer attempt_202105170134457155303182522010940_0004_m_000180_468: needsTaskCommit() Task attempt_202105170134457155303182522010940_0004_m_000180_468: duration 0:00.000s
21/05/17 01:37:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457155303182522010940_0004_m_000180_468
[2021-05-16 22:37:04,734] {docker.py:276} INFO - 21/05/17 01:37:04 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 4544 bytes result sent to driver
[2021-05-16 22:37:04,735] {docker.py:276} INFO - 21/05/17 01:37:04 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (5deb0a1bddc0, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:04,736] {docker.py:276} INFO - 21/05/17 01:37:04 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 2166 ms on 5deb0a1bddc0 (executor driver) (181/200)
[2021-05-16 22:37:04,737] {docker.py:276} INFO - 21/05/17 01:37:04 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-16 22:37:04,747] {docker.py:276} INFO - 21/05/17 01:37:04 INFO ShuffleBlockFetcherIterator: Getting 5 (20.7 KiB) non-empty blocks including 5 (20.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:04,749] {docker.py:276} INFO - 21/05/17 01:37:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453813597299998354445_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453813597299998354445_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453813597299998354445_0004}; taskId=attempt_202105170134453813597299998354445_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@83fa4cd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:04,750] {docker.py:276} INFO - 21/05/17 01:37:04 INFO StagingCommitter: Starting: Task committer attempt_202105170134453813597299998354445_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453813597299998354445_0004_m_000184_472
[2021-05-16 22:37:04,752] {docker.py:276} INFO - 21/05/17 01:37:04 INFO StagingCommitter: Task committer attempt_202105170134453813597299998354445_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453813597299998354445_0004_m_000184_472 : duration 0:00.003s
[2021-05-16 22:37:05,945] {docker.py:276} INFO - 21/05/17 01:37:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134457994830624914345747_0004_m_000181_469: needsTaskCommit() Task attempt_202105170134457994830624914345747_0004_m_000181_469
[2021-05-16 22:37:05,946] {docker.py:276} INFO - 21/05/17 01:37:05 INFO StagingCommitter: Task committer attempt_202105170134457994830624914345747_0004_m_000181_469: needsTaskCommit() Task attempt_202105170134457994830624914345747_0004_m_000181_469: duration 0:00.001s
21/05/17 01:37:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457994830624914345747_0004_m_000181_469
[2021-05-16 22:37:05,946] {docker.py:276} INFO - 21/05/17 01:37:05 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 4544 bytes result sent to driver
[2021-05-16 22:37:05,948] {docker.py:276} INFO - 21/05/17 01:37:05 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (5deb0a1bddc0, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:05,949] {docker.py:276} INFO - 21/05/17 01:37:05 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 2624 ms on 5deb0a1bddc0 (executor driver) (182/200)
[2021-05-16 22:37:05,950] {docker.py:276} INFO - 21/05/17 01:37:05 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-16 22:37:05,957] {docker.py:276} INFO - 21/05/17 01:37:05 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:05,959] {docker.py:276} INFO - 21/05/17 01:37:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:05,959] {docker.py:276} INFO - 21/05/17 01:37:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454148297594807361459_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454148297594807361459_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454148297594807361459_0004}; taskId=attempt_202105170134454148297594807361459_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5371177f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:05 INFO StagingCommitter: Starting: Task committer attempt_202105170134454148297594807361459_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454148297594807361459_0004_m_000185_473
[2021-05-16 22:37:05,963] {docker.py:276} INFO - 21/05/17 01:37:05 INFO StagingCommitter: Task committer attempt_202105170134454148297594807361459_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454148297594807361459_0004_m_000185_473 : duration 0:00.003s
[2021-05-16 22:37:06,108] {docker.py:276} INFO - 21/05/17 01:37:06 INFO StagingCommitter: Starting: Task committer attempt_20210517013445828235116905647077_0004_m_000183_471: needsTaskCommit() Task attempt_20210517013445828235116905647077_0004_m_000183_471
[2021-05-16 22:37:06,108] {docker.py:276} INFO - 21/05/17 01:37:06 INFO StagingCommitter: Task committer attempt_20210517013445828235116905647077_0004_m_000183_471: needsTaskCommit() Task attempt_20210517013445828235116905647077_0004_m_000183_471: duration 0:00.000s
21/05/17 01:37:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445828235116905647077_0004_m_000183_471
[2021-05-16 22:37:06,110] {docker.py:276} INFO - 21/05/17 01:37:06 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 4544 bytes result sent to driver
[2021-05-16 22:37:06,111] {docker.py:276} INFO - 21/05/17 01:37:06 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (5deb0a1bddc0, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 01:37:06 INFO StagingCommitter: Starting: Task committer attempt_202105170134457488606979056111627_0004_m_000182_470: needsTaskCommit() Task attempt_202105170134457488606979056111627_0004_m_000182_470
[2021-05-16 22:37:06,112] {docker.py:276} INFO - 21/05/17 01:37:06 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
21/05/17 01:37:06 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 2586 ms on 5deb0a1bddc0 (executor driver) (183/200)
21/05/17 01:37:06 INFO StagingCommitter: Task committer attempt_202105170134457488606979056111627_0004_m_000182_470: needsTaskCommit() Task attempt_202105170134457488606979056111627_0004_m_000182_470: duration 0:00.001s
21/05/17 01:37:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457488606979056111627_0004_m_000182_470
[2021-05-16 22:37:06,113] {docker.py:276} INFO - 21/05/17 01:37:06 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 4544 bytes result sent to driver
[2021-05-16 22:37:06,114] {docker.py:276} INFO - 21/05/17 01:37:06 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (5deb0a1bddc0, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:06,116] {docker.py:276} INFO - 21/05/17 01:37:06 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
21/05/17 01:37:06 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 2756 ms on 5deb0a1bddc0 (executor driver) (184/200)
[2021-05-16 22:37:06,124] {docker.py:276} INFO - 21/05/17 01:37:06 INFO ShuffleBlockFetcherIterator: Getting 5 (20.9 KiB) non-empty blocks including 5 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:06,125] {docker.py:276} INFO - 21/05/17 01:37:06 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:06,126] {docker.py:276} INFO - 21/05/17 01:37:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453532555494432639102_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453532555494432639102_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453532555494432639102_0004}; taskId=attempt_202105170134453532555494432639102_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@346bedb2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:06,127] {docker.py:276} INFO - 21/05/17 01:37:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:06 INFO StagingCommitter: Starting: Task committer attempt_202105170134453532555494432639102_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453532555494432639102_0004_m_000187_475
[2021-05-16 22:37:06,127] {docker.py:276} INFO - 21/05/17 01:37:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:06,128] {docker.py:276} INFO - 21/05/17 01:37:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457152879758742827782_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457152879758742827782_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457152879758742827782_0004}; taskId=attempt_202105170134457152879758742827782_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f8c2cb7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:06 INFO StagingCommitter: Starting: Task committer attempt_202105170134457152879758742827782_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457152879758742827782_0004_m_000186_474
[2021-05-16 22:37:06,130] {docker.py:276} INFO - 21/05/17 01:37:06 INFO StagingCommitter: Task committer attempt_202105170134453532555494432639102_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453532555494432639102_0004_m_000187_475 : duration 0:00.004s
[2021-05-16 22:37:06,132] {docker.py:276} INFO - 21/05/17 01:37:06 INFO StagingCommitter: Task committer attempt_202105170134457152879758742827782_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457152879758742827782_0004_m_000186_474 : duration 0:00.004s
[2021-05-16 22:37:07,176] {docker.py:276} INFO - 21/05/17 01:37:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134453813597299998354445_0004_m_000184_472: needsTaskCommit() Task attempt_202105170134453813597299998354445_0004_m_000184_472
[2021-05-16 22:37:07,176] {docker.py:276} INFO - 21/05/17 01:37:07 INFO StagingCommitter: Task committer attempt_202105170134453813597299998354445_0004_m_000184_472: needsTaskCommit() Task attempt_202105170134453813597299998354445_0004_m_000184_472: duration 0:00.000s
21/05/17 01:37:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453813597299998354445_0004_m_000184_472
[2021-05-16 22:37:07,178] {docker.py:276} INFO - 21/05/17 01:37:07 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 4544 bytes result sent to driver
[2021-05-16 22:37:07,179] {docker.py:276} INFO - 21/05/17 01:37:07 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (5deb0a1bddc0, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:07,180] {docker.py:276} INFO - 21/05/17 01:37:07 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 2449 ms on 5deb0a1bddc0 (executor driver) (185/200)
[2021-05-16 22:37:07,181] {docker.py:276} INFO - 21/05/17 01:37:07 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-16 22:37:07,191] {docker.py:276} INFO - 21/05/17 01:37:07 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:07,193] {docker.py:276} INFO - 21/05/17 01:37:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:07,193] {docker.py:276} INFO - 21/05/17 01:37:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:07,194] {docker.py:276} INFO - 21/05/17 01:37:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455745313268983621006_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455745313268983621006_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455745313268983621006_0004}; taskId=attempt_202105170134455745313268983621006_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f8451d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:07,194] {docker.py:276} INFO - 21/05/17 01:37:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:07,194] {docker.py:276} INFO - 21/05/17 01:37:07 INFO StagingCommitter: Starting: Task committer attempt_202105170134455745313268983621006_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455745313268983621006_0004_m_000188_476
[2021-05-16 22:37:07,197] {docker.py:276} INFO - 21/05/17 01:37:07 INFO StagingCommitter: Task committer attempt_202105170134455745313268983621006_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455745313268983621006_0004_m_000188_476 : duration 0:00.003s
[2021-05-16 22:37:09,181] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134453532555494432639102_0004_m_000187_475: needsTaskCommit() Task attempt_202105170134453532555494432639102_0004_m_000187_475
[2021-05-16 22:37:09,182] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Task committer attempt_202105170134453532555494432639102_0004_m_000187_475: needsTaskCommit() Task attempt_202105170134453532555494432639102_0004_m_000187_475: duration 0:00.001s
21/05/17 01:37:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453532555494432639102_0004_m_000187_475
[2021-05-16 22:37:09,184] {docker.py:276} INFO - 21/05/17 01:37:09 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 4544 bytes result sent to driver
[2021-05-16 22:37:09,185] {docker.py:276} INFO - 21/05/17 01:37:09 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (5deb0a1bddc0, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:09,186] {docker.py:276} INFO - 21/05/17 01:37:09 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 3075 ms on 5deb0a1bddc0 (executor driver) (186/200)
[2021-05-16 22:37:09,187] {docker.py:276} INFO - 21/05/17 01:37:09 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-16 22:37:09,209] {docker.py:276} INFO - 21/05/17 01:37:09 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 22:37:09,209] {docker.py:276} INFO - 21/05/17 01:37:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:09,210] {docker.py:276} INFO - 21/05/17 01:37:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 22:37:09,211] {docker.py:276} INFO - 21/05/17 01:37:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:09,211] {docker.py:276} INFO - 21/05/17 01:37:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:09,212] {docker.py:276} INFO - 21/05/17 01:37:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134455552777756724531729_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455552777756724531729_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134455552777756724531729_0004}; taskId=attempt_202105170134455552777756724531729_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b1938d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:09,213] {docker.py:276} INFO - 21/05/17 01:37:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:09,213] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134455552777756724531729_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455552777756724531729_0004_m_000189_477
[2021-05-16 22:37:09,215] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Task committer attempt_202105170134455552777756724531729_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134455552777756724531729_0004_m_000189_477 : duration 0:00.003s
[2021-05-16 22:37:09,347] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134457152879758742827782_0004_m_000186_474: needsTaskCommit() Task attempt_202105170134457152879758742827782_0004_m_000186_474
[2021-05-16 22:37:09,348] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Task committer attempt_202105170134457152879758742827782_0004_m_000186_474: needsTaskCommit() Task attempt_202105170134457152879758742827782_0004_m_000186_474: duration 0:00.001s
21/05/17 01:37:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457152879758742827782_0004_m_000186_474
[2021-05-16 22:37:09,349] {docker.py:276} INFO - 21/05/17 01:37:09 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 4587 bytes result sent to driver
[2021-05-16 22:37:09,350] {docker.py:276} INFO - 21/05/17 01:37:09 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (5deb0a1bddc0, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:09,350] {docker.py:276} INFO - 21/05/17 01:37:09 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-16 22:37:09,351] {docker.py:276} INFO - 21/05/17 01:37:09 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 3243 ms on 5deb0a1bddc0 (executor driver) (187/200)
[2021-05-16 22:37:09,358] {docker.py:276} INFO - 21/05/17 01:37:09 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:09,360] {docker.py:276} INFO - 21/05/17 01:37:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134453741020434938449759_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453741020434938449759_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134453741020434938449759_0004}; taskId=attempt_202105170134453741020434938449759_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd02a75}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:09,360] {docker.py:276} INFO - 21/05/17 01:37:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:09 INFO StagingCommitter: Starting: Task committer attempt_202105170134453741020434938449759_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453741020434938449759_0004_m_000190_478
[2021-05-16 22:37:09,363] {docker.py:276} INFO - 21/05/17 01:37:09 INFO StagingCommitter: Task committer attempt_202105170134453741020434938449759_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134453741020434938449759_0004_m_000190_478 : duration 0:00.003s
[2021-05-16 22:37:10,062] {docker.py:276} INFO - 21/05/17 01:37:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134454148297594807361459_0004_m_000185_473: needsTaskCommit() Task attempt_202105170134454148297594807361459_0004_m_000185_473
[2021-05-16 22:37:10,063] {docker.py:276} INFO - 21/05/17 01:37:10 INFO StagingCommitter: Task committer attempt_202105170134454148297594807361459_0004_m_000185_473: needsTaskCommit() Task attempt_202105170134454148297594807361459_0004_m_000185_473: duration 0:00.003s
[2021-05-16 22:37:10,064] {docker.py:276} INFO - 21/05/17 01:37:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454148297594807361459_0004_m_000185_473
[2021-05-16 22:37:10,065] {docker.py:276} INFO - 21/05/17 01:37:10 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 4587 bytes result sent to driver
[2021-05-16 22:37:10,067] {docker.py:276} INFO - 21/05/17 01:37:10 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (5deb0a1bddc0, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:10,069] {docker.py:276} INFO - 21/05/17 01:37:10 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 4126 ms on 5deb0a1bddc0 (executor driver) (188/200)
[2021-05-16 22:37:10,070] {docker.py:276} INFO - 21/05/17 01:37:10 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-16 22:37:10,078] {docker.py:276} INFO - 21/05/17 01:37:10 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:10,080] {docker.py:276} INFO - 21/05/17 01:37:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457728460673044119585_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457728460673044119585_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457728460673044119585_0004}; taskId=attempt_202105170134457728460673044119585_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37ed8948}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134457728460673044119585_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457728460673044119585_0004_m_000191_479
[2021-05-16 22:37:10,083] {docker.py:276} INFO - 21/05/17 01:37:10 INFO StagingCommitter: Task committer attempt_202105170134457728460673044119585_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457728460673044119585_0004_m_000191_479 : duration 0:00.003s
[2021-05-16 22:37:10,427] {docker.py:276} INFO - 21/05/17 01:37:10 INFO StagingCommitter: Starting: Task committer attempt_202105170134455745313268983621006_0004_m_000188_476: needsTaskCommit() Task attempt_202105170134455745313268983621006_0004_m_000188_476
[2021-05-16 22:37:10,427] {docker.py:276} INFO - 21/05/17 01:37:10 INFO StagingCommitter: Task committer attempt_202105170134455745313268983621006_0004_m_000188_476: needsTaskCommit() Task attempt_202105170134455745313268983621006_0004_m_000188_476: duration 0:00.001s
[2021-05-16 22:37:10,428] {docker.py:276} INFO - 21/05/17 01:37:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455745313268983621006_0004_m_000188_476
[2021-05-16 22:37:10,429] {docker.py:276} INFO - 21/05/17 01:37:10 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 4587 bytes result sent to driver
[2021-05-16 22:37:10,430] {docker.py:276} INFO - 21/05/17 01:37:10 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (5deb0a1bddc0, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:10,431] {docker.py:276} INFO - 21/05/17 01:37:10 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
21/05/17 01:37:10 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 3255 ms on 5deb0a1bddc0 (executor driver) (189/200)
[2021-05-16 22:37:10,439] {docker.py:276} INFO - 21/05/17 01:37:10 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:10,441] {docker.py:276} INFO - 21/05/17 01:37:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 22:37:10,441] {docker.py:276} INFO - 21/05/17 01:37:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517013445589355699306966443_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445589355699306966443_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517013445589355699306966443_0004}; taskId=attempt_20210517013445589355699306966443_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ce72b22}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:10 INFO StagingCommitter: Starting: Task committer attempt_20210517013445589355699306966443_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445589355699306966443_0004_m_000192_480
[2021-05-16 22:37:10,443] {docker.py:276} INFO - 21/05/17 01:37:10 INFO StagingCommitter: Task committer attempt_20210517013445589355699306966443_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_20210517013445589355699306966443_0004_m_000192_480 : duration 0:00.002s
[2021-05-16 22:37:11,749] {docker.py:276} INFO - 21/05/17 01:37:11 INFO StagingCommitter: Starting: Task committer attempt_202105170134453741020434938449759_0004_m_000190_478: needsTaskCommit() Task attempt_202105170134453741020434938449759_0004_m_000190_478
[2021-05-16 22:37:11,750] {docker.py:276} INFO - 21/05/17 01:37:11 INFO StagingCommitter: Task committer attempt_202105170134453741020434938449759_0004_m_000190_478: needsTaskCommit() Task attempt_202105170134453741020434938449759_0004_m_000190_478: duration 0:00.000s
21/05/17 01:37:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134453741020434938449759_0004_m_000190_478
[2021-05-16 22:37:11,751] {docker.py:276} INFO - 21/05/17 01:37:11 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 4544 bytes result sent to driver
[2021-05-16 22:37:11,752] {docker.py:276} INFO - 21/05/17 01:37:11 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (5deb0a1bddc0, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:11,753] {docker.py:276} INFO - 21/05/17 01:37:11 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-16 22:37:11,754] {docker.py:276} INFO - 21/05/17 01:37:11 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 2408 ms on 5deb0a1bddc0 (executor driver) (190/200)
[2021-05-16 22:37:11,764] {docker.py:276} INFO - 21/05/17 01:37:11 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:11,765] {docker.py:276} INFO - 21/05/17 01:37:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:11,766] {docker.py:276} INFO - 21/05/17 01:37:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457592800773305271042_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457592800773305271042_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457592800773305271042_0004}; taskId=attempt_202105170134457592800773305271042_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33e56c30}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:11 INFO StagingCommitter: Starting: Task committer attempt_202105170134457592800773305271042_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457592800773305271042_0004_m_000193_481
[2021-05-16 22:37:11,769] {docker.py:276} INFO - 21/05/17 01:37:11 INFO StagingCommitter: Task committer attempt_202105170134457592800773305271042_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457592800773305271042_0004_m_000193_481 : duration 0:00.003s
[2021-05-16 22:37:12,253] {docker.py:276} INFO - 21/05/17 01:37:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134455552777756724531729_0004_m_000189_477: needsTaskCommit() Task attempt_202105170134455552777756724531729_0004_m_000189_477
[2021-05-16 22:37:12,254] {docker.py:276} INFO - 21/05/17 01:37:12 INFO StagingCommitter: Task committer attempt_202105170134455552777756724531729_0004_m_000189_477: needsTaskCommit() Task attempt_202105170134455552777756724531729_0004_m_000189_477: duration 0:00.001s
21/05/17 01:37:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134455552777756724531729_0004_m_000189_477
[2021-05-16 22:37:12,256] {docker.py:276} INFO - 21/05/17 01:37:12 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 4587 bytes result sent to driver
[2021-05-16 22:37:12,257] {docker.py:276} INFO - 21/05/17 01:37:12 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (5deb0a1bddc0, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:12,258] {docker.py:276} INFO - 21/05/17 01:37:12 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-16 22:37:12,260] {docker.py:276} INFO - 21/05/17 01:37:12 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 3078 ms on 5deb0a1bddc0 (executor driver) (191/200)
[2021-05-16 22:37:12,269] {docker.py:276} INFO - 21/05/17 01:37:12 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:12,271] {docker.py:276} INFO - 21/05/17 01:37:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:12,271] {docker.py:276} INFO - 21/05/17 01:37:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134454873505850653097938_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454873505850653097938_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134454873505850653097938_0004}; taskId=attempt_202105170134454873505850653097938_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@167f4788}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134454873505850653097938_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454873505850653097938_0004_m_000194_482
[2021-05-16 22:37:12,275] {docker.py:276} INFO - 21/05/17 01:37:12 INFO StagingCommitter: Task committer attempt_202105170134454873505850653097938_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134454873505850653097938_0004_m_000194_482 : duration 0:00.003s
[2021-05-16 22:37:12,383] {docker.py:276} INFO - 21/05/17 01:37:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134457728460673044119585_0004_m_000191_479: needsTaskCommit() Task attempt_202105170134457728460673044119585_0004_m_000191_479
[2021-05-16 22:37:12,384] {docker.py:276} INFO - 21/05/17 01:37:12 INFO StagingCommitter: Task committer attempt_202105170134457728460673044119585_0004_m_000191_479: needsTaskCommit() Task attempt_202105170134457728460673044119585_0004_m_000191_479: duration 0:00.000s
[2021-05-16 22:37:12,385] {docker.py:276} INFO - 21/05/17 01:37:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457728460673044119585_0004_m_000191_479
[2021-05-16 22:37:12,386] {docker.py:276} INFO - 21/05/17 01:37:12 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 4544 bytes result sent to driver
[2021-05-16 22:37:12,387] {docker.py:276} INFO - 21/05/17 01:37:12 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (5deb0a1bddc0, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:12,388] {docker.py:276} INFO - 21/05/17 01:37:12 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 2325 ms on 5deb0a1bddc0 (executor driver) (192/200)
21/05/17 01:37:12 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-16 22:37:12,397] {docker.py:276} INFO - 21/05/17 01:37:12 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:12,398] {docker.py:276} INFO - 21/05/17 01:37:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:12,399] {docker.py:276} INFO - 21/05/17 01:37:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134457285119019648239012_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457285119019648239012_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134457285119019648239012_0004}; taskId=attempt_202105170134457285119019648239012_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33deab6d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:12,399] {docker.py:276} INFO - 21/05/17 01:37:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:12 INFO StagingCommitter: Starting: Task committer attempt_202105170134457285119019648239012_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457285119019648239012_0004_m_000195_483
[2021-05-16 22:37:12,402] {docker.py:276} INFO - 21/05/17 01:37:12 INFO StagingCommitter: Task committer attempt_202105170134457285119019648239012_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134457285119019648239012_0004_m_000195_483 : duration 0:00.003s
[2021-05-16 22:37:13,253] {docker.py:276} INFO - 21/05/17 01:37:13 INFO StagingCommitter: Starting: Task committer attempt_20210517013445589355699306966443_0004_m_000192_480: needsTaskCommit() Task attempt_20210517013445589355699306966443_0004_m_000192_480
[2021-05-16 22:37:13,254] {docker.py:276} INFO - 21/05/17 01:37:13 INFO StagingCommitter: Task committer attempt_20210517013445589355699306966443_0004_m_000192_480: needsTaskCommit() Task attempt_20210517013445589355699306966443_0004_m_000192_480: duration 0:00.001s
21/05/17 01:37:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517013445589355699306966443_0004_m_000192_480
[2021-05-16 22:37:13,258] {docker.py:276} INFO - 21/05/17 01:37:13 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 4544 bytes result sent to driver
[2021-05-16 22:37:13,259] {docker.py:276} INFO - 21/05/17 01:37:13 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (5deb0a1bddc0, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:13,260] {docker.py:276} INFO - 21/05/17 01:37:13 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 2834 ms on 5deb0a1bddc0 (executor driver) (193/200)
[2021-05-16 22:37:13,262] {docker.py:276} INFO - 21/05/17 01:37:13 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-16 22:37:13,272] {docker.py:276} INFO - 21/05/17 01:37:13 INFO ShuffleBlockFetcherIterator: Getting 5 (20.8 KiB) non-empty blocks including 5 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:13,274] {docker.py:276} INFO - 21/05/17 01:37:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:13,274] {docker.py:276} INFO - 21/05/17 01:37:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134458689130696263598441_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458689130696263598441_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134458689130696263598441_0004}; taskId=attempt_202105170134458689130696263598441_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23ffef94}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:13 INFO StagingCommitter: Starting: Task committer attempt_202105170134458689130696263598441_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458689130696263598441_0004_m_000196_484
[2021-05-16 22:37:13,277] {docker.py:276} INFO - 21/05/17 01:37:13 INFO StagingCommitter: Task committer attempt_202105170134458689130696263598441_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134458689130696263598441_0004_m_000196_484 : duration 0:00.003s
[2021-05-16 22:37:14,336] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Starting: Task committer attempt_202105170134457592800773305271042_0004_m_000193_481: needsTaskCommit() Task attempt_202105170134457592800773305271042_0004_m_000193_481
[2021-05-16 22:37:14,338] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Task committer attempt_202105170134457592800773305271042_0004_m_000193_481: needsTaskCommit() Task attempt_202105170134457592800773305271042_0004_m_000193_481: duration 0:00.001s
21/05/17 01:37:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457592800773305271042_0004_m_000193_481
[2021-05-16 22:37:14,340] {docker.py:276} INFO - 21/05/17 01:37:14 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 4544 bytes result sent to driver
[2021-05-16 22:37:14,342] {docker.py:276} INFO - 21/05/17 01:37:14 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (5deb0a1bddc0, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:14,343] {docker.py:276} INFO - 21/05/17 01:37:14 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 2593 ms on 5deb0a1bddc0 (executor driver) (194/200)
[2021-05-16 22:37:14,345] {docker.py:276} INFO - 21/05/17 01:37:14 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-16 22:37:14,355] {docker.py:276} INFO - 21/05/17 01:37:14 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:14,357] {docker.py:276} INFO - 21/05/17 01:37:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134459061976542353406511_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459061976542353406511_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134459061976542353406511_0004}; taskId=attempt_202105170134459061976542353406511_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a7f81c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:14,357] {docker.py:276} INFO - 21/05/17 01:37:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:14 INFO StagingCommitter: Starting: Task committer attempt_202105170134459061976542353406511_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459061976542353406511_0004_m_000197_485
[2021-05-16 22:37:14,360] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Task committer attempt_202105170134459061976542353406511_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134459061976542353406511_0004_m_000197_485 : duration 0:00.003s
[2021-05-16 22:37:14,838] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Starting: Task committer attempt_202105170134454873505850653097938_0004_m_000194_482: needsTaskCommit() Task attempt_202105170134454873505850653097938_0004_m_000194_482
[2021-05-16 22:37:14,839] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Task committer attempt_202105170134454873505850653097938_0004_m_000194_482: needsTaskCommit() Task attempt_202105170134454873505850653097938_0004_m_000194_482: duration 0:00.000s
21/05/17 01:37:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134454873505850653097938_0004_m_000194_482
[2021-05-16 22:37:14,840] {docker.py:276} INFO - 21/05/17 01:37:14 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 4544 bytes result sent to driver
[2021-05-16 22:37:14,843] {docker.py:276} INFO - 21/05/17 01:37:14 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (5deb0a1bddc0, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:14,843] {docker.py:276} INFO - 21/05/17 01:37:14 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 2590 ms on 5deb0a1bddc0 (executor driver) (195/200)
[2021-05-16 22:37:14,844] {docker.py:276} INFO - 21/05/17 01:37:14 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-16 22:37:14,852] {docker.py:276} INFO - 21/05/17 01:37:14 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:14,854] {docker.py:276} INFO - 21/05/17 01:37:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452751155017400500442_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452751155017400500442_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452751155017400500442_0004}; taskId=attempt_202105170134452751155017400500442_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ce3f623}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 01:37:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 22:37:14,854] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Starting: Task committer attempt_202105170134452751155017400500442_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452751155017400500442_0004_m_000198_486
[2021-05-16 22:37:14,856] {docker.py:276} INFO - 21/05/17 01:37:14 INFO StagingCommitter: Task committer attempt_202105170134452751155017400500442_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452751155017400500442_0004_m_000198_486 : duration 0:00.002s
[2021-05-16 22:37:15,164] {docker.py:276} INFO - 21/05/17 01:37:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134457285119019648239012_0004_m_000195_483: needsTaskCommit() Task attempt_202105170134457285119019648239012_0004_m_000195_483
[2021-05-16 22:37:15,165] {docker.py:276} INFO - 21/05/17 01:37:15 INFO StagingCommitter: Task committer attempt_202105170134457285119019648239012_0004_m_000195_483: needsTaskCommit() Task attempt_202105170134457285119019648239012_0004_m_000195_483: duration 0:00.001s
[2021-05-16 22:37:15,166] {docker.py:276} INFO - 21/05/17 01:37:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134457285119019648239012_0004_m_000195_483
[2021-05-16 22:37:15,168] {docker.py:276} INFO - 21/05/17 01:37:15 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 4544 bytes result sent to driver
[2021-05-16 22:37:15,169] {docker.py:276} INFO - 21/05/17 01:37:15 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (5deb0a1bddc0, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 22:37:15,170] {docker.py:276} INFO - 21/05/17 01:37:15 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-16 22:37:15,171] {docker.py:276} INFO - 21/05/17 01:37:15 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 2787 ms on 5deb0a1bddc0 (executor driver) (196/200)
[2021-05-16 22:37:15,190] {docker.py:276} INFO - 21/05/17 01:37:15 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 01:37:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 22:37:15,192] {docker.py:276} INFO - 21/05/17 01:37:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 01:37:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 01:37:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 01:37:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170134452768136888128945348_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452768136888128945348_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170134452768136888128945348_0004}; taskId=attempt_202105170134452768136888128945348_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bb4f60}; outputPath=file:/home/jovyan/tmp/staging/jovyan/ed19161d-e433-4d47-bc5b-76a48d6c4da5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:15,193] {docker.py:276} INFO - 21/05/17 01:37:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 01:37:15 INFO StagingCommitter: Starting: Task committer attempt_202105170134452768136888128945348_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452768136888128945348_0004_m_000199_487
[2021-05-16 22:37:15,196] {docker.py:276} INFO - 21/05/17 01:37:15 INFO StagingCommitter: Task committer attempt_202105170134452768136888128945348_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/ed19161d-e433-4d47-bc5b-76a48d6c4da5/_temporary/0/_temporary/attempt_202105170134452768136888128945348_0004_m_000199_487 : duration 0:00.003s
[2021-05-16 22:37:16,026] {docker.py:276} INFO - 21/05/17 01:37:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134458689130696263598441_0004_m_000196_484: needsTaskCommit() Task attempt_202105170134458689130696263598441_0004_m_000196_484
[2021-05-16 22:37:16,027] {docker.py:276} INFO - 21/05/17 01:37:16 INFO StagingCommitter: Task committer attempt_202105170134458689130696263598441_0004_m_000196_484: needsTaskCommit() Task attempt_202105170134458689130696263598441_0004_m_000196_484: duration 0:00.000s
21/05/17 01:37:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134458689130696263598441_0004_m_000196_484
[2021-05-16 22:37:16,029] {docker.py:276} INFO - 21/05/17 01:37:16 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 4587 bytes result sent to driver
[2021-05-16 22:37:16,031] {docker.py:276} INFO - 21/05/17 01:37:16 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 2774 ms on 5deb0a1bddc0 (executor driver) (197/200)
[2021-05-16 22:37:16,673] {docker.py:276} INFO - 21/05/17 01:37:16 INFO StagingCommitter: Starting: Task committer attempt_202105170134459061976542353406511_0004_m_000197_485: needsTaskCommit() Task attempt_202105170134459061976542353406511_0004_m_000197_485
21/05/17 01:37:16 INFO StagingCommitter: Task committer attempt_202105170134459061976542353406511_0004_m_000197_485: needsTaskCommit() Task attempt_202105170134459061976542353406511_0004_m_000197_485: duration 0:00.001s
21/05/17 01:37:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134459061976542353406511_0004_m_000197_485
[2021-05-16 22:37:16,674] {docker.py:276} INFO - 21/05/17 01:37:16 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 4587 bytes result sent to driver
[2021-05-16 22:37:16,676] {docker.py:276} INFO - 21/05/17 01:37:16 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 2337 ms on 5deb0a1bddc0 (executor driver) (198/200)
[2021-05-16 22:37:17,147] {docker.py:276} INFO - 21/05/17 01:37:17 INFO StagingCommitter: Starting: Task committer attempt_202105170134452751155017400500442_0004_m_000198_486: needsTaskCommit() Task attempt_202105170134452751155017400500442_0004_m_000198_486
[2021-05-16 22:37:17,148] {docker.py:276} INFO - 21/05/17 01:37:17 INFO StagingCommitter: Task committer attempt_202105170134452751155017400500442_0004_m_000198_486: needsTaskCommit() Task attempt_202105170134452751155017400500442_0004_m_000198_486: duration 0:00.001s
21/05/17 01:37:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452751155017400500442_0004_m_000198_486
[2021-05-16 22:37:17,150] {docker.py:276} INFO - 21/05/17 01:37:17 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 4587 bytes result sent to driver
[2021-05-16 22:37:17,151] {docker.py:276} INFO - 21/05/17 01:37:17 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 2312 ms on 5deb0a1bddc0 (executor driver) (199/200)
[2021-05-16 22:37:17,251] {docker.py:276} INFO - 21/05/17 01:37:17 INFO StagingCommitter: Starting: Task committer attempt_202105170134452768136888128945348_0004_m_000199_487: needsTaskCommit() Task attempt_202105170134452768136888128945348_0004_m_000199_487
21/05/17 01:37:17 INFO StagingCommitter: Task committer attempt_202105170134452768136888128945348_0004_m_000199_487: needsTaskCommit() Task attempt_202105170134452768136888128945348_0004_m_000199_487: duration 0:00.000s
21/05/17 01:37:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170134452768136888128945348_0004_m_000199_487
[2021-05-16 22:37:17,253] {docker.py:276} INFO - 21/05/17 01:37:17 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 4587 bytes result sent to driver
21/05/17 01:37:17 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 2085 ms on 5deb0a1bddc0 (executor driver) (200/200)
21/05/17 01:37:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-16 22:37:17,253] {docker.py:276} INFO - 21/05/17 01:37:17 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 134.129 s
[2021-05-16 22:37:17,254] {docker.py:276} INFO - 21/05/17 01:37:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 22:37:17,255] {docker.py:276} INFO - 21/05/17 01:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-16 22:37:17,255] {docker.py:276} INFO - 21/05/17 01:37:17 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 151.898691 s
[2021-05-16 22:37:17,257] {docker.py:276} INFO - 21/05/17 01:37:17 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105170134448855989266649993838_0000_m_000000_0: commitJob((no job ID))
[2021-05-16 22:37:17,275] {docker.py:276} INFO - 21/05/17 01:37:17 WARN AbstractS3ACommitter: Task committer attempt_202105170134448855989266649993838_0000_m_000000_0: No pending uploads to commit
[2021-05-16 22:37:17,800] {docker.py:276} INFO - 21/05/17 01:37:17 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/17 01:37:17 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-16 22:37:17,979] {docker.py:276} INFO - 21/05/17 01:37:18 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.179s
21/05/17 01:37:18 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.181s
[2021-05-16 22:37:17,981] {docker.py:276} INFO - 21/05/17 01:37:18 INFO AbstractS3ACommitter: Task committer attempt_202105170134448855989266649993838_0000_m_000000_0: commitJob((no job ID)): duration 0:00.724s
[2021-05-16 22:37:18,501] {docker.py:276} INFO - 21/05/17 01:37:18 INFO FileFormatWriter: Write Job ed19161d-e433-4d47-bc5b-76a48d6c4da5 committed.
[2021-05-16 22:37:18,510] {docker.py:276} INFO - 21/05/17 01:37:18 INFO FileFormatWriter: Finished processing stats for write job ed19161d-e433-4d47-bc5b-76a48d6c4da5.
[2021-05-16 22:37:18,621] {docker.py:276} INFO - 21/05/17 01:37:18 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-16 22:37:18,636] {docker.py:276} INFO - 21/05/17 01:37:18 INFO SparkUI: Stopped Spark web UI at http://5deb0a1bddc0:4040
[2021-05-16 22:37:18,658] {docker.py:276} INFO - 21/05/17 01:37:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-16 22:37:18,674] {docker.py:276} INFO - 21/05/17 01:37:18 INFO MemoryStore: MemoryStore cleared
[2021-05-16 22:37:18,675] {docker.py:276} INFO - 21/05/17 01:37:18 INFO BlockManager: BlockManager stopped
[2021-05-16 22:37:18,679] {docker.py:276} INFO - 21/05/17 01:37:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-16 22:37:18,684] {docker.py:276} INFO - 21/05/17 01:37:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-16 22:37:18,692] {docker.py:276} INFO - 21/05/17 01:37:18 INFO SparkContext: Successfully stopped SparkContext
[2021-05-16 22:37:18,692] {docker.py:276} INFO - 21/05/17 01:37:18 INFO ShutdownHookManager: Shutdown hook called
[2021-05-16 22:37:18,694] {docker.py:276} INFO - 21/05/17 01:37:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b63c2c5-a3f8-4734-b537-81d6b87c14dc
[2021-05-16 22:37:18,696] {docker.py:276} INFO - 21/05/17 01:37:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5e5b2a7-4e07-4299-908d-e37a55945ed4
[2021-05-16 22:37:18,698] {docker.py:276} INFO - 21/05/17 01:37:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5e5b2a7-4e07-4299-908d-e37a55945ed4/pyspark-497f34f4-bea8-4164-abd4-d7fe1d3d70de
[2021-05-16 22:37:18,704] {docker.py:276} INFO - 21/05/17 01:37:18 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-16 22:37:18,705] {docker.py:276} INFO - 21/05/17 01:37:18 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-16 22:37:18,706] {docker.py:276} INFO - 21/05/17 01:37:18 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-16 22:37:18,949] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210517T013319, start_date=20210517T013406, end_date=20210517T013718
[2021-05-16 22:37:18,998] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-16 22:37:19,007] {local_task_job.py:146} INFO - Task exited with return code 0
