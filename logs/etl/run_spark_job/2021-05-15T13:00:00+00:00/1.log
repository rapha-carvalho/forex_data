[2021-05-15 11:00:22,351] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T13:00:00+00:00 [queued]>
[2021-05-15 11:00:22,355] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T13:00:00+00:00 [queued]>
[2021-05-15 11:00:22,356] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 11:00:22,356] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-15 11:00:22,356] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 11:00:22,361] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-15T13:00:00+00:00
[2021-05-15 11:00:22,364] {standard_task_runner.py:52} INFO - Started process 27938 to run task
[2021-05-15 11:00:22,370] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-15T13:00:00+00:00', '--job-id', '699', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp4mutcpst', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpe4fknd72']
[2021-05-15 11:00:22,372] {standard_task_runner.py:77} INFO - Job 699: Subtask run_spark_job
[2021-05-15 11:00:22,405] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-15T13:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-15 11:00:22,436] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-15T13:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-15T13:00:00+00:00
[2021-05-15 11:00:22,439] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-15 11:00:25,368] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-15 11:00:25,370] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-15 11:00:25,370] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-15 11:00:25,374] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-15 11:00:27,369] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-15 11:00:28,067] {docker.py:276} INFO - 21/05/15 14:00:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-15 11:00:30,814] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-15 11:00:30,828] {docker.py:276} INFO - 21/05/15 14:00:30 INFO SparkContext: Running Spark version 3.1.1
[2021-05-15 11:00:30,912] {docker.py:276} INFO - 21/05/15 14:00:30 INFO ResourceUtils: ==============================================================
[2021-05-15 11:00:30,913] {docker.py:276} INFO - 21/05/15 14:00:30 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-15 11:00:30,914] {docker.py:276} INFO - 21/05/15 14:00:30 INFO ResourceUtils: ==============================================================
[2021-05-15 11:00:30,916] {docker.py:276} INFO - 21/05/15 14:00:30 INFO SparkContext: Submitted application: spark.py
[2021-05-15 11:00:30,960] {docker.py:276} INFO - 21/05/15 14:00:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-15 11:00:30,982] {docker.py:276} INFO - 21/05/15 14:00:30 INFO ResourceProfile: Limiting resource is cpu
[2021-05-15 11:00:30,984] {docker.py:276} INFO - 21/05/15 14:00:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-15 11:00:31,068] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-15 11:00:31,068] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-15 11:00:31,069] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SecurityManager: Changing view acls groups to:
[2021-05-15 11:00:31,070] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SecurityManager: Changing modify acls groups to:
[2021-05-15 11:00:31,070] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-15 11:00:31,511] {docker.py:276} INFO - 21/05/15 14:00:31 INFO Utils: Successfully started service 'sparkDriver' on port 44737.
[2021-05-15 11:00:31,575] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SparkEnv: Registering MapOutputTracker
[2021-05-15 11:00:31,658] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-15 11:00:31,703] {docker.py:276} INFO - 21/05/15 14:00:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-15 11:00:31,705] {docker.py:276} INFO - 21/05/15 14:00:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-15 11:00:31,712] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-15 11:00:31,732] {docker.py:276} INFO - 21/05/15 14:00:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-de4e2d67-27e2-4352-9958-6469fbacdc74
[2021-05-15 11:00:31,764] {docker.py:276} INFO - 21/05/15 14:00:31 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-15 11:00:31,790] {docker.py:276} INFO - 21/05/15 14:00:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-15 11:00:32,119] {docker.py:276} INFO - 21/05/15 14:00:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-15 11:00:32,224] {docker.py:276} INFO - 21/05/15 14:00:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://8951b5f85146:4040
[2021-05-15 11:00:32,546] {docker.py:276} INFO - 21/05/15 14:00:32 INFO Executor: Starting executor ID driver on host 8951b5f85146
[2021-05-15 11:00:32,595] {docker.py:276} INFO - 21/05/15 14:00:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41629.
[2021-05-15 11:00:32,596] {docker.py:276} INFO - 21/05/15 14:00:32 INFO NettyBlockTransferService: Server created on 8951b5f85146:41629
[2021-05-15 11:00:32,599] {docker.py:276} INFO - 21/05/15 14:00:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-15 11:00:32,611] {docker.py:276} INFO - 21/05/15 14:00:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8951b5f85146, 41629, None)
[2021-05-15 11:00:32,620] {docker.py:276} INFO - 21/05/15 14:00:32 INFO BlockManagerMasterEndpoint: Registering block manager 8951b5f85146:41629 with 934.4 MiB RAM, BlockManagerId(driver, 8951b5f85146, 41629, None)
[2021-05-15 11:00:32,624] {docker.py:276} INFO - 21/05/15 14:00:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8951b5f85146, 41629, None)
[2021-05-15 11:00:32,626] {docker.py:276} INFO - 21/05/15 14:00:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8951b5f85146, 41629, None)
[2021-05-15 11:00:33,239] {docker.py:276} INFO - 21/05/15 14:00:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-15 11:00:33,239] {docker.py:276} INFO - 21/05/15 14:00:33 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-15 11:00:34,385] {docker.py:276} INFO - 21/05/15 14:00:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-15 11:00:34,470] {docker.py:276} INFO - 21/05/15 14:00:34 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-15 11:00:34,471] {docker.py:276} INFO - 21/05/15 14:00:34 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-15 11:00:41,749] {docker.py:276} INFO - 21/05/15 14:00:41 INFO InMemoryFileIndex: It took 1088 ms to list leaf files for 6 paths.
[2021-05-15 11:00:42,904] {docker.py:276} INFO - 21/05/15 14:00:42 INFO InMemoryFileIndex: It took 1053 ms to list leaf files for 6 paths.
[2021-05-15 11:00:45,722] {docker.py:276} INFO - 21/05/15 14:00:45 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 11:00:45,727] {docker.py:276} INFO - 21/05/15 14:00:45 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-15 11:00:45,733] {docker.py:276} INFO - 21/05/15 14:00:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 11:00:46,518] {docker.py:276} INFO - 21/05/15 14:00:46 INFO CodeGenerator: Code generated in 251.0509 ms
[2021-05-15 11:00:46,600] {docker.py:276} INFO - 21/05/15 14:00:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-15 11:00:46,689] {docker.py:276} INFO - 21/05/15 14:00:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-15 11:00:46,693] {docker.py:276} INFO - 21/05/15 14:00:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8951b5f85146:41629 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 11:00:46,698] {docker.py:276} INFO - 21/05/15 14:00:46 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:00:46,725] {docker.py:276} INFO - 21/05/15 14:00:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6451817 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 11:00:46,891] {docker.py:276} INFO - 21/05/15 14:00:46 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:00:46,917] {docker.py:276} INFO - 21/05/15 14:00:46 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-15 11:00:46,918] {docker.py:276} INFO - 21/05/15 14:00:46 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 11:00:46,919] {docker.py:276} INFO - 21/05/15 14:00:46 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 11:00:46,921] {docker.py:276} INFO - 21/05/15 14:00:46 INFO DAGScheduler: Missing parents: List()
[2021-05-15 11:00:46,929] {docker.py:276} INFO - 21/05/15 14:00:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:00:47,020] {docker.py:276} INFO - 21/05/15 14:00:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-15 11:00:47,033] {docker.py:276} INFO - 21/05/15 14:00:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-15 11:00:47,034] {docker.py:276} INFO - 21/05/15 14:00:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8951b5f85146:41629 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 11:00:47,036] {docker.py:276} INFO - 21/05/15 14:00:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:00:47,052] {docker.py:276} INFO - 21/05/15 14:00:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-15 11:00:47,053] {docker.py:276} INFO - 21/05/15 14:00:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-05-15 11:00:47,132] {docker.py:276} INFO - 21/05/15 14:00:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8951b5f85146, executor driver, partition 0, PROCESS_LOCAL, 5015 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:47,158] {docker.py:276} INFO - 21/05/15 14:00:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-15 11:00:47,326] {docker.py:276} INFO - 21/05/15 14:00:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_00_14/from_1621081814_to_1621083614.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:00:47,353] {docker.py:276} INFO - 21/05/15 14:00:47 INFO CodeGenerator: Code generated in 17.9883 ms
[2021-05-15 11:00:47,755] {docker.py:276} INFO - 21/05/15 14:00:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1564 bytes result sent to driver
[2021-05-15 11:00:47,767] {docker.py:276} INFO - 21/05/15 14:00:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 648 ms on 8951b5f85146 (executor driver) (1/1)
[2021-05-15 11:00:47,771] {docker.py:276} INFO - 21/05/15 14:00:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-15 11:00:47,779] {docker.py:276} INFO - 21/05/15 14:00:47 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.832 s
[2021-05-15 11:00:47,785] {docker.py:276} INFO - 21/05/15 14:00:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 11:00:47,786] {docker.py:276} INFO - 21/05/15 14:00:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-15 11:00:47,789] {docker.py:276} INFO - 21/05/15 14:00:47 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.898501 s
[2021-05-15 11:00:47,821] {docker.py:276} INFO - 21/05/15 14:00:47 INFO CodeGenerator: Code generated in 15.1631 ms
[2021-05-15 11:00:47,889] {docker.py:276} INFO - 21/05/15 14:00:47 INFO FileSourceStrategy: Pushed Filters: 
21/05/15 14:00:47 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 11:00:47,890] {docker.py:276} INFO - 21/05/15 14:00:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 11:00:47,898] {docker.py:276} INFO - 21/05/15 14:00:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-15 11:00:47,941] {docker.py:276} INFO - 21/05/15 14:00:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-15 11:00:47,948] {docker.py:276} INFO - 21/05/15 14:00:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8951b5f85146:41629 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 11:00:47,949] {docker.py:276} INFO - 21/05/15 14:00:47 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:00:47,951] {docker.py:276} INFO - 21/05/15 14:00:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6451817 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 11:00:47,968] {docker.py:276} INFO - 21/05/15 14:00:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8951b5f85146:41629 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-15 11:00:48,590] {docker.py:276} INFO - 21/05/15 14:00:48 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 11:00:48,591] {docker.py:276} INFO - 21/05/15 14:00:48 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 11:00:48,592] {docker.py:276} INFO - 21/05/15 14:00:48 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-15 11:00:49,710] {docker.py:276} INFO - 21/05/15 14:00:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:49,713] {docker.py:276} INFO - 21/05/15 14:00:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:49,714] {docker.py:276} INFO - 21/05/15 14:00:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400494724970523776951531_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400494724970523776951531_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400494724970523776951531_0000}; taskId=attempt_202105151400494724970523776951531_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@409f54d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:49,715] {docker.py:276} INFO - 21/05/15 14:00:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:49,742] {docker.py:276} INFO - 21/05/15 14:00:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 11:00:49,835] {docker.py:276} INFO - 21/05/15 14:00:49 INFO CodeGenerator: Code generated in 59.0315 ms
[2021-05-15 11:00:49,837] {docker.py:276} INFO - 21/05/15 14:00:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 11:00:49,883] {docker.py:276} INFO - 21/05/15 14:00:49 INFO CodeGenerator: Code generated in 37.3433 ms
[2021-05-15 11:00:49,888] {docker.py:276} INFO - 21/05/15 14:00:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-15 11:00:49,919] {docker.py:276} INFO - 21/05/15 14:00:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-15 11:00:49,920] {docker.py:276} INFO - 21/05/15 14:00:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8951b5f85146:41629 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 11:00:49,922] {docker.py:276} INFO - 21/05/15 14:00:49 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:00:49,927] {docker.py:276} INFO - 21/05/15 14:00:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6451817 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 11:00:49,967] {docker.py:276} INFO - 21/05/15 14:00:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8951b5f85146:41629 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 11:00:50,044] {docker.py:276} INFO - 21/05/15 14:00:50 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:00:50,049] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Registering RDD 13 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-15 11:00:50,052] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2021-05-15 11:00:50,052] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 11:00:50,053] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2021-05-15 11:00:50,056] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
[2021-05-15 11:00:50,063] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:00:50,081] {docker.py:276} INFO - 21/05/15 14:00:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-15 11:00:50,097] {docker.py:276} INFO - 21/05/15 14:00:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-15 11:00:50,098] {docker.py:276} INFO - 21/05/15 14:00:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8951b5f85146:41629 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-15 11:00:50,099] {docker.py:276} INFO - 21/05/15 14:00:50 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:00:50,102] {docker.py:276} INFO - 21/05/15 14:00:50 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
[2021-05-15 11:00:50,103] {docker.py:276} INFO - 21/05/15 14:00:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0
[2021-05-15 11:00:50,105] {docker.py:276} INFO - 21/05/15 14:00:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8951b5f85146, executor driver, partition 0, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:50,107] {docker.py:276} INFO - 21/05/15 14:00:50 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (8951b5f85146, executor driver, partition 1, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:50,110] {docker.py:276} INFO - 21/05/15 14:00:50 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (8951b5f85146, executor driver, partition 2, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:50,112] {docker.py:276} INFO - 21/05/15 14:00:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2021-05-15 11:00:50,121] {docker.py:276} INFO - 21/05/15 14:00:50 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2021-05-15 11:00:50,126] {docker.py:276} INFO - 21/05/15 14:00:50 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[2021-05-15 11:00:50,186] {docker.py:276} INFO - 21/05/15 14:00:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8951b5f85146:41629 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 11:00:50,246] {docker.py:276} INFO - 21/05/15 14:00:50 INFO CodeGenerator: Code generated in 36.3936 ms
[2021-05-15 11:00:50,278] {docker.py:276} INFO - 21/05/15 14:00:50 INFO CodeGenerator: Code generated in 13.9324 ms
[2021-05-15 11:00:50,304] {docker.py:276} INFO - 21/05/15 14:00:50 INFO CodeGenerator: Code generated in 17.6891 ms
[2021-05-15 11:00:50,326] {docker.py:276} INFO - 21/05/15 14:00:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_00_14/from_1621081814_to_1621083614.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:00:50,327] {docker.py:276} INFO - 21/05/15 14:00:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_00_14/from_1621081814_to_1621083614.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:00:50,328] {docker.py:276} INFO - 21/05/15 14:00:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_00_14/from_1621081814_to_1621083614.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:00:51,144] {docker.py:276} INFO - 21/05/15 14:00:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_00_14/from_1621083614_to_1621085414.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:00:51,636] {docker.py:276} INFO - 21/05/15 14:00:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_00_14/from_1621083614_to_1621085414.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:00:51,668] {docker.py:276} INFO - 21/05/15 14:00:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_00_14/from_1621083614_to_1621085414.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:00:51,818] {docker.py:276} INFO - 21/05/15 14:00:51 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2722 bytes result sent to driver
[2021-05-15 11:00:51,823] {docker.py:276} INFO - 21/05/15 14:00:51 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1718 ms on 8951b5f85146 (executor driver) (1/3)
[2021-05-15 11:00:52,106] {docker.py:276} INFO - 21/05/15 14:00:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2679 bytes result sent to driver
[2021-05-15 11:00:52,116] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1978 ms on 8951b5f85146 (executor driver) (2/3)
[2021-05-15 11:00:52,253] {docker.py:276} INFO - 21/05/15 14:00:52 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2679 bytes result sent to driver
[2021-05-15 11:00:52,256] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2113 ms on 8951b5f85146 (executor driver) (3/3)
[2021-05-15 11:00:52,257] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-15 11:00:52,258] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.155 s
[2021-05-15 11:00:52,259] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: looking for newly runnable stages
[2021-05-15 11:00:52,260] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: running: Set()
[2021-05-15 11:00:52,261] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2021-05-15 11:00:52,262] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: failed: Set()
[2021-05-15 11:00:52,268] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:00:52,321] {docker.py:276} INFO - 21/05/15 14:00:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-15 11:00:52,330] {docker.py:276} INFO - 21/05/15 14:00:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-15 11:00:52,331] {docker.py:276} INFO - 21/05/15 14:00:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8951b5f85146:41629 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-15 11:00:52,332] {docker.py:276} INFO - 21/05/15 14:00:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:00:52,334] {docker.py:276} INFO - 21/05/15 14:00:52 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 11:00:52,335] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2021-05-15 11:00:52,344] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (8951b5f85146, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:52,344] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (8951b5f85146, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:52,345] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 6) (8951b5f85146, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:52,346] {docker.py:276} INFO - 21/05/15 14:00:52 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 7) (8951b5f85146, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:52,347] {docker.py:276} INFO - 21/05/15 14:00:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)
[2021-05-15 11:00:52,347] {docker.py:276} INFO - 21/05/15 14:00:52 INFO Executor: Running task 2.0 in stage 2.0 (TID 6)
[2021-05-15 11:00:52,348] {docker.py:276} INFO - 21/05/15 14:00:52 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)
[2021-05-15 11:00:52,357] {docker.py:276} INFO - 21/05/15 14:00:52 INFO Executor: Running task 3.0 in stage 2.0 (TID 7)
[2021-05-15 11:00:52,478] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Getting 3 (1528.0 B) non-empty blocks including 3 (1528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:52,480] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:52,481] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Getting 3 (1178.0 B) non-empty blocks including 3 (1178.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:52,481] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[2021-05-15 11:00:52,481] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2021-05-15 11:00:52,482] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-15 11:00:52,483] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Getting 3 (1425.0 B) non-empty blocks including 3 (1425.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:52,483] {docker.py:276} INFO - 21/05/15 14:00:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-15 11:00:52,502] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:52,502] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:52,502] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,503] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503811801375761653982_0002_m_000000_4, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503811801375761653982_0002_m_000000_4}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503811801375761653982_0002}; taskId=attempt_202105151400503811801375761653982_0002_m_000000_4, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b5bd018}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,503] {docker.py:276} INFO - 21/05/15 14:00:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:52,506] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:52,508] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:52,508] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:52,509] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,509] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505273338735712650589_0002_m_000001_5, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505273338735712650589_0002_m_000001_5}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505273338735712650589_0002}; taskId=attempt_202105151400505273338735712650589_0002_m_000001_5, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@186f2f3c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,509] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Starting: Task committer attempt_202105151400503811801375761653982_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503811801375761653982_0002_m_000000_4
[2021-05-15 11:00:52,510] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,511] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507035401751133096581_0002_m_000003_7, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507035401751133096581_0002_m_000003_7}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507035401751133096581_0002}; taskId=attempt_202105151400507035401751133096581_0002_m_000003_7, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@750e7b8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,511] {docker.py:276} INFO - 21/05/15 14:00:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:52,511] {docker.py:276} INFO - 21/05/15 14:00:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:52,512] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Starting: Task committer attempt_202105151400507035401751133096581_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507035401751133096581_0002_m_000003_7
[2021-05-15 11:00:52,512] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Starting: Task committer attempt_202105151400505273338735712650589_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505273338735712650589_0002_m_000001_5
[2021-05-15 11:00:52,514] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:52,515] {docker.py:276} INFO - 21/05/15 14:00:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:52,518] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:52,520] {docker.py:276} INFO - 21/05/15 14:00:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508766082726899092738_0002_m_000002_6, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508766082726899092738_0002_m_000002_6}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508766082726899092738_0002}; taskId=attempt_202105151400508766082726899092738_0002_m_000002_6, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@426f60ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:00:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:52,520] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Starting: Task committer attempt_202105151400508766082726899092738_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508766082726899092738_0002_m_000002_6
[2021-05-15 11:00:52,537] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Task committer attempt_202105151400503811801375761653982_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503811801375761653982_0002_m_000000_4 : duration 0:00.026s
[2021-05-15 11:00:52,537] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Task committer attempt_202105151400505273338735712650589_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505273338735712650589_0002_m_000001_5 : duration 0:00.022s
[2021-05-15 11:00:52,550] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Task committer attempt_202105151400508766082726899092738_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508766082726899092738_0002_m_000002_6 : duration 0:00.030s
[2021-05-15 11:00:52,551] {docker.py:276} INFO - 21/05/15 14:00:52 INFO StagingCommitter: Task committer attempt_202105151400507035401751133096581_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507035401751133096581_0002_m_000003_7 : duration 0:00.039s
[2021-05-15 11:00:52,744] {docker.py:276} INFO - 21/05/15 14:00:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 8951b5f85146:41629 in memory (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-15 11:00:54,440] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400505273338735712650589_0002_m_000001_5: needsTaskCommit() Task attempt_202105151400505273338735712650589_0002_m_000001_5
[2021-05-15 11:00:54,441] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400503811801375761653982_0002_m_000000_4: needsTaskCommit() Task attempt_202105151400503811801375761653982_0002_m_000000_4
[2021-05-15 11:00:54,442] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400508766082726899092738_0002_m_000002_6: needsTaskCommit() Task attempt_202105151400508766082726899092738_0002_m_000002_6
[2021-05-15 11:00:54,443] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_202105151400505273338735712650589_0002_m_000001_5: needsTaskCommit() Task attempt_202105151400505273338735712650589_0002_m_000001_5: duration 0:00.001s
[2021-05-15 11:00:54,444] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_202105151400503811801375761653982_0002_m_000000_4: needsTaskCommit() Task attempt_202105151400503811801375761653982_0002_m_000000_4: duration 0:00.002s
[2021-05-15 11:00:54,444] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_202105151400508766082726899092738_0002_m_000002_6: needsTaskCommit() Task attempt_202105151400508766082726899092738_0002_m_000002_6: duration 0:00.000s
[2021-05-15 11:00:54,445] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503811801375761653982_0002_m_000000_4
[2021-05-15 11:00:54,446] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505273338735712650589_0002_m_000001_5
[2021-05-15 11:00:54,447] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508766082726899092738_0002_m_000002_6
[2021-05-15 11:00:54,461] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 4630 bytes result sent to driver
[2021-05-15 11:00:54,462] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 8) (8951b5f85146, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:54,464] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Running task 4.0 in stage 2.0 (TID 8)
21/05/15 14:00:54 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2122 ms on 8951b5f85146 (executor driver) (1/200)
[2021-05-15 11:00:54,468] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Finished task 2.0 in stage 2.0 (TID 6). 4630 bytes result sent to driver
[2021-05-15 11:00:54,470] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 9) (8951b5f85146, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:54,471] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 6) in 2129 ms on 8951b5f85146 (executor driver) (2/200)
[2021-05-15 11:00:54,471] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 4630 bytes result sent to driver
21/05/15 14:00:54 INFO Executor: Running task 5.0 in stage 2.0 (TID 9)
[2021-05-15 11:00:54,473] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 10) (8951b5f85146, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:54,473] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Running task 6.0 in stage 2.0 (TID 10)
[2021-05-15 11:00:54,474] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2135 ms on 8951b5f85146 (executor driver) (3/200)
[2021-05-15 11:00:54,504] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:54,505] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:54,505] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:54,506] {docker.py:276} INFO - 21/05/15 14:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:54,506] {docker.py:276} INFO - 21/05/15 14:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:54,506] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:54,507] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,507] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502148640024821268200_0002_m_000005_9, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502148640024821268200_0002_m_000005_9}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502148640024821268200_0002}; taskId=attempt_202105151400502148640024821268200_0002_m_000005_9, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42015645}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,507] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:54,508] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400502148640024821268200_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502148640024821268200_0002_m_000005_9
[2021-05-15 11:00:54,512] {docker.py:276} INFO - 21/05/15 14:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:54,513] {docker.py:276} INFO - 21/05/15 14:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:54,513] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:54,514] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-15 11:00:54,514] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,515] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050191590906963892970_0002_m_000006_10, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050191590906963892970_0002_m_000006_10}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050191590906963892970_0002}; taskId=attempt_20210515140050191590906963892970_0002_m_000006_10, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71b46e81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,515] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:54,515] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_20210515140050191590906963892970_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050191590906963892970_0002_m_000006_10
[2021-05-15 11:00:54,520] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_202105151400502148640024821268200_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502148640024821268200_0002_m_000005_9 : duration 0:00.014s
[2021-05-15 11:00:54,525] {docker.py:276} INFO - 21/05/15 14:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:54,525] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,526] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508958719662188313361_0002_m_000004_8, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508958719662188313361_0002_m_000004_8}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508958719662188313361_0002}; taskId=attempt_202105151400508958719662188313361_0002_m_000004_8, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d7297d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,526] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:54,527] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400508958719662188313361_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508958719662188313361_0002_m_000004_8
[2021-05-15 11:00:54,533] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_20210515140050191590906963892970_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050191590906963892970_0002_m_000006_10 : duration 0:00.019s
[2021-05-15 11:00:54,544] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_202105151400508958719662188313361_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508958719662188313361_0002_m_000004_8 : duration 0:00.018s
[2021-05-15 11:00:54,798] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400507035401751133096581_0002_m_000003_7: needsTaskCommit() Task attempt_202105151400507035401751133096581_0002_m_000003_7
[2021-05-15 11:00:54,799] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_202105151400507035401751133096581_0002_m_000003_7: needsTaskCommit() Task attempt_202105151400507035401751133096581_0002_m_000003_7: duration 0:00.001s
[2021-05-15 11:00:54,799] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507035401751133096581_0002_m_000003_7
[2021-05-15 11:00:54,803] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Finished task 3.0 in stage 2.0 (TID 7). 4587 bytes result sent to driver
[2021-05-15 11:00:54,804] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 11) (8951b5f85146, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:54,805] {docker.py:276} INFO - 21/05/15 14:00:54 INFO Executor: Running task 7.0 in stage 2.0 (TID 11)
[2021-05-15 11:00:54,807] {docker.py:276} INFO - 21/05/15 14:00:54 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 7) in 2465 ms on 8951b5f85146 (executor driver) (4/200)
[2021-05-15 11:00:54,821] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1184.0 B) non-empty blocks including 3 (1184.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:54,822] {docker.py:276} INFO - 21/05/15 14:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:54,824] {docker.py:276} INFO - 21/05/15 14:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,825] {docker.py:276} INFO - 21/05/15 14:00:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051514005072199796196363317_0002_m_000007_11, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_2021051514005072199796196363317_0002_m_000007_11}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051514005072199796196363317_0002}; taskId=attempt_2021051514005072199796196363317_0002_m_000007_11, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@589fccc1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:54,825] {docker.py:276} INFO - 21/05/15 14:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:54,826] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Starting: Task committer attempt_2021051514005072199796196363317_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_2021051514005072199796196363317_0002_m_000007_11
[2021-05-15 11:00:54,831] {docker.py:276} INFO - 21/05/15 14:00:54 INFO StagingCommitter: Task committer attempt_2021051514005072199796196363317_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_2021051514005072199796196363317_0002_m_000007_11 : duration 0:00.005s
[2021-05-15 11:00:56,247] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_20210515140050191590906963892970_0002_m_000006_10: needsTaskCommit() Task attempt_20210515140050191590906963892970_0002_m_000006_10
[2021-05-15 11:00:56,248] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_20210515140050191590906963892970_0002_m_000006_10: needsTaskCommit() Task attempt_20210515140050191590906963892970_0002_m_000006_10: duration 0:00.001s
21/05/15 14:00:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050191590906963892970_0002_m_000006_10
[2021-05-15 11:00:56,249] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400508958719662188313361_0002_m_000004_8: needsTaskCommit() Task attempt_202105151400508958719662188313361_0002_m_000004_8
[2021-05-15 11:00:56,250] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_202105151400508958719662188313361_0002_m_000004_8: needsTaskCommit() Task attempt_202105151400508958719662188313361_0002_m_000004_8: duration 0:00.000s
[2021-05-15 11:00:56,250] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508958719662188313361_0002_m_000004_8
21/05/15 14:00:56 INFO Executor: Finished task 6.0 in stage 2.0 (TID 10). 4587 bytes result sent to driver
[2021-05-15 11:00:56,252] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 12) (8951b5f85146, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:56,252] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Finished task 4.0 in stage 2.0 (TID 8). 4587 bytes result sent to driver
[2021-05-15 11:00:56,254] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Running task 8.0 in stage 2.0 (TID 12)
[2021-05-15 11:00:56,255] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 13) (8951b5f85146, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:56,257] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 10) in 1787 ms on 8951b5f85146 (executor driver) (5/200)
[2021-05-15 11:00:56,257] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 8) in 1797 ms on 8951b5f85146 (executor driver) (6/200)
[2021-05-15 11:00:56,258] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Running task 9.0 in stage 2.0 (TID 13)
[2021-05-15 11:00:56,271] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400502148640024821268200_0002_m_000005_9: needsTaskCommit() Task attempt_202105151400502148640024821268200_0002_m_000005_9
[2021-05-15 11:00:56,272] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_202105151400502148640024821268200_0002_m_000005_9: needsTaskCommit() Task attempt_202105151400502148640024821268200_0002_m_000005_9: duration 0:00.008s
[2021-05-15 11:00:56,272] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502148640024821268200_0002_m_000005_9
[2021-05-15 11:00:56,274] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Finished task 5.0 in stage 2.0 (TID 9). 4587 bytes result sent to driver
[2021-05-15 11:00:56,277] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 14) (8951b5f85146, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:56,278] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 9) in 1810 ms on 8951b5f85146 (executor driver) (7/200)
[2021-05-15 11:00:56,280] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Running task 10.0 in stage 2.0 (TID 14)
[2021-05-15 11:00:56,282] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1269.0 B) non-empty blocks including 3 (1269.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:56,283] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1380.0 B) non-empty blocks including 3 (1380.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:56,283] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:56,284] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:00:56,285] {docker.py:276} INFO - 21/05/15 14:00:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:56,285] {docker.py:276} INFO - 21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502619245284428071888_0002_m_000009_13, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502619245284428071888_0002_m_000009_13}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502619245284428071888_0002}; taskId=attempt_202105151400502619245284428071888_0002_m_000009_13, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@577e6616}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:56,286] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:56,286] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400502619245284428071888_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502619245284428071888_0002_m_000009_13
[2021-05-15 11:00:56,287] {docker.py:276} INFO - 21/05/15 14:00:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:56,287] {docker.py:276} INFO - 21/05/15 14:00:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:56,288] {docker.py:276} INFO - 21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:56,288] {docker.py:276} INFO - 21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501856597396157287361_0002_m_000008_12, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501856597396157287361_0002_m_000008_12}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501856597396157287361_0002}; taskId=attempt_202105151400501856597396157287361_0002_m_000008_12, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ff29bb4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:56,289] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:56,289] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400501856597396157287361_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501856597396157287361_0002_m_000008_12
[2021-05-15 11:00:56,295] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_202105151400502619245284428071888_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502619245284428071888_0002_m_000009_13 : duration 0:00.009s
[2021-05-15 11:00:56,300] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_202105151400501856597396157287361_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501856597396157287361_0002_m_000008_12 : duration 0:00.012s
[2021-05-15 11:00:56,307] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1470.0 B) non-empty blocks including 3 (1470.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:56,309] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-15 11:00:56,313] {docker.py:276} INFO - 21/05/15 14:00:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:56,313] {docker.py:276} INFO - 21/05/15 14:00:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:56,315] {docker.py:276} INFO - 21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:56,315] {docker.py:276} INFO - 21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501456436613909297139_0002_m_000010_14, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501456436613909297139_0002_m_000010_14}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501456436613909297139_0002}; taskId=attempt_202105151400501456436613909297139_0002_m_000010_14, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b9a145e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:56,315] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:56,316] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400501456436613909297139_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501456436613909297139_0002_m_000010_14
[2021-05-15 11:00:56,320] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_202105151400501456436613909297139_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501456436613909297139_0002_m_000010_14 : duration 0:00.005s
[2021-05-15 11:00:56,571] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_2021051514005072199796196363317_0002_m_000007_11: needsTaskCommit() Task attempt_2021051514005072199796196363317_0002_m_000007_11
[2021-05-15 11:00:56,571] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_2021051514005072199796196363317_0002_m_000007_11: needsTaskCommit() Task attempt_2021051514005072199796196363317_0002_m_000007_11: duration 0:00.001s
[2021-05-15 11:00:56,572] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051514005072199796196363317_0002_m_000007_11
[2021-05-15 11:00:56,574] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Finished task 7.0 in stage 2.0 (TID 11). 4587 bytes result sent to driver
[2021-05-15 11:00:56,575] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 15) (8951b5f85146, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:56,577] {docker.py:276} INFO - 21/05/15 14:00:56 INFO Executor: Running task 11.0 in stage 2.0 (TID 15)
[2021-05-15 11:00:56,578] {docker.py:276} INFO - 21/05/15 14:00:56 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 11) in 1777 ms on 8951b5f85146 (executor driver) (8/200)
[2021-05-15 11:00:56,592] {docker.py:276} INFO - 21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1144.0 B) non-empty blocks including 3 (1144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:00:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:56,595] {docker.py:276} INFO - 21/05/15 14:00:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:56,596] {docker.py:276} INFO - 21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:00:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502626739593372182123_0002_m_000011_15, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502626739593372182123_0002_m_000011_15}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502626739593372182123_0002}; taskId=attempt_202105151400502626739593372182123_0002_m_000011_15, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@758f857b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:56,596] {docker.py:276} INFO - 21/05/15 14:00:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:00:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400502626739593372182123_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502626739593372182123_0002_m_000011_15
[2021-05-15 11:00:56,600] {docker.py:276} INFO - 21/05/15 14:00:56 INFO StagingCommitter: Task committer attempt_202105151400502626739593372182123_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502626739593372182123_0002_m_000011_15 : duration 0:00.004s
[2021-05-15 11:00:57,990] {docker.py:276} INFO - 21/05/15 14:00:57 INFO StagingCommitter: Starting: Task committer attempt_202105151400501456436613909297139_0002_m_000010_14: needsTaskCommit() Task attempt_202105151400501456436613909297139_0002_m_000010_14
[2021-05-15 11:00:57,991] {docker.py:276} INFO - 21/05/15 14:00:57 INFO StagingCommitter: Starting: Task committer attempt_202105151400501856597396157287361_0002_m_000008_12: needsTaskCommit() Task attempt_202105151400501856597396157287361_0002_m_000008_12
[2021-05-15 11:00:57,992] {docker.py:276} INFO - 21/05/15 14:00:57 INFO StagingCommitter: Task committer attempt_202105151400501856597396157287361_0002_m_000008_12: needsTaskCommit() Task attempt_202105151400501856597396157287361_0002_m_000008_12: duration 0:00.001s
[2021-05-15 11:00:57,993] {docker.py:276} INFO - 21/05/15 14:00:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501856597396157287361_0002_m_000008_12
[2021-05-15 11:00:57,994] {docker.py:276} INFO - 21/05/15 14:00:57 INFO StagingCommitter: Task committer attempt_202105151400501456436613909297139_0002_m_000010_14: needsTaskCommit() Task attempt_202105151400501456436613909297139_0002_m_000010_14: duration 0:00.003s
[2021-05-15 11:00:57,995] {docker.py:276} INFO - 21/05/15 14:00:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501456436613909297139_0002_m_000010_14
[2021-05-15 11:00:57,998] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Finished task 8.0 in stage 2.0 (TID 12). 4587 bytes result sent to driver
[2021-05-15 11:00:57,998] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Finished task 10.0 in stage 2.0 (TID 14). 4587 bytes result sent to driver
[2021-05-15 11:00:57,999] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 16) (8951b5f85146, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:58,002] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 17) (8951b5f85146, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:58,003] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Running task 12.0 in stage 2.0 (TID 16)
[2021-05-15 11:00:58,004] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 12) in 1754 ms on 8951b5f85146 (executor driver) (9/200)
[2021-05-15 11:00:58,004] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 14) in 1729 ms on 8951b5f85146 (executor driver) (10/200)
[2021-05-15 11:00:58,005] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Running task 13.0 in stage 2.0 (TID 17)
[2021-05-15 11:00:58,021] {docker.py:276} INFO - 21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:58,024] {docker.py:276} INFO - 21/05/15 14:00:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:58,024] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:58,025] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508398021566515129123_0002_m_000012_16, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508398021566515129123_0002_m_000012_16}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508398021566515129123_0002}; taskId=attempt_202105151400508398021566515129123_0002_m_000012_16, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a2f437a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:00:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:58,025] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400508398021566515129123_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508398021566515129123_0002_m_000012_16
[2021-05-15 11:00:58,026] {docker.py:276} INFO - 21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1429.0 B) non-empty blocks including 3 (1429.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:58,027] {docker.py:276} INFO - 21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:00:58,029] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Task committer attempt_202105151400508398021566515129123_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508398021566515129123_0002_m_000012_16 : duration 0:00.005s
[2021-05-15 11:00:58,031] {docker.py:276} INFO - 21/05/15 14:00:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:00:58,032] {docker.py:276} INFO - 21/05/15 14:00:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:58,033] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:58,033] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502848774853200426654_0002_m_000013_17, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502848774853200426654_0002_m_000013_17}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502848774853200426654_0002}; taskId=attempt_202105151400502848774853200426654_0002_m_000013_17, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dafb64b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:58,033] {docker.py:276} INFO - 21/05/15 14:00:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:58,034] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502848774853200426654_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502848774853200426654_0002_m_000013_17
[2021-05-15 11:00:58,036] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502619245284428071888_0002_m_000009_13: needsTaskCommit() Task attempt_202105151400502619245284428071888_0002_m_000009_13
[2021-05-15 11:00:58,037] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Task committer attempt_202105151400502619245284428071888_0002_m_000009_13: needsTaskCommit() Task attempt_202105151400502619245284428071888_0002_m_000009_13: duration 0:00.002s
[2021-05-15 11:00:58,037] {docker.py:276} INFO - 21/05/15 14:00:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502619245284428071888_0002_m_000009_13
[2021-05-15 11:00:58,042] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Task committer attempt_202105151400502848774853200426654_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502848774853200426654_0002_m_000013_17 : duration 0:00.009s
[2021-05-15 11:00:58,043] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Finished task 9.0 in stage 2.0 (TID 13). 4587 bytes result sent to driver
[2021-05-15 11:00:58,044] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 18) (8951b5f85146, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:58,045] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Running task 14.0 in stage 2.0 (TID 18)
[2021-05-15 11:00:58,045] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 13) in 1793 ms on 8951b5f85146 (executor driver) (11/200)
[2021-05-15 11:00:58,065] {docker.py:276} INFO - 21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:58,065] {docker.py:276} INFO - 21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:58,068] {docker.py:276} INFO - 21/05/15 14:00:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:58,068] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:58,068] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502894168235695443772_0002_m_000014_18, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502894168235695443772_0002_m_000014_18}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502894168235695443772_0002}; taskId=attempt_202105151400502894168235695443772_0002_m_000014_18, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7525448}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:00:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:58,069] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502894168235695443772_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502894168235695443772_0002_m_000014_18
[2021-05-15 11:00:58,073] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Task committer attempt_202105151400502894168235695443772_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502894168235695443772_0002_m_000014_18 : duration 0:00.004s
[2021-05-15 11:00:58,305] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502626739593372182123_0002_m_000011_15: needsTaskCommit() Task attempt_202105151400502626739593372182123_0002_m_000011_15
[2021-05-15 11:00:58,306] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Task committer attempt_202105151400502626739593372182123_0002_m_000011_15: needsTaskCommit() Task attempt_202105151400502626739593372182123_0002_m_000011_15: duration 0:00.003s
[2021-05-15 11:00:58,307] {docker.py:276} INFO - 21/05/15 14:00:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502626739593372182123_0002_m_000011_15
[2021-05-15 11:00:58,311] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Finished task 11.0 in stage 2.0 (TID 15). 4587 bytes result sent to driver
[2021-05-15 11:00:58,313] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 19) (8951b5f85146, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:58,315] {docker.py:276} INFO - 21/05/15 14:00:58 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 15) in 1742 ms on 8951b5f85146 (executor driver) (12/200)
[2021-05-15 11:00:58,316] {docker.py:276} INFO - 21/05/15 14:00:58 INFO Executor: Running task 15.0 in stage 2.0 (TID 19)
[2021-05-15 11:00:58,343] {docker.py:276} INFO - 21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:00:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:58,351] {docker.py:276} INFO - 21/05/15 14:00:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:58,352] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:58,352] {docker.py:276} INFO - 21/05/15 14:00:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505285726536880365851_0002_m_000015_19, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505285726536880365851_0002_m_000015_19}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505285726536880365851_0002}; taskId=attempt_202105151400505285726536880365851_0002_m_000015_19, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e744d68}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:00:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:58,353] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400505285726536880365851_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505285726536880365851_0002_m_000015_19
[2021-05-15 11:00:58,357] {docker.py:276} INFO - 21/05/15 14:00:58 INFO StagingCommitter: Task committer attempt_202105151400505285726536880365851_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505285726536880365851_0002_m_000015_19 : duration 0:00.005s
[2021-05-15 11:00:59,733] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400508398021566515129123_0002_m_000012_16: needsTaskCommit() Task attempt_202105151400508398021566515129123_0002_m_000012_16
[2021-05-15 11:00:59,734] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Task committer attempt_202105151400508398021566515129123_0002_m_000012_16: needsTaskCommit() Task attempt_202105151400508398021566515129123_0002_m_000012_16: duration 0:00.003s
21/05/15 14:00:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508398021566515129123_0002_m_000012_16
[2021-05-15 11:00:59,737] {docker.py:276} INFO - 21/05/15 14:00:59 INFO Executor: Finished task 12.0 in stage 2.0 (TID 16). 4587 bytes result sent to driver
[2021-05-15 11:00:59,739] {docker.py:276} INFO - 21/05/15 14:00:59 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 20) (8951b5f85146, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:59,740] {docker.py:276} INFO - 21/05/15 14:00:59 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 16) in 1744 ms on 8951b5f85146 (executor driver) (13/200)
[2021-05-15 11:00:59,741] {docker.py:276} INFO - 21/05/15 14:00:59 INFO Executor: Running task 16.0 in stage 2.0 (TID 20)
[2021-05-15 11:00:59,744] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400502848774853200426654_0002_m_000013_17: needsTaskCommit() Task attempt_202105151400502848774853200426654_0002_m_000013_17
[2021-05-15 11:00:59,745] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Task committer attempt_202105151400502848774853200426654_0002_m_000013_17: needsTaskCommit() Task attempt_202105151400502848774853200426654_0002_m_000013_17: duration 0:00.000s
[2021-05-15 11:00:59,745] {docker.py:276} INFO - 21/05/15 14:00:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502848774853200426654_0002_m_000013_17
[2021-05-15 11:00:59,746] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400502894168235695443772_0002_m_000014_18: needsTaskCommit() Task attempt_202105151400502894168235695443772_0002_m_000014_18
[2021-05-15 11:00:59,747] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Task committer attempt_202105151400502894168235695443772_0002_m_000014_18: needsTaskCommit() Task attempt_202105151400502894168235695443772_0002_m_000014_18: duration 0:00.001s
[2021-05-15 11:00:59,747] {docker.py:276} INFO - 21/05/15 14:00:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502894168235695443772_0002_m_000014_18
[2021-05-15 11:00:59,748] {docker.py:276} INFO - 21/05/15 14:00:59 INFO Executor: Finished task 13.0 in stage 2.0 (TID 17). 4587 bytes result sent to driver
[2021-05-15 11:00:59,751] {docker.py:276} INFO - 21/05/15 14:00:59 INFO Executor: Finished task 14.0 in stage 2.0 (TID 18). 4587 bytes result sent to driver
[2021-05-15 11:00:59,752] {docker.py:276} INFO - 21/05/15 14:00:59 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 21) (8951b5f85146, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:59,754] {docker.py:276} INFO - 21/05/15 14:00:59 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 22) (8951b5f85146, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:00:59,754] {docker.py:276} INFO - 21/05/15 14:00:59 INFO Executor: Running task 17.0 in stage 2.0 (TID 21)
[2021-05-15 11:00:59,754] {docker.py:276} INFO - 21/05/15 14:00:59 INFO Executor: Running task 18.0 in stage 2.0 (TID 22)
[2021-05-15 11:00:59,755] {docker.py:276} INFO - 21/05/15 14:00:59 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 17) in 1756 ms on 8951b5f85146 (executor driver) (14/200)
[2021-05-15 11:00:59,755] {docker.py:276} INFO - 21/05/15 14:00:59 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 18) in 1713 ms on 8951b5f85146 (executor driver) (15/200)
[2021-05-15 11:00:59,780] {docker.py:276} INFO - 21/05/15 14:00:59 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:59,781] {docker.py:276} INFO - 21/05/15 14:00:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:59,781] {docker.py:276} INFO - 21/05/15 14:00:59 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:59,782] {docker.py:276} INFO - 21/05/15 14:00:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:00:59,783] {docker.py:276} INFO - 21/05/15 14:00:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:59,784] {docker.py:276} INFO - 21/05/15 14:00:59 INFO ShuffleBlockFetcherIterator: Getting 3 (1248.0 B) non-empty blocks including 3 (1248.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:00:59,784] {docker.py:276} INFO - 21/05/15 14:00:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:00:59,785] {docker.py:276} INFO - 21/05/15 14:00:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:59,785] {docker.py:276} INFO - 21/05/15 14:00:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508636571309382520006_0002_m_000018_22, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508636571309382520006_0002_m_000018_22}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508636571309382520006_0002}; taskId=attempt_202105151400508636571309382520006_0002_m_000018_22, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7716b622}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:59,786] {docker.py:276} INFO - 21/05/15 14:00:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:59,786] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400508636571309382520006_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508636571309382520006_0002_m_000018_22
[2021-05-15 11:00:59,788] {docker.py:276} INFO - 21/05/15 14:00:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:59,789] {docker.py:276} INFO - 21/05/15 14:00:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:59,789] {docker.py:276} INFO - 21/05/15 14:00:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507572902626459093859_0002_m_000016_20, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507572902626459093859_0002_m_000016_20}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507572902626459093859_0002}; taskId=attempt_202105151400507572902626459093859_0002_m_000016_20, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ae80078}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:00:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:00:59,790] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400507572902626459093859_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507572902626459093859_0002_m_000016_20
[2021-05-15 11:00:59,803] {docker.py:276} INFO - 21/05/15 14:00:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:00:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:00:59,803] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Task committer attempt_202105151400508636571309382520006_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508636571309382520006_0002_m_000018_22 : duration 0:00.017s
[2021-05-15 11:00:59,804] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Task committer attempt_202105151400507572902626459093859_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507572902626459093859_0002_m_000016_20 : duration 0:00.015s
[2021-05-15 11:00:59,805] {docker.py:276} INFO - 21/05/15 14:00:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:00:59,805] {docker.py:276} INFO - 21/05/15 14:00:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503316394217376657066_0002_m_000017_21, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503316394217376657066_0002_m_000017_21}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503316394217376657066_0002}; taskId=attempt_202105151400503316394217376657066_0002_m_000017_21, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@fcd039a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:00:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:00:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400503316394217376657066_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503316394217376657066_0002_m_000017_21
[2021-05-15 11:00:59,824] {docker.py:276} INFO - 21/05/15 14:00:59 INFO StagingCommitter: Task committer attempt_202105151400503316394217376657066_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503316394217376657066_0002_m_000017_21 : duration 0:00.019s
[2021-05-15 11:01:00,036] {docker.py:276} INFO - 21/05/15 14:01:00 INFO StagingCommitter: Starting: Task committer attempt_202105151400505285726536880365851_0002_m_000015_19: needsTaskCommit() Task attempt_202105151400505285726536880365851_0002_m_000015_19
[2021-05-15 11:01:00,036] {docker.py:276} INFO - 21/05/15 14:01:00 INFO StagingCommitter: Task committer attempt_202105151400505285726536880365851_0002_m_000015_19: needsTaskCommit() Task attempt_202105151400505285726536880365851_0002_m_000015_19: duration 0:00.001s
[2021-05-15 11:01:00,037] {docker.py:276} INFO - 21/05/15 14:01:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505285726536880365851_0002_m_000015_19
[2021-05-15 11:01:00,039] {docker.py:276} INFO - 21/05/15 14:01:00 INFO Executor: Finished task 15.0 in stage 2.0 (TID 19). 4587 bytes result sent to driver
[2021-05-15 11:01:00,043] {docker.py:276} INFO - 21/05/15 14:01:00 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 23) (8951b5f85146, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:00,044] {docker.py:276} INFO - 21/05/15 14:01:00 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 19) in 1735 ms on 8951b5f85146 (executor driver) (16/200)
[2021-05-15 11:01:00,046] {docker.py:276} INFO - 21/05/15 14:01:00 INFO Executor: Running task 19.0 in stage 2.0 (TID 23)
[2021-05-15 11:01:00,060] {docker.py:276} INFO - 21/05/15 14:01:00 INFO ShuffleBlockFetcherIterator: Getting 3 (1434.0 B) non-empty blocks including 3 (1434.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:00,062] {docker.py:276} INFO - 21/05/15 14:01:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:00,065] {docker.py:276} INFO - 21/05/15 14:01:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:00,066] {docker.py:276} INFO - 21/05/15 14:01:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:00,068] {docker.py:276} INFO - 21/05/15 14:01:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506400449050282133507_0002_m_000019_23, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506400449050282133507_0002_m_000019_23}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506400449050282133507_0002}; taskId=attempt_202105151400506400449050282133507_0002_m_000019_23, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16b17378}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:00,068] {docker.py:276} INFO - 21/05/15 14:01:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:00,069] {docker.py:276} INFO - 21/05/15 14:01:00 INFO StagingCommitter: Starting: Task committer attempt_202105151400506400449050282133507_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506400449050282133507_0002_m_000019_23
[2021-05-15 11:01:00,072] {docker.py:276} INFO - 21/05/15 14:01:00 INFO StagingCommitter: Task committer attempt_202105151400506400449050282133507_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506400449050282133507_0002_m_000019_23 : duration 0:00.004s
[2021-05-15 11:01:01,485] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400508636571309382520006_0002_m_000018_22: needsTaskCommit() Task attempt_202105151400508636571309382520006_0002_m_000018_22
[2021-05-15 11:01:01,486] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400508636571309382520006_0002_m_000018_22: needsTaskCommit() Task attempt_202105151400508636571309382520006_0002_m_000018_22: duration 0:00.002s
21/05/15 14:01:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508636571309382520006_0002_m_000018_22
[2021-05-15 11:01:01,489] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Finished task 18.0 in stage 2.0 (TID 22). 4587 bytes result sent to driver
[2021-05-15 11:01:01,493] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400507572902626459093859_0002_m_000016_20: needsTaskCommit() Task attempt_202105151400507572902626459093859_0002_m_000016_20
21/05/15 14:01:01 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 24) (8951b5f85146, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400507572902626459093859_0002_m_000016_20: needsTaskCommit() Task attempt_202105151400507572902626459093859_0002_m_000016_20: duration 0:00.000s
21/05/15 14:01:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507572902626459093859_0002_m_000016_20
21/05/15 14:01:01 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 22) in 1741 ms on 8951b5f85146 (executor driver) (17/200)
[2021-05-15 11:01:01,494] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Running task 20.0 in stage 2.0 (TID 24)
21/05/15 14:01:01 INFO Executor: Finished task 16.0 in stage 2.0 (TID 20). 4587 bytes result sent to driver
[2021-05-15 11:01:01,495] {docker.py:276} INFO - 21/05/15 14:01:01 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 25) (8951b5f85146, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:01,497] {docker.py:276} INFO - 21/05/15 14:01:01 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 20) in 1760 ms on 8951b5f85146 (executor driver) (18/200)
[2021-05-15 11:01:01,497] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Running task 21.0 in stage 2.0 (TID 25)
[2021-05-15 11:01:01,500] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400503316394217376657066_0002_m_000017_21: needsTaskCommit() Task attempt_202105151400503316394217376657066_0002_m_000017_21
[2021-05-15 11:01:01,501] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400503316394217376657066_0002_m_000017_21: needsTaskCommit() Task attempt_202105151400503316394217376657066_0002_m_000017_21: duration 0:00.001s
[2021-05-15 11:01:01,501] {docker.py:276} INFO - 21/05/15 14:01:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503316394217376657066_0002_m_000017_21
[2021-05-15 11:01:01,503] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Finished task 17.0 in stage 2.0 (TID 21). 4587 bytes result sent to driver
[2021-05-15 11:01:01,504] {docker.py:276} INFO - 21/05/15 14:01:01 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 26) (8951b5f85146, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:01,505] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Running task 22.0 in stage 2.0 (TID 26)
[2021-05-15 11:01:01,505] {docker.py:276} INFO - 21/05/15 14:01:01 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 21) in 1756 ms on 8951b5f85146 (executor driver) (19/200)
[2021-05-15 11:01:01,510] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1144.0 B) non-empty blocks including 3 (1144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:01,511] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:01,513] {docker.py:276} INFO - 21/05/15 14:01:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:01,513] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,514] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503141598973033869900_0002_m_000021_25, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503141598973033869900_0002_m_000021_25}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503141598973033869900_0002}; taskId=attempt_202105151400503141598973033869900_0002_m_000021_25, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19216489}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:01,514] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400503141598973033869900_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503141598973033869900_0002_m_000021_25
[2021-05-15 11:01:01,515] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1147.0 B) non-empty blocks including 3 (1147.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:01,516] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:01,520] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400503141598973033869900_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503141598973033869900_0002_m_000021_25 : duration 0:00.007s
[2021-05-15 11:01:01,522] {docker.py:276} INFO - 21/05/15 14:01:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:01,522] {docker.py:276} INFO - 21/05/15 14:01:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:01,523] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,524] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505809677550824273291_0002_m_000020_24, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505809677550824273291_0002_m_000020_24}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505809677550824273291_0002}; taskId=attempt_202105151400505809677550824273291_0002_m_000020_24, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7195132f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,524] {docker.py:276} INFO - 21/05/15 14:01:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:01,524] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400505809677550824273291_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505809677550824273291_0002_m_000020_24
[2021-05-15 11:01:01,533] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400505809677550824273291_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505809677550824273291_0002_m_000020_24 : duration 0:00.009s
[2021-05-15 11:01:01,540] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:01,541] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-15 11:01:01,543] {docker.py:276} INFO - 21/05/15 14:01:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:01,543] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,544] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400509132524754981225279_0002_m_000022_26, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400509132524754981225279_0002_m_000022_26}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400509132524754981225279_0002}; taskId=attempt_202105151400509132524754981225279_0002_m_000022_26, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@630fd775}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,544] {docker.py:276} INFO - 21/05/15 14:01:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:01,544] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400509132524754981225279_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400509132524754981225279_0002_m_000022_26
[2021-05-15 11:01:01,552] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400509132524754981225279_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400509132524754981225279_0002_m_000022_26 : duration 0:00.009s
[2021-05-15 11:01:01,954] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400506400449050282133507_0002_m_000019_23: needsTaskCommit() Task attempt_202105151400506400449050282133507_0002_m_000019_23
[2021-05-15 11:01:01,954] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Task committer attempt_202105151400506400449050282133507_0002_m_000019_23: needsTaskCommit() Task attempt_202105151400506400449050282133507_0002_m_000019_23: duration 0:00.002s
[2021-05-15 11:01:01,957] {docker.py:276} INFO - 21/05/15 14:01:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506400449050282133507_0002_m_000019_23
[2021-05-15 11:01:01,959] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Finished task 19.0 in stage 2.0 (TID 23). 4587 bytes result sent to driver
[2021-05-15 11:01:01,961] {docker.py:276} INFO - 21/05/15 14:01:01 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 27) (8951b5f85146, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:01,961] {docker.py:276} INFO - 21/05/15 14:01:01 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 23) in 1921 ms on 8951b5f85146 (executor driver) (20/200)
[2021-05-15 11:01:01,964] {docker.py:276} INFO - 21/05/15 14:01:01 INFO Executor: Running task 23.0 in stage 2.0 (TID 27)
[2021-05-15 11:01:01,974] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1425.0 B) non-empty blocks including 3 (1425.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:01,974] {docker.py:276} INFO - 21/05/15 14:01:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:01,976] {docker.py:276} INFO - 21/05/15 14:01:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:01,977] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,977] {docker.py:276} INFO - 21/05/15 14:01:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050199453103328947474_0002_m_000023_27, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050199453103328947474_0002_m_000023_27}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050199453103328947474_0002}; taskId=attempt_20210515140050199453103328947474_0002_m_000023_27, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d39c60f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:01,978] {docker.py:276} INFO - 21/05/15 14:01:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:01,978] {docker.py:276} INFO - 21/05/15 14:01:01 INFO StagingCommitter: Starting: Task committer attempt_20210515140050199453103328947474_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050199453103328947474_0002_m_000023_27
[2021-05-15 11:01:01,989] {docker.py:276} INFO - 21/05/15 14:01:02 INFO StagingCommitter: Task committer attempt_20210515140050199453103328947474_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050199453103328947474_0002_m_000023_27 : duration 0:00.012s
[2021-05-15 11:01:03,248] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400505809677550824273291_0002_m_000020_24: needsTaskCommit() Task attempt_202105151400505809677550824273291_0002_m_000020_24
[2021-05-15 11:01:03,249] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400509132524754981225279_0002_m_000022_26: needsTaskCommit() Task attempt_202105151400509132524754981225279_0002_m_000022_26
[2021-05-15 11:01:03,249] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_202105151400509132524754981225279_0002_m_000022_26: needsTaskCommit() Task attempt_202105151400509132524754981225279_0002_m_000022_26: duration 0:00.002s
21/05/15 14:01:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400509132524754981225279_0002_m_000022_26
[2021-05-15 11:01:03,250] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_202105151400505809677550824273291_0002_m_000020_24: needsTaskCommit() Task attempt_202105151400505809677550824273291_0002_m_000020_24: duration 0:00.002s
[2021-05-15 11:01:03,250] {docker.py:276} INFO - 21/05/15 14:01:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505809677550824273291_0002_m_000020_24
[2021-05-15 11:01:03,251] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Finished task 22.0 in stage 2.0 (TID 26). 4587 bytes result sent to driver
[2021-05-15 11:01:03,252] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 28) (8951b5f85146, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:03,252] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Finished task 20.0 in stage 2.0 (TID 24). 4587 bytes result sent to driver
[2021-05-15 11:01:03,252] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Running task 24.0 in stage 2.0 (TID 28)
[2021-05-15 11:01:03,253] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 26) in 1751 ms on 8951b5f85146 (executor driver) (21/200)
[2021-05-15 11:01:03,255] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 29) (8951b5f85146, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:03,255] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 24) in 1767 ms on 8951b5f85146 (executor driver) (22/200)
[2021-05-15 11:01:03,256] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Running task 25.0 in stage 2.0 (TID 29)
[2021-05-15 11:01:03,264] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1528.0 B) non-empty blocks including 3 (1528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:03,264] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:03,266] {docker.py:276} INFO - 21/05/15 14:01:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:03,267] {docker.py:276} INFO - 21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050371099372669499607_0002_m_000024_28, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050371099372669499607_0002_m_000024_28}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050371099372669499607_0002}; taskId=attempt_20210515140050371099372669499607_0002_m_000024_28, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7adf682f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:03,267] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_20210515140050371099372669499607_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050371099372669499607_0002_m_000024_28
[2021-05-15 11:01:03,270] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400503141598973033869900_0002_m_000021_25: needsTaskCommit() Task attempt_202105151400503141598973033869900_0002_m_000021_25
[2021-05-15 11:01:03,270] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_202105151400503141598973033869900_0002_m_000021_25: needsTaskCommit() Task attempt_202105151400503141598973033869900_0002_m_000021_25: duration 0:00.001s
21/05/15 14:01:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503141598973033869900_0002_m_000021_25
[2021-05-15 11:01:03,270] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:03,271] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Finished task 21.0 in stage 2.0 (TID 25). 4587 bytes result sent to driver
[2021-05-15 11:01:03,271] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-15 11:01:03,272] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 30) (8951b5f85146, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:03,276] {docker.py:276} INFO - 21/05/15 14:01:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:03,282] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 25) in 1785 ms on 8951b5f85146 (executor driver) (23/200)
[2021-05-15 11:01:03,283] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Running task 26.0 in stage 2.0 (TID 30)
[2021-05-15 11:01:03,283] {docker.py:276} INFO - 21/05/15 14:01:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:03,284] {docker.py:276} INFO - 21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:03,284] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_20210515140050371099372669499607_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050371099372669499607_0002_m_000024_28 : duration 0:00.013s
[2021-05-15 11:01:03,285] {docker.py:276} INFO - 21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501660440637901850929_0002_m_000025_29, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501660440637901850929_0002_m_000025_29}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501660440637901850929_0002}; taskId=attempt_202105151400501660440637901850929_0002_m_000025_29, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4917abe5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:03,285] {docker.py:276} INFO - 21/05/15 14:01:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:03,286] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400501660440637901850929_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501660440637901850929_0002_m_000025_29
[2021-05-15 11:01:03,320] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1474.0 B) non-empty blocks including 3 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:03,320] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_202105151400501660440637901850929_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501660440637901850929_0002_m_000025_29 : duration 0:00.034s
[2021-05-15 11:01:03,321] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:03,323] {docker.py:276} INFO - 21/05/15 14:01:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:03,324] {docker.py:276} INFO - 21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506294110063737665527_0002_m_000026_30, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506294110063737665527_0002_m_000026_30}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506294110063737665527_0002}; taskId=attempt_202105151400506294110063737665527_0002_m_000026_30, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20831ef3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:03,324] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400506294110063737665527_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506294110063737665527_0002_m_000026_30
[2021-05-15 11:01:03,330] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_202105151400506294110063737665527_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506294110063737665527_0002_m_000026_30 : duration 0:00.007s
[2021-05-15 11:01:03,645] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_20210515140050199453103328947474_0002_m_000023_27: needsTaskCommit() Task attempt_20210515140050199453103328947474_0002_m_000023_27
[2021-05-15 11:01:03,646] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_20210515140050199453103328947474_0002_m_000023_27: needsTaskCommit() Task attempt_20210515140050199453103328947474_0002_m_000023_27: duration 0:00.001s
21/05/15 14:01:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050199453103328947474_0002_m_000023_27
[2021-05-15 11:01:03,647] {docker.py:276} INFO - 21/05/15 14:01:03 INFO Executor: Finished task 23.0 in stage 2.0 (TID 27). 4587 bytes result sent to driver
[2021-05-15 11:01:03,649] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 31) (8951b5f85146, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:03,650] {docker.py:276} INFO - 21/05/15 14:01:03 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 27) in 1692 ms on 8951b5f85146 (executor driver) (24/200)
21/05/15 14:01:03 INFO Executor: Running task 27.0 in stage 2.0 (TID 31)
[2021-05-15 11:01:03,662] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:03,663] {docker.py:276} INFO - 21/05/15 14:01:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:03,665] {docker.py:276} INFO - 21/05/15 14:01:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:03,665] {docker.py:276} INFO - 21/05/15 14:01:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:03,666] {docker.py:276} INFO - 21/05/15 14:01:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506792196433803308639_0002_m_000027_31, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506792196433803308639_0002_m_000027_31}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506792196433803308639_0002}; taskId=attempt_202105151400506792196433803308639_0002_m_000027_31, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@115531f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:03,666] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400506792196433803308639_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506792196433803308639_0002_m_000027_31
[2021-05-15 11:01:03,670] {docker.py:276} INFO - 21/05/15 14:01:03 INFO StagingCommitter: Task committer attempt_202105151400506792196433803308639_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506792196433803308639_0002_m_000027_31 : duration 0:00.004s
[2021-05-15 11:01:04,959] {docker.py:276} INFO - 21/05/15 14:01:04 INFO StagingCommitter: Starting: Task committer attempt_202105151400501660440637901850929_0002_m_000025_29: needsTaskCommit() Task attempt_202105151400501660440637901850929_0002_m_000025_29
[2021-05-15 11:01:04,960] {docker.py:276} INFO - 21/05/15 14:01:04 INFO StagingCommitter: Task committer attempt_202105151400501660440637901850929_0002_m_000025_29: needsTaskCommit() Task attempt_202105151400501660440637901850929_0002_m_000025_29: duration 0:00.003s
21/05/15 14:01:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501660440637901850929_0002_m_000025_29
[2021-05-15 11:01:04,963] {docker.py:276} INFO - 21/05/15 14:01:04 INFO Executor: Finished task 25.0 in stage 2.0 (TID 29). 4587 bytes result sent to driver
[2021-05-15 11:01:04,964] {docker.py:276} INFO - 21/05/15 14:01:04 INFO StagingCommitter: Starting: Task committer attempt_20210515140050371099372669499607_0002_m_000024_28: needsTaskCommit() Task attempt_20210515140050371099372669499607_0002_m_000024_28
[2021-05-15 11:01:04,965] {docker.py:276} INFO - 21/05/15 14:01:04 INFO StagingCommitter: Task committer attempt_20210515140050371099372669499607_0002_m_000024_28: needsTaskCommit() Task attempt_20210515140050371099372669499607_0002_m_000024_28: duration 0:00.001s
21/05/15 14:01:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050371099372669499607_0002_m_000024_28
[2021-05-15 11:01:04,966] {docker.py:276} INFO - 21/05/15 14:01:04 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 32) (8951b5f85146, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:04,967] {docker.py:276} INFO - 21/05/15 14:01:04 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 29) in 1714 ms on 8951b5f85146 (executor driver) (25/200)
[2021-05-15 11:01:04,968] {docker.py:276} INFO - 21/05/15 14:01:04 INFO Executor: Running task 28.0 in stage 2.0 (TID 32)
[2021-05-15 11:01:04,971] {docker.py:276} INFO - 21/05/15 14:01:04 INFO Executor: Finished task 24.0 in stage 2.0 (TID 28). 4587 bytes result sent to driver
[2021-05-15 11:01:04,973] {docker.py:276} INFO - 21/05/15 14:01:04 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 33) (8951b5f85146, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:04,973] {docker.py:276} INFO - 21/05/15 14:01:04 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 28) in 1724 ms on 8951b5f85146 (executor driver) (26/200)
[2021-05-15 11:01:04,974] {docker.py:276} INFO - 21/05/15 14:01:04 INFO Executor: Running task 29.0 in stage 2.0 (TID 33)
[2021-05-15 11:01:04,993] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:04,993] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:04,995] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1425.0 B) non-empty blocks including 3 (1425.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:04,996] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:04,997] {docker.py:276} INFO - 21/05/15 14:01:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:04,997] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504623884174773037350_0002_m_000028_32, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504623884174773037350_0002_m_000028_32}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504623884174773037350_0002}; taskId=attempt_202105151400504623884174773037350_0002_m_000028_32, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@466e43f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:04,998] {docker.py:276} INFO - 21/05/15 14:01:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400504623884174773037350_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504623884174773037350_0002_m_000028_32
[2021-05-15 11:01:05,000] {docker.py:276} INFO - 21/05/15 14:01:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:05,001] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:05,001] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501777731123897824495_0002_m_000029_33, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501777731123897824495_0002_m_000029_33}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501777731123897824495_0002}; taskId=attempt_202105151400501777731123897824495_0002_m_000029_33, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d7e08de}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:05,002] {docker.py:276} INFO - 21/05/15 14:01:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:05,002] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400501777731123897824495_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501777731123897824495_0002_m_000029_33
[2021-05-15 11:01:05,007] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Task committer attempt_202105151400504623884174773037350_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504623884174773037350_0002_m_000028_32 : duration 0:00.009s
[2021-05-15 11:01:05,008] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Task committer attempt_202105151400501777731123897824495_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501777731123897824495_0002_m_000029_33 : duration 0:00.006s
[2021-05-15 11:01:05,055] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400506294110063737665527_0002_m_000026_30: needsTaskCommit() Task attempt_202105151400506294110063737665527_0002_m_000026_30
[2021-05-15 11:01:05,056] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Task committer attempt_202105151400506294110063737665527_0002_m_000026_30: needsTaskCommit() Task attempt_202105151400506294110063737665527_0002_m_000026_30: duration 0:00.000s
21/05/15 14:01:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506294110063737665527_0002_m_000026_30
[2021-05-15 11:01:05,057] {docker.py:276} INFO - 21/05/15 14:01:05 INFO Executor: Finished task 26.0 in stage 2.0 (TID 30). 4587 bytes result sent to driver
[2021-05-15 11:01:05,058] {docker.py:276} INFO - 21/05/15 14:01:05 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 34) (8951b5f85146, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:05,059] {docker.py:276} INFO - 21/05/15 14:01:05 INFO Executor: Running task 30.0 in stage 2.0 (TID 34)
[2021-05-15 11:01:05,060] {docker.py:276} INFO - 21/05/15 14:01:05 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 30) in 1789 ms on 8951b5f85146 (executor driver) (27/200)
[2021-05-15 11:01:05,083] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:05,083] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:05,085] {docker.py:276} INFO - 21/05/15 14:01:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:05,085] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:05,086] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502907515353803744665_0002_m_000030_34, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502907515353803744665_0002_m_000030_34}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502907515353803744665_0002}; taskId=attempt_202105151400502907515353803744665_0002_m_000030_34, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38edc9a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:05,086] {docker.py:276} INFO - 21/05/15 14:01:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:05,086] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400502907515353803744665_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502907515353803744665_0002_m_000030_34
[2021-05-15 11:01:05,090] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Task committer attempt_202105151400502907515353803744665_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502907515353803744665_0002_m_000030_34 : duration 0:00.004s
[2021-05-15 11:01:05,320] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400506792196433803308639_0002_m_000027_31: needsTaskCommit() Task attempt_202105151400506792196433803308639_0002_m_000027_31
[2021-05-15 11:01:05,321] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Task committer attempt_202105151400506792196433803308639_0002_m_000027_31: needsTaskCommit() Task attempt_202105151400506792196433803308639_0002_m_000027_31: duration 0:00.002s
[2021-05-15 11:01:05,321] {docker.py:276} INFO - 21/05/15 14:01:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506792196433803308639_0002_m_000027_31
[2021-05-15 11:01:05,323] {docker.py:276} INFO - 21/05/15 14:01:05 INFO Executor: Finished task 27.0 in stage 2.0 (TID 31). 4587 bytes result sent to driver
[2021-05-15 11:01:05,324] {docker.py:276} INFO - 21/05/15 14:01:05 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 35) (8951b5f85146, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:05,325] {docker.py:276} INFO - 21/05/15 14:01:05 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 31) in 1678 ms on 8951b5f85146 (executor driver) (28/200)
[2021-05-15 11:01:05,326] {docker.py:276} INFO - 21/05/15 14:01:05 INFO Executor: Running task 31.0 in stage 2.0 (TID 35)
[2021-05-15 11:01:05,337] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1425.0 B) non-empty blocks including 3 (1425.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:05,338] {docker.py:276} INFO - 21/05/15 14:01:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:05,340] {docker.py:276} INFO - 21/05/15 14:01:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:05,340] {docker.py:276} INFO - 21/05/15 14:01:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:05,341] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:05,341] {docker.py:276} INFO - 21/05/15 14:01:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508234685594543552705_0002_m_000031_35, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508234685594543552705_0002_m_000031_35}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508234685594543552705_0002}; taskId=attempt_202105151400508234685594543552705_0002_m_000031_35, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c2bdb36}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:05,341] {docker.py:276} INFO - 21/05/15 14:01:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:05,342] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400508234685594543552705_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508234685594543552705_0002_m_000031_35
[2021-05-15 11:01:05,345] {docker.py:276} INFO - 21/05/15 14:01:05 INFO StagingCommitter: Task committer attempt_202105151400508234685594543552705_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508234685594543552705_0002_m_000031_35 : duration 0:00.003s
[2021-05-15 11:01:06,693] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400501777731123897824495_0002_m_000029_33: needsTaskCommit() Task attempt_202105151400501777731123897824495_0002_m_000029_33
[2021-05-15 11:01:06,694] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Task committer attempt_202105151400501777731123897824495_0002_m_000029_33: needsTaskCommit() Task attempt_202105151400501777731123897824495_0002_m_000029_33: duration 0:00.004s
[2021-05-15 11:01:06,694] {docker.py:276} INFO - 21/05/15 14:01:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501777731123897824495_0002_m_000029_33
[2021-05-15 11:01:06,697] {docker.py:276} INFO - 21/05/15 14:01:06 INFO Executor: Finished task 29.0 in stage 2.0 (TID 33). 4587 bytes result sent to driver
[2021-05-15 11:01:06,699] {docker.py:276} INFO - 21/05/15 14:01:06 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 36) (8951b5f85146, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:06,700] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400504623884174773037350_0002_m_000028_32: needsTaskCommit() Task attempt_202105151400504623884174773037350_0002_m_000028_32
[2021-05-15 11:01:06,701] {docker.py:276} INFO - 21/05/15 14:01:06 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 33) in 1730 ms on 8951b5f85146 (executor driver) (29/200)
[2021-05-15 11:01:06,702] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Task committer attempt_202105151400504623884174773037350_0002_m_000028_32: needsTaskCommit() Task attempt_202105151400504623884174773037350_0002_m_000028_32: duration 0:00.003s
[2021-05-15 11:01:06,702] {docker.py:276} INFO - 21/05/15 14:01:06 INFO Executor: Running task 32.0 in stage 2.0 (TID 36)
[2021-05-15 11:01:06,703] {docker.py:276} INFO - 21/05/15 14:01:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504623884174773037350_0002_m_000028_32
[2021-05-15 11:01:06,704] {docker.py:276} INFO - 21/05/15 14:01:06 INFO Executor: Finished task 28.0 in stage 2.0 (TID 32). 4587 bytes result sent to driver
[2021-05-15 11:01:06,705] {docker.py:276} INFO - 21/05/15 14:01:06 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 37) (8951b5f85146, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:06,706] {docker.py:276} INFO - 21/05/15 14:01:06 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 32) in 1744 ms on 8951b5f85146 (executor driver) (30/200)
[2021-05-15 11:01:06,707] {docker.py:276} INFO - 21/05/15 14:01:06 INFO Executor: Running task 33.0 in stage 2.0 (TID 37)
[2021-05-15 11:01:06,714] {docker.py:276} INFO - 21/05/15 14:01:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:06,717] {docker.py:276} INFO - 21/05/15 14:01:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:06,717] {docker.py:276} INFO - 21/05/15 14:01:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:06,718] {docker.py:276} INFO - 21/05/15 14:01:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503860018981599956960_0002_m_000032_36, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503860018981599956960_0002_m_000032_36}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503860018981599956960_0002}; taskId=attempt_202105151400503860018981599956960_0002_m_000032_36, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69a88581}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:06,718] {docker.py:276} INFO - 21/05/15 14:01:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400503860018981599956960_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503860018981599956960_0002_m_000032_36
[2021-05-15 11:01:06,719] {docker.py:276} INFO - 21/05/15 14:01:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:06,720] {docker.py:276} INFO - 21/05/15 14:01:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:06,722] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Task committer attempt_202105151400503860018981599956960_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503860018981599956960_0002_m_000032_36 : duration 0:00.004s
[2021-05-15 11:01:06,723] {docker.py:276} INFO - 21/05/15 14:01:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:06,723] {docker.py:276} INFO - 21/05/15 14:01:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:06,724] {docker.py:276} INFO - 21/05/15 14:01:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502369560446718403616_0002_m_000033_37, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502369560446718403616_0002_m_000033_37}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502369560446718403616_0002}; taskId=attempt_202105151400502369560446718403616_0002_m_000033_37, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46982fde}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:06,724] {docker.py:276} INFO - 21/05/15 14:01:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:06,725] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400502369560446718403616_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502369560446718403616_0002_m_000033_37
[2021-05-15 11:01:06,727] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Task committer attempt_202105151400502369560446718403616_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502369560446718403616_0002_m_000033_37 : duration 0:00.004s
[2021-05-15 11:01:06,856] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400502907515353803744665_0002_m_000030_34: needsTaskCommit() Task attempt_202105151400502907515353803744665_0002_m_000030_34
[2021-05-15 11:01:06,857] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Task committer attempt_202105151400502907515353803744665_0002_m_000030_34: needsTaskCommit() Task attempt_202105151400502907515353803744665_0002_m_000030_34: duration 0:00.001s
21/05/15 14:01:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502907515353803744665_0002_m_000030_34
[2021-05-15 11:01:06,859] {docker.py:276} INFO - 21/05/15 14:01:06 INFO Executor: Finished task 30.0 in stage 2.0 (TID 34). 4587 bytes result sent to driver
[2021-05-15 11:01:06,861] {docker.py:276} INFO - 21/05/15 14:01:06 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 38) (8951b5f85146, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:06,862] {docker.py:276} INFO - 21/05/15 14:01:06 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 34) in 1806 ms on 8951b5f85146 (executor driver) (31/200)
21/05/15 14:01:06 INFO Executor: Running task 34.0 in stage 2.0 (TID 38)
[2021-05-15 11:01:06,879] {docker.py:276} INFO - 21/05/15 14:01:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1269.0 B) non-empty blocks including 3 (1269.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:06,880] {docker.py:276} INFO - 21/05/15 14:01:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505361451518819425976_0002_m_000034_38, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505361451518819425976_0002_m_000034_38}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505361451518819425976_0002}; taskId=attempt_202105151400505361451518819425976_0002_m_000034_38, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1842864f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:06,881] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400505361451518819425976_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505361451518819425976_0002_m_000034_38
[2021-05-15 11:01:06,883] {docker.py:276} INFO - 21/05/15 14:01:06 INFO StagingCommitter: Task committer attempt_202105151400505361451518819425976_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505361451518819425976_0002_m_000034_38 : duration 0:00.003s
[2021-05-15 11:01:07,120] {docker.py:276} INFO - 21/05/15 14:01:07 INFO StagingCommitter: Starting: Task committer attempt_202105151400508234685594543552705_0002_m_000031_35: needsTaskCommit() Task attempt_202105151400508234685594543552705_0002_m_000031_35
[2021-05-15 11:01:07,121] {docker.py:276} INFO - 21/05/15 14:01:07 INFO StagingCommitter: Task committer attempt_202105151400508234685594543552705_0002_m_000031_35: needsTaskCommit() Task attempt_202105151400508234685594543552705_0002_m_000031_35: duration 0:00.002s
[2021-05-15 11:01:07,122] {docker.py:276} INFO - 21/05/15 14:01:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508234685594543552705_0002_m_000031_35
[2021-05-15 11:01:07,124] {docker.py:276} INFO - 21/05/15 14:01:07 INFO Executor: Finished task 31.0 in stage 2.0 (TID 35). 4587 bytes result sent to driver
[2021-05-15 11:01:07,125] {docker.py:276} INFO - 21/05/15 14:01:07 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 39) (8951b5f85146, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:07,127] {docker.py:276} INFO - 21/05/15 14:01:07 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 35) in 1805 ms on 8951b5f85146 (executor driver) (32/200)
21/05/15 14:01:07 INFO Executor: Running task 35.0 in stage 2.0 (TID 39)
[2021-05-15 11:01:07,136] {docker.py:276} INFO - 21/05/15 14:01:07 INFO ShuffleBlockFetcherIterator: Getting 3 (1181.0 B) non-empty blocks including 3 (1181.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:07,139] {docker.py:276} INFO - 21/05/15 14:01:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503836884803195718822_0002_m_000035_39, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503836884803195718822_0002_m_000035_39}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503836884803195718822_0002}; taskId=attempt_202105151400503836884803195718822_0002_m_000035_39, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4320f4ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:07,139] {docker.py:276} INFO - 21/05/15 14:01:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:07 INFO StagingCommitter: Starting: Task committer attempt_202105151400503836884803195718822_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503836884803195718822_0002_m_000035_39
[2021-05-15 11:01:07,142] {docker.py:276} INFO - 21/05/15 14:01:07 INFO StagingCommitter: Task committer attempt_202105151400503836884803195718822_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503836884803195718822_0002_m_000035_39 : duration 0:00.004s
[2021-05-15 11:01:08,408] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400503860018981599956960_0002_m_000032_36: needsTaskCommit() Task attempt_202105151400503860018981599956960_0002_m_000032_36
[2021-05-15 11:01:08,409] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400503860018981599956960_0002_m_000032_36: needsTaskCommit() Task attempt_202105151400503860018981599956960_0002_m_000032_36: duration 0:00.001s
21/05/15 14:01:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503860018981599956960_0002_m_000032_36
[2021-05-15 11:01:08,411] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Finished task 32.0 in stage 2.0 (TID 36). 4587 bytes result sent to driver
[2021-05-15 11:01:08,413] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 40) (8951b5f85146, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:08,414] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Running task 36.0 in stage 2.0 (TID 40)
[2021-05-15 11:01:08,414] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 36) in 1718 ms on 8951b5f85146 (executor driver) (33/200)
[2021-05-15 11:01:08,430] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400502369560446718403616_0002_m_000033_37: needsTaskCommit() Task attempt_202105151400502369560446718403616_0002_m_000033_37
[2021-05-15 11:01:08,430] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400502369560446718403616_0002_m_000033_37: needsTaskCommit() Task attempt_202105151400502369560446718403616_0002_m_000033_37: duration 0:00.000s
[2021-05-15 11:01:08,431] {docker.py:276} INFO - 21/05/15 14:01:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502369560446718403616_0002_m_000033_37
[2021-05-15 11:01:08,431] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Finished task 33.0 in stage 2.0 (TID 37). 4587 bytes result sent to driver
[2021-05-15 11:01:08,432] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:08,433] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:08,433] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 41) (8951b5f85146, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:08,434] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Running task 37.0 in stage 2.0 (TID 41)
21/05/15 14:01:08 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 37) in 1732 ms on 8951b5f85146 (executor driver) (34/200)
[2021-05-15 11:01:08,435] {docker.py:276} INFO - 21/05/15 14:01:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:08,436] {docker.py:276} INFO - 21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:08,436] {docker.py:276} INFO - 21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507914197650686297617_0002_m_000036_40, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507914197650686297617_0002_m_000036_40}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507914197650686297617_0002}; taskId=attempt_202105151400507914197650686297617_0002_m_000036_40, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33c7a6ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:08,437] {docker.py:276} INFO - 21/05/15 14:01:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:08,437] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400507914197650686297617_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507914197650686297617_0002_m_000036_40
[2021-05-15 11:01:08,440] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400507914197650686297617_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507914197650686297617_0002_m_000036_40 : duration 0:00.004s
[2021-05-15 11:01:08,449] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:08,449] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-15 11:01:08,454] {docker.py:276} INFO - 21/05/15 14:01:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:08,455] {docker.py:276} INFO - 21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:08,455] {docker.py:276} INFO - 21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050102857945915553487_0002_m_000037_41, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050102857945915553487_0002_m_000037_41}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050102857945915553487_0002}; taskId=attempt_20210515140050102857945915553487_0002_m_000037_41, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f86a863}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:08,455] {docker.py:276} INFO - 21/05/15 14:01:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:08,456] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_20210515140050102857945915553487_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050102857945915553487_0002_m_000037_41
[2021-05-15 11:01:08,458] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_20210515140050102857945915553487_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050102857945915553487_0002_m_000037_41 : duration 0:00.003s
[2021-05-15 11:01:08,581] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400505361451518819425976_0002_m_000034_38: needsTaskCommit() Task attempt_202105151400505361451518819425976_0002_m_000034_38
[2021-05-15 11:01:08,582] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400505361451518819425976_0002_m_000034_38: needsTaskCommit() Task attempt_202105151400505361451518819425976_0002_m_000034_38: duration 0:00.002s
21/05/15 14:01:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505361451518819425976_0002_m_000034_38
[2021-05-15 11:01:08,584] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Finished task 34.0 in stage 2.0 (TID 38). 4587 bytes result sent to driver
[2021-05-15 11:01:08,586] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 42) (8951b5f85146, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:08,588] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 38) in 1730 ms on 8951b5f85146 (executor driver) (35/200)
21/05/15 14:01:08 INFO Executor: Running task 38.0 in stage 2.0 (TID 42)
[2021-05-15 11:01:08,604] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1184.0 B) non-empty blocks including 3 (1184.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:08,604] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:08,606] {docker.py:276} INFO - 21/05/15 14:01:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:08,606] {docker.py:276} INFO - 21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505456069092145069739_0002_m_000038_42, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505456069092145069739_0002_m_000038_42}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505456069092145069739_0002}; taskId=attempt_202105151400505456069092145069739_0002_m_000038_42, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5fb86b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:08,607] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400505456069092145069739_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505456069092145069739_0002_m_000038_42
[2021-05-15 11:01:08,611] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400505456069092145069739_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505456069092145069739_0002_m_000038_42 : duration 0:00.004s
[2021-05-15 11:01:08,821] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400503836884803195718822_0002_m_000035_39: needsTaskCommit() Task attempt_202105151400503836884803195718822_0002_m_000035_39
[2021-05-15 11:01:08,822] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400503836884803195718822_0002_m_000035_39: needsTaskCommit() Task attempt_202105151400503836884803195718822_0002_m_000035_39: duration 0:00.001s
21/05/15 14:01:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503836884803195718822_0002_m_000035_39
[2021-05-15 11:01:08,823] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Finished task 35.0 in stage 2.0 (TID 39). 4587 bytes result sent to driver
[2021-05-15 11:01:08,826] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 43) (8951b5f85146, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:08,827] {docker.py:276} INFO - 21/05/15 14:01:08 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 39) in 1704 ms on 8951b5f85146 (executor driver) (36/200)
[2021-05-15 11:01:08,828] {docker.py:276} INFO - 21/05/15 14:01:08 INFO Executor: Running task 39.0 in stage 2.0 (TID 43)
[2021-05-15 11:01:08,837] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1226.0 B) non-empty blocks including 3 (1226.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:08,838] {docker.py:276} INFO - 21/05/15 14:01:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:08,840] {docker.py:276} INFO - 21/05/15 14:01:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:08,840] {docker.py:276} INFO - 21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506416385474735630816_0002_m_000039_43, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506416385474735630816_0002_m_000039_43}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506416385474735630816_0002}; taskId=attempt_202105151400506416385474735630816_0002_m_000039_43, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@401df044}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:08,841] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400506416385474735630816_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506416385474735630816_0002_m_000039_43
[2021-05-15 11:01:08,843] {docker.py:276} INFO - 21/05/15 14:01:08 INFO StagingCommitter: Task committer attempt_202105151400506416385474735630816_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506416385474735630816_0002_m_000039_43 : duration 0:00.003s
[2021-05-15 11:01:10,129] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_20210515140050102857945915553487_0002_m_000037_41: needsTaskCommit() Task attempt_20210515140050102857945915553487_0002_m_000037_41
[2021-05-15 11:01:10,130] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_20210515140050102857945915553487_0002_m_000037_41: needsTaskCommit() Task attempt_20210515140050102857945915553487_0002_m_000037_41: duration 0:00.001s
21/05/15 14:01:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050102857945915553487_0002_m_000037_41
[2021-05-15 11:01:10,133] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Finished task 37.0 in stage 2.0 (TID 41). 4587 bytes result sent to driver
[2021-05-15 11:01:10,134] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 44) (8951b5f85146, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:10,135] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 41) in 1704 ms on 8951b5f85146 (executor driver) (37/200)
[2021-05-15 11:01:10,136] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Running task 40.0 in stage 2.0 (TID 44)
[2021-05-15 11:01:10,147] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1113.0 B) non-empty blocks including 3 (1113.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:10,148] {docker.py:276} INFO - 21/05/15 14:01:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:10,149] {docker.py:276} INFO - 21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501562631786705530614_0002_m_000040_44, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501562631786705530614_0002_m_000040_44}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501562631786705530614_0002}; taskId=attempt_202105151400501562631786705530614_0002_m_000040_44, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6190ffef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400501562631786705530614_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501562631786705530614_0002_m_000040_44
[2021-05-15 11:01:10,151] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400501562631786705530614_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501562631786705530614_0002_m_000040_44 : duration 0:00.003s
[2021-05-15 11:01:10,162] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400507914197650686297617_0002_m_000036_40: needsTaskCommit() Task attempt_202105151400507914197650686297617_0002_m_000036_40
[2021-05-15 11:01:10,163] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400507914197650686297617_0002_m_000036_40: needsTaskCommit() Task attempt_202105151400507914197650686297617_0002_m_000036_40: duration 0:00.001s
[2021-05-15 11:01:10,163] {docker.py:276} INFO - 21/05/15 14:01:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507914197650686297617_0002_m_000036_40
[2021-05-15 11:01:10,165] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Finished task 36.0 in stage 2.0 (TID 40). 4587 bytes result sent to driver
[2021-05-15 11:01:10,167] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 45) (8951b5f85146, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:10,167] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Running task 41.0 in stage 2.0 (TID 45)
[2021-05-15 11:01:10,168] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 40) in 1758 ms on 8951b5f85146 (executor driver) (38/200)
[2021-05-15 11:01:10,182] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1434.0 B) non-empty blocks including 3 (1434.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:10,183] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:10,184] {docker.py:276} INFO - 21/05/15 14:01:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:10,185] {docker.py:276} INFO - 21/05/15 14:01:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:10,185] {docker.py:276} INFO - 21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:10,186] {docker.py:276} INFO - 21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506051374255289398031_0002_m_000041_45, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506051374255289398031_0002_m_000041_45}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506051374255289398031_0002}; taskId=attempt_202105151400506051374255289398031_0002_m_000041_45, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5764c5e1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:10,186] {docker.py:276} INFO - 21/05/15 14:01:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:10,186] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400506051374255289398031_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506051374255289398031_0002_m_000041_45
[2021-05-15 11:01:10,189] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400506051374255289398031_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506051374255289398031_0002_m_000041_45 : duration 0:00.003s
[2021-05-15 11:01:10,302] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400505456069092145069739_0002_m_000038_42: needsTaskCommit() Task attempt_202105151400505456069092145069739_0002_m_000038_42
[2021-05-15 11:01:10,304] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400505456069092145069739_0002_m_000038_42: needsTaskCommit() Task attempt_202105151400505456069092145069739_0002_m_000038_42: duration 0:00.001s
21/05/15 14:01:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505456069092145069739_0002_m_000038_42
[2021-05-15 11:01:10,306] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Finished task 38.0 in stage 2.0 (TID 42). 4587 bytes result sent to driver
[2021-05-15 11:01:10,307] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 46) (8951b5f85146, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:10,308] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 42) in 1725 ms on 8951b5f85146 (executor driver) (39/200)
[2021-05-15 11:01:10,311] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Running task 42.0 in stage 2.0 (TID 46)
[2021-05-15 11:01:10,331] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:10,331] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:10,333] {docker.py:276} INFO - 21/05/15 14:01:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:10,334] {docker.py:276} INFO - 21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506684780442934086417_0002_m_000042_46, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506684780442934086417_0002_m_000042_46}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506684780442934086417_0002}; taskId=attempt_202105151400506684780442934086417_0002_m_000042_46, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f63e07e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:10,334] {docker.py:276} INFO - 21/05/15 14:01:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400506684780442934086417_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506684780442934086417_0002_m_000042_46
[2021-05-15 11:01:10,337] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400506684780442934086417_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506684780442934086417_0002_m_000042_46 : duration 0:00.003s
[2021-05-15 11:01:10,597] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400506416385474735630816_0002_m_000039_43: needsTaskCommit() Task attempt_202105151400506416385474735630816_0002_m_000039_43
[2021-05-15 11:01:10,599] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400506416385474735630816_0002_m_000039_43: needsTaskCommit() Task attempt_202105151400506416385474735630816_0002_m_000039_43: duration 0:00.001s
21/05/15 14:01:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506416385474735630816_0002_m_000039_43
[2021-05-15 11:01:10,600] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Finished task 39.0 in stage 2.0 (TID 43). 4587 bytes result sent to driver
[2021-05-15 11:01:10,601] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 47) (8951b5f85146, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:10,603] {docker.py:276} INFO - 21/05/15 14:01:10 INFO Executor: Running task 43.0 in stage 2.0 (TID 47)
[2021-05-15 11:01:10,603] {docker.py:276} INFO - 21/05/15 14:01:10 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 43) in 1780 ms on 8951b5f85146 (executor driver) (40/200)
[2021-05-15 11:01:10,615] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:10,615] {docker.py:276} INFO - 21/05/15 14:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:10,617] {docker.py:276} INFO - 21/05/15 14:01:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:10,618] {docker.py:276} INFO - 21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:10,619] {docker.py:276} INFO - 21/05/15 14:01:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508781312699053804516_0002_m_000043_47, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508781312699053804516_0002_m_000043_47}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508781312699053804516_0002}; taskId=attempt_202105151400508781312699053804516_0002_m_000043_47, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7db3e4d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:10,619] {docker.py:276} INFO - 21/05/15 14:01:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:10,620] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400508781312699053804516_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508781312699053804516_0002_m_000043_47
[2021-05-15 11:01:10,622] {docker.py:276} INFO - 21/05/15 14:01:10 INFO StagingCommitter: Task committer attempt_202105151400508781312699053804516_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508781312699053804516_0002_m_000043_47 : duration 0:00.003s
[2021-05-15 11:01:11,813] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400501562631786705530614_0002_m_000040_44: needsTaskCommit() Task attempt_202105151400501562631786705530614_0002_m_000040_44
[2021-05-15 11:01:11,815] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Task committer attempt_202105151400501562631786705530614_0002_m_000040_44: needsTaskCommit() Task attempt_202105151400501562631786705530614_0002_m_000040_44: duration 0:00.001s
21/05/15 14:01:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501562631786705530614_0002_m_000040_44
[2021-05-15 11:01:11,817] {docker.py:276} INFO - 21/05/15 14:01:11 INFO Executor: Finished task 40.0 in stage 2.0 (TID 44). 4587 bytes result sent to driver
[2021-05-15 11:01:11,819] {docker.py:276} INFO - 21/05/15 14:01:11 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 48) (8951b5f85146, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:11,820] {docker.py:276} INFO - 21/05/15 14:01:11 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 44) in 1688 ms on 8951b5f85146 (executor driver) (41/200)
[2021-05-15 11:01:11,821] {docker.py:276} INFO - 21/05/15 14:01:11 INFO Executor: Running task 44.0 in stage 2.0 (TID 48)
[2021-05-15 11:01:11,839] {docker.py:276} INFO - 21/05/15 14:01:11 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:11,839] {docker.py:276} INFO - 21/05/15 14:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:11,841] {docker.py:276} INFO - 21/05/15 14:01:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:11,842] {docker.py:276} INFO - 21/05/15 14:01:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502261225933696697679_0002_m_000044_48, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502261225933696697679_0002_m_000044_48}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502261225933696697679_0002}; taskId=attempt_202105151400502261225933696697679_0002_m_000044_48, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3612c3b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400502261225933696697679_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502261225933696697679_0002_m_000044_48
[2021-05-15 11:01:11,844] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Task committer attempt_202105151400502261225933696697679_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502261225933696697679_0002_m_000044_48 : duration 0:00.003s
[2021-05-15 11:01:11,876] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400506051374255289398031_0002_m_000041_45: needsTaskCommit() Task attempt_202105151400506051374255289398031_0002_m_000041_45
[2021-05-15 11:01:11,877] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Task committer attempt_202105151400506051374255289398031_0002_m_000041_45: needsTaskCommit() Task attempt_202105151400506051374255289398031_0002_m_000041_45: duration 0:00.001s
21/05/15 14:01:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506051374255289398031_0002_m_000041_45
[2021-05-15 11:01:11,878] {docker.py:276} INFO - 21/05/15 14:01:11 INFO Executor: Finished task 41.0 in stage 2.0 (TID 45). 4587 bytes result sent to driver
[2021-05-15 11:01:11,879] {docker.py:276} INFO - 21/05/15 14:01:11 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 49) (8951b5f85146, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:11,880] {docker.py:276} INFO - 21/05/15 14:01:11 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 45) in 1716 ms on 8951b5f85146 (executor driver) (42/200)
[2021-05-15 11:01:11,881] {docker.py:276} INFO - 21/05/15 14:01:11 INFO Executor: Running task 45.0 in stage 2.0 (TID 49)
[2021-05-15 11:01:11,889] {docker.py:276} INFO - 21/05/15 14:01:11 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:11,891] {docker.py:276} INFO - 21/05/15 14:01:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:11,891] {docker.py:276} INFO - 21/05/15 14:01:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508764063589420547307_0002_m_000045_49, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508764063589420547307_0002_m_000045_49}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508764063589420547307_0002}; taskId=attempt_202105151400508764063589420547307_0002_m_000045_49, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@521918ce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:11,891] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400508764063589420547307_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508764063589420547307_0002_m_000045_49
[2021-05-15 11:01:11,894] {docker.py:276} INFO - 21/05/15 14:01:11 INFO StagingCommitter: Task committer attempt_202105151400508764063589420547307_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508764063589420547307_0002_m_000045_49 : duration 0:00.003s
[2021-05-15 11:01:12,057] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Starting: Task committer attempt_202105151400506684780442934086417_0002_m_000042_46: needsTaskCommit() Task attempt_202105151400506684780442934086417_0002_m_000042_46
[2021-05-15 11:01:12,058] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Task committer attempt_202105151400506684780442934086417_0002_m_000042_46: needsTaskCommit() Task attempt_202105151400506684780442934086417_0002_m_000042_46: duration 0:00.000s
[2021-05-15 11:01:12,058] {docker.py:276} INFO - 21/05/15 14:01:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506684780442934086417_0002_m_000042_46
[2021-05-15 11:01:12,060] {docker.py:276} INFO - 21/05/15 14:01:12 INFO Executor: Finished task 42.0 in stage 2.0 (TID 46). 4587 bytes result sent to driver
[2021-05-15 11:01:12,061] {docker.py:276} INFO - 21/05/15 14:01:12 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 50) (8951b5f85146, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:12,062] {docker.py:276} INFO - 21/05/15 14:01:12 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 46) in 1757 ms on 8951b5f85146 (executor driver) (43/200)
[2021-05-15 11:01:12,063] {docker.py:276} INFO - 21/05/15 14:01:12 INFO Executor: Running task 46.0 in stage 2.0 (TID 50)
[2021-05-15 11:01:12,073] {docker.py:276} INFO - 21/05/15 14:01:12 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:12,075] {docker.py:276} INFO - 21/05/15 14:01:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:12,076] {docker.py:276} INFO - 21/05/15 14:01:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501670068181904549623_0002_m_000046_50, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501670068181904549623_0002_m_000046_50}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501670068181904549623_0002}; taskId=attempt_202105151400501670068181904549623_0002_m_000046_50, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b40e024}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:12 INFO StagingCommitter: Starting: Task committer attempt_202105151400501670068181904549623_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501670068181904549623_0002_m_000046_50
[2021-05-15 11:01:12,079] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Task committer attempt_202105151400501670068181904549623_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501670068181904549623_0002_m_000046_50 : duration 0:00.003s
[2021-05-15 11:01:12,329] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Starting: Task committer attempt_202105151400508781312699053804516_0002_m_000043_47: needsTaskCommit() Task attempt_202105151400508781312699053804516_0002_m_000043_47
[2021-05-15 11:01:12,330] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Task committer attempt_202105151400508781312699053804516_0002_m_000043_47: needsTaskCommit() Task attempt_202105151400508781312699053804516_0002_m_000043_47: duration 0:00.000s
21/05/15 14:01:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508781312699053804516_0002_m_000043_47
[2021-05-15 11:01:12,332] {docker.py:276} INFO - 21/05/15 14:01:12 INFO Executor: Finished task 43.0 in stage 2.0 (TID 47). 4587 bytes result sent to driver
[2021-05-15 11:01:12,333] {docker.py:276} INFO - 21/05/15 14:01:12 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 51) (8951b5f85146, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:12,335] {docker.py:276} INFO - 21/05/15 14:01:12 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 47) in 1735 ms on 8951b5f85146 (executor driver) (44/200)
[2021-05-15 11:01:12,335] {docker.py:276} INFO - 21/05/15 14:01:12 INFO Executor: Running task 47.0 in stage 2.0 (TID 51)
[2021-05-15 11:01:12,353] {docker.py:276} INFO - 21/05/15 14:01:12 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:12,355] {docker.py:276} INFO - 21/05/15 14:01:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:12,356] {docker.py:276} INFO - 21/05/15 14:01:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504894894605662239968_0002_m_000047_51, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504894894605662239968_0002_m_000047_51}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504894894605662239968_0002}; taskId=attempt_202105151400504894894605662239968_0002_m_000047_51, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f0d2821}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:12,356] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Starting: Task committer attempt_202105151400504894894605662239968_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504894894605662239968_0002_m_000047_51
[2021-05-15 11:01:12,359] {docker.py:276} INFO - 21/05/15 14:01:12 INFO StagingCommitter: Task committer attempt_202105151400504894894605662239968_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504894894605662239968_0002_m_000047_51 : duration 0:00.003s
[2021-05-15 11:01:13,497] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400502261225933696697679_0002_m_000044_48: needsTaskCommit() Task attempt_202105151400502261225933696697679_0002_m_000044_48
21/05/15 14:01:13 INFO StagingCommitter: Task committer attempt_202105151400502261225933696697679_0002_m_000044_48: needsTaskCommit() Task attempt_202105151400502261225933696697679_0002_m_000044_48: duration 0:00.001s
21/05/15 14:01:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502261225933696697679_0002_m_000044_48
[2021-05-15 11:01:13,500] {docker.py:276} INFO - 21/05/15 14:01:13 INFO Executor: Finished task 44.0 in stage 2.0 (TID 48). 4587 bytes result sent to driver
[2021-05-15 11:01:13,502] {docker.py:276} INFO - 21/05/15 14:01:13 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 52) (8951b5f85146, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:13,503] {docker.py:276} INFO - 21/05/15 14:01:13 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 48) in 1685 ms on 8951b5f85146 (executor driver) (45/200)
[2021-05-15 11:01:13,504] {docker.py:276} INFO - 21/05/15 14:01:13 INFO Executor: Running task 48.0 in stage 2.0 (TID 52)
[2021-05-15 11:01:13,514] {docker.py:276} INFO - 21/05/15 14:01:13 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:13,515] {docker.py:276} INFO - 21/05/15 14:01:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:13,517] {docker.py:276} INFO - 21/05/15 14:01:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:13,517] {docker.py:276} INFO - 21/05/15 14:01:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507755305479469594975_0002_m_000048_52, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507755305479469594975_0002_m_000048_52}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507755305479469594975_0002}; taskId=attempt_202105151400507755305479469594975_0002_m_000048_52, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33b01943}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:13,517] {docker.py:276} INFO - 21/05/15 14:01:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:13,518] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400507755305479469594975_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507755305479469594975_0002_m_000048_52
[2021-05-15 11:01:13,521] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Task committer attempt_202105151400507755305479469594975_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507755305479469594975_0002_m_000048_52 : duration 0:00.004s
[2021-05-15 11:01:13,565] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400508764063589420547307_0002_m_000045_49: needsTaskCommit() Task attempt_202105151400508764063589420547307_0002_m_000045_49
[2021-05-15 11:01:13,566] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Task committer attempt_202105151400508764063589420547307_0002_m_000045_49: needsTaskCommit() Task attempt_202105151400508764063589420547307_0002_m_000045_49: duration 0:00.001s
21/05/15 14:01:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508764063589420547307_0002_m_000045_49
[2021-05-15 11:01:13,567] {docker.py:276} INFO - 21/05/15 14:01:13 INFO Executor: Finished task 45.0 in stage 2.0 (TID 49). 4587 bytes result sent to driver
[2021-05-15 11:01:13,568] {docker.py:276} INFO - 21/05/15 14:01:13 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 53) (8951b5f85146, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:13,569] {docker.py:276} INFO - 21/05/15 14:01:13 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 49) in 1692 ms on 8951b5f85146 (executor driver) (46/200)
[2021-05-15 11:01:13,570] {docker.py:276} INFO - 21/05/15 14:01:13 INFO Executor: Running task 49.0 in stage 2.0 (TID 53)
[2021-05-15 11:01:13,577] {docker.py:276} INFO - 21/05/15 14:01:13 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:13,580] {docker.py:276} INFO - 21/05/15 14:01:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501620506977606784443_0002_m_000049_53, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501620506977606784443_0002_m_000049_53}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501620506977606784443_0002}; taskId=attempt_202105151400501620506977606784443_0002_m_000049_53, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@345070a2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400501620506977606784443_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501620506977606784443_0002_m_000049_53
[2021-05-15 11:01:13,582] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Task committer attempt_202105151400501620506977606784443_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501620506977606784443_0002_m_000049_53 : duration 0:00.002s
[2021-05-15 11:01:13,796] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400501670068181904549623_0002_m_000046_50: needsTaskCommit() Task attempt_202105151400501670068181904549623_0002_m_000046_50
[2021-05-15 11:01:13,797] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Task committer attempt_202105151400501670068181904549623_0002_m_000046_50: needsTaskCommit() Task attempt_202105151400501670068181904549623_0002_m_000046_50: duration 0:00.000s
21/05/15 14:01:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501670068181904549623_0002_m_000046_50
[2021-05-15 11:01:13,799] {docker.py:276} INFO - 21/05/15 14:01:13 INFO Executor: Finished task 46.0 in stage 2.0 (TID 50). 4587 bytes result sent to driver
[2021-05-15 11:01:13,800] {docker.py:276} INFO - 21/05/15 14:01:13 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 54) (8951b5f85146, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:13,802] {docker.py:276} INFO - 21/05/15 14:01:13 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 50) in 1743 ms on 8951b5f85146 (executor driver) (47/200)
[2021-05-15 11:01:13,803] {docker.py:276} INFO - 21/05/15 14:01:13 INFO Executor: Running task 50.0 in stage 2.0 (TID 54)
[2021-05-15 11:01:13,813] {docker.py:276} INFO - 21/05/15 14:01:13 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:13,815] {docker.py:276} INFO - 21/05/15 14:01:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:13,816] {docker.py:276} INFO - 21/05/15 14:01:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:13,817] {docker.py:276} INFO - 21/05/15 14:01:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501491965406563478491_0002_m_000050_54, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501491965406563478491_0002_m_000050_54}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501491965406563478491_0002}; taskId=attempt_202105151400501491965406563478491_0002_m_000050_54, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2724ed0e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:13,817] {docker.py:276} INFO - 21/05/15 14:01:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:13,817] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400501491965406563478491_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501491965406563478491_0002_m_000050_54
[2021-05-15 11:01:13,820] {docker.py:276} INFO - 21/05/15 14:01:13 INFO StagingCommitter: Task committer attempt_202105151400501491965406563478491_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501491965406563478491_0002_m_000050_54 : duration 0:00.003s
[2021-05-15 11:01:14,049] {docker.py:276} INFO - 21/05/15 14:01:14 INFO StagingCommitter: Starting: Task committer attempt_202105151400504894894605662239968_0002_m_000047_51: needsTaskCommit() Task attempt_202105151400504894894605662239968_0002_m_000047_51
[2021-05-15 11:01:14,050] {docker.py:276} INFO - 21/05/15 14:01:14 INFO StagingCommitter: Task committer attempt_202105151400504894894605662239968_0002_m_000047_51: needsTaskCommit() Task attempt_202105151400504894894605662239968_0002_m_000047_51: duration 0:00.001s
21/05/15 14:01:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504894894605662239968_0002_m_000047_51
[2021-05-15 11:01:14,052] {docker.py:276} INFO - 21/05/15 14:01:14 INFO Executor: Finished task 47.0 in stage 2.0 (TID 51). 4587 bytes result sent to driver
[2021-05-15 11:01:14,053] {docker.py:276} INFO - 21/05/15 14:01:14 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 55) (8951b5f85146, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:14,055] {docker.py:276} INFO - 21/05/15 14:01:14 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 51) in 1724 ms on 8951b5f85146 (executor driver) (48/200)
[2021-05-15 11:01:14,056] {docker.py:276} INFO - 21/05/15 14:01:14 INFO Executor: Running task 51.0 in stage 2.0 (TID 55)
[2021-05-15 11:01:14,066] {docker.py:276} INFO - 21/05/15 14:01:14 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:14,067] {docker.py:276} INFO - 21/05/15 14:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:14,069] {docker.py:276} INFO - 21/05/15 14:01:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:14,069] {docker.py:276} INFO - 21/05/15 14:01:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:14,081] {docker.py:276} INFO - 21/05/15 14:01:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501266717304057410768_0002_m_000051_55, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501266717304057410768_0002_m_000051_55}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501266717304057410768_0002}; taskId=attempt_202105151400501266717304057410768_0002_m_000051_55, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11144076}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:14,082] {docker.py:276} INFO - 21/05/15 14:01:14 INFO StagingCommitter: Starting: Task committer attempt_202105151400501266717304057410768_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501266717304057410768_0002_m_000051_55
[2021-05-15 11:01:14,089] {docker.py:276} INFO - 21/05/15 14:01:14 INFO StagingCommitter: Task committer attempt_202105151400501266717304057410768_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501266717304057410768_0002_m_000051_55 : duration 0:00.008s
[2021-05-15 11:01:15,239] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400507755305479469594975_0002_m_000048_52: needsTaskCommit() Task attempt_202105151400507755305479469594975_0002_m_000048_52
[2021-05-15 11:01:15,240] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400507755305479469594975_0002_m_000048_52: needsTaskCommit() Task attempt_202105151400507755305479469594975_0002_m_000048_52: duration 0:00.002s
21/05/15 14:01:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507755305479469594975_0002_m_000048_52
[2021-05-15 11:01:15,242] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Finished task 48.0 in stage 2.0 (TID 52). 4587 bytes result sent to driver
[2021-05-15 11:01:15,244] {docker.py:276} INFO - 21/05/15 14:01:15 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 56) (8951b5f85146, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:15,245] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Running task 52.0 in stage 2.0 (TID 56)
[2021-05-15 11:01:15,246] {docker.py:276} INFO - 21/05/15 14:01:15 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 52) in 1747 ms on 8951b5f85146 (executor driver) (49/200)
[2021-05-15 11:01:15,256] {docker.py:276} INFO - 21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Getting 3 (1348.0 B) non-empty blocks including 3 (1348.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:15,258] {docker.py:276} INFO - 21/05/15 14:01:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:15,258] {docker.py:276} INFO - 21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050461446831170293769_0002_m_000052_56, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050461446831170293769_0002_m_000052_56}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050461446831170293769_0002}; taskId=attempt_20210515140050461446831170293769_0002_m_000052_56, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@543b353a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_20210515140050461446831170293769_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050461446831170293769_0002_m_000052_56
[2021-05-15 11:01:15,261] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_20210515140050461446831170293769_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050461446831170293769_0002_m_000052_56 : duration 0:00.003s
[2021-05-15 11:01:15,287] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400501620506977606784443_0002_m_000049_53: needsTaskCommit() Task attempt_202105151400501620506977606784443_0002_m_000049_53
[2021-05-15 11:01:15,288] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400501620506977606784443_0002_m_000049_53: needsTaskCommit() Task attempt_202105151400501620506977606784443_0002_m_000049_53: duration 0:00.000s
21/05/15 14:01:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501620506977606784443_0002_m_000049_53
[2021-05-15 11:01:15,289] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Finished task 49.0 in stage 2.0 (TID 53). 4587 bytes result sent to driver
[2021-05-15 11:01:15,297] {docker.py:276} INFO - 21/05/15 14:01:15 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 57) (8951b5f85146, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:15,298] {docker.py:276} INFO - 21/05/15 14:01:15 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 53) in 1732 ms on 8951b5f85146 (executor driver) (50/200)
[2021-05-15 11:01:15,298] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Running task 53.0 in stage 2.0 (TID 57)
[2021-05-15 11:01:15,311] {docker.py:276} INFO - 21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Getting 3 (1269.0 B) non-empty blocks including 3 (1269.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:15,314] {docker.py:276} INFO - 21/05/15 14:01:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:15,315] {docker.py:276} INFO - 21/05/15 14:01:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:15,315] {docker.py:276} INFO - 21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:15,316] {docker.py:276} INFO - 21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502207665639126994689_0002_m_000053_57, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502207665639126994689_0002_m_000053_57}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502207665639126994689_0002}; taskId=attempt_202105151400502207665639126994689_0002_m_000053_57, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@387c1056}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:15,316] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400502207665639126994689_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502207665639126994689_0002_m_000053_57
[2021-05-15 11:01:15,318] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400502207665639126994689_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502207665639126994689_0002_m_000053_57 : duration 0:00.003s
[2021-05-15 11:01:15,496] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400501491965406563478491_0002_m_000050_54: needsTaskCommit() Task attempt_202105151400501491965406563478491_0002_m_000050_54
[2021-05-15 11:01:15,497] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400501491965406563478491_0002_m_000050_54: needsTaskCommit() Task attempt_202105151400501491965406563478491_0002_m_000050_54: duration 0:00.001s
[2021-05-15 11:01:15,497] {docker.py:276} INFO - 21/05/15 14:01:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501491965406563478491_0002_m_000050_54
[2021-05-15 11:01:15,498] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Finished task 50.0 in stage 2.0 (TID 54). 4587 bytes result sent to driver
[2021-05-15 11:01:15,499] {docker.py:276} INFO - 21/05/15 14:01:15 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 58) (8951b5f85146, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:15,500] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Running task 54.0 in stage 2.0 (TID 58)
21/05/15 14:01:15 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 54) in 1702 ms on 8951b5f85146 (executor driver) (51/200)
[2021-05-15 11:01:15,509] {docker.py:276} INFO - 21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Getting 3 (1312.0 B) non-empty blocks including 3 (1312.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:15,509] {docker.py:276} INFO - 21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:15,511] {docker.py:276} INFO - 21/05/15 14:01:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:15,511] {docker.py:276} INFO - 21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:15,512] {docker.py:276} INFO - 21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504742678714466905332_0002_m_000054_58, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504742678714466905332_0002_m_000054_58}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504742678714466905332_0002}; taskId=attempt_202105151400504742678714466905332_0002_m_000054_58, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bcf784f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:15,512] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400504742678714466905332_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504742678714466905332_0002_m_000054_58
[2021-05-15 11:01:15,515] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400504742678714466905332_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504742678714466905332_0002_m_000054_58 : duration 0:00.004s
[2021-05-15 11:01:15,753] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400501266717304057410768_0002_m_000051_55: needsTaskCommit() Task attempt_202105151400501266717304057410768_0002_m_000051_55
[2021-05-15 11:01:15,754] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400501266717304057410768_0002_m_000051_55: needsTaskCommit() Task attempt_202105151400501266717304057410768_0002_m_000051_55: duration 0:00.000s
21/05/15 14:01:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501266717304057410768_0002_m_000051_55
[2021-05-15 11:01:15,759] {docker.py:276} INFO - 21/05/15 14:01:15 INFO Executor: Finished task 51.0 in stage 2.0 (TID 55). 4587 bytes result sent to driver
21/05/15 14:01:15 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 59) (8951b5f85146, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:01:15 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 55) in 1707 ms on 8951b5f85146 (executor driver) (52/200)
21/05/15 14:01:15 INFO Executor: Running task 55.0 in stage 2.0 (TID 59)
[2021-05-15 11:01:15,768] {docker.py:276} INFO - 21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:15,770] {docker.py:276} INFO - 21/05/15 14:01:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:15,771] {docker.py:276} INFO - 21/05/15 14:01:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506725514289739356014_0002_m_000055_59, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506725514289739356014_0002_m_000055_59}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506725514289739356014_0002}; taskId=attempt_202105151400506725514289739356014_0002_m_000055_59, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a31d568}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400506725514289739356014_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506725514289739356014_0002_m_000055_59
[2021-05-15 11:01:15,773] {docker.py:276} INFO - 21/05/15 14:01:15 INFO StagingCommitter: Task committer attempt_202105151400506725514289739356014_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506725514289739356014_0002_m_000055_59 : duration 0:00.003s
[2021-05-15 11:01:16,922] {docker.py:276} INFO - 21/05/15 14:01:16 INFO StagingCommitter: Starting: Task committer attempt_20210515140050461446831170293769_0002_m_000052_56: needsTaskCommit() Task attempt_20210515140050461446831170293769_0002_m_000052_56
[2021-05-15 11:01:16,923] {docker.py:276} INFO - 21/05/15 14:01:16 INFO StagingCommitter: Task committer attempt_20210515140050461446831170293769_0002_m_000052_56: needsTaskCommit() Task attempt_20210515140050461446831170293769_0002_m_000052_56: duration 0:00.002s
[2021-05-15 11:01:16,924] {docker.py:276} INFO - 21/05/15 14:01:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050461446831170293769_0002_m_000052_56
[2021-05-15 11:01:16,925] {docker.py:276} INFO - 21/05/15 14:01:16 INFO Executor: Finished task 52.0 in stage 2.0 (TID 56). 4587 bytes result sent to driver
[2021-05-15 11:01:16,929] {docker.py:276} INFO - 21/05/15 14:01:16 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 60) (8951b5f85146, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:16,930] {docker.py:276} INFO - 21/05/15 14:01:16 INFO Executor: Running task 56.0 in stage 2.0 (TID 60)
[2021-05-15 11:01:16,931] {docker.py:276} INFO - 21/05/15 14:01:16 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 56) in 1686 ms on 8951b5f85146 (executor driver) (53/200)
[2021-05-15 11:01:16,940] {docker.py:276} INFO - 21/05/15 14:01:16 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:16,940] {docker.py:276} INFO - 21/05/15 14:01:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:16,942] {docker.py:276} INFO - 21/05/15 14:01:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:16,943] {docker.py:276} INFO - 21/05/15 14:01:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:16,943] {docker.py:276} INFO - 21/05/15 14:01:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502007903883597605469_0002_m_000056_60, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502007903883597605469_0002_m_000056_60}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502007903883597605469_0002}; taskId=attempt_202105151400502007903883597605469_0002_m_000056_60, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@712f6c7e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:16,943] {docker.py:276} INFO - 21/05/15 14:01:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:16,944] {docker.py:276} INFO - 21/05/15 14:01:16 INFO StagingCommitter: Starting: Task committer attempt_202105151400502007903883597605469_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502007903883597605469_0002_m_000056_60
[2021-05-15 11:01:16,946] {docker.py:276} INFO - 21/05/15 14:01:16 INFO StagingCommitter: Task committer attempt_202105151400502007903883597605469_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502007903883597605469_0002_m_000056_60 : duration 0:00.003s
[2021-05-15 11:01:17,023] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Starting: Task committer attempt_202105151400502207665639126994689_0002_m_000053_57: needsTaskCommit() Task attempt_202105151400502207665639126994689_0002_m_000053_57
[2021-05-15 11:01:17,024] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Task committer attempt_202105151400502207665639126994689_0002_m_000053_57: needsTaskCommit() Task attempt_202105151400502207665639126994689_0002_m_000053_57: duration 0:00.001s
21/05/15 14:01:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502207665639126994689_0002_m_000053_57
[2021-05-15 11:01:17,026] {docker.py:276} INFO - 21/05/15 14:01:17 INFO Executor: Finished task 53.0 in stage 2.0 (TID 57). 4587 bytes result sent to driver
[2021-05-15 11:01:17,028] {docker.py:276} INFO - 21/05/15 14:01:17 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 61) (8951b5f85146, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:17,029] {docker.py:276} INFO - 21/05/15 14:01:17 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 57) in 1734 ms on 8951b5f85146 (executor driver) (54/200)
[2021-05-15 11:01:17,030] {docker.py:276} INFO - 21/05/15 14:01:17 INFO Executor: Running task 57.0 in stage 2.0 (TID 61)
[2021-05-15 11:01:17,039] {docker.py:276} INFO - 21/05/15 14:01:17 INFO ShuffleBlockFetcherIterator: Getting 3 (1181.0 B) non-empty blocks including 3 (1181.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:17,042] {docker.py:276} INFO - 21/05/15 14:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:17,042] {docker.py:276} INFO - 21/05/15 14:01:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505431599529011754872_0002_m_000057_61, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505431599529011754872_0002_m_000057_61}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505431599529011754872_0002}; taskId=attempt_202105151400505431599529011754872_0002_m_000057_61, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5acb9cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:17,043] {docker.py:276} INFO - 21/05/15 14:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:17 INFO StagingCommitter: Starting: Task committer attempt_202105151400505431599529011754872_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505431599529011754872_0002_m_000057_61
[2021-05-15 11:01:17,045] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Task committer attempt_202105151400505431599529011754872_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505431599529011754872_0002_m_000057_61 : duration 0:00.002s
[2021-05-15 11:01:17,206] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Starting: Task committer attempt_202105151400504742678714466905332_0002_m_000054_58: needsTaskCommit() Task attempt_202105151400504742678714466905332_0002_m_000054_58
[2021-05-15 11:01:17,207] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Task committer attempt_202105151400504742678714466905332_0002_m_000054_58: needsTaskCommit() Task attempt_202105151400504742678714466905332_0002_m_000054_58: duration 0:00.002s
21/05/15 14:01:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504742678714466905332_0002_m_000054_58
[2021-05-15 11:01:17,211] {docker.py:276} INFO - 21/05/15 14:01:17 INFO Executor: Finished task 54.0 in stage 2.0 (TID 58). 4587 bytes result sent to driver
[2021-05-15 11:01:17,212] {docker.py:276} INFO - 21/05/15 14:01:17 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 62) (8951b5f85146, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:17,213] {docker.py:276} INFO - 21/05/15 14:01:17 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 58) in 1716 ms on 8951b5f85146 (executor driver) (55/200)
[2021-05-15 11:01:17,214] {docker.py:276} INFO - 21/05/15 14:01:17 INFO Executor: Running task 58.0 in stage 2.0 (TID 62)
[2021-05-15 11:01:17,230] {docker.py:276} INFO - 21/05/15 14:01:17 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:17,231] {docker.py:276} INFO - 21/05/15 14:01:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:17,232] {docker.py:276} INFO - 21/05/15 14:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:17,232] {docker.py:276} INFO - 21/05/15 14:01:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:17,233] {docker.py:276} INFO - 21/05/15 14:01:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050458856619517670877_0002_m_000058_62, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050458856619517670877_0002_m_000058_62}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050458856619517670877_0002}; taskId=attempt_20210515140050458856619517670877_0002_m_000058_62, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23e44e50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:17,233] {docker.py:276} INFO - 21/05/15 14:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:17,233] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Starting: Task committer attempt_20210515140050458856619517670877_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050458856619517670877_0002_m_000058_62
[2021-05-15 11:01:17,235] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Task committer attempt_20210515140050458856619517670877_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050458856619517670877_0002_m_000058_62 : duration 0:00.003s
[2021-05-15 11:01:17,438] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Starting: Task committer attempt_202105151400506725514289739356014_0002_m_000055_59: needsTaskCommit() Task attempt_202105151400506725514289739356014_0002_m_000055_59
[2021-05-15 11:01:17,439] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Task committer attempt_202105151400506725514289739356014_0002_m_000055_59: needsTaskCommit() Task attempt_202105151400506725514289739356014_0002_m_000055_59: duration 0:00.001s
21/05/15 14:01:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506725514289739356014_0002_m_000055_59
[2021-05-15 11:01:17,440] {docker.py:276} INFO - 21/05/15 14:01:17 INFO Executor: Finished task 55.0 in stage 2.0 (TID 59). 4587 bytes result sent to driver
[2021-05-15 11:01:17,442] {docker.py:276} INFO - 21/05/15 14:01:17 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 63) (8951b5f85146, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:17,444] {docker.py:276} INFO - 21/05/15 14:01:17 INFO Executor: Running task 59.0 in stage 2.0 (TID 63)
[2021-05-15 11:01:17,445] {docker.py:276} INFO - 21/05/15 14:01:17 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 59) in 1688 ms on 8951b5f85146 (executor driver) (56/200)
[2021-05-15 11:01:17,454] {docker.py:276} INFO - 21/05/15 14:01:17 INFO ShuffleBlockFetcherIterator: Getting 3 (1248.0 B) non-empty blocks including 3 (1248.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:17,456] {docker.py:276} INFO - 21/05/15 14:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:17,456] {docker.py:276} INFO - 21/05/15 14:01:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501101631521669935788_0002_m_000059_63, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501101631521669935788_0002_m_000059_63}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501101631521669935788_0002}; taskId=attempt_202105151400501101631521669935788_0002_m_000059_63, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4123b72c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:17 INFO StagingCommitter: Starting: Task committer attempt_202105151400501101631521669935788_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501101631521669935788_0002_m_000059_63
[2021-05-15 11:01:17,460] {docker.py:276} INFO - 21/05/15 14:01:17 INFO StagingCommitter: Task committer attempt_202105151400501101631521669935788_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501101631521669935788_0002_m_000059_63 : duration 0:00.003s
[2021-05-15 11:01:18,650] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400502007903883597605469_0002_m_000056_60: needsTaskCommit() Task attempt_202105151400502007903883597605469_0002_m_000056_60
[2021-05-15 11:01:18,651] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Task committer attempt_202105151400502007903883597605469_0002_m_000056_60: needsTaskCommit() Task attempt_202105151400502007903883597605469_0002_m_000056_60: duration 0:00.001s
21/05/15 14:01:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502007903883597605469_0002_m_000056_60
[2021-05-15 11:01:18,654] {docker.py:276} INFO - 21/05/15 14:01:18 INFO Executor: Finished task 56.0 in stage 2.0 (TID 60). 4587 bytes result sent to driver
[2021-05-15 11:01:18,656] {docker.py:276} INFO - 21/05/15 14:01:18 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 64) (8951b5f85146, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:18,658] {docker.py:276} INFO - 21/05/15 14:01:18 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 60) in 1733 ms on 8951b5f85146 (executor driver) (57/200)
[2021-05-15 11:01:18,659] {docker.py:276} INFO - 21/05/15 14:01:18 INFO Executor: Running task 60.0 in stage 2.0 (TID 64)
[2021-05-15 11:01:18,675] {docker.py:276} INFO - 21/05/15 14:01:18 INFO ShuffleBlockFetcherIterator: Getting 3 (1141.0 B) non-empty blocks including 3 (1141.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:18,675] {docker.py:276} INFO - 21/05/15 14:01:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:18,677] {docker.py:276} INFO - 21/05/15 14:01:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:18,678] {docker.py:276} INFO - 21/05/15 14:01:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:18,678] {docker.py:276} INFO - 21/05/15 14:01:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:18,679] {docker.py:276} INFO - 21/05/15 14:01:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502727524881701107868_0002_m_000060_64, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502727524881701107868_0002_m_000060_64}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502727524881701107868_0002}; taskId=attempt_202105151400502727524881701107868_0002_m_000060_64, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c7b998f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:18,679] {docker.py:276} INFO - 21/05/15 14:01:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:18,680] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400502727524881701107868_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502727524881701107868_0002_m_000060_64
[2021-05-15 11:01:18,683] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Task committer attempt_202105151400502727524881701107868_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502727524881701107868_0002_m_000060_64 : duration 0:00.003s
[2021-05-15 11:01:18,737] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400505431599529011754872_0002_m_000057_61: needsTaskCommit() Task attempt_202105151400505431599529011754872_0002_m_000057_61
[2021-05-15 11:01:18,738] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Task committer attempt_202105151400505431599529011754872_0002_m_000057_61: needsTaskCommit() Task attempt_202105151400505431599529011754872_0002_m_000057_61: duration 0:00.000s
21/05/15 14:01:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505431599529011754872_0002_m_000057_61
[2021-05-15 11:01:18,739] {docker.py:276} INFO - 21/05/15 14:01:18 INFO Executor: Finished task 57.0 in stage 2.0 (TID 61). 4587 bytes result sent to driver
[2021-05-15 11:01:18,740] {docker.py:276} INFO - 21/05/15 14:01:18 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 65) (8951b5f85146, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:18,741] {docker.py:276} INFO - 21/05/15 14:01:18 INFO Executor: Running task 61.0 in stage 2.0 (TID 65)
21/05/15 14:01:18 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 61) in 1716 ms on 8951b5f85146 (executor driver) (58/200)
[2021-05-15 11:01:18,750] {docker.py:276} INFO - 21/05/15 14:01:18 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:18,752] {docker.py:276} INFO - 21/05/15 14:01:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050514340031074639339_0002_m_000061_65, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050514340031074639339_0002_m_000061_65}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050514340031074639339_0002}; taskId=attempt_20210515140050514340031074639339_0002_m_000061_65, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@343ff53b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:18,752] {docker.py:276} INFO - 21/05/15 14:01:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:18 INFO StagingCommitter: Starting: Task committer attempt_20210515140050514340031074639339_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050514340031074639339_0002_m_000061_65
[2021-05-15 11:01:18,755] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Task committer attempt_20210515140050514340031074639339_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050514340031074639339_0002_m_000061_65 : duration 0:00.004s
[2021-05-15 11:01:18,910] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Starting: Task committer attempt_20210515140050458856619517670877_0002_m_000058_62: needsTaskCommit() Task attempt_20210515140050458856619517670877_0002_m_000058_62
[2021-05-15 11:01:18,911] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Task committer attempt_20210515140050458856619517670877_0002_m_000058_62: needsTaskCommit() Task attempt_20210515140050458856619517670877_0002_m_000058_62: duration 0:00.000s
[2021-05-15 11:01:18,912] {docker.py:276} INFO - 21/05/15 14:01:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050458856619517670877_0002_m_000058_62
[2021-05-15 11:01:18,915] {docker.py:276} INFO - 21/05/15 14:01:18 INFO Executor: Finished task 58.0 in stage 2.0 (TID 62). 4587 bytes result sent to driver
[2021-05-15 11:01:18,917] {docker.py:276} INFO - 21/05/15 14:01:18 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 66) (8951b5f85146, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:18,918] {docker.py:276} INFO - 21/05/15 14:01:18 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 62) in 1708 ms on 8951b5f85146 (executor driver) (59/200)
21/05/15 14:01:18 INFO Executor: Running task 62.0 in stage 2.0 (TID 66)
[2021-05-15 11:01:18,930] {docker.py:276} INFO - 21/05/15 14:01:18 INFO ShuffleBlockFetcherIterator: Getting 3 (1006.0 B) non-empty blocks including 3 (1006.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:18,937] {docker.py:276} INFO - 21/05/15 14:01:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:18,937] {docker.py:276} INFO - 21/05/15 14:01:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502458418931474854197_0002_m_000062_66, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502458418931474854197_0002_m_000062_66}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502458418931474854197_0002}; taskId=attempt_202105151400502458418931474854197_0002_m_000062_66, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@707e24a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:18,938] {docker.py:276} INFO - 21/05/15 14:01:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400502458418931474854197_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502458418931474854197_0002_m_000062_66
[2021-05-15 11:01:18,941] {docker.py:276} INFO - 21/05/15 14:01:18 INFO StagingCommitter: Task committer attempt_202105151400502458418931474854197_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502458418931474854197_0002_m_000062_66 : duration 0:00.003s
[2021-05-15 11:01:19,108] {docker.py:276} INFO - 21/05/15 14:01:19 INFO StagingCommitter: Starting: Task committer attempt_202105151400501101631521669935788_0002_m_000059_63: needsTaskCommit() Task attempt_202105151400501101631521669935788_0002_m_000059_63
21/05/15 14:01:19 INFO StagingCommitter: Task committer attempt_202105151400501101631521669935788_0002_m_000059_63: needsTaskCommit() Task attempt_202105151400501101631521669935788_0002_m_000059_63: duration 0:00.001s
[2021-05-15 11:01:19,108] {docker.py:276} INFO - 21/05/15 14:01:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501101631521669935788_0002_m_000059_63
[2021-05-15 11:01:19,111] {docker.py:276} INFO - 21/05/15 14:01:19 INFO Executor: Finished task 59.0 in stage 2.0 (TID 63). 4587 bytes result sent to driver
[2021-05-15 11:01:19,112] {docker.py:276} INFO - 21/05/15 14:01:19 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 67) (8951b5f85146, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:19,114] {docker.py:276} INFO - 21/05/15 14:01:19 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 63) in 1674 ms on 8951b5f85146 (executor driver) (60/200)
21/05/15 14:01:19 INFO Executor: Running task 63.0 in stage 2.0 (TID 67)
[2021-05-15 11:01:19,130] {docker.py:276} INFO - 21/05/15 14:01:19 INFO ShuffleBlockFetcherIterator: Getting 3 (1107.0 B) non-empty blocks including 3 (1107.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:19,132] {docker.py:276} INFO - 21/05/15 14:01:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:19,133] {docker.py:276} INFO - 21/05/15 14:01:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504116798218191697018_0002_m_000063_67, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504116798218191697018_0002_m_000063_67}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504116798218191697018_0002}; taskId=attempt_202105151400504116798218191697018_0002_m_000063_67, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ddf194b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:19,133] {docker.py:276} INFO - 21/05/15 14:01:19 INFO StagingCommitter: Starting: Task committer attempt_202105151400504116798218191697018_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504116798218191697018_0002_m_000063_67
[2021-05-15 11:01:19,136] {docker.py:276} INFO - 21/05/15 14:01:19 INFO StagingCommitter: Task committer attempt_202105151400504116798218191697018_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504116798218191697018_0002_m_000063_67 : duration 0:00.004s
[2021-05-15 11:01:20,364] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400502727524881701107868_0002_m_000060_64: needsTaskCommit() Task attempt_202105151400502727524881701107868_0002_m_000060_64
[2021-05-15 11:01:20,365] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400502727524881701107868_0002_m_000060_64: needsTaskCommit() Task attempt_202105151400502727524881701107868_0002_m_000060_64: duration 0:00.001s
21/05/15 14:01:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502727524881701107868_0002_m_000060_64
[2021-05-15 11:01:20,368] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Finished task 60.0 in stage 2.0 (TID 64). 4587 bytes result sent to driver
[2021-05-15 11:01:20,369] {docker.py:276} INFO - 21/05/15 14:01:20 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 68) (8951b5f85146, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:20,370] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Running task 64.0 in stage 2.0 (TID 68)
21/05/15 14:01:20 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 64) in 1717 ms on 8951b5f85146 (executor driver) (61/200)
[2021-05-15 11:01:20,387] {docker.py:276} INFO - 21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Getting 3 (1385.0 B) non-empty blocks including 3 (1385.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:20,389] {docker.py:276} INFO - 21/05/15 14:01:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:20,389] {docker.py:276} INFO - 21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508055049277901852066_0002_m_000064_68, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508055049277901852066_0002_m_000064_68}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508055049277901852066_0002}; taskId=attempt_202105151400508055049277901852066_0002_m_000064_68, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33ee2e84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:20,389] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400508055049277901852066_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508055049277901852066_0002_m_000064_68
[2021-05-15 11:01:20,392] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400508055049277901852066_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508055049277901852066_0002_m_000064_68 : duration 0:00.003s
[2021-05-15 11:01:20,405] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_20210515140050514340031074639339_0002_m_000061_65: needsTaskCommit() Task attempt_20210515140050514340031074639339_0002_m_000061_65
[2021-05-15 11:01:20,406] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_20210515140050514340031074639339_0002_m_000061_65: needsTaskCommit() Task attempt_20210515140050514340031074639339_0002_m_000061_65: duration 0:00.001s
[2021-05-15 11:01:20,406] {docker.py:276} INFO - 21/05/15 14:01:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050514340031074639339_0002_m_000061_65
[2021-05-15 11:01:20,408] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Finished task 61.0 in stage 2.0 (TID 65). 4587 bytes result sent to driver
[2021-05-15 11:01:20,408] {docker.py:276} INFO - 21/05/15 14:01:20 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 69) (8951b5f85146, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:20,409] {docker.py:276} INFO - 21/05/15 14:01:20 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 65) in 1672 ms on 8951b5f85146 (executor driver) (62/200)
[2021-05-15 11:01:20,410] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Running task 65.0 in stage 2.0 (TID 69)
[2021-05-15 11:01:20,418] {docker.py:276} INFO - 21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Getting 3 (1181.0 B) non-empty blocks including 3 (1181.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:20,420] {docker.py:276} INFO - 21/05/15 14:01:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:20,420] {docker.py:276} INFO - 21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503910765860382138900_0002_m_000065_69, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503910765860382138900_0002_m_000065_69}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503910765860382138900_0002}; taskId=attempt_202105151400503910765860382138900_0002_m_000065_69, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c5c9fd7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:20,421] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400503910765860382138900_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503910765860382138900_0002_m_000065_69
[2021-05-15 11:01:20,424] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400503910765860382138900_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503910765860382138900_0002_m_000065_69 : duration 0:00.003s
[2021-05-15 11:01:20,623] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400502458418931474854197_0002_m_000062_66: needsTaskCommit() Task attempt_202105151400502458418931474854197_0002_m_000062_66
[2021-05-15 11:01:20,624] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400502458418931474854197_0002_m_000062_66: needsTaskCommit() Task attempt_202105151400502458418931474854197_0002_m_000062_66: duration 0:00.001s
21/05/15 14:01:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502458418931474854197_0002_m_000062_66
[2021-05-15 11:01:20,626] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Finished task 62.0 in stage 2.0 (TID 66). 4587 bytes result sent to driver
[2021-05-15 11:01:20,627] {docker.py:276} INFO - 21/05/15 14:01:20 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 70) (8951b5f85146, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:20,628] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Running task 66.0 in stage 2.0 (TID 70)
21/05/15 14:01:20 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 66) in 1715 ms on 8951b5f85146 (executor driver) (63/200)
[2021-05-15 11:01:20,639] {docker.py:276} INFO - 21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:20,642] {docker.py:276} INFO - 21/05/15 14:01:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505262542950817191694_0002_m_000066_70, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505262542950817191694_0002_m_000066_70}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505262542950817191694_0002}; taskId=attempt_202105151400505262542950817191694_0002_m_000066_70, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5adcb1b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400505262542950817191694_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505262542950817191694_0002_m_000066_70
[2021-05-15 11:01:20,648] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400505262542950817191694_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505262542950817191694_0002_m_000066_70 : duration 0:00.005s
[2021-05-15 11:01:20,852] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400504116798218191697018_0002_m_000063_67: needsTaskCommit() Task attempt_202105151400504116798218191697018_0002_m_000063_67
[2021-05-15 11:01:20,853] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400504116798218191697018_0002_m_000063_67: needsTaskCommit() Task attempt_202105151400504116798218191697018_0002_m_000063_67: duration 0:00.000s
21/05/15 14:01:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504116798218191697018_0002_m_000063_67
[2021-05-15 11:01:20,855] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Finished task 63.0 in stage 2.0 (TID 67). 4587 bytes result sent to driver
[2021-05-15 11:01:20,856] {docker.py:276} INFO - 21/05/15 14:01:20 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 71) (8951b5f85146, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:20,857] {docker.py:276} INFO - 21/05/15 14:01:20 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 67) in 1747 ms on 8951b5f85146 (executor driver) (64/200)
[2021-05-15 11:01:20,858] {docker.py:276} INFO - 21/05/15 14:01:20 INFO Executor: Running task 67.0 in stage 2.0 (TID 71)
[2021-05-15 11:01:20,868] {docker.py:276} INFO - 21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Getting 3 (1181.0 B) non-empty blocks including 3 (1181.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:20,868] {docker.py:276} INFO - 21/05/15 14:01:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:20,870] {docker.py:276} INFO - 21/05/15 14:01:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:20,871] {docker.py:276} INFO - 21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:20,871] {docker.py:276} INFO - 21/05/15 14:01:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501486061076558200308_0002_m_000067_71, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501486061076558200308_0002_m_000067_71}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501486061076558200308_0002}; taskId=attempt_202105151400501486061076558200308_0002_m_000067_71, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73890a44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:20,872] {docker.py:276} INFO - 21/05/15 14:01:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:20,872] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Starting: Task committer attempt_202105151400501486061076558200308_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501486061076558200308_0002_m_000067_71
[2021-05-15 11:01:20,875] {docker.py:276} INFO - 21/05/15 14:01:20 INFO StagingCommitter: Task committer attempt_202105151400501486061076558200308_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501486061076558200308_0002_m_000067_71 : duration 0:00.003s
[2021-05-15 11:01:22,101] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400503910765860382138900_0002_m_000065_69: needsTaskCommit() Task attempt_202105151400503910765860382138900_0002_m_000065_69
[2021-05-15 11:01:22,103] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400503910765860382138900_0002_m_000065_69: needsTaskCommit() Task attempt_202105151400503910765860382138900_0002_m_000065_69: duration 0:00.002s
21/05/15 14:01:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503910765860382138900_0002_m_000065_69
[2021-05-15 11:01:22,104] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Finished task 65.0 in stage 2.0 (TID 69). 4587 bytes result sent to driver
[2021-05-15 11:01:22,106] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 72) (8951b5f85146, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:22,107] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Finished task 65.0 in stage 2.0 (TID 69) in 1666 ms on 8951b5f85146 (executor driver) (65/200)
[2021-05-15 11:01:22,109] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Running task 68.0 in stage 2.0 (TID 72)
[2021-05-15 11:01:22,118] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Getting 3 (1389.0 B) non-empty blocks including 3 (1389.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:22,118] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:22,120] {docker.py:276} INFO - 21/05/15 14:01:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:22,121] {docker.py:276} INFO - 21/05/15 14:01:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:22,121] {docker.py:276} INFO - 21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:22,121] {docker.py:276} INFO - 21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503028656936877774126_0002_m_000068_72, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503028656936877774126_0002_m_000068_72}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503028656936877774126_0002}; taskId=attempt_202105151400503028656936877774126_0002_m_000068_72, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44ee1d27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:22,122] {docker.py:276} INFO - 21/05/15 14:01:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400503028656936877774126_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503028656936877774126_0002_m_000068_72
[2021-05-15 11:01:22,126] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400503028656936877774126_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503028656936877774126_0002_m_000068_72 : duration 0:00.003s
[2021-05-15 11:01:22,158] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400508055049277901852066_0002_m_000064_68: needsTaskCommit() Task attempt_202105151400508055049277901852066_0002_m_000064_68
[2021-05-15 11:01:22,159] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400508055049277901852066_0002_m_000064_68: needsTaskCommit() Task attempt_202105151400508055049277901852066_0002_m_000064_68: duration 0:00.001s
21/05/15 14:01:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508055049277901852066_0002_m_000064_68
[2021-05-15 11:01:22,160] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Finished task 64.0 in stage 2.0 (TID 68). 4587 bytes result sent to driver
[2021-05-15 11:01:22,162] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 73) (8951b5f85146, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:22,163] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 68) in 1761 ms on 8951b5f85146 (executor driver) (66/200)
[2021-05-15 11:01:22,164] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Running task 69.0 in stage 2.0 (TID 73)
[2021-05-15 11:01:22,180] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Getting 3 (1474.0 B) non-empty blocks including 3 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:22,180] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:22,182] {docker.py:276} INFO - 21/05/15 14:01:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:22,183] {docker.py:276} INFO - 21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502218393944860108306_0002_m_000069_73, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502218393944860108306_0002_m_000069_73}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502218393944860108306_0002}; taskId=attempt_202105151400502218393944860108306_0002_m_000069_73, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a94d7e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:22,183] {docker.py:276} INFO - 21/05/15 14:01:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:22,183] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400502218393944860108306_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502218393944860108306_0002_m_000069_73
[2021-05-15 11:01:22,186] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400502218393944860108306_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502218393944860108306_0002_m_000069_73 : duration 0:00.004s
[2021-05-15 11:01:22,298] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400505262542950817191694_0002_m_000066_70: needsTaskCommit() Task attempt_202105151400505262542950817191694_0002_m_000066_70
21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400505262542950817191694_0002_m_000066_70: needsTaskCommit() Task attempt_202105151400505262542950817191694_0002_m_000066_70: duration 0:00.000s
[2021-05-15 11:01:22,299] {docker.py:276} INFO - 21/05/15 14:01:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505262542950817191694_0002_m_000066_70
[2021-05-15 11:01:22,301] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Finished task 66.0 in stage 2.0 (TID 70). 4587 bytes result sent to driver
[2021-05-15 11:01:22,302] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Starting task 70.0 in stage 2.0 (TID 74) (8951b5f85146, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:22,304] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Finished task 66.0 in stage 2.0 (TID 70) in 1643 ms on 8951b5f85146 (executor driver) (67/200)
[2021-05-15 11:01:22,304] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Running task 70.0 in stage 2.0 (TID 74)
[2021-05-15 11:01:22,321] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Getting 3 (1300.0 B) non-empty blocks including 3 (1300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:22,323] {docker.py:276} INFO - 21/05/15 14:01:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:22,323] {docker.py:276} INFO - 21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506127502626358092316_0002_m_000070_74, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506127502626358092316_0002_m_000070_74}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506127502626358092316_0002}; taskId=attempt_202105151400506127502626358092316_0002_m_000070_74, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@144cd6e6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:22,324] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400506127502626358092316_0002_m_000070_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506127502626358092316_0002_m_000070_74
[2021-05-15 11:01:22,326] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400506127502626358092316_0002_m_000070_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506127502626358092316_0002_m_000070_74 : duration 0:00.003s
[2021-05-15 11:01:22,565] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400501486061076558200308_0002_m_000067_71: needsTaskCommit() Task attempt_202105151400501486061076558200308_0002_m_000067_71
[2021-05-15 11:01:22,567] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400501486061076558200308_0002_m_000067_71: needsTaskCommit() Task attempt_202105151400501486061076558200308_0002_m_000067_71: duration 0:00.001s
21/05/15 14:01:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501486061076558200308_0002_m_000067_71
[2021-05-15 11:01:22,568] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Finished task 67.0 in stage 2.0 (TID 71). 4587 bytes result sent to driver
[2021-05-15 11:01:22,570] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Starting task 71.0 in stage 2.0 (TID 75) (8951b5f85146, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:22,572] {docker.py:276} INFO - 21/05/15 14:01:22 INFO Executor: Running task 71.0 in stage 2.0 (TID 75)
[2021-05-15 11:01:22,572] {docker.py:276} INFO - 21/05/15 14:01:22 INFO TaskSetManager: Finished task 67.0 in stage 2.0 (TID 71) in 1682 ms on 8951b5f85146 (executor driver) (68/200)
[2021-05-15 11:01:22,582] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Getting 3 (1226.0 B) non-empty blocks including 3 (1226.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:22,583] {docker.py:276} INFO - 21/05/15 14:01:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:22,585] {docker.py:276} INFO - 21/05/15 14:01:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:22,586] {docker.py:276} INFO - 21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:22,586] {docker.py:276} INFO - 21/05/15 14:01:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504110165233585616243_0002_m_000071_75, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504110165233585616243_0002_m_000071_75}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504110165233585616243_0002}; taskId=attempt_202105151400504110165233585616243_0002_m_000071_75, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f228a31}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:22,587] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Starting: Task committer attempt_202105151400504110165233585616243_0002_m_000071_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504110165233585616243_0002_m_000071_75
[2021-05-15 11:01:22,590] {docker.py:276} INFO - 21/05/15 14:01:22 INFO StagingCommitter: Task committer attempt_202105151400504110165233585616243_0002_m_000071_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504110165233585616243_0002_m_000071_75 : duration 0:00.004s
[2021-05-15 11:01:23,848] {docker.py:276} INFO - 21/05/15 14:01:23 INFO StagingCommitter: Starting: Task committer attempt_202105151400503028656936877774126_0002_m_000068_72: needsTaskCommit() Task attempt_202105151400503028656936877774126_0002_m_000068_72
[2021-05-15 11:01:23,849] {docker.py:276} INFO - 21/05/15 14:01:23 INFO StagingCommitter: Task committer attempt_202105151400503028656936877774126_0002_m_000068_72: needsTaskCommit() Task attempt_202105151400503028656936877774126_0002_m_000068_72: duration 0:00.001s
[2021-05-15 11:01:23,849] {docker.py:276} INFO - 21/05/15 14:01:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503028656936877774126_0002_m_000068_72
[2021-05-15 11:01:23,850] {docker.py:276} INFO - 21/05/15 14:01:23 INFO Executor: Finished task 68.0 in stage 2.0 (TID 72). 4587 bytes result sent to driver
[2021-05-15 11:01:23,852] {docker.py:276} INFO - 21/05/15 14:01:23 INFO TaskSetManager: Starting task 72.0 in stage 2.0 (TID 76) (8951b5f85146, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:23,853] {docker.py:276} INFO - 21/05/15 14:01:23 INFO Executor: Running task 72.0 in stage 2.0 (TID 76)
21/05/15 14:01:23 INFO TaskSetManager: Finished task 68.0 in stage 2.0 (TID 72) in 1750 ms on 8951b5f85146 (executor driver) (69/200)
[2021-05-15 11:01:23,869] {docker.py:276} INFO - 21/05/15 14:01:23 INFO ShuffleBlockFetcherIterator: Getting 3 (1434.0 B) non-empty blocks including 3 (1434.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:23,871] {docker.py:276} INFO - 21/05/15 14:01:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:23,872] {docker.py:276} INFO - 21/05/15 14:01:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:23,872] {docker.py:276} INFO - 21/05/15 14:01:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504216242453766114264_0002_m_000072_76, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504216242453766114264_0002_m_000072_76}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504216242453766114264_0002}; taskId=attempt_202105151400504216242453766114264_0002_m_000072_76, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69f34250}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:23 INFO StagingCommitter: Starting: Task committer attempt_202105151400504216242453766114264_0002_m_000072_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504216242453766114264_0002_m_000072_76
[2021-05-15 11:01:23,875] {docker.py:276} INFO - 21/05/15 14:01:23 INFO StagingCommitter: Task committer attempt_202105151400504216242453766114264_0002_m_000072_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504216242453766114264_0002_m_000072_76 : duration 0:00.003s
[2021-05-15 11:01:23,896] {docker.py:276} INFO - 21/05/15 14:01:23 INFO StagingCommitter: Starting: Task committer attempt_202105151400502218393944860108306_0002_m_000069_73: needsTaskCommit() Task attempt_202105151400502218393944860108306_0002_m_000069_73
[2021-05-15 11:01:23,897] {docker.py:276} INFO - 21/05/15 14:01:23 INFO StagingCommitter: Task committer attempt_202105151400502218393944860108306_0002_m_000069_73: needsTaskCommit() Task attempt_202105151400502218393944860108306_0002_m_000069_73: duration 0:00.001s
21/05/15 14:01:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502218393944860108306_0002_m_000069_73
[2021-05-15 11:01:23,898] {docker.py:276} INFO - 21/05/15 14:01:23 INFO Executor: Finished task 69.0 in stage 2.0 (TID 73). 4587 bytes result sent to driver
[2021-05-15 11:01:23,899] {docker.py:276} INFO - 21/05/15 14:01:23 INFO TaskSetManager: Starting task 73.0 in stage 2.0 (TID 77) (8951b5f85146, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:23,900] {docker.py:276} INFO - 21/05/15 14:01:23 INFO TaskSetManager: Finished task 69.0 in stage 2.0 (TID 73) in 1741 ms on 8951b5f85146 (executor driver) (70/200)
[2021-05-15 11:01:23,900] {docker.py:276} INFO - 21/05/15 14:01:23 INFO Executor: Running task 73.0 in stage 2.0 (TID 77)
[2021-05-15 11:01:23,909] {docker.py:276} INFO - 21/05/15 14:01:23 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:23,910] {docker.py:276} INFO - 21/05/15 14:01:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:23,911] {docker.py:276} INFO - 21/05/15 14:01:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050266835800128103234_0002_m_000073_77, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050266835800128103234_0002_m_000073_77}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050266835800128103234_0002}; taskId=attempt_20210515140050266835800128103234_0002_m_000073_77, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4626bd6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:23,911] {docker.py:276} INFO - 21/05/15 14:01:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:23 INFO StagingCommitter: Starting: Task committer attempt_20210515140050266835800128103234_0002_m_000073_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050266835800128103234_0002_m_000073_77
[2021-05-15 11:01:23,913] {docker.py:276} INFO - 21/05/15 14:01:23 INFO StagingCommitter: Task committer attempt_20210515140050266835800128103234_0002_m_000073_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050266835800128103234_0002_m_000073_77 : duration 0:00.002s
[2021-05-15 11:01:24,000] {docker.py:276} INFO - 21/05/15 14:01:24 INFO StagingCommitter: Starting: Task committer attempt_202105151400506127502626358092316_0002_m_000070_74: needsTaskCommit() Task attempt_202105151400506127502626358092316_0002_m_000070_74
[2021-05-15 11:01:24,001] {docker.py:276} INFO - 21/05/15 14:01:24 INFO StagingCommitter: Task committer attempt_202105151400506127502626358092316_0002_m_000070_74: needsTaskCommit() Task attempt_202105151400506127502626358092316_0002_m_000070_74: duration 0:00.000s
21/05/15 14:01:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506127502626358092316_0002_m_000070_74
[2021-05-15 11:01:24,003] {docker.py:276} INFO - 21/05/15 14:01:24 INFO Executor: Finished task 70.0 in stage 2.0 (TID 74). 4587 bytes result sent to driver
[2021-05-15 11:01:24,004] {docker.py:276} INFO - 21/05/15 14:01:24 INFO TaskSetManager: Starting task 74.0 in stage 2.0 (TID 78) (8951b5f85146, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:24,005] {docker.py:276} INFO - 21/05/15 14:01:24 INFO TaskSetManager: Finished task 70.0 in stage 2.0 (TID 74) in 1706 ms on 8951b5f85146 (executor driver) (71/200)
21/05/15 14:01:24 INFO Executor: Running task 74.0 in stage 2.0 (TID 78)
[2021-05-15 11:01:24,015] {docker.py:276} INFO - 21/05/15 14:01:24 INFO ShuffleBlockFetcherIterator: Getting 3 (1171.0 B) non-empty blocks including 3 (1171.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:24,016] {docker.py:276} INFO - 21/05/15 14:01:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:24,017] {docker.py:276} INFO - 21/05/15 14:01:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:24,017] {docker.py:276} INFO - 21/05/15 14:01:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:24,018] {docker.py:276} INFO - 21/05/15 14:01:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507921305475169703643_0002_m_000074_78, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507921305475169703643_0002_m_000074_78}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507921305475169703643_0002}; taskId=attempt_202105151400507921305475169703643_0002_m_000074_78, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78dd62e1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:24,018] {docker.py:276} INFO - 21/05/15 14:01:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:24 INFO StagingCommitter: Starting: Task committer attempt_202105151400507921305475169703643_0002_m_000074_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507921305475169703643_0002_m_000074_78
[2021-05-15 11:01:24,021] {docker.py:276} INFO - 21/05/15 14:01:24 INFO StagingCommitter: Task committer attempt_202105151400507921305475169703643_0002_m_000074_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507921305475169703643_0002_m_000074_78 : duration 0:00.003s
[2021-05-15 11:01:24,297] {docker.py:276} INFO - 21/05/15 14:01:24 INFO StagingCommitter: Starting: Task committer attempt_202105151400504110165233585616243_0002_m_000071_75: needsTaskCommit() Task attempt_202105151400504110165233585616243_0002_m_000071_75
[2021-05-15 11:01:24,297] {docker.py:276} INFO - 21/05/15 14:01:24 INFO StagingCommitter: Task committer attempt_202105151400504110165233585616243_0002_m_000071_75: needsTaskCommit() Task attempt_202105151400504110165233585616243_0002_m_000071_75: duration 0:00.001s
21/05/15 14:01:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504110165233585616243_0002_m_000071_75
[2021-05-15 11:01:24,299] {docker.py:276} INFO - 21/05/15 14:01:24 INFO Executor: Finished task 71.0 in stage 2.0 (TID 75). 4587 bytes result sent to driver
[2021-05-15 11:01:24,310] {docker.py:276} INFO - 21/05/15 14:01:24 INFO TaskSetManager: Starting task 75.0 in stage 2.0 (TID 79) (8951b5f85146, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:24,311] {docker.py:276} INFO - 21/05/15 14:01:24 INFO TaskSetManager: Finished task 71.0 in stage 2.0 (TID 75) in 1743 ms on 8951b5f85146 (executor driver) (72/200)
21/05/15 14:01:24 INFO Executor: Running task 75.0 in stage 2.0 (TID 79)
[2021-05-15 11:01:24,320] {docker.py:276} INFO - 21/05/15 14:01:24 INFO ShuffleBlockFetcherIterator: Getting 3 (1429.0 B) non-empty blocks including 3 (1429.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:24,320] {docker.py:276} INFO - 21/05/15 14:01:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:24,323] {docker.py:276} INFO - 21/05/15 14:01:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:24,324] {docker.py:276} INFO - 21/05/15 14:01:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:24,324] {docker.py:276} INFO - 21/05/15 14:01:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505056103903704496321_0002_m_000075_79, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505056103903704496321_0002_m_000075_79}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505056103903704496321_0002}; taskId=attempt_202105151400505056103903704496321_0002_m_000075_79, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@481d0b1d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:24 INFO StagingCommitter: Starting: Task committer attempt_202105151400505056103903704496321_0002_m_000075_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505056103903704496321_0002_m_000075_79
[2021-05-15 11:01:24,328] {docker.py:276} INFO - 21/05/15 14:01:24 INFO StagingCommitter: Task committer attempt_202105151400505056103903704496321_0002_m_000075_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505056103903704496321_0002_m_000075_79 : duration 0:00.003s
[2021-05-15 11:01:25,601] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_20210515140050266835800128103234_0002_m_000073_77: needsTaskCommit() Task attempt_20210515140050266835800128103234_0002_m_000073_77
21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_20210515140050266835800128103234_0002_m_000073_77: needsTaskCommit() Task attempt_20210515140050266835800128103234_0002_m_000073_77: duration 0:00.001s
[2021-05-15 11:01:25,602] {docker.py:276} INFO - 21/05/15 14:01:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050266835800128103234_0002_m_000073_77
[2021-05-15 11:01:25,603] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Finished task 73.0 in stage 2.0 (TID 77). 4587 bytes result sent to driver
[2021-05-15 11:01:25,605] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Starting task 76.0 in stage 2.0 (TID 80) (8951b5f85146, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:25,607] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Finished task 73.0 in stage 2.0 (TID 77) in 1709 ms on 8951b5f85146 (executor driver) (73/200)
[2021-05-15 11:01:25,607] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Running task 76.0 in stage 2.0 (TID 80)
[2021-05-15 11:01:25,624] {docker.py:276} INFO - 21/05/15 14:01:25 INFO ShuffleBlockFetcherIterator: Getting 3 (1215.0 B) non-empty blocks including 3 (1215.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:25,626] {docker.py:276} INFO - 21/05/15 14:01:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501633886641077028049_0002_m_000076_80, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501633886641077028049_0002_m_000076_80}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501633886641077028049_0002}; taskId=attempt_202105151400501633886641077028049_0002_m_000076_80, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ed9f69}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:25,626] {docker.py:276} INFO - 21/05/15 14:01:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_202105151400501633886641077028049_0002_m_000076_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501633886641077028049_0002_m_000076_80
[2021-05-15 11:01:25,628] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_202105151400504216242453766114264_0002_m_000072_76: needsTaskCommit() Task attempt_202105151400504216242453766114264_0002_m_000072_76
[2021-05-15 11:01:25,628] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_202105151400504216242453766114264_0002_m_000072_76: needsTaskCommit() Task attempt_202105151400504216242453766114264_0002_m_000072_76: duration 0:00.000s
21/05/15 14:01:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504216242453766114264_0002_m_000072_76
[2021-05-15 11:01:25,629] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Finished task 72.0 in stage 2.0 (TID 76). 4587 bytes result sent to driver
[2021-05-15 11:01:25,630] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_202105151400501633886641077028049_0002_m_000076_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501633886641077028049_0002_m_000076_80 : duration 0:00.004s
[2021-05-15 11:01:25,631] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Starting task 77.0 in stage 2.0 (TID 81) (8951b5f85146, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:25,632] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Finished task 72.0 in stage 2.0 (TID 76) in 1783 ms on 8951b5f85146 (executor driver) (74/200)
[2021-05-15 11:01:25,632] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Running task 77.0 in stage 2.0 (TID 81)
[2021-05-15 11:01:25,642] {docker.py:276} INFO - 21/05/15 14:01:25 INFO ShuffleBlockFetcherIterator: Getting 3 (1348.0 B) non-empty blocks including 3 (1348.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:25,643] {docker.py:276} INFO - 21/05/15 14:01:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501998066831275378950_0002_m_000077_81, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501998066831275378950_0002_m_000077_81}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501998066831275378950_0002}; taskId=attempt_202105151400501998066831275378950_0002_m_000077_81, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10ad9d76}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_202105151400501998066831275378950_0002_m_000077_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501998066831275378950_0002_m_000077_81
[2021-05-15 11:01:25,646] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_202105151400501998066831275378950_0002_m_000077_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501998066831275378950_0002_m_000077_81 : duration 0:00.003s
[2021-05-15 11:01:25,702] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_202105151400507921305475169703643_0002_m_000074_78: needsTaskCommit() Task attempt_202105151400507921305475169703643_0002_m_000074_78
[2021-05-15 11:01:25,702] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_202105151400507921305475169703643_0002_m_000074_78: needsTaskCommit() Task attempt_202105151400507921305475169703643_0002_m_000074_78: duration 0:00.001s
21/05/15 14:01:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507921305475169703643_0002_m_000074_78
[2021-05-15 11:01:25,705] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Finished task 74.0 in stage 2.0 (TID 78). 4587 bytes result sent to driver
[2021-05-15 11:01:25,707] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Starting task 78.0 in stage 2.0 (TID 82) (8951b5f85146, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:25,708] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Running task 78.0 in stage 2.0 (TID 82)
[2021-05-15 11:01:25,709] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Finished task 74.0 in stage 2.0 (TID 78) in 1707 ms on 8951b5f85146 (executor driver) (75/200)
[2021-05-15 11:01:25,719] {docker.py:276} INFO - 21/05/15 14:01:25 INFO ShuffleBlockFetcherIterator: Getting 3 (1389.0 B) non-empty blocks including 3 (1389.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:25,721] {docker.py:276} INFO - 21/05/15 14:01:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506388218533580048022_0002_m_000078_82, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506388218533580048022_0002_m_000078_82}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506388218533580048022_0002}; taskId=attempt_202105151400506388218533580048022_0002_m_000078_82, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3989dab7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:25,722] {docker.py:276} INFO - 21/05/15 14:01:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:25,722] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_202105151400506388218533580048022_0002_m_000078_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506388218533580048022_0002_m_000078_82
[2021-05-15 11:01:25,725] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_202105151400506388218533580048022_0002_m_000078_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506388218533580048022_0002_m_000078_82 : duration 0:00.003s
[2021-05-15 11:01:25,990] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Starting: Task committer attempt_202105151400505056103903704496321_0002_m_000075_79: needsTaskCommit() Task attempt_202105151400505056103903704496321_0002_m_000075_79
[2021-05-15 11:01:25,991] {docker.py:276} INFO - 21/05/15 14:01:25 INFO StagingCommitter: Task committer attempt_202105151400505056103903704496321_0002_m_000075_79: needsTaskCommit() Task attempt_202105151400505056103903704496321_0002_m_000075_79: duration 0:00.001s
21/05/15 14:01:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505056103903704496321_0002_m_000075_79
[2021-05-15 11:01:25,993] {docker.py:276} INFO - 21/05/15 14:01:25 INFO Executor: Finished task 75.0 in stage 2.0 (TID 79). 4587 bytes result sent to driver
[2021-05-15 11:01:25,995] {docker.py:276} INFO - 21/05/15 14:01:25 INFO TaskSetManager: Starting task 79.0 in stage 2.0 (TID 83) (8951b5f85146, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:25,996] {docker.py:276} INFO - 21/05/15 14:01:26 INFO TaskSetManager: Finished task 75.0 in stage 2.0 (TID 79) in 1688 ms on 8951b5f85146 (executor driver) (76/200)
[2021-05-15 11:01:25,998] {docker.py:276} INFO - 21/05/15 14:01:26 INFO Executor: Running task 79.0 in stage 2.0 (TID 83)
[2021-05-15 11:01:26,013] {docker.py:276} INFO - 21/05/15 14:01:26 INFO ShuffleBlockFetcherIterator: Getting 3 (1348.0 B) non-empty blocks including 3 (1348.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:26,014] {docker.py:276} INFO - 21/05/15 14:01:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:26,015] {docker.py:276} INFO - 21/05/15 14:01:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:26,016] {docker.py:276} INFO - 21/05/15 14:01:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:26,016] {docker.py:276} INFO - 21/05/15 14:01:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:26,016] {docker.py:276} INFO - 21/05/15 14:01:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506076955788584034488_0002_m_000079_83, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506076955788584034488_0002_m_000079_83}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506076955788584034488_0002}; taskId=attempt_202105151400506076955788584034488_0002_m_000079_83, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ca68061}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:26,017] {docker.py:276} INFO - 21/05/15 14:01:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:26,017] {docker.py:276} INFO - 21/05/15 14:01:26 INFO StagingCommitter: Starting: Task committer attempt_202105151400506076955788584034488_0002_m_000079_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506076955788584034488_0002_m_000079_83
[2021-05-15 11:01:26,020] {docker.py:276} INFO - 21/05/15 14:01:26 INFO StagingCommitter: Task committer attempt_202105151400506076955788584034488_0002_m_000079_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506076955788584034488_0002_m_000079_83 : duration 0:00.003s
[2021-05-15 11:01:27,351] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400501633886641077028049_0002_m_000076_80: needsTaskCommit() Task attempt_202105151400501633886641077028049_0002_m_000076_80
[2021-05-15 11:01:27,353] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400501633886641077028049_0002_m_000076_80: needsTaskCommit() Task attempt_202105151400501633886641077028049_0002_m_000076_80: duration 0:00.002s
21/05/15 14:01:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501633886641077028049_0002_m_000076_80
[2021-05-15 11:01:27,356] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Finished task 76.0 in stage 2.0 (TID 80). 4587 bytes result sent to driver
[2021-05-15 11:01:27,357] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Starting task 80.0 in stage 2.0 (TID 84) (8951b5f85146, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:27,358] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Finished task 76.0 in stage 2.0 (TID 80) in 1756 ms on 8951b5f85146 (executor driver) (77/200)
21/05/15 14:01:27 INFO Executor: Running task 80.0 in stage 2.0 (TID 84)
[2021-05-15 11:01:27,373] {docker.py:276} INFO - 21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:27,374] {docker.py:276} INFO - 21/05/15 14:01:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506294068271944892905_0002_m_000080_84, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506294068271944892905_0002_m_000080_84}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506294068271944892905_0002}; taskId=attempt_202105151400506294068271944892905_0002_m_000080_84, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d28acf4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400506294068271944892905_0002_m_000080_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506294068271944892905_0002_m_000080_84
[2021-05-15 11:01:27,377] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400506294068271944892905_0002_m_000080_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506294068271944892905_0002_m_000080_84 : duration 0:00.002s
[2021-05-15 11:01:27,397] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400506388218533580048022_0002_m_000078_82: needsTaskCommit() Task attempt_202105151400506388218533580048022_0002_m_000078_82
[2021-05-15 11:01:27,398] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400506388218533580048022_0002_m_000078_82: needsTaskCommit() Task attempt_202105151400506388218533580048022_0002_m_000078_82: duration 0:00.001s
[2021-05-15 11:01:27,398] {docker.py:276} INFO - 21/05/15 14:01:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506388218533580048022_0002_m_000078_82
[2021-05-15 11:01:27,400] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Finished task 78.0 in stage 2.0 (TID 82). 4587 bytes result sent to driver
[2021-05-15 11:01:27,401] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Starting task 81.0 in stage 2.0 (TID 85) (8951b5f85146, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:27,402] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Finished task 78.0 in stage 2.0 (TID 82) in 1698 ms on 8951b5f85146 (executor driver) (78/200)
[2021-05-15 11:01:27,403] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Running task 81.0 in stage 2.0 (TID 85)
[2021-05-15 11:01:27,403] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400501998066831275378950_0002_m_000077_81: needsTaskCommit() Task attempt_202105151400501998066831275378950_0002_m_000077_81
[2021-05-15 11:01:27,404] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400501998066831275378950_0002_m_000077_81: needsTaskCommit() Task attempt_202105151400501998066831275378950_0002_m_000077_81: duration 0:00.001s
21/05/15 14:01:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501998066831275378950_0002_m_000077_81
[2021-05-15 11:01:27,404] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Finished task 77.0 in stage 2.0 (TID 81). 4587 bytes result sent to driver
[2021-05-15 11:01:27,405] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Starting task 82.0 in stage 2.0 (TID 86) (8951b5f85146, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:27,406] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Finished task 77.0 in stage 2.0 (TID 81) in 1778 ms on 8951b5f85146 (executor driver) (79/200)
[2021-05-15 11:01:27,407] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Running task 82.0 in stage 2.0 (TID 86)
[2021-05-15 11:01:27,413] {docker.py:276} INFO - 21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Getting 3 (1275.0 B) non-empty blocks including 3 (1275.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:27,413] {docker.py:276} INFO - 21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:27,415] {docker.py:276} INFO - 21/05/15 14:01:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:27,416] {docker.py:276} INFO - 21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:27,416] {docker.py:276} INFO - 21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505776595251640665907_0002_m_000081_85, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505776595251640665907_0002_m_000081_85}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505776595251640665907_0002}; taskId=attempt_202105151400505776595251640665907_0002_m_000081_85, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5778b04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:27,417] {docker.py:276} INFO - 21/05/15 14:01:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400505776595251640665907_0002_m_000081_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505776595251640665907_0002_m_000081_85
[2021-05-15 11:01:27,417] {docker.py:276} INFO - 21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Getting 3 (1147.0 B) non-empty blocks including 3 (1147.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:27,417] {docker.py:276} INFO - 21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:27,418] {docker.py:276} INFO - 21/05/15 14:01:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:27,419] {docker.py:276} INFO - 21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503145988345902145945_0002_m_000082_86, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503145988345902145945_0002_m_000082_86}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503145988345902145945_0002}; taskId=attempt_202105151400503145988345902145945_0002_m_000082_86, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40f73e1c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400503145988345902145945_0002_m_000082_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503145988345902145945_0002_m_000082_86
[2021-05-15 11:01:27,419] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400505776595251640665907_0002_m_000081_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505776595251640665907_0002_m_000081_85 : duration 0:00.004s
[2021-05-15 11:01:27,423] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400503145988345902145945_0002_m_000082_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503145988345902145945_0002_m_000082_86 : duration 0:00.004s
[2021-05-15 11:01:27,699] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400506076955788584034488_0002_m_000079_83: needsTaskCommit() Task attempt_202105151400506076955788584034488_0002_m_000079_83
[2021-05-15 11:01:27,700] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400506076955788584034488_0002_m_000079_83: needsTaskCommit() Task attempt_202105151400506076955788584034488_0002_m_000079_83: duration 0:00.001s
21/05/15 14:01:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506076955788584034488_0002_m_000079_83
[2021-05-15 11:01:27,702] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Finished task 79.0 in stage 2.0 (TID 83). 4587 bytes result sent to driver
[2021-05-15 11:01:27,703] {docker.py:276} INFO - 21/05/15 14:01:27 INFO TaskSetManager: Starting task 83.0 in stage 2.0 (TID 87) (8951b5f85146, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:27,704] {docker.py:276} INFO - 21/05/15 14:01:27 INFO Executor: Running task 83.0 in stage 2.0 (TID 87)
21/05/15 14:01:27 INFO TaskSetManager: Finished task 79.0 in stage 2.0 (TID 83) in 1712 ms on 8951b5f85146 (executor driver) (80/200)
[2021-05-15 11:01:27,718] {docker.py:276} INFO - 21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Getting 3 (1126.0 B) non-empty blocks including 3 (1126.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:27,721] {docker.py:276} INFO - 21/05/15 14:01:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:27,721] {docker.py:276} INFO - 21/05/15 14:01:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508399189931646488970_0002_m_000083_87, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508399189931646488970_0002_m_000083_87}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508399189931646488970_0002}; taskId=attempt_202105151400508399189931646488970_0002_m_000083_87, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3af1a65}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:27 INFO StagingCommitter: Starting: Task committer attempt_202105151400508399189931646488970_0002_m_000083_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508399189931646488970_0002_m_000083_87
[2021-05-15 11:01:27,724] {docker.py:276} INFO - 21/05/15 14:01:27 INFO StagingCommitter: Task committer attempt_202105151400508399189931646488970_0002_m_000083_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508399189931646488970_0002_m_000083_87 : duration 0:00.004s
[2021-05-15 11:01:29,068] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400506294068271944892905_0002_m_000080_84: needsTaskCommit() Task attempt_202105151400506294068271944892905_0002_m_000080_84
[2021-05-15 11:01:29,068] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400506294068271944892905_0002_m_000080_84: needsTaskCommit() Task attempt_202105151400506294068271944892905_0002_m_000080_84: duration 0:00.001s
21/05/15 14:01:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506294068271944892905_0002_m_000080_84
[2021-05-15 11:01:29,071] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Finished task 80.0 in stage 2.0 (TID 84). 4587 bytes result sent to driver
[2021-05-15 11:01:29,073] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Finished task 80.0 in stage 2.0 (TID 84) in 1718 ms on 8951b5f85146 (executor driver) (81/200)
[2021-05-15 11:01:29,075] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Starting task 84.0 in stage 2.0 (TID 88) (8951b5f85146, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:29,076] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Running task 84.0 in stage 2.0 (TID 88)
[2021-05-15 11:01:29,092] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Getting 3 (1474.0 B) non-empty blocks including 3 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:29,093] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400505776595251640665907_0002_m_000081_85: needsTaskCommit() Task attempt_202105151400505776595251640665907_0002_m_000081_85
[2021-05-15 11:01:29,094] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400505776595251640665907_0002_m_000081_85: needsTaskCommit() Task attempt_202105151400505776595251640665907_0002_m_000081_85: duration 0:00.001s
21/05/15 14:01:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505776595251640665907_0002_m_000081_85
[2021-05-15 11:01:29,094] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400503145988345902145945_0002_m_000082_86: needsTaskCommit() Task attempt_202105151400503145988345902145945_0002_m_000082_86
[2021-05-15 11:01:29,095] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Finished task 81.0 in stage 2.0 (TID 85). 4587 bytes result sent to driver
[2021-05-15 11:01:29,096] {docker.py:276} INFO - 21/05/15 14:01:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:29,097] {docker.py:276} INFO - 21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508163035348928349055_0002_m_000084_88, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508163035348928349055_0002_m_000084_88}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508163035348928349055_0002}; taskId=attempt_202105151400508163035348928349055_0002_m_000084_88, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f652c41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:29,097] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400508163035348928349055_0002_m_000084_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508163035348928349055_0002_m_000084_88
[2021-05-15 11:01:29,098] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400503145988345902145945_0002_m_000082_86: needsTaskCommit() Task attempt_202105151400503145988345902145945_0002_m_000082_86: duration 0:00.001s
[2021-05-15 11:01:29,098] {docker.py:276} INFO - 21/05/15 14:01:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503145988345902145945_0002_m_000082_86
[2021-05-15 11:01:29,098] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Starting task 85.0 in stage 2.0 (TID 89) (8951b5f85146, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:29,100] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Finished task 81.0 in stage 2.0 (TID 85) in 1700 ms on 8951b5f85146 (executor driver) (82/200)
[2021-05-15 11:01:29,100] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Running task 85.0 in stage 2.0 (TID 89)
[2021-05-15 11:01:29,102] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400508163035348928349055_0002_m_000084_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508163035348928349055_0002_m_000084_88 : duration 0:00.006s
[2021-05-15 11:01:29,102] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Finished task 82.0 in stage 2.0 (TID 86). 4587 bytes result sent to driver
[2021-05-15 11:01:29,103] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Starting task 86.0 in stage 2.0 (TID 90) (8951b5f85146, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:29,104] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Finished task 82.0 in stage 2.0 (TID 86) in 1701 ms on 8951b5f85146 (executor driver) (83/200)
[2021-05-15 11:01:29,105] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Running task 86.0 in stage 2.0 (TID 90)
[2021-05-15 11:01:29,113] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Getting 3 (1528.0 B) non-empty blocks including 3 (1528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:29,114] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:29,117] {docker.py:276} INFO - 21/05/15 14:01:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:29,118] {docker.py:276} INFO - 21/05/15 14:01:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:29,119] {docker.py:276} INFO - 21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:29,119] {docker.py:276} INFO - 21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507406190388398748428_0002_m_000085_89, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507406190388398748428_0002_m_000085_89}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507406190388398748428_0002}; taskId=attempt_202105151400507406190388398748428_0002_m_000085_89, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ebc74a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:29,120] {docker.py:276} INFO - 21/05/15 14:01:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:29,120] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400507406190388398748428_0002_m_000085_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507406190388398748428_0002_m_000085_89
[2021-05-15 11:01:29,122] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Getting 3 (1363.0 B) non-empty blocks including 3 (1363.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:29,123] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:29,124] {docker.py:276} INFO - 21/05/15 14:01:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503094580468897663378_0002_m_000086_90, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503094580468897663378_0002_m_000086_90}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503094580468897663378_0002}; taskId=attempt_202105151400503094580468897663378_0002_m_000086_90, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@372059b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:29,125] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400503094580468897663378_0002_m_000086_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503094580468897663378_0002_m_000086_90
[2021-05-15 11:01:29,126] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400507406190388398748428_0002_m_000085_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507406190388398748428_0002_m_000085_89 : duration 0:00.006s
[2021-05-15 11:01:29,128] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400503094580468897663378_0002_m_000086_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503094580468897663378_0002_m_000086_90 : duration 0:00.004s
[2021-05-15 11:01:29,388] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400508399189931646488970_0002_m_000083_87: needsTaskCommit() Task attempt_202105151400508399189931646488970_0002_m_000083_87
[2021-05-15 11:01:29,390] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400508399189931646488970_0002_m_000083_87: needsTaskCommit() Task attempt_202105151400508399189931646488970_0002_m_000083_87: duration 0:00.002s
[2021-05-15 11:01:29,391] {docker.py:276} INFO - 21/05/15 14:01:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508399189931646488970_0002_m_000083_87
[2021-05-15 11:01:29,392] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Finished task 83.0 in stage 2.0 (TID 87). 4587 bytes result sent to driver
[2021-05-15 11:01:29,393] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Starting task 87.0 in stage 2.0 (TID 91) (8951b5f85146, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:29,402] {docker.py:276} INFO - 21/05/15 14:01:29 INFO Executor: Running task 87.0 in stage 2.0 (TID 91)
[2021-05-15 11:01:29,403] {docker.py:276} INFO - 21/05/15 14:01:29 INFO TaskSetManager: Finished task 83.0 in stage 2.0 (TID 87) in 1703 ms on 8951b5f85146 (executor driver) (84/200)
[2021-05-15 11:01:29,413] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:29,413] {docker.py:276} INFO - 21/05/15 14:01:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:29,415] {docker.py:276} INFO - 21/05/15 14:01:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:29,415] {docker.py:276} INFO - 21/05/15 14:01:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505387325964754636967_0002_m_000087_91, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505387325964754636967_0002_m_000087_91}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505387325964754636967_0002}; taskId=attempt_202105151400505387325964754636967_0002_m_000087_91, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c851f78}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:29,416] {docker.py:276} INFO - 21/05/15 14:01:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:29 INFO StagingCommitter: Starting: Task committer attempt_202105151400505387325964754636967_0002_m_000087_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505387325964754636967_0002_m_000087_91
[2021-05-15 11:01:29,419] {docker.py:276} INFO - 21/05/15 14:01:29 INFO StagingCommitter: Task committer attempt_202105151400505387325964754636967_0002_m_000087_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505387325964754636967_0002_m_000087_91 : duration 0:00.003s
[2021-05-15 11:01:30,768] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Starting: Task committer attempt_202105151400508163035348928349055_0002_m_000084_88: needsTaskCommit() Task attempt_202105151400508163035348928349055_0002_m_000084_88
[2021-05-15 11:01:30,769] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Starting: Task committer attempt_202105151400503094580468897663378_0002_m_000086_90: needsTaskCommit() Task attempt_202105151400503094580468897663378_0002_m_000086_90
[2021-05-15 11:01:30,770] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Task committer attempt_202105151400503094580468897663378_0002_m_000086_90: needsTaskCommit() Task attempt_202105151400503094580468897663378_0002_m_000086_90: duration 0:00.001s
21/05/15 14:01:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503094580468897663378_0002_m_000086_90
21/05/15 14:01:30 INFO StagingCommitter: Task committer attempt_202105151400508163035348928349055_0002_m_000084_88: needsTaskCommit() Task attempt_202105151400508163035348928349055_0002_m_000084_88: duration 0:00.002s
[2021-05-15 11:01:30,770] {docker.py:276} INFO - 21/05/15 14:01:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508163035348928349055_0002_m_000084_88
[2021-05-15 11:01:30,771] {docker.py:276} INFO - 21/05/15 14:01:30 INFO Executor: Finished task 86.0 in stage 2.0 (TID 90). 4587 bytes result sent to driver
[2021-05-15 11:01:30,773] {docker.py:276} INFO - 21/05/15 14:01:30 INFO Executor: Finished task 84.0 in stage 2.0 (TID 88). 4587 bytes result sent to driver
[2021-05-15 11:01:30,774] {docker.py:276} INFO - 21/05/15 14:01:30 INFO TaskSetManager: Starting task 88.0 in stage 2.0 (TID 92) (8951b5f85146, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:30,775] {docker.py:276} INFO - 21/05/15 14:01:30 INFO TaskSetManager: Finished task 86.0 in stage 2.0 (TID 90) in 1674 ms on 8951b5f85146 (executor driver) (85/200)
[2021-05-15 11:01:30,776] {docker.py:276} INFO - 21/05/15 14:01:30 INFO Executor: Running task 88.0 in stage 2.0 (TID 92)
[2021-05-15 11:01:30,777] {docker.py:276} INFO - 21/05/15 14:01:30 INFO TaskSetManager: Starting task 89.0 in stage 2.0 (TID 93) (8951b5f85146, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:30,778] {docker.py:276} INFO - 21/05/15 14:01:30 INFO TaskSetManager: Finished task 84.0 in stage 2.0 (TID 88) in 1706 ms on 8951b5f85146 (executor driver) (86/200)
[2021-05-15 11:01:30,779] {docker.py:276} INFO - 21/05/15 14:01:30 INFO Executor: Running task 89.0 in stage 2.0 (TID 93)
[2021-05-15 11:01:30,787] {docker.py:276} INFO - 21/05/15 14:01:30 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:30 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:30,788] {docker.py:276} INFO - 21/05/15 14:01:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:30,788] {docker.py:276} INFO - 21/05/15 14:01:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:30,790] {docker.py:276} INFO - 21/05/15 14:01:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:30,791] {docker.py:276} INFO - 21/05/15 14:01:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506564596913808143586_0002_m_000088_92, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506564596913808143586_0002_m_000088_92}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506564596913808143586_0002}; taskId=attempt_202105151400506564596913808143586_0002_m_000088_92, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20e92144}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:30,792] {docker.py:276} INFO - 21/05/15 14:01:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:30 INFO StagingCommitter: Starting: Task committer attempt_202105151400506564596913808143586_0002_m_000088_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506564596913808143586_0002_m_000088_92
[2021-05-15 11:01:30,792] {docker.py:276} INFO - 21/05/15 14:01:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:30,793] {docker.py:276} INFO - 21/05/15 14:01:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:30,793] {docker.py:276} INFO - 21/05/15 14:01:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505801073716507536570_0002_m_000089_93, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505801073716507536570_0002_m_000089_93}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505801073716507536570_0002}; taskId=attempt_202105151400505801073716507536570_0002_m_000089_93, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69f1812d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:30,793] {docker.py:276} INFO - 21/05/15 14:01:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:30,794] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Starting: Task committer attempt_202105151400505801073716507536570_0002_m_000089_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505801073716507536570_0002_m_000089_93
[2021-05-15 11:01:30,794] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Task committer attempt_202105151400506564596913808143586_0002_m_000088_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506564596913808143586_0002_m_000088_92 : duration 0:00.004s
[2021-05-15 11:01:30,795] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Task committer attempt_202105151400505801073716507536570_0002_m_000089_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505801073716507536570_0002_m_000089_93 : duration 0:00.003s
[2021-05-15 11:01:30,803] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Starting: Task committer attempt_202105151400507406190388398748428_0002_m_000085_89: needsTaskCommit() Task attempt_202105151400507406190388398748428_0002_m_000085_89
[2021-05-15 11:01:30,803] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Task committer attempt_202105151400507406190388398748428_0002_m_000085_89: needsTaskCommit() Task attempt_202105151400507406190388398748428_0002_m_000085_89: duration 0:00.000s
21/05/15 14:01:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507406190388398748428_0002_m_000085_89
[2021-05-15 11:01:30,804] {docker.py:276} INFO - 21/05/15 14:01:30 INFO Executor: Finished task 85.0 in stage 2.0 (TID 89). 4587 bytes result sent to driver
[2021-05-15 11:01:30,805] {docker.py:276} INFO - 21/05/15 14:01:30 INFO TaskSetManager: Starting task 90.0 in stage 2.0 (TID 94) (8951b5f85146, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:30,806] {docker.py:276} INFO - 21/05/15 14:01:30 INFO Executor: Running task 90.0 in stage 2.0 (TID 94)
[2021-05-15 11:01:30,807] {docker.py:276} INFO - 21/05/15 14:01:30 INFO TaskSetManager: Finished task 85.0 in stage 2.0 (TID 89) in 1712 ms on 8951b5f85146 (executor driver) (87/200)
[2021-05-15 11:01:30,819] {docker.py:276} INFO - 21/05/15 14:01:30 INFO ShuffleBlockFetcherIterator: Getting 3 (1141.0 B) non-empty blocks including 3 (1141.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:30,820] {docker.py:276} INFO - 21/05/15 14:01:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:30,821] {docker.py:276} INFO - 21/05/15 14:01:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:30,822] {docker.py:276} INFO - 21/05/15 14:01:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504220957057456455263_0002_m_000090_94, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504220957057456455263_0002_m_000090_94}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504220957057456455263_0002}; taskId=attempt_202105151400504220957057456455263_0002_m_000090_94, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36b7bb8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:30,822] {docker.py:276} INFO - 21/05/15 14:01:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:30,823] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Starting: Task committer attempt_202105151400504220957057456455263_0002_m_000090_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504220957057456455263_0002_m_000090_94
[2021-05-15 11:01:30,825] {docker.py:276} INFO - 21/05/15 14:01:30 INFO StagingCommitter: Task committer attempt_202105151400504220957057456455263_0002_m_000090_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504220957057456455263_0002_m_000090_94 : duration 0:00.003s
[2021-05-15 11:01:31,114] {docker.py:276} INFO - 21/05/15 14:01:31 INFO StagingCommitter: Starting: Task committer attempt_202105151400505387325964754636967_0002_m_000087_91: needsTaskCommit() Task attempt_202105151400505387325964754636967_0002_m_000087_91
[2021-05-15 11:01:31,115] {docker.py:276} INFO - 21/05/15 14:01:31 INFO StagingCommitter: Task committer attempt_202105151400505387325964754636967_0002_m_000087_91: needsTaskCommit() Task attempt_202105151400505387325964754636967_0002_m_000087_91: duration 0:00.001s
[2021-05-15 11:01:31,116] {docker.py:276} INFO - 21/05/15 14:01:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505387325964754636967_0002_m_000087_91
[2021-05-15 11:01:31,117] {docker.py:276} INFO - 21/05/15 14:01:31 INFO Executor: Finished task 87.0 in stage 2.0 (TID 91). 4587 bytes result sent to driver
[2021-05-15 11:01:31,119] {docker.py:276} INFO - 21/05/15 14:01:31 INFO TaskSetManager: Starting task 91.0 in stage 2.0 (TID 95) (8951b5f85146, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:31,120] {docker.py:276} INFO - 21/05/15 14:01:31 INFO TaskSetManager: Finished task 87.0 in stage 2.0 (TID 91) in 1727 ms on 8951b5f85146 (executor driver) (88/200)
[2021-05-15 11:01:31,120] {docker.py:276} INFO - 21/05/15 14:01:31 INFO Executor: Running task 91.0 in stage 2.0 (TID 95)
[2021-05-15 11:01:31,130] {docker.py:276} INFO - 21/05/15 14:01:31 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:31,132] {docker.py:276} INFO - 21/05/15 14:01:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:31,132] {docker.py:276} INFO - 21/05/15 14:01:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507077107538047799844_0002_m_000091_95, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507077107538047799844_0002_m_000091_95}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507077107538047799844_0002}; taskId=attempt_202105151400507077107538047799844_0002_m_000091_95, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ee4e25e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:31,133] {docker.py:276} INFO - 21/05/15 14:01:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:31 INFO StagingCommitter: Starting: Task committer attempt_202105151400507077107538047799844_0002_m_000091_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507077107538047799844_0002_m_000091_95
[2021-05-15 11:01:31,135] {docker.py:276} INFO - 21/05/15 14:01:31 INFO StagingCommitter: Task committer attempt_202105151400507077107538047799844_0002_m_000091_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507077107538047799844_0002_m_000091_95 : duration 0:00.003s
[2021-05-15 11:01:32,498] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400505801073716507536570_0002_m_000089_93: needsTaskCommit() Task attempt_202105151400505801073716507536570_0002_m_000089_93
21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400506564596913808143586_0002_m_000088_92: needsTaskCommit() Task attempt_202105151400506564596913808143586_0002_m_000088_92
21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400505801073716507536570_0002_m_000089_93: needsTaskCommit() Task attempt_202105151400505801073716507536570_0002_m_000089_93: duration 0:00.000s
21/05/15 14:01:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505801073716507536570_0002_m_000089_93
21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400506564596913808143586_0002_m_000088_92: needsTaskCommit() Task attempt_202105151400506564596913808143586_0002_m_000088_92: duration 0:00.001s
21/05/15 14:01:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506564596913808143586_0002_m_000088_92
21/05/15 14:01:32 INFO Executor: Finished task 89.0 in stage 2.0 (TID 93). 4587 bytes result sent to driver
[2021-05-15 11:01:32,499] {docker.py:276} INFO - 21/05/15 14:01:32 INFO Executor: Finished task 88.0 in stage 2.0 (TID 92). 4587 bytes result sent to driver
21/05/15 14:01:32 INFO TaskSetManager: Starting task 92.0 in stage 2.0 (TID 96) (8951b5f85146, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:32,501] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Starting task 93.0 in stage 2.0 (TID 97) (8951b5f85146, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:01:32 INFO Executor: Running task 92.0 in stage 2.0 (TID 96)
[2021-05-15 11:01:32,502] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Finished task 88.0 in stage 2.0 (TID 92) in 1730 ms on 8951b5f85146 (executor driver) (89/200)
[2021-05-15 11:01:32,503] {docker.py:276} INFO - 21/05/15 14:01:32 INFO Executor: Running task 93.0 in stage 2.0 (TID 97)
[2021-05-15 11:01:32,504] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Finished task 89.0 in stage 2.0 (TID 93) in 1727 ms on 8951b5f85146 (executor driver) (90/200)
[2021-05-15 11:01:32,517] {docker.py:276} INFO - 21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Getting 3 (1144.0 B) non-empty blocks including 3 (1144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:32,517] {docker.py:276} INFO - 21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:32,518] {docker.py:276} INFO - 21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:32,518] {docker.py:276} INFO - 21/05/15 14:01:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:32,519] {docker.py:276} INFO - 21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:32,519] {docker.py:276} INFO - 21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503008815187249246327_0002_m_000093_97, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503008815187249246327_0002_m_000093_97}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503008815187249246327_0002}; taskId=attempt_202105151400503008815187249246327_0002_m_000093_97, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50c8fd9e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400503008815187249246327_0002_m_000093_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503008815187249246327_0002_m_000093_97
[2021-05-15 11:01:32,520] {docker.py:276} INFO - 21/05/15 14:01:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:32,520] {docker.py:276} INFO - 21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502473089467883244233_0002_m_000092_96, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502473089467883244233_0002_m_000092_96}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502473089467883244233_0002}; taskId=attempt_202105151400502473089467883244233_0002_m_000092_96, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55daf05f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:32,521] {docker.py:276} INFO - 21/05/15 14:01:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:32,521] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400502473089467883244233_0002_m_000092_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502473089467883244233_0002_m_000092_96
[2021-05-15 11:01:32,522] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400503008815187249246327_0002_m_000093_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503008815187249246327_0002_m_000093_97 : duration 0:00.003s
[2021-05-15 11:01:32,526] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400502473089467883244233_0002_m_000092_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502473089467883244233_0002_m_000092_96 : duration 0:00.005s
[2021-05-15 11:01:32,538] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400504220957057456455263_0002_m_000090_94: needsTaskCommit() Task attempt_202105151400504220957057456455263_0002_m_000090_94
[2021-05-15 11:01:32,539] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400504220957057456455263_0002_m_000090_94: needsTaskCommit() Task attempt_202105151400504220957057456455263_0002_m_000090_94: duration 0:00.001s
21/05/15 14:01:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504220957057456455263_0002_m_000090_94
[2021-05-15 11:01:32,539] {docker.py:276} INFO - 21/05/15 14:01:32 INFO Executor: Finished task 90.0 in stage 2.0 (TID 94). 4587 bytes result sent to driver
[2021-05-15 11:01:32,541] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Starting task 94.0 in stage 2.0 (TID 98) (8951b5f85146, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:32,541] {docker.py:276} INFO - 21/05/15 14:01:32 INFO Executor: Running task 94.0 in stage 2.0 (TID 98)
[2021-05-15 11:01:32,542] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Finished task 90.0 in stage 2.0 (TID 94) in 1739 ms on 8951b5f85146 (executor driver) (91/200)
[2021-05-15 11:01:32,554] {docker.py:276} INFO - 21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:32,554] {docker.py:276} INFO - 21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:32,555] {docker.py:276} INFO - 21/05/15 14:01:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:32,556] {docker.py:276} INFO - 21/05/15 14:01:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:32,556] {docker.py:276} INFO - 21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506050191221301052380_0002_m_000094_98, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506050191221301052380_0002_m_000094_98}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506050191221301052380_0002}; taskId=attempt_202105151400506050191221301052380_0002_m_000094_98, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@675a9a48}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:32,556] {docker.py:276} INFO - 21/05/15 14:01:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400506050191221301052380_0002_m_000094_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506050191221301052380_0002_m_000094_98
[2021-05-15 11:01:32,559] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400506050191221301052380_0002_m_000094_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506050191221301052380_0002_m_000094_98 : duration 0:00.003s
[2021-05-15 11:01:32,832] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400507077107538047799844_0002_m_000091_95: needsTaskCommit() Task attempt_202105151400507077107538047799844_0002_m_000091_95
[2021-05-15 11:01:32,833] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400507077107538047799844_0002_m_000091_95: needsTaskCommit() Task attempt_202105151400507077107538047799844_0002_m_000091_95: duration 0:00.001s
21/05/15 14:01:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507077107538047799844_0002_m_000091_95
[2021-05-15 11:01:32,835] {docker.py:276} INFO - 21/05/15 14:01:32 INFO Executor: Finished task 91.0 in stage 2.0 (TID 95). 4587 bytes result sent to driver
[2021-05-15 11:01:32,836] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Starting task 95.0 in stage 2.0 (TID 99) (8951b5f85146, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:32,838] {docker.py:276} INFO - 21/05/15 14:01:32 INFO Executor: Running task 95.0 in stage 2.0 (TID 99)
[2021-05-15 11:01:32,838] {docker.py:276} INFO - 21/05/15 14:01:32 INFO TaskSetManager: Finished task 91.0 in stage 2.0 (TID 95) in 1722 ms on 8951b5f85146 (executor driver) (92/200)
[2021-05-15 11:01:32,848] {docker.py:276} INFO - 21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:32,850] {docker.py:276} INFO - 21/05/15 14:01:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:32,850] {docker.py:276} INFO - 21/05/15 14:01:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503987413174891580927_0002_m_000095_99, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503987413174891580927_0002_m_000095_99}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503987413174891580927_0002}; taskId=attempt_202105151400503987413174891580927_0002_m_000095_99, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@553d6b20}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:32,851] {docker.py:276} INFO - 21/05/15 14:01:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:32,851] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Starting: Task committer attempt_202105151400503987413174891580927_0002_m_000095_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503987413174891580927_0002_m_000095_99
[2021-05-15 11:01:32,855] {docker.py:276} INFO - 21/05/15 14:01:32 INFO StagingCommitter: Task committer attempt_202105151400503987413174891580927_0002_m_000095_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503987413174891580927_0002_m_000095_99 : duration 0:00.004s
[2021-05-15 11:01:34,197] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400502473089467883244233_0002_m_000092_96: needsTaskCommit() Task attempt_202105151400502473089467883244233_0002_m_000092_96
[2021-05-15 11:01:34,198] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400502473089467883244233_0002_m_000092_96: needsTaskCommit() Task attempt_202105151400502473089467883244233_0002_m_000092_96: duration 0:00.001s
21/05/15 14:01:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502473089467883244233_0002_m_000092_96
[2021-05-15 11:01:34,200] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Finished task 92.0 in stage 2.0 (TID 96). 4587 bytes result sent to driver
[2021-05-15 11:01:34,201] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Starting task 96.0 in stage 2.0 (TID 100) (8951b5f85146, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:34,202] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Running task 96.0 in stage 2.0 (TID 100)
[2021-05-15 11:01:34,203] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Finished task 92.0 in stage 2.0 (TID 96) in 1709 ms on 8951b5f85146 (executor driver) (93/200)
[2021-05-15 11:01:34,212] {docker.py:276} INFO - 21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Getting 3 (1104.0 B) non-empty blocks including 3 (1104.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:34,214] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400503008815187249246327_0002_m_000093_97: needsTaskCommit() Task attempt_202105151400503008815187249246327_0002_m_000093_97
[2021-05-15 11:01:34,214] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400503008815187249246327_0002_m_000093_97: needsTaskCommit() Task attempt_202105151400503008815187249246327_0002_m_000093_97: duration 0:00.000s
21/05/15 14:01:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503008815187249246327_0002_m_000093_97
[2021-05-15 11:01:34,215] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Finished task 93.0 in stage 2.0 (TID 97). 4587 bytes result sent to driver
21/05/15 14:01:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:34,216] {docker.py:276} INFO - 21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,216] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400506050191221301052380_0002_m_000094_98: needsTaskCommit() Task attempt_202105151400506050191221301052380_0002_m_000094_98
21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504379747324342707263_0002_m_000096_100, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504379747324342707263_0002_m_000096_100}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504379747324342707263_0002}; taskId=attempt_202105151400504379747324342707263_0002_m_000096_100, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c4ab6f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,217] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400506050191221301052380_0002_m_000094_98: needsTaskCommit() Task attempt_202105151400506050191221301052380_0002_m_000094_98: duration 0:00.001s
21/05/15 14:01:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506050191221301052380_0002_m_000094_98
[2021-05-15 11:01:34,217] {docker.py:276} INFO - 21/05/15 14:01:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400504379747324342707263_0002_m_000096_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504379747324342707263_0002_m_000096_100
[2021-05-15 11:01:34,218] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Starting task 97.0 in stage 2.0 (TID 101) (8951b5f85146, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:01:34 INFO Executor: Finished task 94.0 in stage 2.0 (TID 98). 4587 bytes result sent to driver
[2021-05-15 11:01:34,219] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Finished task 93.0 in stage 2.0 (TID 97) in 1721 ms on 8951b5f85146 (executor driver) (94/200)
[2021-05-15 11:01:34,220] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Starting task 98.0 in stage 2.0 (TID 102) (8951b5f85146, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:34,221] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Finished task 94.0 in stage 2.0 (TID 98) in 1682 ms on 8951b5f85146 (executor driver) (95/200)
[2021-05-15 11:01:34,222] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Running task 98.0 in stage 2.0 (TID 102)
[2021-05-15 11:01:34,223] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400504379747324342707263_0002_m_000096_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504379747324342707263_0002_m_000096_100 : duration 0:00.006s
[2021-05-15 11:01:34,228] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Running task 97.0 in stage 2.0 (TID 101)
[2021-05-15 11:01:34,241] {docker.py:276} INFO - 21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Getting 3 (1224.0 B) non-empty blocks including 3 (1224.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:34,241] {docker.py:276} INFO - 21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:34,243] {docker.py:276} INFO - 21/05/15 14:01:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,243] {docker.py:276} INFO - 21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508908539541445764895_0002_m_000098_102, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508908539541445764895_0002_m_000098_102}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508908539541445764895_0002}; taskId=attempt_202105151400508908539541445764895_0002_m_000098_102, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bbccf28}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
21/05/15 14:01:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:34,244] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400508908539541445764895_0002_m_000098_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508908539541445764895_0002_m_000098_102
[2021-05-15 11:01:34,244] {docker.py:276} INFO - 21/05/15 14:01:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:34,245] {docker.py:276} INFO - 21/05/15 14:01:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,245] {docker.py:276} INFO - 21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508212964437404272180_0002_m_000097_101, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508212964437404272180_0002_m_000097_101}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508212964437404272180_0002}; taskId=attempt_202105151400508212964437404272180_0002_m_000097_101, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@452228dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,245] {docker.py:276} INFO - 21/05/15 14:01:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:34,246] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400508212964437404272180_0002_m_000097_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508212964437404272180_0002_m_000097_101
[2021-05-15 11:01:34,248] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400508908539541445764895_0002_m_000098_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508908539541445764895_0002_m_000098_102 : duration 0:00.005s
[2021-05-15 11:01:34,249] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400508212964437404272180_0002_m_000097_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508212964437404272180_0002_m_000097_101 : duration 0:00.003s
[2021-05-15 11:01:34,567] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400503987413174891580927_0002_m_000095_99: needsTaskCommit() Task attempt_202105151400503987413174891580927_0002_m_000095_99
[2021-05-15 11:01:34,568] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400503987413174891580927_0002_m_000095_99: needsTaskCommit() Task attempt_202105151400503987413174891580927_0002_m_000095_99: duration 0:00.001s
[2021-05-15 11:01:34,569] {docker.py:276} INFO - 21/05/15 14:01:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503987413174891580927_0002_m_000095_99
[2021-05-15 11:01:34,570] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Finished task 95.0 in stage 2.0 (TID 99). 4587 bytes result sent to driver
[2021-05-15 11:01:34,572] {docker.py:276} INFO - 21/05/15 14:01:34 INFO TaskSetManager: Starting task 99.0 in stage 2.0 (TID 103) (8951b5f85146, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:34,573] {docker.py:276} INFO - 21/05/15 14:01:34 INFO Executor: Running task 99.0 in stage 2.0 (TID 103)
21/05/15 14:01:34 INFO TaskSetManager: Finished task 95.0 in stage 2.0 (TID 99) in 1739 ms on 8951b5f85146 (executor driver) (96/200)
[2021-05-15 11:01:34,588] {docker.py:276} INFO - 21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Getting 3 (1372.0 B) non-empty blocks including 3 (1372.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:34,591] {docker.py:276} INFO - 21/05/15 14:01:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,591] {docker.py:276} INFO - 21/05/15 14:01:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506612747946534925576_0002_m_000099_103, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506612747946534925576_0002_m_000099_103}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506612747946534925576_0002}; taskId=attempt_202105151400506612747946534925576_0002_m_000099_103, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8d7315c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:34,592] {docker.py:276} INFO - 21/05/15 14:01:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:34 INFO StagingCommitter: Starting: Task committer attempt_202105151400506612747946534925576_0002_m_000099_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506612747946534925576_0002_m_000099_103
[2021-05-15 11:01:34,594] {docker.py:276} INFO - 21/05/15 14:01:34 INFO StagingCommitter: Task committer attempt_202105151400506612747946534925576_0002_m_000099_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506612747946534925576_0002_m_000099_103 : duration 0:00.003s
[2021-05-15 11:01:35,864] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Starting: Task committer attempt_202105151400504379747324342707263_0002_m_000096_100: needsTaskCommit() Task attempt_202105151400504379747324342707263_0002_m_000096_100
[2021-05-15 11:01:35,865] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Task committer attempt_202105151400504379747324342707263_0002_m_000096_100: needsTaskCommit() Task attempt_202105151400504379747324342707263_0002_m_000096_100: duration 0:00.001s
[2021-05-15 11:01:35,866] {docker.py:276} INFO - 21/05/15 14:01:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504379747324342707263_0002_m_000096_100
[2021-05-15 11:01:35,868] {docker.py:276} INFO - 21/05/15 14:01:35 INFO Executor: Finished task 96.0 in stage 2.0 (TID 100). 4587 bytes result sent to driver
[2021-05-15 11:01:35,871] {docker.py:276} INFO - 21/05/15 14:01:35 INFO TaskSetManager: Starting task 100.0 in stage 2.0 (TID 104) (8951b5f85146, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:35,872] {docker.py:276} INFO - 21/05/15 14:01:35 INFO TaskSetManager: Finished task 96.0 in stage 2.0 (TID 100) in 1673 ms on 8951b5f85146 (executor driver) (97/200)
[2021-05-15 11:01:35,873] {docker.py:276} INFO - 21/05/15 14:01:35 INFO Executor: Running task 100.0 in stage 2.0 (TID 104)
[2021-05-15 11:01:35,888] {docker.py:276} INFO - 21/05/15 14:01:35 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:35,890] {docker.py:276} INFO - 21/05/15 14:01:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:35,891] {docker.py:276} INFO - 21/05/15 14:01:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505842393193910629200_0002_m_000100_104, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505842393193910629200_0002_m_000100_104}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505842393193910629200_0002}; taskId=attempt_202105151400505842393193910629200_0002_m_000100_104, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e986014}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:35,891] {docker.py:276} INFO - 21/05/15 14:01:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:35,891] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Starting: Task committer attempt_202105151400505842393193910629200_0002_m_000100_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505842393193910629200_0002_m_000100_104
[2021-05-15 11:01:35,895] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Task committer attempt_202105151400505842393193910629200_0002_m_000100_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505842393193910629200_0002_m_000100_104 : duration 0:00.004s
[2021-05-15 11:01:35,910] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Starting: Task committer attempt_202105151400508908539541445764895_0002_m_000098_102: needsTaskCommit() Task attempt_202105151400508908539541445764895_0002_m_000098_102
[2021-05-15 11:01:35,911] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Task committer attempt_202105151400508908539541445764895_0002_m_000098_102: needsTaskCommit() Task attempt_202105151400508908539541445764895_0002_m_000098_102: duration 0:00.001s
21/05/15 14:01:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508908539541445764895_0002_m_000098_102
[2021-05-15 11:01:35,913] {docker.py:276} INFO - 21/05/15 14:01:35 INFO Executor: Finished task 98.0 in stage 2.0 (TID 102). 4587 bytes result sent to driver
[2021-05-15 11:01:35,914] {docker.py:276} INFO - 21/05/15 14:01:35 INFO TaskSetManager: Starting task 101.0 in stage 2.0 (TID 105) (8951b5f85146, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:35,915] {docker.py:276} INFO - 21/05/15 14:01:35 INFO Executor: Running task 101.0 in stage 2.0 (TID 105)
[2021-05-15 11:01:35,915] {docker.py:276} INFO - 21/05/15 14:01:35 INFO TaskSetManager: Finished task 98.0 in stage 2.0 (TID 102) in 1697 ms on 8951b5f85146 (executor driver) (98/200)
[2021-05-15 11:01:35,929] {docker.py:276} INFO - 21/05/15 14:01:35 INFO ShuffleBlockFetcherIterator: Getting 3 (1389.0 B) non-empty blocks including 3 (1389.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:35,931] {docker.py:276} INFO - 21/05/15 14:01:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502246739312344772364_0002_m_000101_105, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502246739312344772364_0002_m_000101_105}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502246739312344772364_0002}; taskId=attempt_202105151400502246739312344772364_0002_m_000101_105, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c53a399}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:35,931] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Starting: Task committer attempt_202105151400502246739312344772364_0002_m_000101_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502246739312344772364_0002_m_000101_105
[2021-05-15 11:01:35,939] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Task committer attempt_202105151400502246739312344772364_0002_m_000101_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502246739312344772364_0002_m_000101_105 : duration 0:00.008s
[2021-05-15 11:01:35,945] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Starting: Task committer attempt_202105151400508212964437404272180_0002_m_000097_101: needsTaskCommit() Task attempt_202105151400508212964437404272180_0002_m_000097_101
21/05/15 14:01:35 INFO StagingCommitter: Task committer attempt_202105151400508212964437404272180_0002_m_000097_101: needsTaskCommit() Task attempt_202105151400508212964437404272180_0002_m_000097_101: duration 0:00.000s
[2021-05-15 11:01:35,946] {docker.py:276} INFO - 21/05/15 14:01:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508212964437404272180_0002_m_000097_101
[2021-05-15 11:01:35,947] {docker.py:276} INFO - 21/05/15 14:01:35 INFO Executor: Finished task 97.0 in stage 2.0 (TID 101). 4587 bytes result sent to driver
[2021-05-15 11:01:35,948] {docker.py:276} INFO - 21/05/15 14:01:35 INFO TaskSetManager: Starting task 102.0 in stage 2.0 (TID 106) (8951b5f85146, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:35,949] {docker.py:276} INFO - 21/05/15 14:01:35 INFO TaskSetManager: Finished task 97.0 in stage 2.0 (TID 101) in 1735 ms on 8951b5f85146 (executor driver) (99/200)
[2021-05-15 11:01:35,949] {docker.py:276} INFO - 21/05/15 14:01:35 INFO Executor: Running task 102.0 in stage 2.0 (TID 106)
[2021-05-15 11:01:35,957] {docker.py:276} INFO - 21/05/15 14:01:35 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:35,958] {docker.py:276} INFO - 21/05/15 14:01:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:35,959] {docker.py:276} INFO - 21/05/15 14:01:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505268187404056060709_0002_m_000102_106, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505268187404056060709_0002_m_000102_106}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505268187404056060709_0002}; taskId=attempt_202105151400505268187404056060709_0002_m_000102_106, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7bc6ee62}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:35,959] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Starting: Task committer attempt_202105151400505268187404056060709_0002_m_000102_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505268187404056060709_0002_m_000102_106
[2021-05-15 11:01:35,961] {docker.py:276} INFO - 21/05/15 14:01:35 INFO StagingCommitter: Task committer attempt_202105151400505268187404056060709_0002_m_000102_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505268187404056060709_0002_m_000102_106 : duration 0:00.002s
[2021-05-15 11:01:36,283] {docker.py:276} INFO - 21/05/15 14:01:36 INFO StagingCommitter: Starting: Task committer attempt_202105151400506612747946534925576_0002_m_000099_103: needsTaskCommit() Task attempt_202105151400506612747946534925576_0002_m_000099_103
[2021-05-15 11:01:36,284] {docker.py:276} INFO - 21/05/15 14:01:36 INFO StagingCommitter: Task committer attempt_202105151400506612747946534925576_0002_m_000099_103: needsTaskCommit() Task attempt_202105151400506612747946534925576_0002_m_000099_103: duration 0:00.001s
21/05/15 14:01:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506612747946534925576_0002_m_000099_103
[2021-05-15 11:01:36,286] {docker.py:276} INFO - 21/05/15 14:01:36 INFO Executor: Finished task 99.0 in stage 2.0 (TID 103). 4587 bytes result sent to driver
[2021-05-15 11:01:36,289] {docker.py:276} INFO - 21/05/15 14:01:36 INFO TaskSetManager: Starting task 103.0 in stage 2.0 (TID 107) (8951b5f85146, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:36,290] {docker.py:276} INFO - 21/05/15 14:01:36 INFO TaskSetManager: Finished task 99.0 in stage 2.0 (TID 103) in 1720 ms on 8951b5f85146 (executor driver) (100/200)
[2021-05-15 11:01:36,291] {docker.py:276} INFO - 21/05/15 14:01:36 INFO Executor: Running task 103.0 in stage 2.0 (TID 107)
[2021-05-15 11:01:36,301] {docker.py:276} INFO - 21/05/15 14:01:36 INFO ShuffleBlockFetcherIterator: Getting 3 (1300.0 B) non-empty blocks including 3 (1300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:36,303] {docker.py:276} INFO - 21/05/15 14:01:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:36,304] {docker.py:276} INFO - 21/05/15 14:01:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:36,304] {docker.py:276} INFO - 21/05/15 14:01:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:36,304] {docker.py:276} INFO - 21/05/15 14:01:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505763059095357908355_0002_m_000103_107, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505763059095357908355_0002_m_000103_107}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505763059095357908355_0002}; taskId=attempt_202105151400505763059095357908355_0002_m_000103_107, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8939cab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:36,305] {docker.py:276} INFO - 21/05/15 14:01:36 INFO StagingCommitter: Starting: Task committer attempt_202105151400505763059095357908355_0002_m_000103_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505763059095357908355_0002_m_000103_107
[2021-05-15 11:01:36,307] {docker.py:276} INFO - 21/05/15 14:01:36 INFO StagingCommitter: Task committer attempt_202105151400505763059095357908355_0002_m_000103_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505763059095357908355_0002_m_000103_107 : duration 0:00.003s
[2021-05-15 11:01:37,569] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Starting: Task committer attempt_202105151400505842393193910629200_0002_m_000100_104: needsTaskCommit() Task attempt_202105151400505842393193910629200_0002_m_000100_104
21/05/15 14:01:37 INFO StagingCommitter: Task committer attempt_202105151400505842393193910629200_0002_m_000100_104: needsTaskCommit() Task attempt_202105151400505842393193910629200_0002_m_000100_104: duration 0:00.001s
21/05/15 14:01:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505842393193910629200_0002_m_000100_104
[2021-05-15 11:01:37,570] {docker.py:276} INFO - 21/05/15 14:01:37 INFO Executor: Finished task 100.0 in stage 2.0 (TID 104). 4587 bytes result sent to driver
[2021-05-15 11:01:37,571] {docker.py:276} INFO - 21/05/15 14:01:37 INFO TaskSetManager: Starting task 104.0 in stage 2.0 (TID 108) (8951b5f85146, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:37,573] {docker.py:276} INFO - 21/05/15 14:01:37 INFO TaskSetManager: Finished task 100.0 in stage 2.0 (TID 104) in 1705 ms on 8951b5f85146 (executor driver) (101/200)
[2021-05-15 11:01:37,574] {docker.py:276} INFO - 21/05/15 14:01:37 INFO Executor: Running task 104.0 in stage 2.0 (TID 108)
[2021-05-15 11:01:37,584] {docker.py:276} INFO - 21/05/15 14:01:37 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:37,586] {docker.py:276} INFO - 21/05/15 14:01:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503281183524291015174_0002_m_000104_108, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503281183524291015174_0002_m_000104_108}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503281183524291015174_0002}; taskId=attempt_202105151400503281183524291015174_0002_m_000104_108, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28c054f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:37,586] {docker.py:276} INFO - 21/05/15 14:01:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:37 INFO StagingCommitter: Starting: Task committer attempt_202105151400503281183524291015174_0002_m_000104_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503281183524291015174_0002_m_000104_108
[2021-05-15 11:01:37,589] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Task committer attempt_202105151400503281183524291015174_0002_m_000104_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503281183524291015174_0002_m_000104_108 : duration 0:00.003s
[2021-05-15 11:01:37,593] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Starting: Task committer attempt_202105151400502246739312344772364_0002_m_000101_105: needsTaskCommit() Task attempt_202105151400502246739312344772364_0002_m_000101_105
[2021-05-15 11:01:37,593] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Task committer attempt_202105151400502246739312344772364_0002_m_000101_105: needsTaskCommit() Task attempt_202105151400502246739312344772364_0002_m_000101_105: duration 0:00.000s
[2021-05-15 11:01:37,594] {docker.py:276} INFO - 21/05/15 14:01:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502246739312344772364_0002_m_000101_105
[2021-05-15 11:01:37,595] {docker.py:276} INFO - 21/05/15 14:01:37 INFO Executor: Finished task 101.0 in stage 2.0 (TID 105). 4587 bytes result sent to driver
[2021-05-15 11:01:37,596] {docker.py:276} INFO - 21/05/15 14:01:37 INFO TaskSetManager: Starting task 105.0 in stage 2.0 (TID 109) (8951b5f85146, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:37,597] {docker.py:276} INFO - 21/05/15 14:01:37 INFO TaskSetManager: Finished task 101.0 in stage 2.0 (TID 105) in 1686 ms on 8951b5f85146 (executor driver) (102/200)
[2021-05-15 11:01:37,599] {docker.py:276} INFO - 21/05/15 14:01:37 INFO Executor: Running task 105.0 in stage 2.0 (TID 109)
[2021-05-15 11:01:37,616] {docker.py:276} INFO - 21/05/15 14:01:37 INFO ShuffleBlockFetcherIterator: Getting 3 (1141.0 B) non-empty blocks including 3 (1141.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:37,618] {docker.py:276} INFO - 21/05/15 14:01:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:37,618] {docker.py:276} INFO - 21/05/15 14:01:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501215952651470287716_0002_m_000105_109, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501215952651470287716_0002_m_000105_109}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501215952651470287716_0002}; taskId=attempt_202105151400501215952651470287716_0002_m_000105_109, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a015ff1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:37,618] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Starting: Task committer attempt_202105151400501215952651470287716_0002_m_000105_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501215952651470287716_0002_m_000105_109
[2021-05-15 11:01:37,621] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Task committer attempt_202105151400501215952651470287716_0002_m_000105_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501215952651470287716_0002_m_000105_109 : duration 0:00.003s
[2021-05-15 11:01:37,652] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Starting: Task committer attempt_202105151400505268187404056060709_0002_m_000102_106: needsTaskCommit() Task attempt_202105151400505268187404056060709_0002_m_000102_106
[2021-05-15 11:01:37,653] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Task committer attempt_202105151400505268187404056060709_0002_m_000102_106: needsTaskCommit() Task attempt_202105151400505268187404056060709_0002_m_000102_106: duration 0:00.001s
21/05/15 14:01:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505268187404056060709_0002_m_000102_106
[2021-05-15 11:01:37,654] {docker.py:276} INFO - 21/05/15 14:01:37 INFO Executor: Finished task 102.0 in stage 2.0 (TID 106). 4587 bytes result sent to driver
[2021-05-15 11:01:37,655] {docker.py:276} INFO - 21/05/15 14:01:37 INFO TaskSetManager: Starting task 106.0 in stage 2.0 (TID 110) (8951b5f85146, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:37,656] {docker.py:276} INFO - 21/05/15 14:01:37 INFO TaskSetManager: Finished task 102.0 in stage 2.0 (TID 106) in 1710 ms on 8951b5f85146 (executor driver) (103/200)
[2021-05-15 11:01:37,657] {docker.py:276} INFO - 21/05/15 14:01:37 INFO Executor: Running task 106.0 in stage 2.0 (TID 110)
[2021-05-15 11:01:37,664] {docker.py:276} INFO - 21/05/15 14:01:37 INFO ShuffleBlockFetcherIterator: Getting 3 (1241.0 B) non-empty blocks including 3 (1241.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:37,666] {docker.py:276} INFO - 21/05/15 14:01:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:37,666] {docker.py:276} INFO - 21/05/15 14:01:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:37,667] {docker.py:276} INFO - 21/05/15 14:01:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:37,667] {docker.py:276} INFO - 21/05/15 14:01:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501987444125142738935_0002_m_000106_110, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501987444125142738935_0002_m_000106_110}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501987444125142738935_0002}; taskId=attempt_202105151400501987444125142738935_0002_m_000106_110, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@741ca2b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:37,667] {docker.py:276} INFO - 21/05/15 14:01:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:37 INFO StagingCommitter: Starting: Task committer attempt_202105151400501987444125142738935_0002_m_000106_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501987444125142738935_0002_m_000106_110
[2021-05-15 11:01:37,670] {docker.py:276} INFO - 21/05/15 14:01:37 INFO StagingCommitter: Task committer attempt_202105151400501987444125142738935_0002_m_000106_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501987444125142738935_0002_m_000106_110 : duration 0:00.003s
[2021-05-15 11:01:38,010] {docker.py:276} INFO - 21/05/15 14:01:38 INFO StagingCommitter: Starting: Task committer attempt_202105151400505763059095357908355_0002_m_000103_107: needsTaskCommit() Task attempt_202105151400505763059095357908355_0002_m_000103_107
[2021-05-15 11:01:38,012] {docker.py:276} INFO - 21/05/15 14:01:38 INFO StagingCommitter: Task committer attempt_202105151400505763059095357908355_0002_m_000103_107: needsTaskCommit() Task attempt_202105151400505763059095357908355_0002_m_000103_107: duration 0:00.002s
21/05/15 14:01:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505763059095357908355_0002_m_000103_107
[2021-05-15 11:01:38,014] {docker.py:276} INFO - 21/05/15 14:01:38 INFO Executor: Finished task 103.0 in stage 2.0 (TID 107). 4587 bytes result sent to driver
[2021-05-15 11:01:38,016] {docker.py:276} INFO - 21/05/15 14:01:38 INFO TaskSetManager: Starting task 107.0 in stage 2.0 (TID 111) (8951b5f85146, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:38,017] {docker.py:276} INFO - 21/05/15 14:01:38 INFO Executor: Running task 107.0 in stage 2.0 (TID 111)
21/05/15 14:01:38 INFO TaskSetManager: Finished task 103.0 in stage 2.0 (TID 107) in 1730 ms on 8951b5f85146 (executor driver) (104/200)
[2021-05-15 11:01:38,034] {docker.py:276} INFO - 21/05/15 14:01:38 INFO ShuffleBlockFetcherIterator: Getting 3 (1147.0 B) non-empty blocks including 3 (1147.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:38,035] {docker.py:276} INFO - 21/05/15 14:01:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:38,036] {docker.py:276} INFO - 21/05/15 14:01:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502205264352017694691_0002_m_000107_111, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502205264352017694691_0002_m_000107_111}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502205264352017694691_0002}; taskId=attempt_202105151400502205264352017694691_0002_m_000107_111, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ced4547}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:38 INFO StagingCommitter: Starting: Task committer attempt_202105151400502205264352017694691_0002_m_000107_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502205264352017694691_0002_m_000107_111
[2021-05-15 11:01:38,039] {docker.py:276} INFO - 21/05/15 14:01:38 INFO StagingCommitter: Task committer attempt_202105151400502205264352017694691_0002_m_000107_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502205264352017694691_0002_m_000107_111 : duration 0:00.004s
[2021-05-15 11:01:39,250] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400503281183524291015174_0002_m_000104_108: needsTaskCommit() Task attempt_202105151400503281183524291015174_0002_m_000104_108
[2021-05-15 11:01:39,251] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400503281183524291015174_0002_m_000104_108: needsTaskCommit() Task attempt_202105151400503281183524291015174_0002_m_000104_108: duration 0:00.001s
21/05/15 14:01:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503281183524291015174_0002_m_000104_108
[2021-05-15 11:01:39,256] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Finished task 104.0 in stage 2.0 (TID 108). 4587 bytes result sent to driver
[2021-05-15 11:01:39,261] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Starting task 108.0 in stage 2.0 (TID 112) (8951b5f85146, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:39,264] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Running task 108.0 in stage 2.0 (TID 112)
[2021-05-15 11:01:39,265] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Finished task 104.0 in stage 2.0 (TID 108) in 1693 ms on 8951b5f85146 (executor driver) (105/200)
[2021-05-15 11:01:39,288] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Getting 3 (1523.0 B) non-empty blocks including 3 (1523.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:39,289] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:39,292] {docker.py:276} INFO - 21/05/15 14:01:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:39,293] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502620913148083174147_0002_m_000108_112, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502620913148083174147_0002_m_000108_112}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502620913148083174147_0002}; taskId=attempt_202105151400502620913148083174147_0002_m_000108_112, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2074d5cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:39,294] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400502620913148083174147_0002_m_000108_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502620913148083174147_0002_m_000108_112
[2021-05-15 11:01:39,296] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400501215952651470287716_0002_m_000105_109: needsTaskCommit() Task attempt_202105151400501215952651470287716_0002_m_000105_109
[2021-05-15 11:01:39,296] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400501215952651470287716_0002_m_000105_109: needsTaskCommit() Task attempt_202105151400501215952651470287716_0002_m_000105_109: duration 0:00.000s
[2021-05-15 11:01:39,297] {docker.py:276} INFO - 21/05/15 14:01:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501215952651470287716_0002_m_000105_109
[2021-05-15 11:01:39,299] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Finished task 105.0 in stage 2.0 (TID 109). 4587 bytes result sent to driver
[2021-05-15 11:01:39,300] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400502620913148083174147_0002_m_000108_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502620913148083174147_0002_m_000108_112 : duration 0:00.006s
[2021-05-15 11:01:39,301] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Starting task 109.0 in stage 2.0 (TID 113) (8951b5f85146, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:39,302] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Finished task 105.0 in stage 2.0 (TID 109) in 1708 ms on 8951b5f85146 (executor driver) (106/200)
[2021-05-15 11:01:39,303] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Running task 109.0 in stage 2.0 (TID 113)
[2021-05-15 11:01:39,319] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Getting 3 (1335.0 B) non-empty blocks including 3 (1335.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:39,320] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:39,321] {docker.py:276} INFO - 21/05/15 14:01:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:39,322] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:39,323] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508576466113480406387_0002_m_000109_113, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508576466113480406387_0002_m_000109_113}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508576466113480406387_0002}; taskId=attempt_202105151400508576466113480406387_0002_m_000109_113, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e8bc0f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:39,325] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400508576466113480406387_0002_m_000109_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508576466113480406387_0002_m_000109_113
[2021-05-15 11:01:39,328] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400508576466113480406387_0002_m_000109_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508576466113480406387_0002_m_000109_113 : duration 0:00.006s
[2021-05-15 11:01:39,335] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400501987444125142738935_0002_m_000106_110: needsTaskCommit() Task attempt_202105151400501987444125142738935_0002_m_000106_110
[2021-05-15 11:01:39,335] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400501987444125142738935_0002_m_000106_110: needsTaskCommit() Task attempt_202105151400501987444125142738935_0002_m_000106_110: duration 0:00.000s
21/05/15 14:01:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501987444125142738935_0002_m_000106_110
[2021-05-15 11:01:39,336] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Finished task 106.0 in stage 2.0 (TID 110). 4587 bytes result sent to driver
[2021-05-15 11:01:39,338] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Starting task 110.0 in stage 2.0 (TID 114) (8951b5f85146, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:39,340] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Finished task 106.0 in stage 2.0 (TID 110) in 1687 ms on 8951b5f85146 (executor driver) (107/200)
[2021-05-15 11:01:39,341] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Running task 110.0 in stage 2.0 (TID 114)
[2021-05-15 11:01:39,353] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:39,354] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:39,357] {docker.py:276} INFO - 21/05/15 14:01:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:39,357] {docker.py:276} INFO - 21/05/15 14:01:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:39,358] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:39,358] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504829757338271176916_0002_m_000110_114, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504829757338271176916_0002_m_000110_114}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504829757338271176916_0002}; taskId=attempt_202105151400504829757338271176916_0002_m_000110_114, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37dcb18a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:39,359] {docker.py:276} INFO - 21/05/15 14:01:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:39,360] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400504829757338271176916_0002_m_000110_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504829757338271176916_0002_m_000110_114
[2021-05-15 11:01:39,363] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400504829757338271176916_0002_m_000110_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504829757338271176916_0002_m_000110_114 : duration 0:00.005s
[2021-05-15 11:01:39,694] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400502205264352017694691_0002_m_000107_111: needsTaskCommit() Task attempt_202105151400502205264352017694691_0002_m_000107_111
[2021-05-15 11:01:39,695] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400502205264352017694691_0002_m_000107_111: needsTaskCommit() Task attempt_202105151400502205264352017694691_0002_m_000107_111: duration 0:00.001s
[2021-05-15 11:01:39,695] {docker.py:276} INFO - 21/05/15 14:01:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502205264352017694691_0002_m_000107_111
[2021-05-15 11:01:39,697] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Finished task 107.0 in stage 2.0 (TID 111). 4587 bytes result sent to driver
[2021-05-15 11:01:39,699] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Starting task 111.0 in stage 2.0 (TID 115) (8951b5f85146, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:39,701] {docker.py:276} INFO - 21/05/15 14:01:39 INFO TaskSetManager: Finished task 107.0 in stage 2.0 (TID 111) in 1687 ms on 8951b5f85146 (executor driver) (108/200)
[2021-05-15 11:01:39,702] {docker.py:276} INFO - 21/05/15 14:01:39 INFO Executor: Running task 111.0 in stage 2.0 (TID 115)
[2021-05-15 11:01:39,711] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Getting 3 (1144.0 B) non-empty blocks including 3 (1144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:39,712] {docker.py:276} INFO - 21/05/15 14:01:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:39,713] {docker.py:276} INFO - 21/05/15 14:01:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:39,713] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:39,714] {docker.py:276} INFO - 21/05/15 14:01:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502351014603977799453_0002_m_000111_115, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502351014603977799453_0002_m_000111_115}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502351014603977799453_0002}; taskId=attempt_202105151400502351014603977799453_0002_m_000111_115, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b69a2a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:39,714] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Starting: Task committer attempt_202105151400502351014603977799453_0002_m_000111_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502351014603977799453_0002_m_000111_115
[2021-05-15 11:01:39,717] {docker.py:276} INFO - 21/05/15 14:01:39 INFO StagingCommitter: Task committer attempt_202105151400502351014603977799453_0002_m_000111_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502351014603977799453_0002_m_000111_115 : duration 0:00.003s
[2021-05-15 11:01:41,005] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400504829757338271176916_0002_m_000110_114: needsTaskCommit() Task attempt_202105151400504829757338271176916_0002_m_000110_114
[2021-05-15 11:01:41,006] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400504829757338271176916_0002_m_000110_114: needsTaskCommit() Task attempt_202105151400504829757338271176916_0002_m_000110_114: duration 0:00.001s
21/05/15 14:01:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504829757338271176916_0002_m_000110_114
[2021-05-15 11:01:41,008] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Finished task 110.0 in stage 2.0 (TID 114). 4587 bytes result sent to driver
21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400502620913148083174147_0002_m_000108_112: needsTaskCommit() Task attempt_202105151400502620913148083174147_0002_m_000108_112
[2021-05-15 11:01:41,009] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400502620913148083174147_0002_m_000108_112: needsTaskCommit() Task attempt_202105151400502620913148083174147_0002_m_000108_112: duration 0:00.001s
21/05/15 14:01:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502620913148083174147_0002_m_000108_112
[2021-05-15 11:01:41,010] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Starting task 112.0 in stage 2.0 (TID 116) (8951b5f85146, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:41,011] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Running task 112.0 in stage 2.0 (TID 116)
[2021-05-15 11:01:41,012] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Finished task 110.0 in stage 2.0 (TID 114) in 1675 ms on 8951b5f85146 (executor driver) (109/200)
21/05/15 14:01:41 INFO Executor: Finished task 108.0 in stage 2.0 (TID 112). 4587 bytes result sent to driver
[2021-05-15 11:01:41,013] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Starting task 113.0 in stage 2.0 (TID 117) (8951b5f85146, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:41,013] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Finished task 108.0 in stage 2.0 (TID 112) in 1758 ms on 8951b5f85146 (executor driver) (110/200)
[2021-05-15 11:01:41,015] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Running task 113.0 in stage 2.0 (TID 117)
[2021-05-15 11:01:41,021] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:41,022] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:41,023] {docker.py:276} INFO - 21/05/15 14:01:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:41,024] {docker.py:276} INFO - 21/05/15 14:01:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:41,024] {docker.py:276} INFO - 21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:41,025] {docker.py:276} INFO - 21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508805919646843451756_0002_m_000112_116, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508805919646843451756_0002_m_000112_116}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508805919646843451756_0002}; taskId=attempt_202105151400508805919646843451756_0002_m_000112_116, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51146a34}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:41,025] {docker.py:276} INFO - 21/05/15 14:01:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:41,026] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400508805919646843451756_0002_m_000112_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508805919646843451756_0002_m_000112_116
[2021-05-15 11:01:41,026] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:41,026] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:41,028] {docker.py:276} INFO - 21/05/15 14:01:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:41,028] {docker.py:276} INFO - 21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:41,029] {docker.py:276} INFO - 21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501842607979763943392_0002_m_000113_117, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501842607979763943392_0002_m_000113_117}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501842607979763943392_0002}; taskId=attempt_202105151400501842607979763943392_0002_m_000113_117, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50d8d1c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:41,029] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400501842607979763943392_0002_m_000113_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501842607979763943392_0002_m_000113_117
[2021-05-15 11:01:41,030] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400508805919646843451756_0002_m_000112_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508805919646843451756_0002_m_000112_116 : duration 0:00.005s
[2021-05-15 11:01:41,033] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400501842607979763943392_0002_m_000113_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501842607979763943392_0002_m_000113_117 : duration 0:00.004s
[2021-05-15 11:01:41,151] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400508576466113480406387_0002_m_000109_113: needsTaskCommit() Task attempt_202105151400508576466113480406387_0002_m_000109_113
[2021-05-15 11:01:41,152] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400508576466113480406387_0002_m_000109_113: needsTaskCommit() Task attempt_202105151400508576466113480406387_0002_m_000109_113: duration 0:00.000s
21/05/15 14:01:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508576466113480406387_0002_m_000109_113
[2021-05-15 11:01:41,153] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Finished task 109.0 in stage 2.0 (TID 113). 4587 bytes result sent to driver
[2021-05-15 11:01:41,154] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Starting task 114.0 in stage 2.0 (TID 118) (8951b5f85146, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:41,155] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Finished task 109.0 in stage 2.0 (TID 113) in 1857 ms on 8951b5f85146 (executor driver) (111/200)
[2021-05-15 11:01:41,156] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Running task 114.0 in stage 2.0 (TID 118)
[2021-05-15 11:01:41,173] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Getting 3 (1154.0 B) non-empty blocks including 3 (1154.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:41,175] {docker.py:276} INFO - 21/05/15 14:01:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504560359453740876905_0002_m_000114_118, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504560359453740876905_0002_m_000114_118}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504560359453740876905_0002}; taskId=attempt_202105151400504560359453740876905_0002_m_000114_118, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1acaea70}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:41,176] {docker.py:276} INFO - 21/05/15 14:01:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400504560359453740876905_0002_m_000114_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504560359453740876905_0002_m_000114_118
[2021-05-15 11:01:41,179] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400504560359453740876905_0002_m_000114_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504560359453740876905_0002_m_000114_118 : duration 0:00.004s
[2021-05-15 11:01:41,409] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_202105151400502351014603977799453_0002_m_000111_115: needsTaskCommit() Task attempt_202105151400502351014603977799453_0002_m_000111_115
[2021-05-15 11:01:41,410] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_202105151400502351014603977799453_0002_m_000111_115: needsTaskCommit() Task attempt_202105151400502351014603977799453_0002_m_000111_115: duration 0:00.000s
21/05/15 14:01:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502351014603977799453_0002_m_000111_115
[2021-05-15 11:01:41,412] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Finished task 111.0 in stage 2.0 (TID 115). 4587 bytes result sent to driver
[2021-05-15 11:01:41,413] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Starting task 115.0 in stage 2.0 (TID 119) (8951b5f85146, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:41,414] {docker.py:276} INFO - 21/05/15 14:01:41 INFO TaskSetManager: Finished task 111.0 in stage 2.0 (TID 115) in 1718 ms on 8951b5f85146 (executor driver) (112/200)
[2021-05-15 11:01:41,415] {docker.py:276} INFO - 21/05/15 14:01:41 INFO Executor: Running task 115.0 in stage 2.0 (TID 119)
[2021-05-15 11:01:41,423] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Getting 3 (1114.0 B) non-empty blocks including 3 (1114.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:41,424] {docker.py:276} INFO - 21/05/15 14:01:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:41,426] {docker.py:276} INFO - 21/05/15 14:01:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:41,427] {docker.py:276} INFO - 21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:41,427] {docker.py:276} INFO - 21/05/15 14:01:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050989030002688603671_0002_m_000115_119, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050989030002688603671_0002_m_000115_119}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050989030002688603671_0002}; taskId=attempt_20210515140050989030002688603671_0002_m_000115_119, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f05f1ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:41,428] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Starting: Task committer attempt_20210515140050989030002688603671_0002_m_000115_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050989030002688603671_0002_m_000115_119
[2021-05-15 11:01:41,431] {docker.py:276} INFO - 21/05/15 14:01:41 INFO StagingCommitter: Task committer attempt_20210515140050989030002688603671_0002_m_000115_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050989030002688603671_0002_m_000115_119 : duration 0:00.003s
[2021-05-15 11:01:42,756] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Starting: Task committer attempt_202105151400508805919646843451756_0002_m_000112_116: needsTaskCommit() Task attempt_202105151400508805919646843451756_0002_m_000112_116
[2021-05-15 11:01:42,757] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Task committer attempt_202105151400508805919646843451756_0002_m_000112_116: needsTaskCommit() Task attempt_202105151400508805919646843451756_0002_m_000112_116: duration 0:00.001s
[2021-05-15 11:01:42,759] {docker.py:276} INFO - 21/05/15 14:01:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508805919646843451756_0002_m_000112_116
[2021-05-15 11:01:42,761] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Starting: Task committer attempt_202105151400501842607979763943392_0002_m_000113_117: needsTaskCommit() Task attempt_202105151400501842607979763943392_0002_m_000113_117
[2021-05-15 11:01:42,762] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Task committer attempt_202105151400501842607979763943392_0002_m_000113_117: needsTaskCommit() Task attempt_202105151400501842607979763943392_0002_m_000113_117: duration 0:00.001s
[2021-05-15 11:01:42,762] {docker.py:276} INFO - 21/05/15 14:01:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501842607979763943392_0002_m_000113_117
[2021-05-15 11:01:42,763] {docker.py:276} INFO - 21/05/15 14:01:42 INFO Executor: Finished task 112.0 in stage 2.0 (TID 116). 4587 bytes result sent to driver
21/05/15 14:01:42 INFO Executor: Finished task 113.0 in stage 2.0 (TID 117). 4587 bytes result sent to driver
[2021-05-15 11:01:42,764] {docker.py:276} INFO - 21/05/15 14:01:42 INFO TaskSetManager: Starting task 116.0 in stage 2.0 (TID 120) (8951b5f85146, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:42,765] {docker.py:276} INFO - 21/05/15 14:01:42 INFO TaskSetManager: Finished task 112.0 in stage 2.0 (TID 116) in 1758 ms on 8951b5f85146 (executor driver) (113/200)
[2021-05-15 11:01:42,766] {docker.py:276} INFO - 21/05/15 14:01:42 INFO Executor: Running task 116.0 in stage 2.0 (TID 120)
[2021-05-15 11:01:42,767] {docker.py:276} INFO - 21/05/15 14:01:42 INFO TaskSetManager: Starting task 117.0 in stage 2.0 (TID 121) (8951b5f85146, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:42,768] {docker.py:276} INFO - 21/05/15 14:01:42 INFO TaskSetManager: Finished task 113.0 in stage 2.0 (TID 117) in 1758 ms on 8951b5f85146 (executor driver) (114/200)
[2021-05-15 11:01:42,769] {docker.py:276} INFO - 21/05/15 14:01:42 INFO Executor: Running task 117.0 in stage 2.0 (TID 121)
[2021-05-15 11:01:42,786] {docker.py:276} INFO - 21/05/15 14:01:42 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:01:42,787] {docker.py:276} INFO - 21/05/15 14:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:42,788] {docker.py:276} INFO - 21/05/15 14:01:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505583251682599893366_0002_m_000116_120, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505583251682599893366_0002_m_000116_120}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505583251682599893366_0002}; taskId=attempt_202105151400505583251682599893366_0002_m_000116_120, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45d8e4e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:42 INFO StagingCommitter: Starting: Task committer attempt_202105151400505583251682599893366_0002_m_000116_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505583251682599893366_0002_m_000116_120
[2021-05-15 11:01:42,791] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Task committer attempt_202105151400505583251682599893366_0002_m_000116_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505583251682599893366_0002_m_000116_120 : duration 0:00.004s
[2021-05-15 11:01:42,793] {docker.py:276} INFO - 21/05/15 14:01:42 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:42,795] {docker.py:276} INFO - 21/05/15 14:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:42,796] {docker.py:276} INFO - 21/05/15 14:01:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507958495572562854188_0002_m_000117_121, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507958495572562854188_0002_m_000117_121}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507958495572562854188_0002}; taskId=attempt_202105151400507958495572562854188_0002_m_000117_121, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d7b40a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:42,797] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Starting: Task committer attempt_202105151400507958495572562854188_0002_m_000117_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507958495572562854188_0002_m_000117_121
[2021-05-15 11:01:42,801] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Task committer attempt_202105151400507958495572562854188_0002_m_000117_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507958495572562854188_0002_m_000117_121 : duration 0:00.004s
[2021-05-15 11:01:42,908] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Starting: Task committer attempt_202105151400504560359453740876905_0002_m_000114_118: needsTaskCommit() Task attempt_202105151400504560359453740876905_0002_m_000114_118
[2021-05-15 11:01:42,910] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Task committer attempt_202105151400504560359453740876905_0002_m_000114_118: needsTaskCommit() Task attempt_202105151400504560359453740876905_0002_m_000114_118: duration 0:00.001s
21/05/15 14:01:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504560359453740876905_0002_m_000114_118
[2021-05-15 11:01:42,911] {docker.py:276} INFO - 21/05/15 14:01:42 INFO Executor: Finished task 114.0 in stage 2.0 (TID 118). 4587 bytes result sent to driver
[2021-05-15 11:01:42,913] {docker.py:276} INFO - 21/05/15 14:01:42 INFO TaskSetManager: Starting task 118.0 in stage 2.0 (TID 122) (8951b5f85146, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:42,914] {docker.py:276} INFO - 21/05/15 14:01:42 INFO TaskSetManager: Finished task 114.0 in stage 2.0 (TID 118) in 1762 ms on 8951b5f85146 (executor driver) (115/200)
[2021-05-15 11:01:42,915] {docker.py:276} INFO - 21/05/15 14:01:42 INFO Executor: Running task 118.0 in stage 2.0 (TID 122)
[2021-05-15 11:01:42,928] {docker.py:276} INFO - 21/05/15 14:01:42 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:42,931] {docker.py:276} INFO - 21/05/15 14:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:42,932] {docker.py:276} INFO - 21/05/15 14:01:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503095775330455599985_0002_m_000118_122, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503095775330455599985_0002_m_000118_122}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503095775330455599985_0002}; taskId=attempt_202105151400503095775330455599985_0002_m_000118_122, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74455fa2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:42,932] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Starting: Task committer attempt_202105151400503095775330455599985_0002_m_000118_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503095775330455599985_0002_m_000118_122
[2021-05-15 11:01:42,935] {docker.py:276} INFO - 21/05/15 14:01:42 INFO StagingCommitter: Task committer attempt_202105151400503095775330455599985_0002_m_000118_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503095775330455599985_0002_m_000118_122 : duration 0:00.003s
[2021-05-15 11:01:43,135] {docker.py:276} INFO - 21/05/15 14:01:43 INFO StagingCommitter: Starting: Task committer attempt_20210515140050989030002688603671_0002_m_000115_119: needsTaskCommit() Task attempt_20210515140050989030002688603671_0002_m_000115_119
[2021-05-15 11:01:43,135] {docker.py:276} INFO - 21/05/15 14:01:43 INFO StagingCommitter: Task committer attempt_20210515140050989030002688603671_0002_m_000115_119: needsTaskCommit() Task attempt_20210515140050989030002688603671_0002_m_000115_119: duration 0:00.000s
21/05/15 14:01:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050989030002688603671_0002_m_000115_119
[2021-05-15 11:01:43,136] {docker.py:276} INFO - 21/05/15 14:01:43 INFO Executor: Finished task 115.0 in stage 2.0 (TID 119). 4587 bytes result sent to driver
[2021-05-15 11:01:43,138] {docker.py:276} INFO - 21/05/15 14:01:43 INFO TaskSetManager: Starting task 119.0 in stage 2.0 (TID 123) (8951b5f85146, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:43,139] {docker.py:276} INFO - 21/05/15 14:01:43 INFO Executor: Running task 119.0 in stage 2.0 (TID 123)
21/05/15 14:01:43 INFO TaskSetManager: Finished task 115.0 in stage 2.0 (TID 119) in 1729 ms on 8951b5f85146 (executor driver) (116/200)
[2021-05-15 11:01:43,150] {docker.py:276} INFO - 21/05/15 14:01:43 INFO ShuffleBlockFetcherIterator: Getting 3 (1483.0 B) non-empty blocks including 3 (1483.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:43,153] {docker.py:276} INFO - 21/05/15 14:01:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503829162184569834623_0002_m_000119_123, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503829162184569834623_0002_m_000119_123}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503829162184569834623_0002}; taskId=attempt_202105151400503829162184569834623_0002_m_000119_123, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@556dc724}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:43,154] {docker.py:276} INFO - 21/05/15 14:01:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:43 INFO StagingCommitter: Starting: Task committer attempt_202105151400503829162184569834623_0002_m_000119_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503829162184569834623_0002_m_000119_123
[2021-05-15 11:01:43,157] {docker.py:276} INFO - 21/05/15 14:01:43 INFO StagingCommitter: Task committer attempt_202105151400503829162184569834623_0002_m_000119_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503829162184569834623_0002_m_000119_123 : duration 0:00.003s
[2021-05-15 11:01:44,489] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400507958495572562854188_0002_m_000117_121: needsTaskCommit() Task attempt_202105151400507958495572562854188_0002_m_000117_121
[2021-05-15 11:01:44,490] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400507958495572562854188_0002_m_000117_121: needsTaskCommit() Task attempt_202105151400507958495572562854188_0002_m_000117_121: duration 0:00.001s
[2021-05-15 11:01:44,491] {docker.py:276} INFO - 21/05/15 14:01:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507958495572562854188_0002_m_000117_121
[2021-05-15 11:01:44,494] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400505583251682599893366_0002_m_000116_120: needsTaskCommit() Task attempt_202105151400505583251682599893366_0002_m_000116_120
[2021-05-15 11:01:44,495] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Finished task 117.0 in stage 2.0 (TID 121). 4587 bytes result sent to driver
[2021-05-15 11:01:44,496] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400505583251682599893366_0002_m_000116_120: needsTaskCommit() Task attempt_202105151400505583251682599893366_0002_m_000116_120: duration 0:00.002s
21/05/15 14:01:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505583251682599893366_0002_m_000116_120
[2021-05-15 11:01:44,496] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Starting task 120.0 in stage 2.0 (TID 124) (8951b5f85146, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:44,497] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Finished task 116.0 in stage 2.0 (TID 120). 4587 bytes result sent to driver
[2021-05-15 11:01:44,498] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Finished task 117.0 in stage 2.0 (TID 121) in 1733 ms on 8951b5f85146 (executor driver) (117/200)
[2021-05-15 11:01:44,499] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Running task 120.0 in stage 2.0 (TID 124)
[2021-05-15 11:01:44,501] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Starting task 121.0 in stage 2.0 (TID 125) (8951b5f85146, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:44,502] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Running task 121.0 in stage 2.0 (TID 125)
21/05/15 14:01:44 INFO TaskSetManager: Finished task 116.0 in stage 2.0 (TID 120) in 1740 ms on 8951b5f85146 (executor driver) (118/200)
[2021-05-15 11:01:44,516] {docker.py:276} INFO - 21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:44,517] {docker.py:276} INFO - 21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Getting 3 (1385.0 B) non-empty blocks including 3 (1385.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:44,517] {docker.py:276} INFO - 21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:44,518] {docker.py:276} INFO - 21/05/15 14:01:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:44,519] {docker.py:276} INFO - 21/05/15 14:01:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:44,520] {docker.py:276} INFO - 21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:44,520] {docker.py:276} INFO - 21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503443532060436803288_0002_m_000120_124, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503443532060436803288_0002_m_000120_124}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503443532060436803288_0002}; taskId=attempt_202105151400503443532060436803288_0002_m_000120_124, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25fc971d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:44,520] {docker.py:276} INFO - 21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:44,520] {docker.py:276} INFO - 21/05/15 14:01:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:44,521] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400503443532060436803288_0002_m_000120_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503443532060436803288_0002_m_000120_124
[2021-05-15 11:01:44,521] {docker.py:276} INFO - 21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506382145659597596491_0002_m_000121_125, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506382145659597596491_0002_m_000121_125}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506382145659597596491_0002}; taskId=attempt_202105151400506382145659597596491_0002_m_000121_125, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f980785}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:44,522] {docker.py:276} INFO - 21/05/15 14:01:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:44,522] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400506382145659597596491_0002_m_000121_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506382145659597596491_0002_m_000121_125
[2021-05-15 11:01:44,524] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400503443532060436803288_0002_m_000120_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503443532060436803288_0002_m_000120_124 : duration 0:00.004s
[2021-05-15 11:01:44,525] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400506382145659597596491_0002_m_000121_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506382145659597596491_0002_m_000121_125 : duration 0:00.004s
[2021-05-15 11:01:44,594] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400503095775330455599985_0002_m_000118_122: needsTaskCommit() Task attempt_202105151400503095775330455599985_0002_m_000118_122
[2021-05-15 11:01:44,595] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400503095775330455599985_0002_m_000118_122: needsTaskCommit() Task attempt_202105151400503095775330455599985_0002_m_000118_122: duration 0:00.001s
21/05/15 14:01:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503095775330455599985_0002_m_000118_122
[2021-05-15 11:01:44,598] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Finished task 118.0 in stage 2.0 (TID 122). 4587 bytes result sent to driver
[2021-05-15 11:01:44,599] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Starting task 122.0 in stage 2.0 (TID 126) (8951b5f85146, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:44,601] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Running task 122.0 in stage 2.0 (TID 126)
[2021-05-15 11:01:44,601] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Finished task 118.0 in stage 2.0 (TID 122) in 1690 ms on 8951b5f85146 (executor driver) (119/200)
[2021-05-15 11:01:44,618] {docker.py:276} INFO - 21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Getting 3 (1229.0 B) non-empty blocks including 3 (1229.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:44,618] {docker.py:276} INFO - 21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:44,620] {docker.py:276} INFO - 21/05/15 14:01:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:44,620] {docker.py:276} INFO - 21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507924844663875288812_0002_m_000122_126, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507924844663875288812_0002_m_000122_126}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507924844663875288812_0002}; taskId=attempt_202105151400507924844663875288812_0002_m_000122_126, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f4afdc2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:44,621] {docker.py:276} INFO - 21/05/15 14:01:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:44,621] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400507924844663875288812_0002_m_000122_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507924844663875288812_0002_m_000122_126
[2021-05-15 11:01:44,623] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400507924844663875288812_0002_m_000122_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507924844663875288812_0002_m_000122_126 : duration 0:00.003s
[2021-05-15 11:01:44,763] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400503829162184569834623_0002_m_000119_123: needsTaskCommit() Task attempt_202105151400503829162184569834623_0002_m_000119_123
[2021-05-15 11:01:44,764] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400503829162184569834623_0002_m_000119_123: needsTaskCommit() Task attempt_202105151400503829162184569834623_0002_m_000119_123: duration 0:00.001s
21/05/15 14:01:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503829162184569834623_0002_m_000119_123
[2021-05-15 11:01:44,765] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Finished task 119.0 in stage 2.0 (TID 123). 4587 bytes result sent to driver
[2021-05-15 11:01:44,768] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Starting task 123.0 in stage 2.0 (TID 127) (8951b5f85146, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:44,770] {docker.py:276} INFO - 21/05/15 14:01:44 INFO TaskSetManager: Finished task 119.0 in stage 2.0 (TID 123) in 1634 ms on 8951b5f85146 (executor driver) (120/200)
[2021-05-15 11:01:44,771] {docker.py:276} INFO - 21/05/15 14:01:44 INFO Executor: Running task 123.0 in stage 2.0 (TID 127)
[2021-05-15 11:01:44,783] {docker.py:276} INFO - 21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Getting 3 (1397.0 B) non-empty blocks including 3 (1397.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:44,785] {docker.py:276} INFO - 21/05/15 14:01:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502330138634064617211_0002_m_000123_127, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502330138634064617211_0002_m_000123_127}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502330138634064617211_0002}; taskId=attempt_202105151400502330138634064617211_0002_m_000123_127, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7083322d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:44 INFO StagingCommitter: Starting: Task committer attempt_202105151400502330138634064617211_0002_m_000123_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502330138634064617211_0002_m_000123_127
[2021-05-15 11:01:44,788] {docker.py:276} INFO - 21/05/15 14:01:44 INFO StagingCommitter: Task committer attempt_202105151400502330138634064617211_0002_m_000123_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502330138634064617211_0002_m_000123_127 : duration 0:00.003s
[2021-05-15 11:01:46,187] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400503443532060436803288_0002_m_000120_124: needsTaskCommit() Task attempt_202105151400503443532060436803288_0002_m_000120_124
[2021-05-15 11:01:46,189] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400503443532060436803288_0002_m_000120_124: needsTaskCommit() Task attempt_202105151400503443532060436803288_0002_m_000120_124: duration 0:00.002s
21/05/15 14:01:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503443532060436803288_0002_m_000120_124
[2021-05-15 11:01:46,192] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Finished task 120.0 in stage 2.0 (TID 124). 4587 bytes result sent to driver
[2021-05-15 11:01:46,194] {docker.py:276} INFO - 21/05/15 14:01:46 INFO TaskSetManager: Starting task 124.0 in stage 2.0 (TID 128) (8951b5f85146, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:46,195] {docker.py:276} INFO - 21/05/15 14:01:46 INFO TaskSetManager: Finished task 120.0 in stage 2.0 (TID 124) in 1701 ms on 8951b5f85146 (executor driver) (121/200)
[2021-05-15 11:01:46,196] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Running task 124.0 in stage 2.0 (TID 128)
[2021-05-15 11:01:46,205] {docker.py:276} INFO - 21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:46,207] {docker.py:276} INFO - 21/05/15 14:01:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:46,208] {docker.py:276} INFO - 21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507731816467987870365_0002_m_000124_128, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507731816467987870365_0002_m_000124_128}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507731816467987870365_0002}; taskId=attempt_202105151400507731816467987870365_0002_m_000124_128, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c017e18}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:46,208] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400507731816467987870365_0002_m_000124_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507731816467987870365_0002_m_000124_128
[2021-05-15 11:01:46,211] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400507731816467987870365_0002_m_000124_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507731816467987870365_0002_m_000124_128 : duration 0:00.004s
[2021-05-15 11:01:46,216] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400506382145659597596491_0002_m_000121_125: needsTaskCommit() Task attempt_202105151400506382145659597596491_0002_m_000121_125
21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400506382145659597596491_0002_m_000121_125: needsTaskCommit() Task attempt_202105151400506382145659597596491_0002_m_000121_125: duration 0:00.000s
21/05/15 14:01:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506382145659597596491_0002_m_000121_125
[2021-05-15 11:01:46,217] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Finished task 121.0 in stage 2.0 (TID 125). 4587 bytes result sent to driver
[2021-05-15 11:01:46,218] {docker.py:276} INFO - 21/05/15 14:01:46 INFO TaskSetManager: Starting task 125.0 in stage 2.0 (TID 129) (8951b5f85146, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:46,219] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Running task 125.0 in stage 2.0 (TID 129)
21/05/15 14:01:46 INFO TaskSetManager: Finished task 121.0 in stage 2.0 (TID 125) in 1720 ms on 8951b5f85146 (executor driver) (122/200)
[2021-05-15 11:01:46,232] {docker.py:276} INFO - 21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Getting 3 (1278.0 B) non-empty blocks including 3 (1278.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:46,234] {docker.py:276} INFO - 21/05/15 14:01:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502170251446400845644_0002_m_000125_129, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502170251446400845644_0002_m_000125_129}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502170251446400845644_0002}; taskId=attempt_202105151400502170251446400845644_0002_m_000125_129, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8a3e813}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:46,234] {docker.py:276} INFO - 21/05/15 14:01:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400502170251446400845644_0002_m_000125_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502170251446400845644_0002_m_000125_129
[2021-05-15 11:01:46,237] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400502170251446400845644_0002_m_000125_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502170251446400845644_0002_m_000125_129 : duration 0:00.003s
[2021-05-15 11:01:46,362] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400507924844663875288812_0002_m_000122_126: needsTaskCommit() Task attempt_202105151400507924844663875288812_0002_m_000122_126
[2021-05-15 11:01:46,363] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400507924844663875288812_0002_m_000122_126: needsTaskCommit() Task attempt_202105151400507924844663875288812_0002_m_000122_126: duration 0:00.001s
[2021-05-15 11:01:46,364] {docker.py:276} INFO - 21/05/15 14:01:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507924844663875288812_0002_m_000122_126
[2021-05-15 11:01:46,365] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Finished task 122.0 in stage 2.0 (TID 126). 4587 bytes result sent to driver
[2021-05-15 11:01:46,366] {docker.py:276} INFO - 21/05/15 14:01:46 INFO TaskSetManager: Starting task 126.0 in stage 2.0 (TID 130) (8951b5f85146, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:46,367] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Running task 126.0 in stage 2.0 (TID 130)
[2021-05-15 11:01:46,367] {docker.py:276} INFO - 21/05/15 14:01:46 INFO TaskSetManager: Finished task 122.0 in stage 2.0 (TID 126) in 1771 ms on 8951b5f85146 (executor driver) (123/200)
[2021-05-15 11:01:46,381] {docker.py:276} INFO - 21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Getting 3 (1269.0 B) non-empty blocks including 3 (1269.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:46,382] {docker.py:276} INFO - 21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:46,384] {docker.py:276} INFO - 21/05/15 14:01:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:46,384] {docker.py:276} INFO - 21/05/15 14:01:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:46,385] {docker.py:276} INFO - 21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:46,385] {docker.py:276} INFO - 21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503295052679728888677_0002_m_000126_130, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503295052679728888677_0002_m_000126_130}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503295052679728888677_0002}; taskId=attempt_202105151400503295052679728888677_0002_m_000126_130, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c2283eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:46,386] {docker.py:276} INFO - 21/05/15 14:01:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:46,386] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400503295052679728888677_0002_m_000126_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503295052679728888677_0002_m_000126_130
[2021-05-15 11:01:46,389] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400503295052679728888677_0002_m_000126_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503295052679728888677_0002_m_000126_130 : duration 0:00.003s
[2021-05-15 11:01:46,446] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400502330138634064617211_0002_m_000123_127: needsTaskCommit() Task attempt_202105151400502330138634064617211_0002_m_000123_127
21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400502330138634064617211_0002_m_000123_127: needsTaskCommit() Task attempt_202105151400502330138634064617211_0002_m_000123_127: duration 0:00.001s
[2021-05-15 11:01:46,447] {docker.py:276} INFO - 21/05/15 14:01:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502330138634064617211_0002_m_000123_127
[2021-05-15 11:01:46,450] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Finished task 123.0 in stage 2.0 (TID 127). 4587 bytes result sent to driver
[2021-05-15 11:01:46,451] {docker.py:276} INFO - 21/05/15 14:01:46 INFO TaskSetManager: Starting task 127.0 in stage 2.0 (TID 131) (8951b5f85146, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:46,453] {docker.py:276} INFO - 21/05/15 14:01:46 INFO Executor: Running task 127.0 in stage 2.0 (TID 131)
21/05/15 14:01:46 INFO TaskSetManager: Finished task 123.0 in stage 2.0 (TID 127) in 1688 ms on 8951b5f85146 (executor driver) (124/200)
[2021-05-15 11:01:46,469] {docker.py:276} INFO - 21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:46,470] {docker.py:276} INFO - 21/05/15 14:01:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:46,471] {docker.py:276} INFO - 21/05/15 14:01:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507258182256771443434_0002_m_000127_131, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507258182256771443434_0002_m_000127_131}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507258182256771443434_0002}; taskId=attempt_202105151400507258182256771443434_0002_m_000127_131, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@572484c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:46 INFO StagingCommitter: Starting: Task committer attempt_202105151400507258182256771443434_0002_m_000127_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507258182256771443434_0002_m_000127_131
[2021-05-15 11:01:46,474] {docker.py:276} INFO - 21/05/15 14:01:46 INFO StagingCommitter: Task committer attempt_202105151400507258182256771443434_0002_m_000127_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507258182256771443434_0002_m_000127_131 : duration 0:00.002s
[2021-05-15 11:01:47,867] {docker.py:276} INFO - 21/05/15 14:01:47 INFO StagingCommitter: Starting: Task committer attempt_202105151400507731816467987870365_0002_m_000124_128: needsTaskCommit() Task attempt_202105151400507731816467987870365_0002_m_000124_128
[2021-05-15 11:01:47,868] {docker.py:276} INFO - 21/05/15 14:01:47 INFO StagingCommitter: Task committer attempt_202105151400507731816467987870365_0002_m_000124_128: needsTaskCommit() Task attempt_202105151400507731816467987870365_0002_m_000124_128: duration 0:00.001s
21/05/15 14:01:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507731816467987870365_0002_m_000124_128
[2021-05-15 11:01:47,869] {docker.py:276} INFO - 21/05/15 14:01:47 INFO Executor: Finished task 124.0 in stage 2.0 (TID 128). 4587 bytes result sent to driver
[2021-05-15 11:01:47,870] {docker.py:276} INFO - 21/05/15 14:01:47 INFO TaskSetManager: Starting task 128.0 in stage 2.0 (TID 132) (8951b5f85146, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:47,871] {docker.py:276} INFO - 21/05/15 14:01:47 INFO TaskSetManager: Finished task 124.0 in stage 2.0 (TID 128) in 1680 ms on 8951b5f85146 (executor driver) (125/200)
[2021-05-15 11:01:47,873] {docker.py:276} INFO - 21/05/15 14:01:47 INFO Executor: Running task 128.0 in stage 2.0 (TID 132)
[2021-05-15 11:01:47,890] {docker.py:276} INFO - 21/05/15 14:01:47 INFO ShuffleBlockFetcherIterator: Getting 3 (1107.0 B) non-empty blocks including 3 (1107.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:47,890] {docker.py:276} INFO - 21/05/15 14:01:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:47,893] {docker.py:276} INFO - 21/05/15 14:01:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:47,894] {docker.py:276} INFO - 21/05/15 14:01:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:47,894] {docker.py:276} INFO - 21/05/15 14:01:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050540966888818554311_0002_m_000128_132, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050540966888818554311_0002_m_000128_132}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050540966888818554311_0002}; taskId=attempt_20210515140050540966888818554311_0002_m_000128_132, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52ec2cdc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:47,895] {docker.py:276} INFO - 21/05/15 14:01:47 INFO StagingCommitter: Starting: Task committer attempt_20210515140050540966888818554311_0002_m_000128_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050540966888818554311_0002_m_000128_132
[2021-05-15 11:01:47,897] {docker.py:276} INFO - 21/05/15 14:01:47 INFO StagingCommitter: Task committer attempt_20210515140050540966888818554311_0002_m_000128_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050540966888818554311_0002_m_000128_132 : duration 0:00.004s
[2021-05-15 11:01:47,975] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Starting: Task committer attempt_202105151400502170251446400845644_0002_m_000125_129: needsTaskCommit() Task attempt_202105151400502170251446400845644_0002_m_000125_129
21/05/15 14:01:48 INFO StagingCommitter: Task committer attempt_202105151400502170251446400845644_0002_m_000125_129: needsTaskCommit() Task attempt_202105151400502170251446400845644_0002_m_000125_129: duration 0:00.000s
21/05/15 14:01:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502170251446400845644_0002_m_000125_129
[2021-05-15 11:01:47,976] {docker.py:276} INFO - 21/05/15 14:01:48 INFO Executor: Finished task 125.0 in stage 2.0 (TID 129). 4587 bytes result sent to driver
[2021-05-15 11:01:47,977] {docker.py:276} INFO - 21/05/15 14:01:48 INFO TaskSetManager: Starting task 129.0 in stage 2.0 (TID 133) (8951b5f85146, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:47,978] {docker.py:276} INFO - 21/05/15 14:01:48 INFO Executor: Running task 129.0 in stage 2.0 (TID 133)
21/05/15 14:01:48 INFO TaskSetManager: Finished task 125.0 in stage 2.0 (TID 129) in 1762 ms on 8951b5f85146 (executor driver) (126/200)
[2021-05-15 11:01:47,995] {docker.py:276} INFO - 21/05/15 14:01:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1178.0 B) non-empty blocks including 3 (1178.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:47,997] {docker.py:276} INFO - 21/05/15 14:01:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501387198820355085183_0002_m_000129_133, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501387198820355085183_0002_m_000129_133}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501387198820355085183_0002}; taskId=attempt_202105151400501387198820355085183_0002_m_000129_133, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@373b4a03}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:48 INFO StagingCommitter: Starting: Task committer attempt_202105151400501387198820355085183_0002_m_000129_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501387198820355085183_0002_m_000129_133
[2021-05-15 11:01:48,001] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Task committer attempt_202105151400501387198820355085183_0002_m_000129_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501387198820355085183_0002_m_000129_133 : duration 0:00.004s
[2021-05-15 11:01:48,070] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Starting: Task committer attempt_202105151400503295052679728888677_0002_m_000126_130: needsTaskCommit() Task attempt_202105151400503295052679728888677_0002_m_000126_130
[2021-05-15 11:01:48,071] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Task committer attempt_202105151400503295052679728888677_0002_m_000126_130: needsTaskCommit() Task attempt_202105151400503295052679728888677_0002_m_000126_130: duration 0:00.001s
21/05/15 14:01:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503295052679728888677_0002_m_000126_130
[2021-05-15 11:01:48,074] {docker.py:276} INFO - 21/05/15 14:01:48 INFO Executor: Finished task 126.0 in stage 2.0 (TID 130). 4587 bytes result sent to driver
[2021-05-15 11:01:48,076] {docker.py:276} INFO - 21/05/15 14:01:48 INFO TaskSetManager: Starting task 130.0 in stage 2.0 (TID 134) (8951b5f85146, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:48,077] {docker.py:276} INFO - 21/05/15 14:01:48 INFO Executor: Running task 130.0 in stage 2.0 (TID 134)
[2021-05-15 11:01:48,077] {docker.py:276} INFO - 21/05/15 14:01:48 INFO TaskSetManager: Finished task 126.0 in stage 2.0 (TID 130) in 1713 ms on 8951b5f85146 (executor driver) (127/200)
[2021-05-15 11:01:48,092] {docker.py:276} INFO - 21/05/15 14:01:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:48,094] {docker.py:276} INFO - 21/05/15 14:01:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505048091943216696199_0002_m_000130_134, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505048091943216696199_0002_m_000130_134}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505048091943216696199_0002}; taskId=attempt_202105151400505048091943216696199_0002_m_000130_134, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a90ef59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:48,094] {docker.py:276} INFO - 21/05/15 14:01:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:48 INFO StagingCommitter: Starting: Task committer attempt_202105151400505048091943216696199_0002_m_000130_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505048091943216696199_0002_m_000130_134
[2021-05-15 11:01:48,097] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Task committer attempt_202105151400505048091943216696199_0002_m_000130_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505048091943216696199_0002_m_000130_134 : duration 0:00.002s
[2021-05-15 11:01:48,176] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Starting: Task committer attempt_202105151400507258182256771443434_0002_m_000127_131: needsTaskCommit() Task attempt_202105151400507258182256771443434_0002_m_000127_131
[2021-05-15 11:01:48,177] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Task committer attempt_202105151400507258182256771443434_0002_m_000127_131: needsTaskCommit() Task attempt_202105151400507258182256771443434_0002_m_000127_131: duration 0:00.001s
[2021-05-15 11:01:48,178] {docker.py:276} INFO - 21/05/15 14:01:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507258182256771443434_0002_m_000127_131
[2021-05-15 11:01:48,180] {docker.py:276} INFO - 21/05/15 14:01:48 INFO Executor: Finished task 127.0 in stage 2.0 (TID 131). 4587 bytes result sent to driver
[2021-05-15 11:01:48,181] {docker.py:276} INFO - 21/05/15 14:01:48 INFO TaskSetManager: Starting task 131.0 in stage 2.0 (TID 135) (8951b5f85146, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:48,182] {docker.py:276} INFO - 21/05/15 14:01:48 INFO Executor: Running task 131.0 in stage 2.0 (TID 135)
[2021-05-15 11:01:48,182] {docker.py:276} INFO - 21/05/15 14:01:48 INFO TaskSetManager: Finished task 127.0 in stage 2.0 (TID 131) in 1733 ms on 8951b5f85146 (executor driver) (128/200)
[2021-05-15 11:01:48,196] {docker.py:276} INFO - 21/05/15 14:01:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:48,198] {docker.py:276} INFO - 21/05/15 14:01:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:48,198] {docker.py:276} INFO - 21/05/15 14:01:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506500665449404140962_0002_m_000131_135, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506500665449404140962_0002_m_000131_135}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506500665449404140962_0002}; taskId=attempt_202105151400506500665449404140962_0002_m_000131_135, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34760b83}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:48 INFO StagingCommitter: Starting: Task committer attempt_202105151400506500665449404140962_0002_m_000131_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506500665449404140962_0002_m_000131_135
[2021-05-15 11:01:48,200] {docker.py:276} INFO - 21/05/15 14:01:48 INFO StagingCommitter: Task committer attempt_202105151400506500665449404140962_0002_m_000131_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506500665449404140962_0002_m_000131_135 : duration 0:00.002s
[2021-05-15 11:01:49,594] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_20210515140050540966888818554311_0002_m_000128_132: needsTaskCommit() Task attempt_20210515140050540966888818554311_0002_m_000128_132
[2021-05-15 11:01:49,595] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_20210515140050540966888818554311_0002_m_000128_132: needsTaskCommit() Task attempt_20210515140050540966888818554311_0002_m_000128_132: duration 0:00.001s
[2021-05-15 11:01:49,596] {docker.py:276} INFO - 21/05/15 14:01:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050540966888818554311_0002_m_000128_132
[2021-05-15 11:01:49,598] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Finished task 128.0 in stage 2.0 (TID 132). 4587 bytes result sent to driver
[2021-05-15 11:01:49,600] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Starting task 132.0 in stage 2.0 (TID 136) (8951b5f85146, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:49,601] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Finished task 128.0 in stage 2.0 (TID 132) in 1733 ms on 8951b5f85146 (executor driver) (129/200)
[2021-05-15 11:01:49,601] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Running task 132.0 in stage 2.0 (TID 136)
[2021-05-15 11:01:49,616] {docker.py:276} INFO - 21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:49,618] {docker.py:276} INFO - 21/05/15 14:01:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:49,618] {docker.py:276} INFO - 21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501388560922822445782_0002_m_000132_136, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501388560922822445782_0002_m_000132_136}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501388560922822445782_0002}; taskId=attempt_202105151400501388560922822445782_0002_m_000132_136, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b29205a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_202105151400501388560922822445782_0002_m_000132_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501388560922822445782_0002_m_000132_136
[2021-05-15 11:01:49,621] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_202105151400501388560922822445782_0002_m_000132_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501388560922822445782_0002_m_000132_136 : duration 0:00.004s
[2021-05-15 11:01:49,743] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_202105151400501387198820355085183_0002_m_000129_133: needsTaskCommit() Task attempt_202105151400501387198820355085183_0002_m_000129_133
[2021-05-15 11:01:49,743] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_202105151400501387198820355085183_0002_m_000129_133: needsTaskCommit() Task attempt_202105151400501387198820355085183_0002_m_000129_133: duration 0:00.001s
21/05/15 14:01:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501387198820355085183_0002_m_000129_133
[2021-05-15 11:01:49,746] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Finished task 129.0 in stage 2.0 (TID 133). 4587 bytes result sent to driver
[2021-05-15 11:01:49,747] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Starting task 133.0 in stage 2.0 (TID 137) (8951b5f85146, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:49,749] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Running task 133.0 in stage 2.0 (TID 137)
[2021-05-15 11:01:49,749] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Finished task 129.0 in stage 2.0 (TID 133) in 1773 ms on 8951b5f85146 (executor driver) (130/200)
[2021-05-15 11:01:49,765] {docker.py:276} INFO - 21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Getting 3 (1385.0 B) non-empty blocks including 3 (1385.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:49,766] {docker.py:276} INFO - 21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:49,767] {docker.py:276} INFO - 21/05/15 14:01:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:49,767] {docker.py:276} INFO - 21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050738670491958927284_0002_m_000133_137, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050738670491958927284_0002_m_000133_137}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050738670491958927284_0002}; taskId=attempt_20210515140050738670491958927284_0002_m_000133_137, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1db6e648}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_20210515140050738670491958927284_0002_m_000133_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050738670491958927284_0002_m_000133_137
[2021-05-15 11:01:49,769] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_20210515140050738670491958927284_0002_m_000133_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050738670491958927284_0002_m_000133_137 : duration 0:00.002s
[2021-05-15 11:01:49,788] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_202105151400505048091943216696199_0002_m_000130_134: needsTaskCommit() Task attempt_202105151400505048091943216696199_0002_m_000130_134
[2021-05-15 11:01:49,788] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_202105151400505048091943216696199_0002_m_000130_134: needsTaskCommit() Task attempt_202105151400505048091943216696199_0002_m_000130_134: duration 0:00.001s
21/05/15 14:01:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505048091943216696199_0002_m_000130_134
[2021-05-15 11:01:49,789] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Finished task 130.0 in stage 2.0 (TID 134). 4587 bytes result sent to driver
[2021-05-15 11:01:49,790] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Starting task 134.0 in stage 2.0 (TID 138) (8951b5f85146, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:49,791] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Finished task 130.0 in stage 2.0 (TID 134) in 1719 ms on 8951b5f85146 (executor driver) (131/200)
[2021-05-15 11:01:49,792] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Running task 134.0 in stage 2.0 (TID 138)
[2021-05-15 11:01:49,807] {docker.py:276} INFO - 21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Getting 3 (1389.0 B) non-empty blocks including 3 (1389.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:49,808] {docker.py:276} INFO - 21/05/15 14:01:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507399340785311858810_0002_m_000134_138, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507399340785311858810_0002_m_000134_138}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507399340785311858810_0002}; taskId=attempt_202105151400507399340785311858810_0002_m_000134_138, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@185e1403}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_202105151400507399340785311858810_0002_m_000134_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507399340785311858810_0002_m_000134_138
[2021-05-15 11:01:49,811] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_202105151400507399340785311858810_0002_m_000134_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507399340785311858810_0002_m_000134_138 : duration 0:00.002s
[2021-05-15 11:01:49,859] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_202105151400506500665449404140962_0002_m_000131_135: needsTaskCommit() Task attempt_202105151400506500665449404140962_0002_m_000131_135
[2021-05-15 11:01:49,860] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_202105151400506500665449404140962_0002_m_000131_135: needsTaskCommit() Task attempt_202105151400506500665449404140962_0002_m_000131_135: duration 0:00.002s
21/05/15 14:01:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506500665449404140962_0002_m_000131_135
[2021-05-15 11:01:49,863] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Finished task 131.0 in stage 2.0 (TID 135). 4587 bytes result sent to driver
[2021-05-15 11:01:49,865] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Starting task 135.0 in stage 2.0 (TID 139) (8951b5f85146, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:49,866] {docker.py:276} INFO - 21/05/15 14:01:49 INFO Executor: Running task 135.0 in stage 2.0 (TID 139)
[2021-05-15 11:01:49,866] {docker.py:276} INFO - 21/05/15 14:01:49 INFO TaskSetManager: Finished task 131.0 in stage 2.0 (TID 135) in 1687 ms on 8951b5f85146 (executor driver) (132/200)
[2021-05-15 11:01:49,880] {docker.py:276} INFO - 21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Getting 3 (1300.0 B) non-empty blocks including 3 (1300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:49,882] {docker.py:276} INFO - 21/05/15 14:01:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:49,882] {docker.py:276} INFO - 21/05/15 14:01:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501520109146395919855_0002_m_000135_139, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501520109146395919855_0002_m_000135_139}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501520109146395919855_0002}; taskId=attempt_202105151400501520109146395919855_0002_m_000135_139, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd9b7cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:49 INFO StagingCommitter: Starting: Task committer attempt_202105151400501520109146395919855_0002_m_000135_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501520109146395919855_0002_m_000135_139
[2021-05-15 11:01:49,885] {docker.py:276} INFO - 21/05/15 14:01:49 INFO StagingCommitter: Task committer attempt_202105151400501520109146395919855_0002_m_000135_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501520109146395919855_0002_m_000135_139 : duration 0:00.003s
[2021-05-15 11:01:51,326] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400501388560922822445782_0002_m_000132_136: needsTaskCommit() Task attempt_202105151400501388560922822445782_0002_m_000132_136
[2021-05-15 11:01:51,327] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400501388560922822445782_0002_m_000132_136: needsTaskCommit() Task attempt_202105151400501388560922822445782_0002_m_000132_136: duration 0:00.001s
21/05/15 14:01:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501388560922822445782_0002_m_000132_136
[2021-05-15 11:01:51,329] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Finished task 132.0 in stage 2.0 (TID 136). 4587 bytes result sent to driver
[2021-05-15 11:01:51,331] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Starting task 136.0 in stage 2.0 (TID 140) (8951b5f85146, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:51,332] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Finished task 132.0 in stage 2.0 (TID 136) in 1735 ms on 8951b5f85146 (executor driver) (133/200)
21/05/15 14:01:51 INFO Executor: Running task 136.0 in stage 2.0 (TID 140)
[2021-05-15 11:01:51,350] {docker.py:276} INFO - 21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Getting 3 (1380.0 B) non-empty blocks including 3 (1380.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:51,351] {docker.py:276} INFO - 21/05/15 14:01:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505321016364481892049_0002_m_000136_140, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505321016364481892049_0002_m_000136_140}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505321016364481892049_0002}; taskId=attempt_202105151400505321016364481892049_0002_m_000136_140, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23e58baa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400505321016364481892049_0002_m_000136_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505321016364481892049_0002_m_000136_140
[2021-05-15 11:01:51,354] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400505321016364481892049_0002_m_000136_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505321016364481892049_0002_m_000136_140 : duration 0:00.002s
[2021-05-15 11:01:51,443] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_20210515140050738670491958927284_0002_m_000133_137: needsTaskCommit() Task attempt_20210515140050738670491958927284_0002_m_000133_137
[2021-05-15 11:01:51,444] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_20210515140050738670491958927284_0002_m_000133_137: needsTaskCommit() Task attempt_20210515140050738670491958927284_0002_m_000133_137: duration 0:00.001s
[2021-05-15 11:01:51,445] {docker.py:276} INFO - 21/05/15 14:01:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050738670491958927284_0002_m_000133_137
[2021-05-15 11:01:51,446] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Finished task 133.0 in stage 2.0 (TID 137). 4587 bytes result sent to driver
[2021-05-15 11:01:51,447] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Starting task 137.0 in stage 2.0 (TID 141) (8951b5f85146, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:51,448] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Running task 137.0 in stage 2.0 (TID 141)
21/05/15 14:01:51 INFO TaskSetManager: Finished task 133.0 in stage 2.0 (TID 137) in 1704 ms on 8951b5f85146 (executor driver) (134/200)
[2021-05-15 11:01:51,457] {docker.py:276} INFO - 21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:51,459] {docker.py:276} INFO - 21/05/15 14:01:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505925278452252122972_0002_m_000137_141, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505925278452252122972_0002_m_000137_141}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505925278452252122972_0002}; taskId=attempt_202105151400505925278452252122972_0002_m_000137_141, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fa4b29a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:51,459] {docker.py:276} INFO - 21/05/15 14:01:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:51,459] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400505925278452252122972_0002_m_000137_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505925278452252122972_0002_m_000137_141
[2021-05-15 11:01:51,461] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400505925278452252122972_0002_m_000137_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505925278452252122972_0002_m_000137_141 : duration 0:00.002s
[2021-05-15 11:01:51,500] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400507399340785311858810_0002_m_000134_138: needsTaskCommit() Task attempt_202105151400507399340785311858810_0002_m_000134_138
[2021-05-15 11:01:51,501] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400507399340785311858810_0002_m_000134_138: needsTaskCommit() Task attempt_202105151400507399340785311858810_0002_m_000134_138: duration 0:00.002s
[2021-05-15 11:01:51,502] {docker.py:276} INFO - 21/05/15 14:01:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507399340785311858810_0002_m_000134_138
[2021-05-15 11:01:51,504] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Finished task 134.0 in stage 2.0 (TID 138). 4587 bytes result sent to driver
[2021-05-15 11:01:51,506] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Starting task 138.0 in stage 2.0 (TID 142) (8951b5f85146, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:51,507] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Finished task 134.0 in stage 2.0 (TID 138) in 1718 ms on 8951b5f85146 (executor driver) (135/200)
[2021-05-15 11:01:51,508] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Running task 138.0 in stage 2.0 (TID 142)
[2021-05-15 11:01:51,519] {docker.py:276} INFO - 21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:51,522] {docker.py:276} INFO - 21/05/15 14:01:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:51,522] {docker.py:276} INFO - 21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507759420151938354556_0002_m_000138_142, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507759420151938354556_0002_m_000138_142}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507759420151938354556_0002}; taskId=attempt_202105151400507759420151938354556_0002_m_000138_142, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@203ce0f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:51,523] {docker.py:276} INFO - 21/05/15 14:01:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400507759420151938354556_0002_m_000138_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507759420151938354556_0002_m_000138_142
[2021-05-15 11:01:51,526] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400507759420151938354556_0002_m_000138_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507759420151938354556_0002_m_000138_142 : duration 0:00.004s
[2021-05-15 11:01:51,588] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400501520109146395919855_0002_m_000135_139: needsTaskCommit() Task attempt_202105151400501520109146395919855_0002_m_000135_139
[2021-05-15 11:01:51,589] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400501520109146395919855_0002_m_000135_139: needsTaskCommit() Task attempt_202105151400501520109146395919855_0002_m_000135_139: duration 0:00.002s
[2021-05-15 11:01:51,590] {docker.py:276} INFO - 21/05/15 14:01:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501520109146395919855_0002_m_000135_139
[2021-05-15 11:01:51,592] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Finished task 135.0 in stage 2.0 (TID 139). 4587 bytes result sent to driver
[2021-05-15 11:01:51,593] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Starting task 139.0 in stage 2.0 (TID 143) (8951b5f85146, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:51,594] {docker.py:276} INFO - 21/05/15 14:01:51 INFO TaskSetManager: Finished task 135.0 in stage 2.0 (TID 139) in 1732 ms on 8951b5f85146 (executor driver) (136/200)
[2021-05-15 11:01:51,595] {docker.py:276} INFO - 21/05/15 14:01:51 INFO Executor: Running task 139.0 in stage 2.0 (TID 143)
[2021-05-15 11:01:51,604] {docker.py:276} INFO - 21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Getting 3 (1117.0 B) non-empty blocks including 3 (1117.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:51,605] {docker.py:276} INFO - 21/05/15 14:01:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:51,606] {docker.py:276} INFO - 21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:51,606] {docker.py:276} INFO - 21/05/15 14:01:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507680126649253554706_0002_m_000139_143, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507680126649253554706_0002_m_000139_143}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507680126649253554706_0002}; taskId=attempt_202105151400507680126649253554706_0002_m_000139_143, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2edf035c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:51,607] {docker.py:276} INFO - 21/05/15 14:01:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:51 INFO StagingCommitter: Starting: Task committer attempt_202105151400507680126649253554706_0002_m_000139_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507680126649253554706_0002_m_000139_143
[2021-05-15 11:01:51,609] {docker.py:276} INFO - 21/05/15 14:01:51 INFO StagingCommitter: Task committer attempt_202105151400507680126649253554706_0002_m_000139_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507680126649253554706_0002_m_000139_143 : duration 0:00.003s
[2021-05-15 11:01:53,057] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400505321016364481892049_0002_m_000136_140: needsTaskCommit() Task attempt_202105151400505321016364481892049_0002_m_000136_140
[2021-05-15 11:01:53,058] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400505321016364481892049_0002_m_000136_140: needsTaskCommit() Task attempt_202105151400505321016364481892049_0002_m_000136_140: duration 0:00.001s
21/05/15 14:01:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505321016364481892049_0002_m_000136_140
[2021-05-15 11:01:53,059] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Finished task 136.0 in stage 2.0 (TID 140). 4587 bytes result sent to driver
[2021-05-15 11:01:53,061] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Starting task 140.0 in stage 2.0 (TID 144) (8951b5f85146, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:53,063] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Running task 140.0 in stage 2.0 (TID 144)
21/05/15 14:01:53 INFO TaskSetManager: Finished task 136.0 in stage 2.0 (TID 140) in 1699 ms on 8951b5f85146 (executor driver) (137/200)
[2021-05-15 11:01:53,072] {docker.py:276} INFO - 21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:53,073] {docker.py:276} INFO - 21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:53,075] {docker.py:276} INFO - 21/05/15 14:01:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:53,076] {docker.py:276} INFO - 21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508361709126789090548_0002_m_000140_144, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508361709126789090548_0002_m_000140_144}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508361709126789090548_0002}; taskId=attempt_202105151400508361709126789090548_0002_m_000140_144, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6608fab5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:53,076] {docker.py:276} INFO - 21/05/15 14:01:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400508361709126789090548_0002_m_000140_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508361709126789090548_0002_m_000140_144
[2021-05-15 11:01:53,080] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400508361709126789090548_0002_m_000140_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508361709126789090548_0002_m_000140_144 : duration 0:00.003s
[2021-05-15 11:01:53,117] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400505925278452252122972_0002_m_000137_141: needsTaskCommit() Task attempt_202105151400505925278452252122972_0002_m_000137_141
[2021-05-15 11:01:53,118] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400505925278452252122972_0002_m_000137_141: needsTaskCommit() Task attempt_202105151400505925278452252122972_0002_m_000137_141: duration 0:00.001s
21/05/15 14:01:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505925278452252122972_0002_m_000137_141
[2021-05-15 11:01:53,119] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Finished task 137.0 in stage 2.0 (TID 141). 4587 bytes result sent to driver
[2021-05-15 11:01:53,121] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Starting task 141.0 in stage 2.0 (TID 145) (8951b5f85146, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:53,123] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Running task 141.0 in stage 2.0 (TID 145)
[2021-05-15 11:01:53,124] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Finished task 137.0 in stage 2.0 (TID 141) in 1642 ms on 8951b5f85146 (executor driver) (138/200)
[2021-05-15 11:01:53,133] {docker.py:276} INFO - 21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Getting 3 (1429.0 B) non-empty blocks including 3 (1429.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:53,135] {docker.py:276} INFO - 21/05/15 14:01:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:53,135] {docker.py:276} INFO - 21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507508056890033403406_0002_m_000141_145, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507508056890033403406_0002_m_000141_145}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507508056890033403406_0002}; taskId=attempt_202105151400507508056890033403406_0002_m_000141_145, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7042335a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:53,136] {docker.py:276} INFO - 21/05/15 14:01:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400507508056890033403406_0002_m_000141_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507508056890033403406_0002_m_000141_145
[2021-05-15 11:01:53,137] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400507759420151938354556_0002_m_000138_142: needsTaskCommit() Task attempt_202105151400507759420151938354556_0002_m_000138_142
[2021-05-15 11:01:53,138] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400507759420151938354556_0002_m_000138_142: needsTaskCommit() Task attempt_202105151400507759420151938354556_0002_m_000138_142: duration 0:00.001s
[2021-05-15 11:01:53,138] {docker.py:276} INFO - 21/05/15 14:01:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507759420151938354556_0002_m_000138_142
[2021-05-15 11:01:53,139] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400507508056890033403406_0002_m_000141_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507508056890033403406_0002_m_000141_145 : duration 0:00.004s
[2021-05-15 11:01:53,141] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Finished task 138.0 in stage 2.0 (TID 142). 4587 bytes result sent to driver
[2021-05-15 11:01:53,142] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Starting task 142.0 in stage 2.0 (TID 146) (8951b5f85146, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:53,143] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Finished task 138.0 in stage 2.0 (TID 142) in 1604 ms on 8951b5f85146 (executor driver) (139/200)
[2021-05-15 11:01:53,144] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Running task 142.0 in stage 2.0 (TID 146)
[2021-05-15 11:01:53,151] {docker.py:276} INFO - 21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Getting 3 (1178.0 B) non-empty blocks including 3 (1178.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:53,152] {docker.py:276} INFO - 21/05/15 14:01:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506618108122808768376_0002_m_000142_146, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506618108122808768376_0002_m_000142_146}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506618108122808768376_0002}; taskId=attempt_202105151400506618108122808768376_0002_m_000142_146, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e627edb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:53,152] {docker.py:276} INFO - 21/05/15 14:01:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400506618108122808768376_0002_m_000142_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506618108122808768376_0002_m_000142_146
[2021-05-15 11:01:53,156] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400506618108122808768376_0002_m_000142_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506618108122808768376_0002_m_000142_146 : duration 0:00.003s
[2021-05-15 11:01:53,293] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_202105151400507680126649253554706_0002_m_000139_143: needsTaskCommit() Task attempt_202105151400507680126649253554706_0002_m_000139_143
[2021-05-15 11:01:53,294] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_202105151400507680126649253554706_0002_m_000139_143: needsTaskCommit() Task attempt_202105151400507680126649253554706_0002_m_000139_143: duration 0:00.001s
21/05/15 14:01:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507680126649253554706_0002_m_000139_143
[2021-05-15 11:01:53,296] {docker.py:276} INFO - 21/05/15 14:01:53 INFO Executor: Finished task 139.0 in stage 2.0 (TID 143). 4587 bytes result sent to driver
[2021-05-15 11:01:53,298] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Starting task 143.0 in stage 2.0 (TID 147) (8951b5f85146, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:53,300] {docker.py:276} INFO - 21/05/15 14:01:53 INFO TaskSetManager: Finished task 139.0 in stage 2.0 (TID 143) in 1673 ms on 8951b5f85146 (executor driver) (140/200)
21/05/15 14:01:53 INFO Executor: Running task 143.0 in stage 2.0 (TID 147)
[2021-05-15 11:01:53,318] {docker.py:276} INFO - 21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Getting 3 (1036.0 B) non-empty blocks including 3 (1036.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:53,320] {docker.py:276} INFO - 21/05/15 14:01:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050450027131510885985_0002_m_000143_147, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050450027131510885985_0002_m_000143_147}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050450027131510885985_0002}; taskId=attempt_20210515140050450027131510885985_0002_m_000143_147, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32bd3552}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:53,320] {docker.py:276} INFO - 21/05/15 14:01:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:53 INFO StagingCommitter: Starting: Task committer attempt_20210515140050450027131510885985_0002_m_000143_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050450027131510885985_0002_m_000143_147
[2021-05-15 11:01:53,323] {docker.py:276} INFO - 21/05/15 14:01:53 INFO StagingCommitter: Task committer attempt_20210515140050450027131510885985_0002_m_000143_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050450027131510885985_0002_m_000143_147 : duration 0:00.003s
[2021-05-15 11:01:54,757] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400508361709126789090548_0002_m_000140_144: needsTaskCommit() Task attempt_202105151400508361709126789090548_0002_m_000140_144
21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400508361709126789090548_0002_m_000140_144: needsTaskCommit() Task attempt_202105151400508361709126789090548_0002_m_000140_144: duration 0:00.000s
21/05/15 14:01:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508361709126789090548_0002_m_000140_144
[2021-05-15 11:01:54,759] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Finished task 140.0 in stage 2.0 (TID 144). 4587 bytes result sent to driver
[2021-05-15 11:01:54,760] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Starting task 144.0 in stage 2.0 (TID 148) (8951b5f85146, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:54,762] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Finished task 140.0 in stage 2.0 (TID 144) in 1703 ms on 8951b5f85146 (executor driver) (141/200)
[2021-05-15 11:01:54,763] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Running task 144.0 in stage 2.0 (TID 148)
[2021-05-15 11:01:54,771] {docker.py:276} INFO - 21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1199.0 B) non-empty blocks including 3 (1199.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:54,774] {docker.py:276} INFO - 21/05/15 14:01:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:54,775] {docker.py:276} INFO - 21/05/15 14:01:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:54,775] {docker.py:276} INFO - 21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508459043729785330836_0002_m_000144_148, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508459043729785330836_0002_m_000144_148}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508459043729785330836_0002}; taskId=attempt_202105151400508459043729785330836_0002_m_000144_148, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52179a6b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:54,776] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400508459043729785330836_0002_m_000144_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508459043729785330836_0002_m_000144_148
[2021-05-15 11:01:54,779] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400508459043729785330836_0002_m_000144_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508459043729785330836_0002_m_000144_148 : duration 0:00.003s
[2021-05-15 11:01:54,840] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400507508056890033403406_0002_m_000141_145: needsTaskCommit() Task attempt_202105151400507508056890033403406_0002_m_000141_145
[2021-05-15 11:01:54,842] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400507508056890033403406_0002_m_000141_145: needsTaskCommit() Task attempt_202105151400507508056890033403406_0002_m_000141_145: duration 0:00.001s
21/05/15 14:01:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507508056890033403406_0002_m_000141_145
[2021-05-15 11:01:54,844] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Finished task 141.0 in stage 2.0 (TID 145). 4587 bytes result sent to driver
[2021-05-15 11:01:54,845] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Starting task 145.0 in stage 2.0 (TID 149) (8951b5f85146, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:54,847] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Running task 145.0 in stage 2.0 (TID 149)
[2021-05-15 11:01:54,847] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Finished task 141.0 in stage 2.0 (TID 145) in 1728 ms on 8951b5f85146 (executor driver) (142/200)
[2021-05-15 11:01:54,848] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400506618108122808768376_0002_m_000142_146: needsTaskCommit() Task attempt_202105151400506618108122808768376_0002_m_000142_146
[2021-05-15 11:01:54,849] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400506618108122808768376_0002_m_000142_146: needsTaskCommit() Task attempt_202105151400506618108122808768376_0002_m_000142_146: duration 0:00.000s
[2021-05-15 11:01:54,850] {docker.py:276} INFO - 21/05/15 14:01:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506618108122808768376_0002_m_000142_146
[2021-05-15 11:01:54,851] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Finished task 142.0 in stage 2.0 (TID 146). 4587 bytes result sent to driver
[2021-05-15 11:01:54,851] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Starting task 146.0 in stage 2.0 (TID 150) (8951b5f85146, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:54,853] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Finished task 142.0 in stage 2.0 (TID 146) in 1712 ms on 8951b5f85146 (executor driver) (143/200)
[2021-05-15 11:01:54,853] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Running task 146.0 in stage 2.0 (TID 150)
[2021-05-15 11:01:54,867] {docker.py:276} INFO - 21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1429.0 B) non-empty blocks including 3 (1429.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:54,868] {docker.py:276} INFO - 21/05/15 14:01:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503011555732460939260_0002_m_000145_149, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503011555732460939260_0002_m_000145_149}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503011555732460939260_0002}; taskId=attempt_202105151400503011555732460939260_0002_m_000145_149, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e29ab9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400503011555732460939260_0002_m_000145_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503011555732460939260_0002_m_000145_149
[2021-05-15 11:01:54,870] {docker.py:276} INFO - 21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:54,871] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400503011555732460939260_0002_m_000145_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503011555732460939260_0002_m_000145_149 : duration 0:00.003s
[2021-05-15 11:01:54,872] {docker.py:276} INFO - 21/05/15 14:01:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:54,872] {docker.py:276} INFO - 21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:54,873] {docker.py:276} INFO - 21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508781504823739539564_0002_m_000146_150, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508781504823739539564_0002_m_000146_150}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508781504823739539564_0002}; taskId=attempt_202105151400508781504823739539564_0002_m_000146_150, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63267503}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:54,873] {docker.py:276} INFO - 21/05/15 14:01:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400508781504823739539564_0002_m_000146_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508781504823739539564_0002_m_000146_150
[2021-05-15 11:01:54,875] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400508781504823739539564_0002_m_000146_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508781504823739539564_0002_m_000146_150 : duration 0:00.003s
[2021-05-15 11:01:54,945] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_20210515140050450027131510885985_0002_m_000143_147: needsTaskCommit() Task attempt_20210515140050450027131510885985_0002_m_000143_147
[2021-05-15 11:01:54,946] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_20210515140050450027131510885985_0002_m_000143_147: needsTaskCommit() Task attempt_20210515140050450027131510885985_0002_m_000143_147: duration 0:00.001s
21/05/15 14:01:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050450027131510885985_0002_m_000143_147
[2021-05-15 11:01:54,948] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Finished task 143.0 in stage 2.0 (TID 147). 4587 bytes result sent to driver
[2021-05-15 11:01:54,949] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Starting task 147.0 in stage 2.0 (TID 151) (8951b5f85146, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:54,950] {docker.py:276} INFO - 21/05/15 14:01:54 INFO TaskSetManager: Finished task 143.0 in stage 2.0 (TID 147) in 1654 ms on 8951b5f85146 (executor driver) (144/200)
[2021-05-15 11:01:54,951] {docker.py:276} INFO - 21/05/15 14:01:54 INFO Executor: Running task 147.0 in stage 2.0 (TID 151)
[2021-05-15 11:01:54,960] {docker.py:276} INFO - 21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Getting 3 (1184.0 B) non-empty blocks including 3 (1184.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:54,962] {docker.py:276} INFO - 21/05/15 14:01:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:54,962] {docker.py:276} INFO - 21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:54,962] {docker.py:276} INFO - 21/05/15 14:01:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502255848828764674477_0002_m_000147_151, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502255848828764674477_0002_m_000147_151}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502255848828764674477_0002}; taskId=attempt_202105151400502255848828764674477_0002_m_000147_151, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3709a2f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:54,963] {docker.py:276} INFO - 21/05/15 14:01:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:54,963] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Starting: Task committer attempt_202105151400502255848828764674477_0002_m_000147_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502255848828764674477_0002_m_000147_151
[2021-05-15 11:01:54,965] {docker.py:276} INFO - 21/05/15 14:01:54 INFO StagingCommitter: Task committer attempt_202105151400502255848828764674477_0002_m_000147_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502255848828764674477_0002_m_000147_151 : duration 0:00.003s
[2021-05-15 11:01:56,421] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400508459043729785330836_0002_m_000144_148: needsTaskCommit() Task attempt_202105151400508459043729785330836_0002_m_000144_148
[2021-05-15 11:01:56,422] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400508459043729785330836_0002_m_000144_148: needsTaskCommit() Task attempt_202105151400508459043729785330836_0002_m_000144_148: duration 0:00.002s
21/05/15 14:01:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508459043729785330836_0002_m_000144_148
[2021-05-15 11:01:56,425] {docker.py:276} INFO - 21/05/15 14:01:56 INFO Executor: Finished task 144.0 in stage 2.0 (TID 148). 4587 bytes result sent to driver
[2021-05-15 11:01:56,426] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Starting task 148.0 in stage 2.0 (TID 152) (8951b5f85146, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:56,427] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Finished task 144.0 in stage 2.0 (TID 148) in 1669 ms on 8951b5f85146 (executor driver) (145/200)
21/05/15 14:01:56 INFO Executor: Running task 148.0 in stage 2.0 (TID 152)
[2021-05-15 11:01:56,444] {docker.py:276} INFO - 21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:56,444] {docker.py:276} INFO - 21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:56,446] {docker.py:276} INFO - 21/05/15 14:01:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:56,447] {docker.py:276} INFO - 21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502551132917052213414_0002_m_000148_152, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502551132917052213414_0002_m_000148_152}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502551132917052213414_0002}; taskId=attempt_202105151400502551132917052213414_0002_m_000148_152, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71d1d154}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400502551132917052213414_0002_m_000148_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502551132917052213414_0002_m_000148_152
[2021-05-15 11:01:56,450] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400502551132917052213414_0002_m_000148_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502551132917052213414_0002_m_000148_152 : duration 0:00.003s
[2021-05-15 11:01:56,497] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400503011555732460939260_0002_m_000145_149: needsTaskCommit() Task attempt_202105151400503011555732460939260_0002_m_000145_149
[2021-05-15 11:01:56,498] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400503011555732460939260_0002_m_000145_149: needsTaskCommit() Task attempt_202105151400503011555732460939260_0002_m_000145_149: duration 0:00.000s
[2021-05-15 11:01:56,498] {docker.py:276} INFO - 21/05/15 14:01:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503011555732460939260_0002_m_000145_149
[2021-05-15 11:01:56,499] {docker.py:276} INFO - 21/05/15 14:01:56 INFO Executor: Finished task 145.0 in stage 2.0 (TID 149). 4587 bytes result sent to driver
[2021-05-15 11:01:56,500] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Starting task 149.0 in stage 2.0 (TID 153) (8951b5f85146, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:56,501] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Finished task 145.0 in stage 2.0 (TID 149) in 1659 ms on 8951b5f85146 (executor driver) (146/200)
21/05/15 14:01:56 INFO Executor: Running task 149.0 in stage 2.0 (TID 153)
[2021-05-15 11:01:56,508] {docker.py:276} INFO - 21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1474.0 B) non-empty blocks including 3 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:56,509] {docker.py:276} INFO - 21/05/15 14:01:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:56,510] {docker.py:276} INFO - 21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504032612857811827355_0002_m_000149_153, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504032612857811827355_0002_m_000149_153}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504032612857811827355_0002}; taskId=attempt_202105151400504032612857811827355_0002_m_000149_153, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@340920c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400504032612857811827355_0002_m_000149_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504032612857811827355_0002_m_000149_153
[2021-05-15 11:01:56,512] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400504032612857811827355_0002_m_000149_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504032612857811827355_0002_m_000149_153 : duration 0:00.003s
[2021-05-15 11:01:56,591] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400508781504823739539564_0002_m_000146_150: needsTaskCommit() Task attempt_202105151400508781504823739539564_0002_m_000146_150
[2021-05-15 11:01:56,592] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400508781504823739539564_0002_m_000146_150: needsTaskCommit() Task attempt_202105151400508781504823739539564_0002_m_000146_150: duration 0:00.001s
21/05/15 14:01:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508781504823739539564_0002_m_000146_150
[2021-05-15 11:01:56,593] {docker.py:276} INFO - 21/05/15 14:01:56 INFO Executor: Finished task 146.0 in stage 2.0 (TID 150). 4587 bytes result sent to driver
[2021-05-15 11:01:56,596] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Starting task 150.0 in stage 2.0 (TID 154) (8951b5f85146, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:01:56 INFO Executor: Running task 150.0 in stage 2.0 (TID 154)
[2021-05-15 11:01:56,596] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Finished task 146.0 in stage 2.0 (TID 150) in 1746 ms on 8951b5f85146 (executor driver) (147/200)
[2021-05-15 11:01:56,605] {docker.py:276} INFO - 21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:56,607] {docker.py:276} INFO - 21/05/15 14:01:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:56,608] {docker.py:276} INFO - 21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507593469396865606083_0002_m_000150_154, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507593469396865606083_0002_m_000150_154}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507593469396865606083_0002}; taskId=attempt_202105151400507593469396865606083_0002_m_000150_154, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f335e13}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:56,608] {docker.py:276} INFO - 21/05/15 14:01:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400507593469396865606083_0002_m_000150_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507593469396865606083_0002_m_000150_154
[2021-05-15 11:01:56,611] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400507593469396865606083_0002_m_000150_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507593469396865606083_0002_m_000150_154 : duration 0:00.003s
[2021-05-15 11:01:56,671] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400502255848828764674477_0002_m_000147_151: needsTaskCommit() Task attempt_202105151400502255848828764674477_0002_m_000147_151
[2021-05-15 11:01:56,672] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400502255848828764674477_0002_m_000147_151: needsTaskCommit() Task attempt_202105151400502255848828764674477_0002_m_000147_151: duration 0:00.002s
21/05/15 14:01:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502255848828764674477_0002_m_000147_151
[2021-05-15 11:01:56,674] {docker.py:276} INFO - 21/05/15 14:01:56 INFO Executor: Finished task 147.0 in stage 2.0 (TID 151). 4587 bytes result sent to driver
[2021-05-15 11:01:56,676] {docker.py:276} INFO - 21/05/15 14:01:56 INFO TaskSetManager: Starting task 151.0 in stage 2.0 (TID 155) (8951b5f85146, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:01:56 INFO TaskSetManager: Finished task 147.0 in stage 2.0 (TID 151) in 1729 ms on 8951b5f85146 (executor driver) (148/200)
[2021-05-15 11:01:56,677] {docker.py:276} INFO - 21/05/15 14:01:56 INFO Executor: Running task 151.0 in stage 2.0 (TID 155)
[2021-05-15 11:01:56,686] {docker.py:276} INFO - 21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1385.0 B) non-empty blocks including 3 (1385.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:56,688] {docker.py:276} INFO - 21/05/15 14:01:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501032834224824975028_0002_m_000151_155, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501032834224824975028_0002_m_000151_155}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501032834224824975028_0002}; taskId=attempt_202105151400501032834224824975028_0002_m_000151_155, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20cceb7c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:56 INFO StagingCommitter: Starting: Task committer attempt_202105151400501032834224824975028_0002_m_000151_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501032834224824975028_0002_m_000151_155
[2021-05-15 11:01:56,691] {docker.py:276} INFO - 21/05/15 14:01:56 INFO StagingCommitter: Task committer attempt_202105151400501032834224824975028_0002_m_000151_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501032834224824975028_0002_m_000151_155 : duration 0:00.003s
[2021-05-15 11:01:58,148] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400504032612857811827355_0002_m_000149_153: needsTaskCommit() Task attempt_202105151400504032612857811827355_0002_m_000149_153
21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502551132917052213414_0002_m_000148_152: needsTaskCommit() Task attempt_202105151400502551132917052213414_0002_m_000148_152
[2021-05-15 11:01:58,149] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400502551132917052213414_0002_m_000148_152: needsTaskCommit() Task attempt_202105151400502551132917052213414_0002_m_000148_152: duration 0:00.001s
21/05/15 14:01:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502551132917052213414_0002_m_000148_152
[2021-05-15 11:01:58,151] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400504032612857811827355_0002_m_000149_153: needsTaskCommit() Task attempt_202105151400504032612857811827355_0002_m_000149_153: duration 0:00.002s
[2021-05-15 11:01:58,152] {docker.py:276} INFO - 21/05/15 14:01:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504032612857811827355_0002_m_000149_153
[2021-05-15 11:01:58,152] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Finished task 148.0 in stage 2.0 (TID 152). 4587 bytes result sent to driver
[2021-05-15 11:01:58,153] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Starting task 152.0 in stage 2.0 (TID 156) (8951b5f85146, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:58,153] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Finished task 149.0 in stage 2.0 (TID 153). 4587 bytes result sent to driver
[2021-05-15 11:01:58,154] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Finished task 148.0 in stage 2.0 (TID 152) in 1727 ms on 8951b5f85146 (executor driver) (149/200)
[2021-05-15 11:01:58,154] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Starting task 153.0 in stage 2.0 (TID 157) (8951b5f85146, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:58,154] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Running task 152.0 in stage 2.0 (TID 156)
[2021-05-15 11:01:58,155] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Finished task 149.0 in stage 2.0 (TID 153) in 1654 ms on 8951b5f85146 (executor driver) (150/200)
[2021-05-15 11:01:58,155] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Running task 153.0 in stage 2.0 (TID 157)
[2021-05-15 11:01:58,163] {docker.py:276} INFO - 21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:58,163] {docker.py:276} INFO - 21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:58,164] {docker.py:276} INFO - 21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1528.0 B) non-empty blocks including 3 (1528.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:01:58,164] {docker.py:276} INFO - 21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:58,165] {docker.py:276} INFO - 21/05/15 14:01:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:58,166] {docker.py:276} INFO - 21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503636087126082363837_0002_m_000152_156, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503636087126082363837_0002_m_000152_156}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503636087126082363837_0002}; taskId=attempt_202105151400503636087126082363837_0002_m_000152_156, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fc86f99}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:58,166] {docker.py:276} INFO - 21/05/15 14:01:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:58,166] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400503636087126082363837_0002_m_000152_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503636087126082363837_0002_m_000152_156
[2021-05-15 11:01:58,167] {docker.py:276} INFO - 21/05/15 14:01:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:01:58,167] {docker.py:276} INFO - 21/05/15 14:01:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:58,167] {docker.py:276} INFO - 21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:58,168] {docker.py:276} INFO - 21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507709964417090595625_0002_m_000153_157, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507709964417090595625_0002_m_000153_157}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507709964417090595625_0002}; taskId=attempt_202105151400507709964417090595625_0002_m_000153_157, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3037a243}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:58,168] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400507709964417090595625_0002_m_000153_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507709964417090595625_0002_m_000153_157
[2021-05-15 11:01:58,170] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400507709964417090595625_0002_m_000153_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507709964417090595625_0002_m_000153_157 : duration 0:00.003s
[2021-05-15 11:01:58,170] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400503636087126082363837_0002_m_000152_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503636087126082363837_0002_m_000152_156 : duration 0:00.004s
[2021-05-15 11:01:58,279] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400507593469396865606083_0002_m_000150_154: needsTaskCommit() Task attempt_202105151400507593469396865606083_0002_m_000150_154
[2021-05-15 11:01:58,281] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400507593469396865606083_0002_m_000150_154: needsTaskCommit() Task attempt_202105151400507593469396865606083_0002_m_000150_154: duration 0:00.002s
21/05/15 14:01:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507593469396865606083_0002_m_000150_154
[2021-05-15 11:01:58,283] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Finished task 150.0 in stage 2.0 (TID 154). 4587 bytes result sent to driver
[2021-05-15 11:01:58,284] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Finished task 150.0 in stage 2.0 (TID 154) in 1692 ms on 8951b5f85146 (executor driver) (151/200)
[2021-05-15 11:01:58,285] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Starting task 154.0 in stage 2.0 (TID 158) (8951b5f85146, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:58,287] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Running task 154.0 in stage 2.0 (TID 158)
[2021-05-15 11:01:58,296] {docker.py:276} INFO - 21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1278.0 B) non-empty blocks including 3 (1278.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:58,298] {docker.py:276} INFO - 21/05/15 14:01:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502314579782702953992_0002_m_000154_158, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502314579782702953992_0002_m_000154_158}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502314579782702953992_0002}; taskId=attempt_202105151400502314579782702953992_0002_m_000154_158, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@548632c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502314579782702953992_0002_m_000154_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502314579782702953992_0002_m_000154_158
[2021-05-15 11:01:58,300] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400502314579782702953992_0002_m_000154_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502314579782702953992_0002_m_000154_158 : duration 0:00.002s
[2021-05-15 11:01:58,329] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400501032834224824975028_0002_m_000151_155: needsTaskCommit() Task attempt_202105151400501032834224824975028_0002_m_000151_155
[2021-05-15 11:01:58,330] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400501032834224824975028_0002_m_000151_155: needsTaskCommit() Task attempt_202105151400501032834224824975028_0002_m_000151_155: duration 0:00.000s
21/05/15 14:01:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501032834224824975028_0002_m_000151_155
[2021-05-15 11:01:58,331] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Finished task 151.0 in stage 2.0 (TID 155). 4587 bytes result sent to driver
[2021-05-15 11:01:58,332] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Starting task 155.0 in stage 2.0 (TID 159) (8951b5f85146, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:58,333] {docker.py:276} INFO - 21/05/15 14:01:58 INFO Executor: Running task 155.0 in stage 2.0 (TID 159)
[2021-05-15 11:01:58,333] {docker.py:276} INFO - 21/05/15 14:01:58 INFO TaskSetManager: Finished task 151.0 in stage 2.0 (TID 155) in 1661 ms on 8951b5f85146 (executor driver) (152/200)
[2021-05-15 11:01:58,348] {docker.py:276} INFO - 21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Getting 3 (1226.0 B) non-empty blocks including 3 (1226.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:58,350] {docker.py:276} INFO - 21/05/15 14:01:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:01:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502315065301424845713_0002_m_000155_159, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502315065301424845713_0002_m_000155_159}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502315065301424845713_0002}; taskId=attempt_202105151400502315065301424845713_0002_m_000155_159, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2523f1f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:58,350] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Starting: Task committer attempt_202105151400502315065301424845713_0002_m_000155_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502315065301424845713_0002_m_000155_159
[2021-05-15 11:01:58,353] {docker.py:276} INFO - 21/05/15 14:01:58 INFO StagingCommitter: Task committer attempt_202105151400502315065301424845713_0002_m_000155_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502315065301424845713_0002_m_000155_159 : duration 0:00.003s
[2021-05-15 11:01:59,820] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400507709964417090595625_0002_m_000153_157: needsTaskCommit() Task attempt_202105151400507709964417090595625_0002_m_000153_157
[2021-05-15 11:01:59,821] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Task committer attempt_202105151400507709964417090595625_0002_m_000153_157: needsTaskCommit() Task attempt_202105151400507709964417090595625_0002_m_000153_157: duration 0:00.001s
21/05/15 14:01:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507709964417090595625_0002_m_000153_157
[2021-05-15 11:01:59,823] {docker.py:276} INFO - 21/05/15 14:01:59 INFO Executor: Finished task 153.0 in stage 2.0 (TID 157). 4587 bytes result sent to driver
[2021-05-15 11:01:59,825] {docker.py:276} INFO - 21/05/15 14:01:59 INFO TaskSetManager: Starting task 156.0 in stage 2.0 (TID 160) (8951b5f85146, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:59,826] {docker.py:276} INFO - 21/05/15 14:01:59 INFO Executor: Running task 156.0 in stage 2.0 (TID 160)
[2021-05-15 11:01:59,827] {docker.py:276} INFO - 21/05/15 14:01:59 INFO TaskSetManager: Finished task 153.0 in stage 2.0 (TID 157) in 1678 ms on 8951b5f85146 (executor driver) (153/200)
[2021-05-15 11:01:59,834] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400503636087126082363837_0002_m_000152_156: needsTaskCommit() Task attempt_202105151400503636087126082363837_0002_m_000152_156
[2021-05-15 11:01:59,835] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Task committer attempt_202105151400503636087126082363837_0002_m_000152_156: needsTaskCommit() Task attempt_202105151400503636087126082363837_0002_m_000152_156: duration 0:00.001s
[2021-05-15 11:01:59,835] {docker.py:276} INFO - 21/05/15 14:01:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503636087126082363837_0002_m_000152_156
[2021-05-15 11:01:59,837] {docker.py:276} INFO - 21/05/15 14:01:59 INFO ShuffleBlockFetcherIterator: Getting 3 (1380.0 B) non-empty blocks including 3 (1380.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:59,839] {docker.py:276} INFO - 21/05/15 14:01:59 INFO Executor: Finished task 152.0 in stage 2.0 (TID 156). 4587 bytes result sent to driver
[2021-05-15 11:01:59,840] {docker.py:276} INFO - 21/05/15 14:01:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:59,840] {docker.py:276} INFO - 21/05/15 14:01:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:59,841] {docker.py:276} INFO - 21/05/15 14:01:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501596644532657019928_0002_m_000156_160, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501596644532657019928_0002_m_000156_160}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501596644532657019928_0002}; taskId=attempt_202105151400501596644532657019928_0002_m_000156_160, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b1fdb61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:59,842] {docker.py:276} INFO - 21/05/15 14:01:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:59,842] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400501596644532657019928_0002_m_000156_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501596644532657019928_0002_m_000156_160
[2021-05-15 11:01:59,843] {docker.py:276} INFO - 21/05/15 14:01:59 INFO TaskSetManager: Starting task 157.0 in stage 2.0 (TID 161) (8951b5f85146, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:59,843] {docker.py:276} INFO - 21/05/15 14:01:59 INFO TaskSetManager: Finished task 152.0 in stage 2.0 (TID 156) in 1697 ms on 8951b5f85146 (executor driver) (154/200)
[2021-05-15 11:01:59,844] {docker.py:276} INFO - 21/05/15 14:01:59 INFO Executor: Running task 157.0 in stage 2.0 (TID 161)
[2021-05-15 11:01:59,845] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Task committer attempt_202105151400501596644532657019928_0002_m_000156_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501596644532657019928_0002_m_000156_160 : duration 0:00.005s
[2021-05-15 11:01:59,860] {docker.py:276} INFO - 21/05/15 14:01:59 INFO ShuffleBlockFetcherIterator: Getting 3 (1474.0 B) non-empty blocks including 3 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:59,862] {docker.py:276} INFO - 21/05/15 14:01:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:01:59,862] {docker.py:276} INFO - 21/05/15 14:01:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:59,863] {docker.py:276} INFO - 21/05/15 14:01:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506622071542578229212_0002_m_000157_161, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506622071542578229212_0002_m_000157_161}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506622071542578229212_0002}; taskId=attempt_202105151400506622071542578229212_0002_m_000157_161, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d2badbc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:59,863] {docker.py:276} INFO - 21/05/15 14:01:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:01:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400506622071542578229212_0002_m_000157_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506622071542578229212_0002_m_000157_161
[2021-05-15 11:01:59,866] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Task committer attempt_202105151400506622071542578229212_0002_m_000157_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506622071542578229212_0002_m_000157_161 : duration 0:00.003s
[2021-05-15 11:01:59,964] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400502314579782702953992_0002_m_000154_158: needsTaskCommit() Task attempt_202105151400502314579782702953992_0002_m_000154_158
[2021-05-15 11:01:59,965] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Task committer attempt_202105151400502314579782702953992_0002_m_000154_158: needsTaskCommit() Task attempt_202105151400502314579782702953992_0002_m_000154_158: duration 0:00.000s
[2021-05-15 11:01:59,965] {docker.py:276} INFO - 21/05/15 14:01:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502314579782702953992_0002_m_000154_158
[2021-05-15 11:01:59,967] {docker.py:276} INFO - 21/05/15 14:01:59 INFO Executor: Finished task 154.0 in stage 2.0 (TID 158). 4587 bytes result sent to driver
[2021-05-15 11:01:59,968] {docker.py:276} INFO - 21/05/15 14:01:59 INFO TaskSetManager: Starting task 158.0 in stage 2.0 (TID 162) (8951b5f85146, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:01:59,968] {docker.py:276} INFO - 21/05/15 14:01:59 INFO TaskSetManager: Finished task 154.0 in stage 2.0 (TID 158) in 1685 ms on 8951b5f85146 (executor driver) (155/200)
[2021-05-15 11:01:59,969] {docker.py:276} INFO - 21/05/15 14:01:59 INFO Executor: Running task 158.0 in stage 2.0 (TID 162)
[2021-05-15 11:01:59,977] {docker.py:276} INFO - 21/05/15 14:01:59 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:01:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:01:59,979] {docker.py:276} INFO - 21/05/15 14:01:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:01:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:01:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:01:59,979] {docker.py:276} INFO - 21/05/15 14:01:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506126572026752361906_0002_m_000158_162, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506126572026752361906_0002_m_000158_162}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506126572026752361906_0002}; taskId=attempt_202105151400506126572026752361906_0002_m_000158_162, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5cd7b302}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:01:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:01:59,979] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Starting: Task committer attempt_202105151400506126572026752361906_0002_m_000158_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506126572026752361906_0002_m_000158_162
[2021-05-15 11:01:59,982] {docker.py:276} INFO - 21/05/15 14:01:59 INFO StagingCommitter: Task committer attempt_202105151400506126572026752361906_0002_m_000158_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506126572026752361906_0002_m_000158_162 : duration 0:00.003s
[2021-05-15 11:02:00,012] {docker.py:276} INFO - 21/05/15 14:02:00 INFO StagingCommitter: Starting: Task committer attempt_202105151400502315065301424845713_0002_m_000155_159: needsTaskCommit() Task attempt_202105151400502315065301424845713_0002_m_000155_159
[2021-05-15 11:02:00,013] {docker.py:276} INFO - 21/05/15 14:02:00 INFO StagingCommitter: Task committer attempt_202105151400502315065301424845713_0002_m_000155_159: needsTaskCommit() Task attempt_202105151400502315065301424845713_0002_m_000155_159: duration 0:00.001s
21/05/15 14:02:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502315065301424845713_0002_m_000155_159
[2021-05-15 11:02:00,014] {docker.py:276} INFO - 21/05/15 14:02:00 INFO Executor: Finished task 155.0 in stage 2.0 (TID 159). 4587 bytes result sent to driver
[2021-05-15 11:02:00,016] {docker.py:276} INFO - 21/05/15 14:02:00 INFO TaskSetManager: Starting task 159.0 in stage 2.0 (TID 163) (8951b5f85146, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:00,017] {docker.py:276} INFO - 21/05/15 14:02:00 INFO TaskSetManager: Finished task 155.0 in stage 2.0 (TID 159) in 1687 ms on 8951b5f85146 (executor driver) (156/200)
[2021-05-15 11:02:00,018] {docker.py:276} INFO - 21/05/15 14:02:00 INFO Executor: Running task 159.0 in stage 2.0 (TID 163)
[2021-05-15 11:02:00,026] {docker.py:276} INFO - 21/05/15 14:02:00 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:00,026] {docker.py:276} INFO - 21/05/15 14:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:00,028] {docker.py:276} INFO - 21/05/15 14:02:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:02:00,028] {docker.py:276} INFO - 21/05/15 14:02:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:00,028] {docker.py:276} INFO - 21/05/15 14:02:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:00,029] {docker.py:276} INFO - 21/05/15 14:02:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503950878944229310840_0002_m_000159_163, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503950878944229310840_0002_m_000159_163}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503950878944229310840_0002}; taskId=attempt_202105151400503950878944229310840_0002_m_000159_163, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4dacdfa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:00,029] {docker.py:276} INFO - 21/05/15 14:02:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:00 INFO StagingCommitter: Starting: Task committer attempt_202105151400503950878944229310840_0002_m_000159_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503950878944229310840_0002_m_000159_163
[2021-05-15 11:02:00,031] {docker.py:276} INFO - 21/05/15 14:02:00 INFO StagingCommitter: Task committer attempt_202105151400503950878944229310840_0002_m_000159_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503950878944229310840_0002_m_000159_163 : duration 0:00.002s
[2021-05-15 11:02:01,505] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400506622071542578229212_0002_m_000157_161: needsTaskCommit() Task attempt_202105151400506622071542578229212_0002_m_000157_161
[2021-05-15 11:02:01,506] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400506622071542578229212_0002_m_000157_161: needsTaskCommit() Task attempt_202105151400506622071542578229212_0002_m_000157_161: duration 0:00.001s
21/05/15 14:02:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506622071542578229212_0002_m_000157_161
[2021-05-15 11:02:01,508] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Finished task 157.0 in stage 2.0 (TID 161). 4587 bytes result sent to driver
[2021-05-15 11:02:01,509] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Starting task 160.0 in stage 2.0 (TID 164) (8951b5f85146, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:01,510] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Finished task 157.0 in stage 2.0 (TID 161) in 1671 ms on 8951b5f85146 (executor driver) (157/200)
[2021-05-15 11:02:01,511] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Running task 160.0 in stage 2.0 (TID 164)
[2021-05-15 11:02:01,512] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400501596644532657019928_0002_m_000156_160: needsTaskCommit() Task attempt_202105151400501596644532657019928_0002_m_000156_160
[2021-05-15 11:02:01,513] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400501596644532657019928_0002_m_000156_160: needsTaskCommit() Task attempt_202105151400501596644532657019928_0002_m_000156_160: duration 0:00.000s
21/05/15 14:02:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501596644532657019928_0002_m_000156_160
[2021-05-15 11:02:01,514] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Finished task 156.0 in stage 2.0 (TID 160). 4587 bytes result sent to driver
[2021-05-15 11:02:01,515] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Starting task 161.0 in stage 2.0 (TID 165) (8951b5f85146, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:01,516] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Finished task 156.0 in stage 2.0 (TID 160) in 1694 ms on 8951b5f85146 (executor driver) (158/200)
[2021-05-15 11:02:01,518] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Running task 161.0 in stage 2.0 (TID 165)
[2021-05-15 11:02:01,524] {docker.py:276} INFO - 21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1215.0 B) non-empty blocks including 3 (1215.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:01,524] {docker.py:276} INFO - 21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:01,526] {docker.py:276} INFO - 21/05/15 14:02:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:01,526] {docker.py:276} INFO - 21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050146255533454944817_0002_m_000160_164, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050146255533454944817_0002_m_000160_164}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050146255533454944817_0002}; taskId=attempt_20210515140050146255533454944817_0002_m_000160_164, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54890250}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:01,526] {docker.py:276} INFO - 21/05/15 14:02:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_20210515140050146255533454944817_0002_m_000160_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050146255533454944817_0002_m_000160_164
[2021-05-15 11:02:01,528] {docker.py:276} INFO - 21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1300.0 B) non-empty blocks including 3 (1300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:01,529] {docker.py:276} INFO - 21/05/15 14:02:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:01,530] {docker.py:276} INFO - 21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508275210972293740682_0002_m_000161_165, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508275210972293740682_0002_m_000161_165}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508275210972293740682_0002}; taskId=attempt_202105151400508275210972293740682_0002_m_000161_165, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52ee5149}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400508275210972293740682_0002_m_000161_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508275210972293740682_0002_m_000161_165
[2021-05-15 11:02:01,530] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_20210515140050146255533454944817_0002_m_000160_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050146255533454944817_0002_m_000160_164 : duration 0:00.004s
[2021-05-15 11:02:01,533] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400508275210972293740682_0002_m_000161_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508275210972293740682_0002_m_000161_165 : duration 0:00.004s
[2021-05-15 11:02:01,657] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400506126572026752361906_0002_m_000158_162: needsTaskCommit() Task attempt_202105151400506126572026752361906_0002_m_000158_162
[2021-05-15 11:02:01,658] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400506126572026752361906_0002_m_000158_162: needsTaskCommit() Task attempt_202105151400506126572026752361906_0002_m_000158_162: duration 0:00.002s
21/05/15 14:02:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506126572026752361906_0002_m_000158_162
[2021-05-15 11:02:01,660] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Finished task 158.0 in stage 2.0 (TID 162). 4587 bytes result sent to driver
[2021-05-15 11:02:01,662] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Starting task 162.0 in stage 2.0 (TID 166) (8951b5f85146, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:01,666] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Finished task 158.0 in stage 2.0 (TID 162) in 1698 ms on 8951b5f85146 (executor driver) (159/200)
21/05/15 14:02:01 INFO Executor: Running task 162.0 in stage 2.0 (TID 166)
[2021-05-15 11:02:01,673] {docker.py:276} INFO - 21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1389.0 B) non-empty blocks including 3 (1389.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:01,675] {docker.py:276} INFO - 21/05/15 14:02:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:01,675] {docker.py:276} INFO - 21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506351234503829170078_0002_m_000162_166, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506351234503829170078_0002_m_000162_166}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506351234503829170078_0002}; taskId=attempt_202105151400506351234503829170078_0002_m_000162_166, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@740e764b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400506351234503829170078_0002_m_000162_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506351234503829170078_0002_m_000162_166
[2021-05-15 11:02:01,678] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400506351234503829170078_0002_m_000162_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506351234503829170078_0002_m_000162_166 : duration 0:00.003s
[2021-05-15 11:02:01,731] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400503950878944229310840_0002_m_000159_163: needsTaskCommit() Task attempt_202105151400503950878944229310840_0002_m_000159_163
[2021-05-15 11:02:01,732] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400503950878944229310840_0002_m_000159_163: needsTaskCommit() Task attempt_202105151400503950878944229310840_0002_m_000159_163: duration 0:00.001s
21/05/15 14:02:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503950878944229310840_0002_m_000159_163
[2021-05-15 11:02:01,734] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Finished task 159.0 in stage 2.0 (TID 163). 4587 bytes result sent to driver
[2021-05-15 11:02:01,735] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Starting task 163.0 in stage 2.0 (TID 167) (8951b5f85146, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:01,736] {docker.py:276} INFO - 21/05/15 14:02:01 INFO TaskSetManager: Finished task 159.0 in stage 2.0 (TID 163) in 1722 ms on 8951b5f85146 (executor driver) (160/200)
[2021-05-15 11:02:01,737] {docker.py:276} INFO - 21/05/15 14:02:01 INFO Executor: Running task 163.0 in stage 2.0 (TID 167)
[2021-05-15 11:02:01,754] {docker.py:276} INFO - 21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1352.0 B) non-empty blocks including 3 (1352.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:01,756] {docker.py:276} INFO - 21/05/15 14:02:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:01,756] {docker.py:276} INFO - 21/05/15 14:02:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507400425603409714302_0002_m_000163_167, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507400425603409714302_0002_m_000163_167}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507400425603409714302_0002}; taskId=attempt_202105151400507400425603409714302_0002_m_000163_167, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45ab0845}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:01 INFO StagingCommitter: Starting: Task committer attempt_202105151400507400425603409714302_0002_m_000163_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507400425603409714302_0002_m_000163_167
[2021-05-15 11:02:01,759] {docker.py:276} INFO - 21/05/15 14:02:01 INFO StagingCommitter: Task committer attempt_202105151400507400425603409714302_0002_m_000163_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507400425603409714302_0002_m_000163_167 : duration 0:00.002s
[2021-05-15 11:02:03,181] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400508275210972293740682_0002_m_000161_165: needsTaskCommit() Task attempt_202105151400508275210972293740682_0002_m_000161_165
[2021-05-15 11:02:03,181] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400508275210972293740682_0002_m_000161_165: needsTaskCommit() Task attempt_202105151400508275210972293740682_0002_m_000161_165: duration 0:00.001s
[2021-05-15 11:02:03,182] {docker.py:276} INFO - 21/05/15 14:02:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508275210972293740682_0002_m_000161_165
[2021-05-15 11:02:03,182] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Finished task 161.0 in stage 2.0 (TID 165). 4587 bytes result sent to driver
[2021-05-15 11:02:03,183] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Starting task 164.0 in stage 2.0 (TID 168) (8951b5f85146, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:03,185] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Finished task 161.0 in stage 2.0 (TID 165) in 1671 ms on 8951b5f85146 (executor driver) (161/200)
[2021-05-15 11:02:03,186] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Running task 164.0 in stage 2.0 (TID 168)
[2021-05-15 11:02:03,193] {docker.py:276} INFO - 21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1147.0 B) non-empty blocks including 3 (1147.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:03,194] {docker.py:276} INFO - 21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:03,195] {docker.py:276} INFO - 21/05/15 14:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:02:03,196] {docker.py:276} INFO - 21/05/15 14:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:03,196] {docker.py:276} INFO - 21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:03,196] {docker.py:276} INFO - 21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508089896428660183584_0002_m_000164_168, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508089896428660183584_0002_m_000164_168}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508089896428660183584_0002}; taskId=attempt_202105151400508089896428660183584_0002_m_000164_168, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52f0d7db}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:03,197] {docker.py:276} INFO - 21/05/15 14:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:03,197] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400508089896428660183584_0002_m_000164_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508089896428660183584_0002_m_000164_168
[2021-05-15 11:02:03,199] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400508089896428660183584_0002_m_000164_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508089896428660183584_0002_m_000164_168 : duration 0:00.003s
[2021-05-15 11:02:03,234] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_20210515140050146255533454944817_0002_m_000160_164: needsTaskCommit() Task attempt_20210515140050146255533454944817_0002_m_000160_164
[2021-05-15 11:02:03,235] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_20210515140050146255533454944817_0002_m_000160_164: needsTaskCommit() Task attempt_20210515140050146255533454944817_0002_m_000160_164: duration 0:00.001s
21/05/15 14:02:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050146255533454944817_0002_m_000160_164
[2021-05-15 11:02:03,236] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Finished task 160.0 in stage 2.0 (TID 164). 4587 bytes result sent to driver
[2021-05-15 11:02:03,238] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Starting task 165.0 in stage 2.0 (TID 169) (8951b5f85146, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:03,239] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Running task 165.0 in stage 2.0 (TID 169)
[2021-05-15 11:02:03,240] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Finished task 160.0 in stage 2.0 (TID 164) in 1732 ms on 8951b5f85146 (executor driver) (162/200)
[2021-05-15 11:02:03,259] {docker.py:276} INFO - 21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:03,259] {docker.py:276} INFO - 21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:02:03,262] {docker.py:276} INFO - 21/05/15 14:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:02:03,262] {docker.py:276} INFO - 21/05/15 14:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:03,263] {docker.py:276} INFO - 21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:03,263] {docker.py:276} INFO - 21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501130506839324490386_0002_m_000165_169, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501130506839324490386_0002_m_000165_169}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501130506839324490386_0002}; taskId=attempt_202105151400501130506839324490386_0002_m_000165_169, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c5b84d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:03,263] {docker.py:276} INFO - 21/05/15 14:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400501130506839324490386_0002_m_000165_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501130506839324490386_0002_m_000165_169
[2021-05-15 11:02:03,266] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400501130506839324490386_0002_m_000165_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501130506839324490386_0002_m_000165_169 : duration 0:00.002s
[2021-05-15 11:02:03,424] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400507400425603409714302_0002_m_000163_167: needsTaskCommit() Task attempt_202105151400507400425603409714302_0002_m_000163_167
[2021-05-15 11:02:03,425] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400507400425603409714302_0002_m_000163_167: needsTaskCommit() Task attempt_202105151400507400425603409714302_0002_m_000163_167: duration 0:00.001s
21/05/15 14:02:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507400425603409714302_0002_m_000163_167
[2021-05-15 11:02:03,426] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Finished task 163.0 in stage 2.0 (TID 167). 4587 bytes result sent to driver
[2021-05-15 11:02:03,427] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Starting task 166.0 in stage 2.0 (TID 170) (8951b5f85146, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:03,430] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Finished task 163.0 in stage 2.0 (TID 167) in 1697 ms on 8951b5f85146 (executor driver) (163/200)
[2021-05-15 11:02:03,430] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Running task 166.0 in stage 2.0 (TID 170)
[2021-05-15 11:02:03,434] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400506351234503829170078_0002_m_000162_166: needsTaskCommit() Task attempt_202105151400506351234503829170078_0002_m_000162_166
[2021-05-15 11:02:03,435] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400506351234503829170078_0002_m_000162_166: needsTaskCommit() Task attempt_202105151400506351234503829170078_0002_m_000162_166: duration 0:00.000s
21/05/15 14:02:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506351234503829170078_0002_m_000162_166
[2021-05-15 11:02:03,435] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Finished task 162.0 in stage 2.0 (TID 166). 4587 bytes result sent to driver
[2021-05-15 11:02:03,437] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Starting task 167.0 in stage 2.0 (TID 171) (8951b5f85146, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:03,438] {docker.py:276} INFO - 21/05/15 14:02:03 INFO TaskSetManager: Finished task 162.0 in stage 2.0 (TID 166) in 1779 ms on 8951b5f85146 (executor driver) (164/200)
[2021-05-15 11:02:03,438] {docker.py:276} INFO - 21/05/15 14:02:03 INFO Executor: Running task 167.0 in stage 2.0 (TID 171)
[2021-05-15 11:02:03,445] {docker.py:276} INFO - 21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1385.0 B) non-empty blocks including 3 (1385.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:03,449] {docker.py:276} INFO - 21/05/15 14:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502283656395931814657_0002_m_000166_170, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502283656395931814657_0002_m_000166_170}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502283656395931814657_0002}; taskId=attempt_202105151400502283656395931814657_0002_m_000166_170, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16211268}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400502283656395931814657_0002_m_000166_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502283656395931814657_0002_m_000166_170
[2021-05-15 11:02:03,450] {docker.py:276} INFO - 21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1258.0 B) non-empty blocks including 3 (1258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:03,452] {docker.py:276} INFO - 21/05/15 14:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:03,452] {docker.py:276} INFO - 21/05/15 14:02:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502009661545072163107_0002_m_000167_171, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502009661545072163107_0002_m_000167_171}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502009661545072163107_0002}; taskId=attempt_202105151400502009661545072163107_0002_m_000167_171, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78210298}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:03,453] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Starting: Task committer attempt_202105151400502009661545072163107_0002_m_000167_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502009661545072163107_0002_m_000167_171
[2021-05-15 11:02:03,455] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400502283656395931814657_0002_m_000166_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502283656395931814657_0002_m_000166_170 : duration 0:00.005s
[2021-05-15 11:02:03,457] {docker.py:276} INFO - 21/05/15 14:02:03 INFO StagingCommitter: Task committer attempt_202105151400502009661545072163107_0002_m_000167_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502009661545072163107_0002_m_000167_171 : duration 0:00.005s
[2021-05-15 11:02:04,877] {docker.py:276} INFO - 21/05/15 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105151400508089896428660183584_0002_m_000164_168: needsTaskCommit() Task attempt_202105151400508089896428660183584_0002_m_000164_168
21/05/15 14:02:04 INFO StagingCommitter: Task committer attempt_202105151400508089896428660183584_0002_m_000164_168: needsTaskCommit() Task attempt_202105151400508089896428660183584_0002_m_000164_168: duration 0:00.000s
21/05/15 14:02:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508089896428660183584_0002_m_000164_168
[2021-05-15 11:02:04,879] {docker.py:276} INFO - 21/05/15 14:02:04 INFO Executor: Finished task 164.0 in stage 2.0 (TID 168). 4587 bytes result sent to driver
[2021-05-15 11:02:04,881] {docker.py:276} INFO - 21/05/15 14:02:04 INFO TaskSetManager: Starting task 168.0 in stage 2.0 (TID 172) (8951b5f85146, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:04,882] {docker.py:276} INFO - 21/05/15 14:02:04 INFO Executor: Running task 168.0 in stage 2.0 (TID 172)
21/05/15 14:02:04 INFO TaskSetManager: Finished task 164.0 in stage 2.0 (TID 168) in 1700 ms on 8951b5f85146 (executor driver) (165/200)
[2021-05-15 11:02:04,892] {docker.py:276} INFO - 21/05/15 14:02:04 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:04,894] {docker.py:276} INFO - 21/05/15 14:02:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:04,895] {docker.py:276} INFO - 21/05/15 14:02:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507262388031727701816_0002_m_000168_172, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507262388031727701816_0002_m_000168_172}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507262388031727701816_0002}; taskId=attempt_202105151400507262388031727701816_0002_m_000168_172, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6494a5b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:04,895] {docker.py:276} INFO - 21/05/15 14:02:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:04,896] {docker.py:276} INFO - 21/05/15 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105151400507262388031727701816_0002_m_000168_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507262388031727701816_0002_m_000168_172
[2021-05-15 11:02:04,899] {docker.py:276} INFO - 21/05/15 14:02:04 INFO StagingCommitter: Task committer attempt_202105151400507262388031727701816_0002_m_000168_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507262388031727701816_0002_m_000168_172 : duration 0:00.003s
[2021-05-15 11:02:04,959] {docker.py:276} INFO - 21/05/15 14:02:04 INFO StagingCommitter: Starting: Task committer attempt_202105151400501130506839324490386_0002_m_000165_169: needsTaskCommit() Task attempt_202105151400501130506839324490386_0002_m_000165_169
[2021-05-15 11:02:04,960] {docker.py:276} INFO - 21/05/15 14:02:04 INFO StagingCommitter: Task committer attempt_202105151400501130506839324490386_0002_m_000165_169: needsTaskCommit() Task attempt_202105151400501130506839324490386_0002_m_000165_169: duration 0:00.001s
21/05/15 14:02:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501130506839324490386_0002_m_000165_169
[2021-05-15 11:02:04,962] {docker.py:276} INFO - 21/05/15 14:02:04 INFO Executor: Finished task 165.0 in stage 2.0 (TID 169). 4587 bytes result sent to driver
[2021-05-15 11:02:04,964] {docker.py:276} INFO - 21/05/15 14:02:04 INFO TaskSetManager: Starting task 169.0 in stage 2.0 (TID 173) (8951b5f85146, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:04,966] {docker.py:276} INFO - 21/05/15 14:02:04 INFO Executor: Running task 169.0 in stage 2.0 (TID 173)
[2021-05-15 11:02:04,967] {docker.py:276} INFO - 21/05/15 14:02:04 INFO TaskSetManager: Finished task 165.0 in stage 2.0 (TID 169) in 1732 ms on 8951b5f85146 (executor driver) (166/200)
[2021-05-15 11:02:04,985] {docker.py:276} INFO - 21/05/15 14:02:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:04,987] {docker.py:276} INFO - 21/05/15 14:02:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:04,988] {docker.py:276} INFO - 21/05/15 14:02:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:04,988] {docker.py:276} INFO - 21/05/15 14:02:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504896742596613403709_0002_m_000169_173, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504896742596613403709_0002_m_000169_173}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504896742596613403709_0002}; taskId=attempt_202105151400504896742596613403709_0002_m_000169_173, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ee4a299}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:04,989] {docker.py:276} INFO - 21/05/15 14:02:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:04,989] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400504896742596613403709_0002_m_000169_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504896742596613403709_0002_m_000169_173
[2021-05-15 11:02:04,993] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Task committer attempt_202105151400504896742596613403709_0002_m_000169_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504896742596613403709_0002_m_000169_173 : duration 0:00.004s
[2021-05-15 11:02:05,106] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400502283656395931814657_0002_m_000166_170: needsTaskCommit() Task attempt_202105151400502283656395931814657_0002_m_000166_170
21/05/15 14:02:05 INFO StagingCommitter: Task committer attempt_202105151400502283656395931814657_0002_m_000166_170: needsTaskCommit() Task attempt_202105151400502283656395931814657_0002_m_000166_170: duration 0:00.001s
21/05/15 14:02:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502283656395931814657_0002_m_000166_170
[2021-05-15 11:02:05,108] {docker.py:276} INFO - 21/05/15 14:02:05 INFO Executor: Finished task 166.0 in stage 2.0 (TID 170). 4587 bytes result sent to driver
[2021-05-15 11:02:05,110] {docker.py:276} INFO - 21/05/15 14:02:05 INFO TaskSetManager: Starting task 170.0 in stage 2.0 (TID 174) (8951b5f85146, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:05,111] {docker.py:276} INFO - 21/05/15 14:02:05 INFO Executor: Running task 170.0 in stage 2.0 (TID 174)
21/05/15 14:02:05 INFO TaskSetManager: Finished task 166.0 in stage 2.0 (TID 170) in 1685 ms on 8951b5f85146 (executor driver) (167/200)
[2021-05-15 11:02:05,121] {docker.py:276} INFO - 21/05/15 14:02:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1488.0 B) non-empty blocks including 3 (1488.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:05,123] {docker.py:276} INFO - 21/05/15 14:02:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:05,124] {docker.py:276} INFO - 21/05/15 14:02:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504639262151302742209_0002_m_000170_174, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504639262151302742209_0002_m_000170_174}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504639262151302742209_0002}; taskId=attempt_202105151400504639262151302742209_0002_m_000170_174, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62bf1905}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:05,124] {docker.py:276} INFO - 21/05/15 14:02:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400504639262151302742209_0002_m_000170_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504639262151302742209_0002_m_000170_174
[2021-05-15 11:02:05,127] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Task committer attempt_202105151400504639262151302742209_0002_m_000170_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504639262151302742209_0002_m_000170_174 : duration 0:00.002s
[2021-05-15 11:02:05,183] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400502009661545072163107_0002_m_000167_171: needsTaskCommit() Task attempt_202105151400502009661545072163107_0002_m_000167_171
[2021-05-15 11:02:05,183] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Task committer attempt_202105151400502009661545072163107_0002_m_000167_171: needsTaskCommit() Task attempt_202105151400502009661545072163107_0002_m_000167_171: duration 0:00.001s
21/05/15 14:02:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502009661545072163107_0002_m_000167_171
[2021-05-15 11:02:05,184] {docker.py:276} INFO - 21/05/15 14:02:05 INFO Executor: Finished task 167.0 in stage 2.0 (TID 171). 4587 bytes result sent to driver
[2021-05-15 11:02:05,187] {docker.py:276} INFO - 21/05/15 14:02:05 INFO TaskSetManager: Starting task 171.0 in stage 2.0 (TID 175) (8951b5f85146, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:05,188] {docker.py:276} INFO - 21/05/15 14:02:05 INFO TaskSetManager: Finished task 167.0 in stage 2.0 (TID 171) in 1753 ms on 8951b5f85146 (executor driver) (168/200)
21/05/15 14:02:05 INFO Executor: Running task 171.0 in stage 2.0 (TID 175)
[2021-05-15 11:02:05,199] {docker.py:276} INFO - 21/05/15 14:02:05 INFO ShuffleBlockFetcherIterator: Getting 3 (1239.0 B) non-empty blocks including 3 (1239.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:05,200] {docker.py:276} INFO - 21/05/15 14:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:05,202] {docker.py:276} INFO - 21/05/15 14:02:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:05,202] {docker.py:276} INFO - 21/05/15 14:02:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501201146405949536779_0002_m_000171_175, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501201146405949536779_0002_m_000171_175}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501201146405949536779_0002}; taskId=attempt_202105151400501201146405949536779_0002_m_000171_175, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2af71251}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:05 INFO StagingCommitter: Starting: Task committer attempt_202105151400501201146405949536779_0002_m_000171_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501201146405949536779_0002_m_000171_175
[2021-05-15 11:02:05,206] {docker.py:276} INFO - 21/05/15 14:02:05 INFO StagingCommitter: Task committer attempt_202105151400501201146405949536779_0002_m_000171_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501201146405949536779_0002_m_000171_175 : duration 0:00.003s
[2021-05-15 11:02:06,582] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400507262388031727701816_0002_m_000168_172: needsTaskCommit() Task attempt_202105151400507262388031727701816_0002_m_000168_172
21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400507262388031727701816_0002_m_000168_172: needsTaskCommit() Task attempt_202105151400507262388031727701816_0002_m_000168_172: duration 0:00.000s
21/05/15 14:02:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507262388031727701816_0002_m_000168_172
[2021-05-15 11:02:06,583] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Finished task 168.0 in stage 2.0 (TID 172). 4587 bytes result sent to driver
[2021-05-15 11:02:06,585] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Starting task 172.0 in stage 2.0 (TID 176) (8951b5f85146, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:06,587] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Running task 172.0 in stage 2.0 (TID 176)
[2021-05-15 11:02:06,588] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Finished task 168.0 in stage 2.0 (TID 172) in 1709 ms on 8951b5f85146 (executor driver) (169/200)
[2021-05-15 11:02:06,597] {docker.py:276} INFO - 21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:06,599] {docker.py:276} INFO - 21/05/15 14:02:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504050334421649494409_0002_m_000172_176, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504050334421649494409_0002_m_000172_176}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504050334421649494409_0002}; taskId=attempt_202105151400504050334421649494409_0002_m_000172_176, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53d24836}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400504050334421649494409_0002_m_000172_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504050334421649494409_0002_m_000172_176
[2021-05-15 11:02:06,602] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400504050334421649494409_0002_m_000172_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504050334421649494409_0002_m_000172_176 : duration 0:00.004s
[2021-05-15 11:02:06,727] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400504896742596613403709_0002_m_000169_173: needsTaskCommit() Task attempt_202105151400504896742596613403709_0002_m_000169_173
[2021-05-15 11:02:06,728] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400504896742596613403709_0002_m_000169_173: needsTaskCommit() Task attempt_202105151400504896742596613403709_0002_m_000169_173: duration 0:00.001s
21/05/15 14:02:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504896742596613403709_0002_m_000169_173
[2021-05-15 11:02:06,729] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Finished task 169.0 in stage 2.0 (TID 173). 4587 bytes result sent to driver
[2021-05-15 11:02:06,730] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Starting task 173.0 in stage 2.0 (TID 177) (8951b5f85146, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:06,731] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Finished task 169.0 in stage 2.0 (TID 173) in 1770 ms on 8951b5f85146 (executor driver) (170/200)
[2021-05-15 11:02:06,732] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Running task 173.0 in stage 2.0 (TID 177)
[2021-05-15 11:02:06,741] {docker.py:276} INFO - 21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1288.0 B) non-empty blocks including 3 (1288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:06,744] {docker.py:276} INFO - 21/05/15 14:02:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507409405899972407742_0002_m_000173_177, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507409405899972407742_0002_m_000173_177}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507409405899972407742_0002}; taskId=attempt_202105151400507409405899972407742_0002_m_000173_177, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77adab59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400507409405899972407742_0002_m_000173_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507409405899972407742_0002_m_000173_177
[2021-05-15 11:02:06,747] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400507409405899972407742_0002_m_000173_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507409405899972407742_0002_m_000173_177 : duration 0:00.003s
[2021-05-15 11:02:06,772] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400504639262151302742209_0002_m_000170_174: needsTaskCommit() Task attempt_202105151400504639262151302742209_0002_m_000170_174
21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400504639262151302742209_0002_m_000170_174: needsTaskCommit() Task attempt_202105151400504639262151302742209_0002_m_000170_174: duration 0:00.000s
[2021-05-15 11:02:06,773] {docker.py:276} INFO - 21/05/15 14:02:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504639262151302742209_0002_m_000170_174
[2021-05-15 11:02:06,773] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Finished task 170.0 in stage 2.0 (TID 174). 4587 bytes result sent to driver
[2021-05-15 11:02:06,774] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Starting task 174.0 in stage 2.0 (TID 178) (8951b5f85146, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:06,774] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Running task 174.0 in stage 2.0 (TID 178)
[2021-05-15 11:02:06,775] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Finished task 170.0 in stage 2.0 (TID 174) in 1667 ms on 8951b5f85146 (executor driver) (171/200)
[2021-05-15 11:02:06,783] {docker.py:276} INFO - 21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1318.0 B) non-empty blocks including 3 (1318.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:06,784] {docker.py:276} INFO - 21/05/15 14:02:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400505904904137649751955_0002_m_000174_178, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505904904137649751955_0002_m_000174_178}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400505904904137649751955_0002}; taskId=attempt_202105151400505904904137649751955_0002_m_000174_178, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37af4aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400505904904137649751955_0002_m_000174_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505904904137649751955_0002_m_000174_178
[2021-05-15 11:02:06,788] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400505904904137649751955_0002_m_000174_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400505904904137649751955_0002_m_000174_178 : duration 0:00.004s
[2021-05-15 11:02:06,959] {docker.py:276} INFO - 21/05/15 14:02:06 INFO StagingCommitter: Starting: Task committer attempt_202105151400501201146405949536779_0002_m_000171_175: needsTaskCommit() Task attempt_202105151400501201146405949536779_0002_m_000171_175
21/05/15 14:02:06 INFO StagingCommitter: Task committer attempt_202105151400501201146405949536779_0002_m_000171_175: needsTaskCommit() Task attempt_202105151400501201146405949536779_0002_m_000171_175: duration 0:00.001s
21/05/15 14:02:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501201146405949536779_0002_m_000171_175
[2021-05-15 11:02:06,961] {docker.py:276} INFO - 21/05/15 14:02:06 INFO Executor: Finished task 171.0 in stage 2.0 (TID 175). 4587 bytes result sent to driver
[2021-05-15 11:02:06,963] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Starting task 175.0 in stage 2.0 (TID 179) (8951b5f85146, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:06,964] {docker.py:276} INFO - 21/05/15 14:02:06 INFO TaskSetManager: Finished task 171.0 in stage 2.0 (TID 175) in 1780 ms on 8951b5f85146 (executor driver) (172/200)
21/05/15 14:02:06 INFO Executor: Running task 175.0 in stage 2.0 (TID 179)
[2021-05-15 11:02:06,980] {docker.py:276} INFO - 21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1218.0 B) non-empty blocks including 3 (1218.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:06,982] {docker.py:276} INFO - 21/05/15 14:02:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:06,982] {docker.py:276} INFO - 21/05/15 14:02:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504515014269643221202_0002_m_000175_179, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504515014269643221202_0002_m_000175_179}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504515014269643221202_0002}; taskId=attempt_202105151400504515014269643221202_0002_m_000175_179, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50655b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:06,982] {docker.py:276} INFO - 21/05/15 14:02:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:06,982] {docker.py:276} INFO - 21/05/15 14:02:07 INFO StagingCommitter: Starting: Task committer attempt_202105151400504515014269643221202_0002_m_000175_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504515014269643221202_0002_m_000175_179
[2021-05-15 11:02:06,985] {docker.py:276} INFO - 21/05/15 14:02:07 INFO StagingCommitter: Task committer attempt_202105151400504515014269643221202_0002_m_000175_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504515014269643221202_0002_m_000175_179 : duration 0:00.002s
[2021-05-15 11:02:08,350] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400504050334421649494409_0002_m_000172_176: needsTaskCommit() Task attempt_202105151400504050334421649494409_0002_m_000172_176
[2021-05-15 11:02:08,350] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400504050334421649494409_0002_m_000172_176: needsTaskCommit() Task attempt_202105151400504050334421649494409_0002_m_000172_176: duration 0:00.000s
21/05/15 14:02:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504050334421649494409_0002_m_000172_176
[2021-05-15 11:02:08,353] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Finished task 172.0 in stage 2.0 (TID 176). 4587 bytes result sent to driver
[2021-05-15 11:02:08,354] {docker.py:276} INFO - 21/05/15 14:02:08 INFO TaskSetManager: Starting task 176.0 in stage 2.0 (TID 180) (8951b5f85146, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:02:08 INFO TaskSetManager: Finished task 172.0 in stage 2.0 (TID 176) in 1772 ms on 8951b5f85146 (executor driver) (173/200)
[2021-05-15 11:02:08,355] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Running task 176.0 in stage 2.0 (TID 180)
[2021-05-15 11:02:08,364] {docker.py:276} INFO - 21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:08,365] {docker.py:276} INFO - 21/05/15 14:02:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400506903458414696756776_0002_m_000176_180, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506903458414696756776_0002_m_000176_180}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400506903458414696756776_0002}; taskId=attempt_202105151400506903458414696756776_0002_m_000176_180, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c71dfcb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400506903458414696756776_0002_m_000176_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506903458414696756776_0002_m_000176_180
[2021-05-15 11:02:08,368] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400506903458414696756776_0002_m_000176_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400506903458414696756776_0002_m_000176_180 : duration 0:00.003s
[2021-05-15 11:02:08,453] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400507409405899972407742_0002_m_000173_177: needsTaskCommit() Task attempt_202105151400507409405899972407742_0002_m_000173_177
[2021-05-15 11:02:08,454] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400507409405899972407742_0002_m_000173_177: needsTaskCommit() Task attempt_202105151400507409405899972407742_0002_m_000173_177: duration 0:00.000s
21/05/15 14:02:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507409405899972407742_0002_m_000173_177
[2021-05-15 11:02:08,455] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Finished task 173.0 in stage 2.0 (TID 177). 4587 bytes result sent to driver
[2021-05-15 11:02:08,456] {docker.py:276} INFO - 21/05/15 14:02:08 INFO TaskSetManager: Starting task 177.0 in stage 2.0 (TID 181) (8951b5f85146, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:08,458] {docker.py:276} INFO - 21/05/15 14:02:08 INFO TaskSetManager: Finished task 173.0 in stage 2.0 (TID 177) in 1729 ms on 8951b5f85146 (executor driver) (174/200)
[2021-05-15 11:02:08,459] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Running task 177.0 in stage 2.0 (TID 181)
[2021-05-15 11:02:08,472] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400505904904137649751955_0002_m_000174_178: needsTaskCommit() Task attempt_202105151400505904904137649751955_0002_m_000174_178
[2021-05-15 11:02:08,473] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400505904904137649751955_0002_m_000174_178: needsTaskCommit() Task attempt_202105151400505904904137649751955_0002_m_000174_178: duration 0:00.001s
[2021-05-15 11:02:08,473] {docker.py:276} INFO - 21/05/15 14:02:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400505904904137649751955_0002_m_000174_178
[2021-05-15 11:02:08,474] {docker.py:276} INFO - 21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1352.0 B) non-empty blocks including 3 (1352.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:08,474] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Finished task 174.0 in stage 2.0 (TID 178). 4587 bytes result sent to driver
[2021-05-15 11:02:08,476] {docker.py:276} INFO - 21/05/15 14:02:08 INFO TaskSetManager: Starting task 178.0 in stage 2.0 (TID 182) (8951b5f85146, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:08,477] {docker.py:276} INFO - 21/05/15 14:02:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:02:08,477] {docker.py:276} INFO - 21/05/15 14:02:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:08 INFO TaskSetManager: Finished task 174.0 in stage 2.0 (TID 178) in 1705 ms on 8951b5f85146 (executor driver) (175/200)
[2021-05-15 11:02:08,477] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Running task 178.0 in stage 2.0 (TID 182)
[2021-05-15 11:02:08,477] {docker.py:276} INFO - 21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501580221944962660318_0002_m_000177_181, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501580221944962660318_0002_m_000177_181}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501580221944962660318_0002}; taskId=attempt_202105151400501580221944962660318_0002_m_000177_181, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32e46722}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:08,478] {docker.py:276} INFO - 21/05/15 14:02:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:08,479] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400501580221944962660318_0002_m_000177_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501580221944962660318_0002_m_000177_181
[2021-05-15 11:02:08,484] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400501580221944962660318_0002_m_000177_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501580221944962660318_0002_m_000177_181 : duration 0:00.006s
[2021-05-15 11:02:08,487] {docker.py:276} INFO - 21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1255.0 B) non-empty blocks including 3 (1255.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:08,489] {docker.py:276} INFO - 21/05/15 14:02:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:08,489] {docker.py:276} INFO - 21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507800973917649384273_0002_m_000178_182, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507800973917649384273_0002_m_000178_182}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507800973917649384273_0002}; taskId=attempt_202105151400507800973917649384273_0002_m_000178_182, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26c843ed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:08,489] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400507800973917649384273_0002_m_000178_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507800973917649384273_0002_m_000178_182
[2021-05-15 11:02:08,491] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400507800973917649384273_0002_m_000178_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507800973917649384273_0002_m_000178_182 : duration 0:00.002s
[2021-05-15 11:02:08,764] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400504515014269643221202_0002_m_000175_179: needsTaskCommit() Task attempt_202105151400504515014269643221202_0002_m_000175_179
21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400504515014269643221202_0002_m_000175_179: needsTaskCommit() Task attempt_202105151400504515014269643221202_0002_m_000175_179: duration 0:00.000s
21/05/15 14:02:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504515014269643221202_0002_m_000175_179
[2021-05-15 11:02:08,766] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Finished task 175.0 in stage 2.0 (TID 179). 4587 bytes result sent to driver
[2021-05-15 11:02:08,768] {docker.py:276} INFO - 21/05/15 14:02:08 INFO TaskSetManager: Starting task 179.0 in stage 2.0 (TID 183) (8951b5f85146, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:08,768] {docker.py:276} INFO - 21/05/15 14:02:08 INFO TaskSetManager: Finished task 175.0 in stage 2.0 (TID 179) in 1808 ms on 8951b5f85146 (executor driver) (176/200)
[2021-05-15 11:02:08,769] {docker.py:276} INFO - 21/05/15 14:02:08 INFO Executor: Running task 179.0 in stage 2.0 (TID 183)
[2021-05-15 11:02:08,787] {docker.py:276} INFO - 21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1126.0 B) non-empty blocks including 3 (1126.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:08,789] {docker.py:276} INFO - 21/05/15 14:02:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400504508070451244696424_0002_m_000179_183, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504508070451244696424_0002_m_000179_183}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400504508070451244696424_0002}; taskId=attempt_202105151400504508070451244696424_0002_m_000179_183, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fe2f315}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:08,790] {docker.py:276} INFO - 21/05/15 14:02:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:08 INFO StagingCommitter: Starting: Task committer attempt_202105151400504508070451244696424_0002_m_000179_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504508070451244696424_0002_m_000179_183
[2021-05-15 11:02:08,792] {docker.py:276} INFO - 21/05/15 14:02:08 INFO StagingCommitter: Task committer attempt_202105151400504508070451244696424_0002_m_000179_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400504508070451244696424_0002_m_000179_183 : duration 0:00.003s
[2021-05-15 11:02:10,066] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400506903458414696756776_0002_m_000176_180: needsTaskCommit() Task attempt_202105151400506903458414696756776_0002_m_000176_180
21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_202105151400506903458414696756776_0002_m_000176_180: needsTaskCommit() Task attempt_202105151400506903458414696756776_0002_m_000176_180: duration 0:00.001s
21/05/15 14:02:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400506903458414696756776_0002_m_000176_180
[2021-05-15 11:02:10,069] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Finished task 176.0 in stage 2.0 (TID 180). 4587 bytes result sent to driver
[2021-05-15 11:02:10,071] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Starting task 180.0 in stage 2.0 (TID 184) (8951b5f85146, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:10,072] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Finished task 176.0 in stage 2.0 (TID 180) in 1721 ms on 8951b5f85146 (executor driver) (177/200)
[2021-05-15 11:02:10,073] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Running task 180.0 in stage 2.0 (TID 184)
[2021-05-15 11:02:10,082] {docker.py:276} INFO - 21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:10,082] {docker.py:276} INFO - 21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:10,084] {docker.py:276} INFO - 21/05/15 14:02:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:10,084] {docker.py:276} INFO - 21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:10,085] {docker.py:276} INFO - 21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050408494399207664594_0002_m_000180_184, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050408494399207664594_0002_m_000180_184}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050408494399207664594_0002}; taskId=attempt_20210515140050408494399207664594_0002_m_000180_184, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61fb064d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:10,085] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_20210515140050408494399207664594_0002_m_000180_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050408494399207664594_0002_m_000180_184
[2021-05-15 11:02:10,093] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_20210515140050408494399207664594_0002_m_000180_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050408494399207664594_0002_m_000180_184 : duration 0:00.008s
[2021-05-15 11:02:10,140] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400501580221944962660318_0002_m_000177_181: needsTaskCommit() Task attempt_202105151400501580221944962660318_0002_m_000177_181
21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_202105151400501580221944962660318_0002_m_000177_181: needsTaskCommit() Task attempt_202105151400501580221944962660318_0002_m_000177_181: duration 0:00.000s
21/05/15 14:02:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501580221944962660318_0002_m_000177_181
[2021-05-15 11:02:10,142] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Finished task 177.0 in stage 2.0 (TID 181). 4587 bytes result sent to driver
[2021-05-15 11:02:10,142] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Starting task 181.0 in stage 2.0 (TID 185) (8951b5f85146, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:10,143] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Finished task 177.0 in stage 2.0 (TID 181) in 1689 ms on 8951b5f85146 (executor driver) (178/200)
21/05/15 14:02:10 INFO Executor: Running task 181.0 in stage 2.0 (TID 185)
[2021-05-15 11:02:10,152] {docker.py:276} INFO - 21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1385.0 B) non-empty blocks including 3 (1385.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:10,154] {docker.py:276} INFO - 21/05/15 14:02:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:10,154] {docker.py:276} INFO - 21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507527554661280878943_0002_m_000181_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507527554661280878943_0002_m_000181_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507527554661280878943_0002}; taskId=attempt_202105151400507527554661280878943_0002_m_000181_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f5421fb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400507527554661280878943_0002_m_000181_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507527554661280878943_0002_m_000181_185
[2021-05-15 11:02:10,157] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_202105151400507527554661280878943_0002_m_000181_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507527554661280878943_0002_m_000181_185 : duration 0:00.003s
[2021-05-15 11:02:10,238] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400507800973917649384273_0002_m_000178_182: needsTaskCommit() Task attempt_202105151400507800973917649384273_0002_m_000178_182
[2021-05-15 11:02:10,239] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_202105151400507800973917649384273_0002_m_000178_182: needsTaskCommit() Task attempt_202105151400507800973917649384273_0002_m_000178_182: duration 0:00.002s
21/05/15 14:02:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507800973917649384273_0002_m_000178_182
[2021-05-15 11:02:10,242] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Finished task 178.0 in stage 2.0 (TID 182). 4587 bytes result sent to driver
[2021-05-15 11:02:10,243] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Starting task 182.0 in stage 2.0 (TID 186) (8951b5f85146, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:10,245] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Running task 182.0 in stage 2.0 (TID 186)
[2021-05-15 11:02:10,246] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Finished task 178.0 in stage 2.0 (TID 182) in 1771 ms on 8951b5f85146 (executor driver) (179/200)
[2021-05-15 11:02:10,265] {docker.py:276} INFO - 21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:10,266] {docker.py:276} INFO - 21/05/15 14:02:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:10,267] {docker.py:276} INFO - 21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050391567294375723280_0002_m_000182_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050391567294375723280_0002_m_000182_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050391567294375723280_0002}; taskId=attempt_20210515140050391567294375723280_0002_m_000182_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50744013}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_20210515140050391567294375723280_0002_m_000182_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050391567294375723280_0002_m_000182_186
[2021-05-15 11:02:10,270] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_20210515140050391567294375723280_0002_m_000182_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050391567294375723280_0002_m_000182_186 : duration 0:00.003s
[2021-05-15 11:02:10,453] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400504508070451244696424_0002_m_000179_183: needsTaskCommit() Task attempt_202105151400504508070451244696424_0002_m_000179_183
[2021-05-15 11:02:10,454] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_202105151400504508070451244696424_0002_m_000179_183: needsTaskCommit() Task attempt_202105151400504508070451244696424_0002_m_000179_183: duration 0:00.001s
[2021-05-15 11:02:10,455] {docker.py:276} INFO - 21/05/15 14:02:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400504508070451244696424_0002_m_000179_183
[2021-05-15 11:02:10,456] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Finished task 179.0 in stage 2.0 (TID 183). 4587 bytes result sent to driver
[2021-05-15 11:02:10,457] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Starting task 183.0 in stage 2.0 (TID 187) (8951b5f85146, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:10,458] {docker.py:276} INFO - 21/05/15 14:02:10 INFO TaskSetManager: Finished task 179.0 in stage 2.0 (TID 183) in 1693 ms on 8951b5f85146 (executor driver) (180/200)
[2021-05-15 11:02:10,458] {docker.py:276} INFO - 21/05/15 14:02:10 INFO Executor: Running task 183.0 in stage 2.0 (TID 187)
[2021-05-15 11:02:10,475] {docker.py:276} INFO - 21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:10,476] {docker.py:276} INFO - 21/05/15 14:02:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:10,477] {docker.py:276} INFO - 21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:10,477] {docker.py:276} INFO - 21/05/15 14:02:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502792058899977460486_0002_m_000183_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502792058899977460486_0002_m_000183_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502792058899977460486_0002}; taskId=attempt_202105151400502792058899977460486_0002_m_000183_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@769bae61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:10 INFO StagingCommitter: Starting: Task committer attempt_202105151400502792058899977460486_0002_m_000183_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502792058899977460486_0002_m_000183_187
[2021-05-15 11:02:10,480] {docker.py:276} INFO - 21/05/15 14:02:10 INFO StagingCommitter: Task committer attempt_202105151400502792058899977460486_0002_m_000183_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502792058899977460486_0002_m_000183_187 : duration 0:00.003s
[2021-05-15 11:02:11,821] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400507527554661280878943_0002_m_000181_185: needsTaskCommit() Task attempt_202105151400507527554661280878943_0002_m_000181_185
[2021-05-15 11:02:11,822] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Task committer attempt_202105151400507527554661280878943_0002_m_000181_185: needsTaskCommit() Task attempt_202105151400507527554661280878943_0002_m_000181_185: duration 0:00.001s
[2021-05-15 11:02:11,823] {docker.py:276} INFO - 21/05/15 14:02:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507527554661280878943_0002_m_000181_185
[2021-05-15 11:02:11,825] {docker.py:276} INFO - 21/05/15 14:02:11 INFO Executor: Finished task 181.0 in stage 2.0 (TID 185). 4587 bytes result sent to driver
21/05/15 14:02:11 INFO TaskSetManager: Starting task 184.0 in stage 2.0 (TID 188) (8951b5f85146, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:11,826] {docker.py:276} INFO - 21/05/15 14:02:11 INFO TaskSetManager: Finished task 181.0 in stage 2.0 (TID 185) in 1686 ms on 8951b5f85146 (executor driver) (181/200)
[2021-05-15 11:02:11,827] {docker.py:276} INFO - 21/05/15 14:02:11 INFO Executor: Running task 184.0 in stage 2.0 (TID 188)
[2021-05-15 11:02:11,844] {docker.py:276} INFO - 21/05/15 14:02:11 INFO ShuffleBlockFetcherIterator: Getting 3 (1181.0 B) non-empty blocks including 3 (1181.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:11,844] {docker.py:276} INFO - 21/05/15 14:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:11,847] {docker.py:276} INFO - 21/05/15 14:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:11,847] {docker.py:276} INFO - 21/05/15 14:02:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:11,848] {docker.py:276} INFO - 21/05/15 14:02:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508183041310643445032_0002_m_000184_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508183041310643445032_0002_m_000184_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508183041310643445032_0002}; taskId=attempt_202105151400508183041310643445032_0002_m_000184_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58fe1e8b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:11,848] {docker.py:276} INFO - 21/05/15 14:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:11,848] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400508183041310643445032_0002_m_000184_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508183041310643445032_0002_m_000184_188
[2021-05-15 11:02:11,850] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Task committer attempt_202105151400508183041310643445032_0002_m_000184_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508183041310643445032_0002_m_000184_188 : duration 0:00.002s
[2021-05-15 11:02:11,947] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_20210515140050391567294375723280_0002_m_000182_186: needsTaskCommit() Task attempt_20210515140050391567294375723280_0002_m_000182_186
[2021-05-15 11:02:11,948] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Task committer attempt_20210515140050391567294375723280_0002_m_000182_186: needsTaskCommit() Task attempt_20210515140050391567294375723280_0002_m_000182_186: duration 0:00.001s
[2021-05-15 11:02:11,949] {docker.py:276} INFO - 21/05/15 14:02:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050391567294375723280_0002_m_000182_186
[2021-05-15 11:02:11,950] {docker.py:276} INFO - 21/05/15 14:02:11 INFO Executor: Finished task 182.0 in stage 2.0 (TID 186). 4587 bytes result sent to driver
[2021-05-15 11:02:11,952] {docker.py:276} INFO - 21/05/15 14:02:11 INFO TaskSetManager: Starting task 185.0 in stage 2.0 (TID 189) (8951b5f85146, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:11,953] {docker.py:276} INFO - 21/05/15 14:02:11 INFO TaskSetManager: Finished task 182.0 in stage 2.0 (TID 186) in 1712 ms on 8951b5f85146 (executor driver) (182/200)
21/05/15 14:02:11 INFO Executor: Running task 185.0 in stage 2.0 (TID 189)
[2021-05-15 11:02:11,967] {docker.py:276} INFO - 21/05/15 14:02:11 INFO ShuffleBlockFetcherIterator: Getting 3 (1446.0 B) non-empty blocks including 3 (1446.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:11,969] {docker.py:276} INFO - 21/05/15 14:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501945084949012501464_0002_m_000185_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501945084949012501464_0002_m_000185_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501945084949012501464_0002}; taskId=attempt_202105151400501945084949012501464_0002_m_000185_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@272eceac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:11 INFO StagingCommitter: Starting: Task committer attempt_202105151400501945084949012501464_0002_m_000185_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501945084949012501464_0002_m_000185_189
[2021-05-15 11:02:11,972] {docker.py:276} INFO - 21/05/15 14:02:11 INFO StagingCommitter: Task committer attempt_202105151400501945084949012501464_0002_m_000185_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501945084949012501464_0002_m_000185_189 : duration 0:00.003s
[2021-05-15 11:02:12,157] {docker.py:276} INFO - 21/05/15 14:02:12 INFO StagingCommitter: Starting: Task committer attempt_202105151400502792058899977460486_0002_m_000183_187: needsTaskCommit() Task attempt_202105151400502792058899977460486_0002_m_000183_187
21/05/15 14:02:12 INFO StagingCommitter: Task committer attempt_202105151400502792058899977460486_0002_m_000183_187: needsTaskCommit() Task attempt_202105151400502792058899977460486_0002_m_000183_187: duration 0:00.000s
21/05/15 14:02:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502792058899977460486_0002_m_000183_187
[2021-05-15 11:02:12,159] {docker.py:276} INFO - 21/05/15 14:02:12 INFO Executor: Finished task 183.0 in stage 2.0 (TID 187). 4587 bytes result sent to driver
[2021-05-15 11:02:12,160] {docker.py:276} INFO - 21/05/15 14:02:12 INFO TaskSetManager: Starting task 186.0 in stage 2.0 (TID 190) (8951b5f85146, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:12,162] {docker.py:276} INFO - 21/05/15 14:02:12 INFO TaskSetManager: Finished task 183.0 in stage 2.0 (TID 187) in 1707 ms on 8951b5f85146 (executor driver) (183/200)
[2021-05-15 11:02:12,162] {docker.py:276} INFO - 21/05/15 14:02:12 INFO Executor: Running task 186.0 in stage 2.0 (TID 190)
[2021-05-15 11:02:12,177] {docker.py:276} INFO - 21/05/15 14:02:12 INFO ShuffleBlockFetcherIterator: Getting 3 (1352.0 B) non-empty blocks including 3 (1352.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:12,178] {docker.py:276} INFO - 21/05/15 14:02:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:12,178] {docker.py:276} INFO - 21/05/15 14:02:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050790726122288542219_0002_m_000186_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050790726122288542219_0002_m_000186_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050790726122288542219_0002}; taskId=attempt_20210515140050790726122288542219_0002_m_000186_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cfc65e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:12 INFO StagingCommitter: Starting: Task committer attempt_20210515140050790726122288542219_0002_m_000186_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050790726122288542219_0002_m_000186_190
[2021-05-15 11:02:12,181] {docker.py:276} INFO - 21/05/15 14:02:12 INFO StagingCommitter: Task committer attempt_20210515140050790726122288542219_0002_m_000186_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050790726122288542219_0002_m_000186_190 : duration 0:00.002s
[2021-05-15 11:02:12,358] {docker.py:276} INFO - 21/05/15 14:02:12 INFO StagingCommitter: Starting: Task committer attempt_20210515140050408494399207664594_0002_m_000180_184: needsTaskCommit() Task attempt_20210515140050408494399207664594_0002_m_000180_184
[2021-05-15 11:02:12,359] {docker.py:276} INFO - 21/05/15 14:02:12 INFO StagingCommitter: Task committer attempt_20210515140050408494399207664594_0002_m_000180_184: needsTaskCommit() Task attempt_20210515140050408494399207664594_0002_m_000180_184: duration 0:00.000s
21/05/15 14:02:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050408494399207664594_0002_m_000180_184
[2021-05-15 11:02:12,360] {docker.py:276} INFO - 21/05/15 14:02:12 INFO Executor: Finished task 180.0 in stage 2.0 (TID 184). 4587 bytes result sent to driver
[2021-05-15 11:02:12,361] {docker.py:276} INFO - 21/05/15 14:02:12 INFO TaskSetManager: Starting task 187.0 in stage 2.0 (TID 191) (8951b5f85146, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:12,361] {docker.py:276} INFO - 21/05/15 14:02:12 INFO TaskSetManager: Finished task 180.0 in stage 2.0 (TID 184) in 2294 ms on 8951b5f85146 (executor driver) (184/200)
[2021-05-15 11:02:12,362] {docker.py:276} INFO - 21/05/15 14:02:12 INFO Executor: Running task 187.0 in stage 2.0 (TID 191)
[2021-05-15 11:02:12,376] {docker.py:276} INFO - 21/05/15 14:02:12 INFO ShuffleBlockFetcherIterator: Getting 3 (1263.0 B) non-empty blocks including 3 (1263.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:12,378] {docker.py:276} INFO - 21/05/15 14:02:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:12,378] {docker.py:276} INFO - 21/05/15 14:02:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502505643887772951033_0002_m_000187_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502505643887772951033_0002_m_000187_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502505643887772951033_0002}; taskId=attempt_202105151400502505643887772951033_0002_m_000187_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2910684d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:12,378] {docker.py:276} INFO - 21/05/15 14:02:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:12,379] {docker.py:276} INFO - 21/05/15 14:02:12 INFO StagingCommitter: Starting: Task committer attempt_202105151400502505643887772951033_0002_m_000187_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502505643887772951033_0002_m_000187_191
[2021-05-15 11:02:12,381] {docker.py:276} INFO - 21/05/15 14:02:12 INFO StagingCommitter: Task committer attempt_202105151400502505643887772951033_0002_m_000187_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502505643887772951033_0002_m_000187_191 : duration 0:00.003s
[2021-05-15 11:02:13,180] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400501945084949012501464_0002_m_000185_189: needsTaskCommit() Task attempt_202105151400501945084949012501464_0002_m_000185_189
[2021-05-15 11:02:13,181] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Task committer attempt_202105151400501945084949012501464_0002_m_000185_189: needsTaskCommit() Task attempt_202105151400501945084949012501464_0002_m_000185_189: duration 0:00.001s
[2021-05-15 11:02:13,182] {docker.py:276} INFO - 21/05/15 14:02:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501945084949012501464_0002_m_000185_189
[2021-05-15 11:02:13,184] {docker.py:276} INFO - 21/05/15 14:02:13 INFO Executor: Finished task 185.0 in stage 2.0 (TID 189). 4587 bytes result sent to driver
[2021-05-15 11:02:13,194] {docker.py:276} INFO - 21/05/15 14:02:13 INFO TaskSetManager: Starting task 188.0 in stage 2.0 (TID 192) (8951b5f85146, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:13,195] {docker.py:276} INFO - 21/05/15 14:02:13 INFO Executor: Running task 188.0 in stage 2.0 (TID 192)
[2021-05-15 11:02:13,196] {docker.py:276} INFO - 21/05/15 14:02:13 INFO TaskSetManager: Finished task 185.0 in stage 2.0 (TID 189) in 1246 ms on 8951b5f85146 (executor driver) (185/200)
[2021-05-15 11:02:13,205] {docker.py:276} INFO - 21/05/15 14:02:13 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:13,206] {docker.py:276} INFO - 21/05/15 14:02:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:13,208] {docker.py:276} INFO - 21/05/15 14:02:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:02:13,208] {docker.py:276} INFO - 21/05/15 14:02:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:13,209] {docker.py:276} INFO - 21/05/15 14:02:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:13,210] {docker.py:276} INFO - 21/05/15 14:02:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400508071943240109898535_0002_m_000188_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508071943240109898535_0002_m_000188_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400508071943240109898535_0002}; taskId=attempt_202105151400508071943240109898535_0002_m_000188_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@440e847f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:13,210] {docker.py:276} INFO - 21/05/15 14:02:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400508071943240109898535_0002_m_000188_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508071943240109898535_0002_m_000188_192
[2021-05-15 11:02:13,213] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Task committer attempt_202105151400508071943240109898535_0002_m_000188_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400508071943240109898535_0002_m_000188_192 : duration 0:00.003s
[2021-05-15 11:02:13,362] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Starting: Task committer attempt_20210515140050790726122288542219_0002_m_000186_190: needsTaskCommit() Task attempt_20210515140050790726122288542219_0002_m_000186_190
[2021-05-15 11:02:13,362] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Task committer attempt_20210515140050790726122288542219_0002_m_000186_190: needsTaskCommit() Task attempt_20210515140050790726122288542219_0002_m_000186_190: duration 0:00.001s
21/05/15 14:02:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050790726122288542219_0002_m_000186_190
[2021-05-15 11:02:13,363] {docker.py:276} INFO - 21/05/15 14:02:13 INFO Executor: Finished task 186.0 in stage 2.0 (TID 190). 4587 bytes result sent to driver
[2021-05-15 11:02:13,365] {docker.py:276} INFO - 21/05/15 14:02:13 INFO TaskSetManager: Starting task 189.0 in stage 2.0 (TID 193) (8951b5f85146, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:13,366] {docker.py:276} INFO - 21/05/15 14:02:13 INFO Executor: Running task 189.0 in stage 2.0 (TID 193)
[2021-05-15 11:02:13,366] {docker.py:276} INFO - 21/05/15 14:02:13 INFO TaskSetManager: Finished task 186.0 in stage 2.0 (TID 190) in 1207 ms on 8951b5f85146 (executor driver) (186/200)
[2021-05-15 11:02:13,376] {docker.py:276} INFO - 21/05/15 14:02:13 INFO ShuffleBlockFetcherIterator: Getting 3 (1178.0 B) non-empty blocks including 3 (1178.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:13,377] {docker.py:276} INFO - 21/05/15 14:02:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400501667140441001270459_0002_m_000189_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501667140441001270459_0002_m_000189_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400501667140441001270459_0002}; taskId=attempt_202105151400501667140441001270459_0002_m_000189_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59aa15f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:13,378] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400501667140441001270459_0002_m_000189_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501667140441001270459_0002_m_000189_193
[2021-05-15 11:02:13,389] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Task committer attempt_202105151400501667140441001270459_0002_m_000189_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400501667140441001270459_0002_m_000189_193 : duration 0:00.012s
[2021-05-15 11:02:13,520] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400508183041310643445032_0002_m_000184_188: needsTaskCommit() Task attempt_202105151400508183041310643445032_0002_m_000184_188
[2021-05-15 11:02:13,522] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Task committer attempt_202105151400508183041310643445032_0002_m_000184_188: needsTaskCommit() Task attempt_202105151400508183041310643445032_0002_m_000184_188: duration 0:00.001s
21/05/15 14:02:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508183041310643445032_0002_m_000184_188
[2021-05-15 11:02:13,524] {docker.py:276} INFO - 21/05/15 14:02:13 INFO Executor: Finished task 184.0 in stage 2.0 (TID 188). 4587 bytes result sent to driver
[2021-05-15 11:02:13,526] {docker.py:276} INFO - 21/05/15 14:02:13 INFO TaskSetManager: Starting task 190.0 in stage 2.0 (TID 194) (8951b5f85146, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:13,527] {docker.py:276} INFO - 21/05/15 14:02:13 INFO TaskSetManager: Finished task 184.0 in stage 2.0 (TID 188) in 1705 ms on 8951b5f85146 (executor driver) (187/200)
[2021-05-15 11:02:13,529] {docker.py:276} INFO - 21/05/15 14:02:13 INFO Executor: Running task 190.0 in stage 2.0 (TID 194)
[2021-05-15 11:02:13,544] {docker.py:276} INFO - 21/05/15 14:02:13 INFO ShuffleBlockFetcherIterator: Getting 3 (1380.0 B) non-empty blocks including 3 (1380.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:02:13,545] {docker.py:276} INFO - 21/05/15 14:02:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:13,546] {docker.py:276} INFO - 21/05/15 14:02:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507951014284628941154_0002_m_000190_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507951014284628941154_0002_m_000190_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507951014284628941154_0002}; taskId=attempt_202105151400507951014284628941154_0002_m_000190_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b110f0f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:13,547] {docker.py:276} INFO - 21/05/15 14:02:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:13,548] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Starting: Task committer attempt_202105151400507951014284628941154_0002_m_000190_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507951014284628941154_0002_m_000190_194
[2021-05-15 11:02:13,550] {docker.py:276} INFO - 21/05/15 14:02:13 INFO StagingCommitter: Task committer attempt_202105151400507951014284628941154_0002_m_000190_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507951014284628941154_0002_m_000190_194 : duration 0:00.003s
[2021-05-15 11:02:14,919] {docker.py:276} INFO - 21/05/15 14:02:14 INFO StagingCommitter: Starting: Task committer attempt_202105151400502505643887772951033_0002_m_000187_191: needsTaskCommit() Task attempt_202105151400502505643887772951033_0002_m_000187_191
[2021-05-15 11:02:14,920] {docker.py:276} INFO - 21/05/15 14:02:14 INFO StagingCommitter: Starting: Task committer attempt_202105151400508071943240109898535_0002_m_000188_192: needsTaskCommit() Task attempt_202105151400508071943240109898535_0002_m_000188_192
21/05/15 14:02:14 INFO StagingCommitter: Task committer attempt_202105151400502505643887772951033_0002_m_000187_191: needsTaskCommit() Task attempt_202105151400502505643887772951033_0002_m_000187_191: duration 0:00.001s
21/05/15 14:02:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502505643887772951033_0002_m_000187_191
[2021-05-15 11:02:14,921] {docker.py:276} INFO - 21/05/15 14:02:14 INFO StagingCommitter: Task committer attempt_202105151400508071943240109898535_0002_m_000188_192: needsTaskCommit() Task attempt_202105151400508071943240109898535_0002_m_000188_192: duration 0:00.001s
[2021-05-15 11:02:14,921] {docker.py:276} INFO - 21/05/15 14:02:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400508071943240109898535_0002_m_000188_192
[2021-05-15 11:02:14,922] {docker.py:276} INFO - 21/05/15 14:02:14 INFO Executor: Finished task 187.0 in stage 2.0 (TID 191). 4587 bytes result sent to driver
[2021-05-15 11:02:14,923] {docker.py:276} INFO - 21/05/15 14:02:14 INFO TaskSetManager: Starting task 191.0 in stage 2.0 (TID 195) (8951b5f85146, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:14,923] {docker.py:276} INFO - 21/05/15 14:02:14 INFO Executor: Finished task 188.0 in stage 2.0 (TID 192). 4587 bytes result sent to driver
[2021-05-15 11:02:14,923] {docker.py:276} INFO - 21/05/15 14:02:14 INFO TaskSetManager: Starting task 192.0 in stage 2.0 (TID 196) (8951b5f85146, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:14,924] {docker.py:276} INFO - 21/05/15 14:02:14 INFO Executor: Running task 191.0 in stage 2.0 (TID 195)
[2021-05-15 11:02:14,925] {docker.py:276} INFO - 21/05/15 14:02:14 INFO TaskSetManager: Finished task 187.0 in stage 2.0 (TID 191) in 2567 ms on 8951b5f85146 (executor driver) (188/200)
[2021-05-15 11:02:14,926] {docker.py:276} INFO - 21/05/15 14:02:14 INFO TaskSetManager: Finished task 188.0 in stage 2.0 (TID 192) in 1734 ms on 8951b5f85146 (executor driver) (189/200)
[2021-05-15 11:02:14,927] {docker.py:276} INFO - 21/05/15 14:02:14 INFO Executor: Running task 192.0 in stage 2.0 (TID 196)
[2021-05-15 11:02:14,946] {docker.py:276} INFO - 21/05/15 14:02:14 INFO ShuffleBlockFetcherIterator: Getting 3 (1335.0 B) non-empty blocks including 3 (1335.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:14 INFO ShuffleBlockFetcherIterator: Getting 3 (1303.0 B) non-empty blocks including 3 (1303.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:14,946] {docker.py:276} INFO - 21/05/15 14:02:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:14,947] {docker.py:276} INFO - 21/05/15 14:02:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:02:14,948] {docker.py:276} INFO - 21/05/15 14:02:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050613360470131794623_0002_m_000192_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050613360470131794623_0002_m_000192_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050613360470131794623_0002}; taskId=attempt_20210515140050613360470131794623_0002_m_000192_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5546609e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:14 INFO StagingCommitter: Starting: Task committer attempt_20210515140050613360470131794623_0002_m_000192_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050613360470131794623_0002_m_000192_196 
21/05/15 14:02:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503705680648883641064_0002_m_000191_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503705680648883641064_0002_m_000191_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503705680648883641064_0002}; taskId=attempt_202105151400503705680648883641064_0002_m_000191_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40bf7604}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:14,948] {docker.py:276} INFO - 21/05/15 14:02:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:14 INFO StagingCommitter: Starting: Task committer attempt_202105151400503705680648883641064_0002_m_000191_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503705680648883641064_0002_m_000191_195
[2021-05-15 11:02:14,950] {docker.py:276} INFO - 21/05/15 14:02:14 INFO StagingCommitter: Task committer attempt_20210515140050613360470131794623_0002_m_000192_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050613360470131794623_0002_m_000192_196 : duration 0:00.003s
[2021-05-15 11:02:14,955] {docker.py:276} INFO - 21/05/15 14:02:14 INFO StagingCommitter: Task committer attempt_202105151400503705680648883641064_0002_m_000191_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503705680648883641064_0002_m_000191_195 : duration 0:00.006s
[2021-05-15 11:02:15,046] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400501667140441001270459_0002_m_000189_193: needsTaskCommit() Task attempt_202105151400501667140441001270459_0002_m_000189_193
[2021-05-15 11:02:15,046] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Task committer attempt_202105151400501667140441001270459_0002_m_000189_193: needsTaskCommit() Task attempt_202105151400501667140441001270459_0002_m_000189_193: duration 0:00.000s
21/05/15 14:02:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400501667140441001270459_0002_m_000189_193
[2021-05-15 11:02:15,047] {docker.py:276} INFO - 21/05/15 14:02:15 INFO Executor: Finished task 189.0 in stage 2.0 (TID 193). 4587 bytes result sent to driver
[2021-05-15 11:02:15,049] {docker.py:276} INFO - 21/05/15 14:02:15 INFO TaskSetManager: Starting task 193.0 in stage 2.0 (TID 197) (8951b5f85146, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:15,049] {docker.py:276} INFO - 21/05/15 14:02:15 INFO TaskSetManager: Finished task 189.0 in stage 2.0 (TID 193) in 1687 ms on 8951b5f85146 (executor driver) (190/200)
[2021-05-15 11:02:15,050] {docker.py:276} INFO - 21/05/15 14:02:15 INFO Executor: Running task 193.0 in stage 2.0 (TID 197)
[2021-05-15 11:02:15,065] {docker.py:276} INFO - 21/05/15 14:02:15 INFO ShuffleBlockFetcherIterator: Getting 3 (1340.0 B) non-empty blocks including 3 (1340.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:15,066] {docker.py:276} INFO - 21/05/15 14:02:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503772788591712871235_0002_m_000193_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503772788591712871235_0002_m_000193_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503772788591712871235_0002}; taskId=attempt_202105151400503772788591712871235_0002_m_000193_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@175ced7a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:15,067] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400503772788591712871235_0002_m_000193_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503772788591712871235_0002_m_000193_197
[2021-05-15 11:02:15,069] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Task committer attempt_202105151400503772788591712871235_0002_m_000193_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503772788591712871235_0002_m_000193_197 : duration 0:00.003s
[2021-05-15 11:02:15,197] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Starting: Task committer attempt_202105151400507951014284628941154_0002_m_000190_194: needsTaskCommit() Task attempt_202105151400507951014284628941154_0002_m_000190_194
[2021-05-15 11:02:15,198] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Task committer attempt_202105151400507951014284628941154_0002_m_000190_194: needsTaskCommit() Task attempt_202105151400507951014284628941154_0002_m_000190_194: duration 0:00.002s
21/05/15 14:02:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507951014284628941154_0002_m_000190_194
[2021-05-15 11:02:15,200] {docker.py:276} INFO - 21/05/15 14:02:15 INFO Executor: Finished task 190.0 in stage 2.0 (TID 194). 4587 bytes result sent to driver
[2021-05-15 11:02:15,201] {docker.py:276} INFO - 21/05/15 14:02:15 INFO TaskSetManager: Starting task 194.0 in stage 2.0 (TID 198) (8951b5f85146, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:15,202] {docker.py:276} INFO - 21/05/15 14:02:15 INFO TaskSetManager: Finished task 190.0 in stage 2.0 (TID 194) in 1679 ms on 8951b5f85146 (executor driver) (191/200)
21/05/15 14:02:15 INFO Executor: Running task 194.0 in stage 2.0 (TID 198)
[2021-05-15 11:02:15,219] {docker.py:276} INFO - 21/05/15 14:02:15 INFO ShuffleBlockFetcherIterator: Getting 3 (1380.0 B) non-empty blocks including 3 (1380.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:15,222] {docker.py:276} INFO - 21/05/15 14:02:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:15,222] {docker.py:276} INFO - 21/05/15 14:02:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050160640211241231715_0002_m_000194_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050160640211241231715_0002_m_000194_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050160640211241231715_0002}; taskId=attempt_20210515140050160640211241231715_0002_m_000194_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49e97c98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:15,223] {docker.py:276} INFO - 21/05/15 14:02:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:15,223] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Starting: Task committer attempt_20210515140050160640211241231715_0002_m_000194_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050160640211241231715_0002_m_000194_198
[2021-05-15 11:02:15,225] {docker.py:276} INFO - 21/05/15 14:02:15 INFO StagingCommitter: Task committer attempt_20210515140050160640211241231715_0002_m_000194_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050160640211241231715_0002_m_000194_198 : duration 0:00.002s
[2021-05-15 11:02:16,621] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_202105151400503705680648883641064_0002_m_000191_195: needsTaskCommit() Task attempt_202105151400503705680648883641064_0002_m_000191_195
[2021-05-15 11:02:16,623] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_202105151400503705680648883641064_0002_m_000191_195: needsTaskCommit() Task attempt_202105151400503705680648883641064_0002_m_000191_195: duration 0:00.001s
21/05/15 14:02:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503705680648883641064_0002_m_000191_195
[2021-05-15 11:02:16,623] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_20210515140050613360470131794623_0002_m_000192_196: needsTaskCommit() Task attempt_20210515140050613360470131794623_0002_m_000192_196
[2021-05-15 11:02:16,624] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Finished task 191.0 in stage 2.0 (TID 195). 4587 bytes result sent to driver
[2021-05-15 11:02:16,625] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_20210515140050613360470131794623_0002_m_000192_196: needsTaskCommit() Task attempt_20210515140050613360470131794623_0002_m_000192_196: duration 0:00.004s
21/05/15 14:02:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050613360470131794623_0002_m_000192_196
[2021-05-15 11:02:16,626] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Starting task 195.0 in stage 2.0 (TID 199) (8951b5f85146, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:16,627] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Running task 195.0 in stage 2.0 (TID 199)
[2021-05-15 11:02:16,628] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Finished task 191.0 in stage 2.0 (TID 195) in 1708 ms on 8951b5f85146 (executor driver) (192/200)
[2021-05-15 11:02:16,629] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Finished task 192.0 in stage 2.0 (TID 196). 4587 bytes result sent to driver
[2021-05-15 11:02:16,630] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Starting task 196.0 in stage 2.0 (TID 200) (8951b5f85146, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:16,631] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Running task 196.0 in stage 2.0 (TID 200)
21/05/15 14:02:16 INFO TaskSetManager: Finished task 192.0 in stage 2.0 (TID 196) in 1709 ms on 8951b5f85146 (executor driver) (193/200)
[2021-05-15 11:02:16,638] {docker.py:276} INFO - 21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Getting 3 (1104.0 B) non-empty blocks including 3 (1104.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:16,639] {docker.py:276} INFO - 21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:16,640] {docker.py:276} INFO - 21/05/15 14:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050365464501654351257_0002_m_000195_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050365464501654351257_0002_m_000195_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050365464501654351257_0002}; taskId=attempt_20210515140050365464501654351257_0002_m_000195_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@760efb9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:16,641] {docker.py:276} INFO - 21/05/15 14:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_20210515140050365464501654351257_0002_m_000195_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050365464501654351257_0002_m_000195_199 
21/05/15 14:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:02:16,641] {docker.py:276} INFO - 21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400507067442348448650254_0002_m_000196_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507067442348448650254_0002_m_000196_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400507067442348448650254_0002}; taskId=attempt_202105151400507067442348448650254_0002_m_000196_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ada8fba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:16,641] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_202105151400507067442348448650254_0002_m_000196_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507067442348448650254_0002_m_000196_200
[2021-05-15 11:02:16,644] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_20210515140050365464501654351257_0002_m_000195_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050365464501654351257_0002_m_000195_199 : duration 0:00.004s
[2021-05-15 11:02:16,645] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_202105151400507067442348448650254_0002_m_000196_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400507067442348448650254_0002_m_000196_200 : duration 0:00.004s
[2021-05-15 11:02:16,681] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_202105151400503772788591712871235_0002_m_000193_197: needsTaskCommit() Task attempt_202105151400503772788591712871235_0002_m_000193_197
[2021-05-15 11:02:16,682] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_202105151400503772788591712871235_0002_m_000193_197: needsTaskCommit() Task attempt_202105151400503772788591712871235_0002_m_000193_197: duration 0:00.001s
21/05/15 14:02:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503772788591712871235_0002_m_000193_197
[2021-05-15 11:02:16,684] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Finished task 193.0 in stage 2.0 (TID 197). 4587 bytes result sent to driver
[2021-05-15 11:02:16,686] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Starting task 197.0 in stage 2.0 (TID 201) (8951b5f85146, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:16,687] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Running task 197.0 in stage 2.0 (TID 201)
[2021-05-15 11:02:16,688] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Finished task 193.0 in stage 2.0 (TID 197) in 1640 ms on 8951b5f85146 (executor driver) (194/200)
[2021-05-15 11:02:16,704] {docker.py:276} INFO - 21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Getting 3 (1295.0 B) non-empty blocks including 3 (1295.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:16,705] {docker.py:276} INFO - 21/05/15 14:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:16,706] {docker.py:276} INFO - 21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400503683492598760896049_0002_m_000197_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503683492598760896049_0002_m_000197_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400503683492598760896049_0002}; taskId=attempt_202105151400503683492598760896049_0002_m_000197_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@654a814c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:02:16,706] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_202105151400503683492598760896049_0002_m_000197_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503683492598760896049_0002_m_000197_201
[2021-05-15 11:02:16,708] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_202105151400503683492598760896049_0002_m_000197_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400503683492598760896049_0002_m_000197_201 : duration 0:00.003s
[2021-05-15 11:02:16,953] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_20210515140050160640211241231715_0002_m_000194_198: needsTaskCommit() Task attempt_20210515140050160640211241231715_0002_m_000194_198
[2021-05-15 11:02:16,954] {docker.py:276} INFO - 21/05/15 14:02:16 INFO StagingCommitter: Task committer attempt_20210515140050160640211241231715_0002_m_000194_198: needsTaskCommit() Task attempt_20210515140050160640211241231715_0002_m_000194_198: duration 0:00.000s
21/05/15 14:02:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050160640211241231715_0002_m_000194_198
[2021-05-15 11:02:16,956] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Finished task 194.0 in stage 2.0 (TID 198). 4587 bytes result sent to driver
[2021-05-15 11:02:16,956] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Starting task 198.0 in stage 2.0 (TID 202) (8951b5f85146, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:16,957] {docker.py:276} INFO - 21/05/15 14:02:16 INFO TaskSetManager: Finished task 194.0 in stage 2.0 (TID 198) in 1759 ms on 8951b5f85146 (executor driver) (195/200)
[2021-05-15 11:02:16,958] {docker.py:276} INFO - 21/05/15 14:02:16 INFO Executor: Running task 198.0 in stage 2.0 (TID 202)
[2021-05-15 11:02:16,967] {docker.py:276} INFO - 21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Getting 3 (1036.0 B) non-empty blocks including 3 (1036.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:16,970] {docker.py:276} INFO - 21/05/15 14:02:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515140050778182457935839567_0002_m_000198_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050778182457935839567_0002_m_000198_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515140050778182457935839567_0002}; taskId=attempt_20210515140050778182457935839567_0002_m_000198_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@494b05cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:16,970] {docker.py:276} INFO - 21/05/15 14:02:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:16 INFO StagingCommitter: Starting: Task committer attempt_20210515140050778182457935839567_0002_m_000198_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050778182457935839567_0002_m_000198_202
[2021-05-15 11:02:16,973] {docker.py:276} INFO - 21/05/15 14:02:17 INFO StagingCommitter: Task committer attempt_20210515140050778182457935839567_0002_m_000198_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_20210515140050778182457935839567_0002_m_000198_202 : duration 0:00.003s
[2021-05-15 11:02:18,272] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_20210515140050365464501654351257_0002_m_000195_199: needsTaskCommit() Task attempt_20210515140050365464501654351257_0002_m_000195_199
[2021-05-15 11:02:18,273] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Task committer attempt_20210515140050365464501654351257_0002_m_000195_199: needsTaskCommit() Task attempt_20210515140050365464501654351257_0002_m_000195_199: duration 0:00.000s
21/05/15 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050365464501654351257_0002_m_000195_199
[2021-05-15 11:02:18,275] {docker.py:276} INFO - 21/05/15 14:02:18 INFO Executor: Finished task 195.0 in stage 2.0 (TID 199). 4587 bytes result sent to driver
[2021-05-15 11:02:18,276] {docker.py:276} INFO - 21/05/15 14:02:18 INFO TaskSetManager: Starting task 199.0 in stage 2.0 (TID 203) (8951b5f85146, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:02:18,277] {docker.py:276} INFO - 21/05/15 14:02:18 INFO Executor: Running task 199.0 in stage 2.0 (TID 203)
21/05/15 14:02:18 INFO TaskSetManager: Finished task 195.0 in stage 2.0 (TID 199) in 1655 ms on 8951b5f85146 (executor driver) (196/200)
[2021-05-15 11:02:18,291] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400507067442348448650254_0002_m_000196_200: needsTaskCommit() Task attempt_202105151400507067442348448650254_0002_m_000196_200
[2021-05-15 11:02:18,291] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Task committer attempt_202105151400507067442348448650254_0002_m_000196_200: needsTaskCommit() Task attempt_202105151400507067442348448650254_0002_m_000196_200: duration 0:00.000s
21/05/15 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400507067442348448650254_0002_m_000196_200
[2021-05-15 11:02:18,292] {docker.py:276} INFO - 21/05/15 14:02:18 INFO Executor: Finished task 196.0 in stage 2.0 (TID 200). 4587 bytes result sent to driver
[2021-05-15 11:02:18,295] {docker.py:276} INFO - 21/05/15 14:02:18 INFO TaskSetManager: Finished task 196.0 in stage 2.0 (TID 200) in 1667 ms on 8951b5f85146 (executor driver) (197/200)
[2021-05-15 11:02:18,296] {docker.py:276} INFO - 21/05/15 14:02:18 INFO ShuffleBlockFetcherIterator: Getting 3 (1500.0 B) non-empty blocks including 3 (1500.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:02:18,298] {docker.py:276} INFO - 21/05/15 14:02:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:02:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:02:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:02:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151400502867828161976803239_0002_m_000199_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502867828161976803239_0002_m_000199_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151400502867828161976803239_0002}; taskId=attempt_202105151400502867828161976803239_0002_m_000199_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73a05e5b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/2f1e8adf-ab28-45b9-bba8-f220100f37f5/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:02:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400502867828161976803239_0002_m_000199_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502867828161976803239_0002_m_000199_203
[2021-05-15 11:02:18,302] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Task committer attempt_202105151400502867828161976803239_0002_m_000199_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/2f1e8adf-ab28-45b9-bba8-f220100f37f5/_temporary/0/_temporary/attempt_202105151400502867828161976803239_0002_m_000199_203 : duration 0:00.004s
[2021-05-15 11:02:18,336] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_202105151400503683492598760896049_0002_m_000197_201: needsTaskCommit() Task attempt_202105151400503683492598760896049_0002_m_000197_201
[2021-05-15 11:02:18,337] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Task committer attempt_202105151400503683492598760896049_0002_m_000197_201: needsTaskCommit() Task attempt_202105151400503683492598760896049_0002_m_000197_201: duration 0:00.001s
[2021-05-15 11:02:18,338] {docker.py:276} INFO - 21/05/15 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400503683492598760896049_0002_m_000197_201
[2021-05-15 11:02:18,339] {docker.py:276} INFO - 21/05/15 14:02:18 INFO Executor: Finished task 197.0 in stage 2.0 (TID 201). 4587 bytes result sent to driver
[2021-05-15 11:02:18,341] {docker.py:276} INFO - 21/05/15 14:02:18 INFO TaskSetManager: Finished task 197.0 in stage 2.0 (TID 201) in 1658 ms on 8951b5f85146 (executor driver) (198/200)
[2021-05-15 11:02:18,639] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Starting: Task committer attempt_20210515140050778182457935839567_0002_m_000198_202: needsTaskCommit() Task attempt_20210515140050778182457935839567_0002_m_000198_202
[2021-05-15 11:02:18,640] {docker.py:276} INFO - 21/05/15 14:02:18 INFO StagingCommitter: Task committer attempt_20210515140050778182457935839567_0002_m_000198_202: needsTaskCommit() Task attempt_20210515140050778182457935839567_0002_m_000198_202: duration 0:00.002s
21/05/15 14:02:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515140050778182457935839567_0002_m_000198_202
[2021-05-15 11:02:18,642] {docker.py:276} INFO - 21/05/15 14:02:18 INFO Executor: Finished task 198.0 in stage 2.0 (TID 202). 4587 bytes result sent to driver
[2021-05-15 11:02:18,645] {docker.py:276} INFO - 21/05/15 14:02:18 INFO TaskSetManager: Finished task 198.0 in stage 2.0 (TID 202) in 1691 ms on 8951b5f85146 (executor driver) (199/200)
[2021-05-15 11:02:19,475] {docker.py:276} INFO - 21/05/15 14:02:19 INFO StagingCommitter: Starting: Task committer attempt_202105151400502867828161976803239_0002_m_000199_203: needsTaskCommit() Task attempt_202105151400502867828161976803239_0002_m_000199_203
[2021-05-15 11:02:19,477] {docker.py:276} INFO - 21/05/15 14:02:19 INFO StagingCommitter: Task committer attempt_202105151400502867828161976803239_0002_m_000199_203: needsTaskCommit() Task attempt_202105151400502867828161976803239_0002_m_000199_203: duration 0:00.001s
21/05/15 14:02:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151400502867828161976803239_0002_m_000199_203
[2021-05-15 11:02:19,479] {docker.py:276} INFO - 21/05/15 14:02:19 INFO Executor: Finished task 199.0 in stage 2.0 (TID 203). 4587 bytes result sent to driver
[2021-05-15 11:02:19,488] {docker.py:276} INFO - 21/05/15 14:02:19 INFO TaskSetManager: Finished task 199.0 in stage 2.0 (TID 203) in 1206 ms on 8951b5f85146 (executor driver) (200/200)
21/05/15 14:02:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-15 11:02:19,489] {docker.py:276} INFO - 21/05/15 14:02:19 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 87.208 s
[2021-05-15 11:02:19,489] {docker.py:276} INFO - 21/05/15 14:02:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 14:02:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-15 11:02:19,489] {docker.py:276} INFO - 21/05/15 14:02:19 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 89.544735 s
[2021-05-15 11:02:19,489] {docker.py:276} INFO - 21/05/15 14:02:19 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105151400494724970523776951531_0000_m_000000_0: commitJob((no job ID))
[2021-05-15 11:02:19,504] {docker.py:276} INFO - 21/05/15 14:02:19 WARN AbstractS3ACommitter: Task committer attempt_202105151400494724970523776951531_0000_m_000000_0: No pending uploads to commit
[2021-05-15 11:02:20,000] {docker.py:276} INFO - 21/05/15 14:02:20 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/15 14:02:20 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-15 11:02:20,180] {docker.py:276} INFO - 21/05/15 14:02:20 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.180s
21/05/15 14:02:20 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.181s
[2021-05-15 11:02:20,181] {docker.py:276} INFO - 21/05/15 14:02:20 INFO AbstractS3ACommitter: Task committer attempt_202105151400494724970523776951531_0000_m_000000_0: commitJob((no job ID)): duration 0:00.697s
[2021-05-15 11:02:20,689] {docker.py:276} INFO - 21/05/15 14:02:20 INFO FileFormatWriter: Write Job 2f1e8adf-ab28-45b9-bba8-f220100f37f5 committed.
[2021-05-15 11:02:20,699] {docker.py:276} INFO - 21/05/15 14:02:20 INFO FileFormatWriter: Finished processing stats for write job 2f1e8adf-ab28-45b9-bba8-f220100f37f5.
[2021-05-15 11:02:20,814] {docker.py:276} INFO - 21/05/15 14:02:20 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-15 11:02:20,831] {docker.py:276} INFO - 21/05/15 14:02:20 INFO SparkUI: Stopped Spark web UI at http://8951b5f85146:4040
[2021-05-15 11:02:20,852] {docker.py:276} INFO - 21/05/15 14:02:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-15 11:02:20,869] {docker.py:276} INFO - 21/05/15 14:02:20 INFO MemoryStore: MemoryStore cleared
[2021-05-15 11:02:20,870] {docker.py:276} INFO - 21/05/15 14:02:20 INFO BlockManager: BlockManager stopped
[2021-05-15 11:02:20,873] {docker.py:276} INFO - 21/05/15 14:02:20 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-15 11:02:20,878] {docker.py:276} INFO - 21/05/15 14:02:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-15 11:02:20,897] {docker.py:276} INFO - 21/05/15 14:02:20 INFO SparkContext: Successfully stopped SparkContext
[2021-05-15 11:02:20,897] {docker.py:276} INFO - 21/05/15 14:02:20 INFO ShutdownHookManager: Shutdown hook called
[2021-05-15 11:02:20,899] {docker.py:276} INFO - 21/05/15 14:02:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7dc860ef-d89c-43b6-a4bf-12742a4e34c2
[2021-05-15 11:02:20,902] {docker.py:276} INFO - 21/05/15 14:02:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7dc860ef-d89c-43b6-a4bf-12742a4e34c2/pyspark-210087ff-298e-4ccc-b806-02065607a7ba
[2021-05-15 11:02:20,905] {docker.py:276} INFO - 21/05/15 14:02:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-830c8d5d-995f-45bb-81d8-7ca6d2ac7ea5
[2021-05-15 11:02:20,914] {docker.py:276} INFO - 21/05/15 14:02:20 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-15 11:02:20,915] {docker.py:276} INFO - 21/05/15 14:02:20 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-15 11:02:20,915] {docker.py:276} INFO - 21/05/15 14:02:20 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-15 11:02:21,137] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210515T130000, start_date=20210515T140022, end_date=20210515T140221
[2021-05-15 11:02:21,185] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-15 11:02:21,201] {local_task_job.py:146} INFO - Task exited with return code 0
