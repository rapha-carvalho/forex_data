[2021-05-15 10:01:31,019] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T04:00:00+00:00 [queued]>
[2021-05-15 10:01:31,025] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T04:00:00+00:00 [queued]>
[2021-05-15 10:01:31,025] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 10:01:31,025] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-15 10:01:31,025] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 10:01:31,030] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-15T04:00:00+00:00
[2021-05-15 10:01:31,033] {standard_task_runner.py:52} INFO - Started process 26997 to run task
[2021-05-15 10:01:31,039] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-15T04:00:00+00:00', '--job-id', '643', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmphuaplq19', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpv8sf1r4d']
[2021-05-15 10:01:31,041] {standard_task_runner.py:77} INFO - Job 643: Subtask run_spark_job
[2021-05-15 10:01:31,075] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-15T04:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-15 10:01:31,102] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-15T04:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-15T04:00:00+00:00
[2021-05-15 10:01:31,104] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-15 10:01:33,982] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-15 10:01:33,984] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-15 10:01:33,984] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-15 10:01:33,988] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-15 10:01:35,978] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-15 10:01:36,601] {docker.py:276} INFO - 21/05/15 13:01:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-15 10:01:39,231] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-15 10:01:39,246] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SparkContext: Running Spark version 3.1.1
[2021-05-15 10:01:39,319] {docker.py:276} INFO - 21/05/15 13:01:39 INFO ResourceUtils: ==============================================================
[2021-05-15 10:01:39,320] {docker.py:276} INFO - 21/05/15 13:01:39 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-15 10:01:39,320] {docker.py:276} INFO - 21/05/15 13:01:39 INFO ResourceUtils: ==============================================================
[2021-05-15 10:01:39,321] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SparkContext: Submitted application: spark.py
[2021-05-15 10:01:39,360] {docker.py:276} INFO - 21/05/15 13:01:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-15 10:01:39,377] {docker.py:276} INFO - 21/05/15 13:01:39 INFO ResourceProfile: Limiting resource is cpu
[2021-05-15 10:01:39,378] {docker.py:276} INFO - 21/05/15 13:01:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-15 10:01:39,485] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-15 10:01:39,486] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-15 10:01:39,486] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SecurityManager: Changing view acls groups to:
[2021-05-15 10:01:39,487] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SecurityManager: Changing modify acls groups to: 
21/05/15 13:01:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-15 10:01:39,893] {docker.py:276} INFO - 21/05/15 13:01:39 INFO Utils: Successfully started service 'sparkDriver' on port 35741.
[2021-05-15 10:01:39,936] {docker.py:276} INFO - 21/05/15 13:01:39 INFO SparkEnv: Registering MapOutputTracker
[2021-05-15 10:01:39,990] {docker.py:276} INFO - 21/05/15 13:01:40 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-15 10:01:40,031] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-15 10:01:40,033] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-15 10:01:40,043] {docker.py:276} INFO - 21/05/15 13:01:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-15 10:01:40,067] {docker.py:276} INFO - 21/05/15 13:01:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d967f19-f310-4e0a-8408-36558ee14c5b
[2021-05-15 10:01:40,100] {docker.py:276} INFO - 21/05/15 13:01:40 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-15 10:01:40,127] {docker.py:276} INFO - 21/05/15 13:01:40 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-15 10:01:40,455] {docker.py:276} INFO - 21/05/15 13:01:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-15 10:01:40,554] {docker.py:276} INFO - 21/05/15 13:01:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://4e8a4a26f4b5:4040
[2021-05-15 10:01:40,858] {docker.py:276} INFO - 21/05/15 13:01:40 INFO Executor: Starting executor ID driver on host 4e8a4a26f4b5
[2021-05-15 10:01:40,911] {docker.py:276} INFO - 21/05/15 13:01:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43543.
21/05/15 13:01:40 INFO NettyBlockTransferService: Server created on 4e8a4a26f4b5:43543
[2021-05-15 10:01:40,915] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-15 10:01:40,931] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4e8a4a26f4b5, 43543, None)
[2021-05-15 10:01:40,942] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManagerMasterEndpoint: Registering block manager 4e8a4a26f4b5:43543 with 934.4 MiB RAM, BlockManagerId(driver, 4e8a4a26f4b5, 43543, None)
[2021-05-15 10:01:40,947] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4e8a4a26f4b5, 43543, None)
[2021-05-15 10:01:40,949] {docker.py:276} INFO - 21/05/15 13:01:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4e8a4a26f4b5, 43543, None)
[2021-05-15 10:01:41,704] {docker.py:276} INFO - 21/05/15 13:01:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-15 10:01:41,706] {docker.py:276} INFO - 21/05/15 13:01:41 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-15 10:01:43,056] {docker.py:276} INFO - 21/05/15 13:01:43 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-15 10:01:43,117] {docker.py:276} INFO - 21/05/15 13:01:43 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/15 13:01:43 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-15 10:01:49,235] {docker.py:276} INFO - 21/05/15 13:01:49 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 87 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621029665_to_1621031465.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621031465_to_1621033265.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621033265_to_1621035065.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621035065_to_1621036865.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621036865_to_1621038665.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621038665_to_1621040465.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621040465_to_1621042265.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621042265_to_1621044065.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621044065_to_1621045865.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621045865_to_1621047665.csv.
[2021-05-15 10:01:49,738] {docker.py:276} INFO - 21/05/15 13:01:49 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:01:49,762] {docker.py:276} INFO - 21/05/15 13:01:49 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 87 output partitions
[2021-05-15 10:01:49,762] {docker.py:276} INFO - 21/05/15 13:01:49 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 10:01:49,763] {docker.py:276} INFO - 21/05/15 13:01:49 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 10:01:49,766] {docker.py:276} INFO - 21/05/15 13:01:49 INFO DAGScheduler: Missing parents: List()
[2021-05-15 10:01:49,774] {docker.py:276} INFO - 21/05/15 13:01:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:01:49,867] {docker.py:276} INFO - 21/05/15 13:01:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-15 10:01:49,966] {docker.py:276} INFO - 21/05/15 13:01:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-15 10:01:49,970] {docker.py:276} INFO - 21/05/15 13:01:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4e8a4a26f4b5:43543 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:01:49,978] {docker.py:276} INFO - 21/05/15 13:01:50 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:01:50,010] {docker.py:276} INFO - 21/05/15 13:01:50 INFO DAGScheduler: Submitting 87 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 10:01:50,011] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 87 tasks resource profile 0
[2021-05-15 10:01:50,107] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4e8a4a26f4b5, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,111] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (4e8a4a26f4b5, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,112] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (4e8a4a26f4b5, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,113] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (4e8a4a26f4b5, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,136] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
21/05/15 13:01:50 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-15 10:01:50,137] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-15 10:01:50,138] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-15 10:01:50,536] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-15 10:01:50,542] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (4e8a4a26f4b5, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,544] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-15 10:01:50,549] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 436 ms on 4e8a4a26f4b5 (executor driver) (1/87)
[2021-05-15 10:01:50,729] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1886 bytes result sent to driver
[2021-05-15 10:01:50,730] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (4e8a4a26f4b5, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,732] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-15 10:01:50,733] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 192 ms on 4e8a4a26f4b5 (executor driver) (2/87)
[2021-05-15 10:01:50,929] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-15 10:01:50,932] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (4e8a4a26f4b5, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:50,933] {docker.py:276} INFO - 21/05/15 13:01:50 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-15 10:01:50,933] {docker.py:276} INFO - 21/05/15 13:01:50 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 203 ms on 4e8a4a26f4b5 (executor driver) (3/87)
[2021-05-15 10:01:51,049] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-15 10:01:51,051] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-15 10:01:51,053] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-15 10:01:51,054] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (4e8a4a26f4b5, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,055] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-15 10:01:51,056] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (4e8a4a26f4b5, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,058] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-15 10:01:51,059] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (4e8a4a26f4b5, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,059] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-15 10:01:51,061] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 949 ms on 4e8a4a26f4b5 (executor driver) (4/87)
[2021-05-15 10:01:51,061] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 990 ms on 4e8a4a26f4b5 (executor driver) (5/87)
[2021-05-15 10:01:51,063] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 954 ms on 4e8a4a26f4b5 (executor driver) (6/87)
[2021-05-15 10:01:51,124] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-15 10:01:51,127] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (4e8a4a26f4b5, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,128] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-15 10:01:51,129] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 198 ms on 4e8a4a26f4b5 (executor driver) (7/87)
[2021-05-15 10:01:51,251] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-15 10:01:51,252] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-15 10:01:51,253] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (4e8a4a26f4b5, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,255] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-15 10:01:51,255] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-15 10:01:51,256] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (4e8a4a26f4b5, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,257] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-15 10:01:51,258] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (4e8a4a26f4b5, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,259] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 203 ms on 4e8a4a26f4b5 (executor driver) (8/87)
[2021-05-15 10:01:51,259] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 202 ms on 4e8a4a26f4b5 (executor driver) (9/87)
[2021-05-15 10:01:51,260] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 207 ms on 4e8a4a26f4b5 (executor driver) (10/87)
[2021-05-15 10:01:51,261] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-15 10:01:51,314] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-15 10:01:51,315] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (4e8a4a26f4b5, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,316] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-15 10:01:51,317] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 192 ms on 4e8a4a26f4b5 (executor driver) (11/87)
[2021-05-15 10:01:51,445] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-15 10:01:51,449] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (4e8a4a26f4b5, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,450] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-15 10:01:51,450] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 199 ms on 4e8a4a26f4b5 (executor driver) (12/87)
[2021-05-15 10:01:51,452] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-15 10:01:51,454] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (4e8a4a26f4b5, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,455] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 198 ms on 4e8a4a26f4b5 (executor driver) (13/87)
[2021-05-15 10:01:51,461] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-15 10:01:51,506] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-15 10:01:51,507] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (4e8a4a26f4b5, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,508] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-15 10:01:51,509] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 193 ms on 4e8a4a26f4b5 (executor driver) (14/87)
[2021-05-15 10:01:51,520] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-15 10:01:51,521] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (4e8a4a26f4b5, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,522] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 268 ms on 4e8a4a26f4b5 (executor driver) (15/87)
[2021-05-15 10:01:51,523] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-15 10:01:51,653] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1886 bytes result sent to driver
[2021-05-15 10:01:51,655] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (4e8a4a26f4b5, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,656] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-15 10:01:51,657] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 204 ms on 4e8a4a26f4b5 (executor driver) (16/87)
[2021-05-15 10:01:51,660] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1886 bytes result sent to driver
[2021-05-15 10:01:51,663] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (4e8a4a26f4b5, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,663] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-15 10:01:51,664] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 218 ms on 4e8a4a26f4b5 (executor driver) (17/87)
[2021-05-15 10:01:51,695] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1886 bytes result sent to driver
[2021-05-15 10:01:51,697] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (4e8a4a26f4b5, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,698] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 191 ms on 4e8a4a26f4b5 (executor driver) (18/87)
21/05/15 13:01:51 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-15 10:01:51,703] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-15 10:01:51,704] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (4e8a4a26f4b5, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,705] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 184 ms on 4e8a4a26f4b5 (executor driver) (19/87)
[2021-05-15 10:01:51,707] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-15 10:01:51,843] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-15 10:01:51,846] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
[2021-05-15 10:01:51,848] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (4e8a4a26f4b5, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,850] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-15 10:01:51,851] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (4e8a4a26f4b5, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,852] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-15 10:01:51,853] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 199 ms on 4e8a4a26f4b5 (executor driver) (20/87)
[2021-05-15 10:01:51,854] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 191 ms on 4e8a4a26f4b5 (executor driver) (21/87)
[2021-05-15 10:01:51,883] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-15 10:01:51,884] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-15 10:01:51,885] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (4e8a4a26f4b5, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,887] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-15 10:01:51,888] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (4e8a4a26f4b5, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:51,890] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 186 ms on 4e8a4a26f4b5 (executor driver) (22/87)
[2021-05-15 10:01:51,891] {docker.py:276} INFO - 21/05/15 13:01:51 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 194 ms on 4e8a4a26f4b5 (executor driver) (23/87)
[2021-05-15 10:01:51,893] {docker.py:276} INFO - 21/05/15 13:01:51 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-15 10:01:52,032] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
21/05/15 13:01:52 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-15 10:01:52,034] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (4e8a4a26f4b5, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,035] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-15 10:01:52,037] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (4e8a4a26f4b5, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,038] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 191 ms on 4e8a4a26f4b5 (executor driver) (24/87)
[2021-05-15 10:01:52,039] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-15 10:01:52,039] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 188 ms on 4e8a4a26f4b5 (executor driver) (25/87)
[2021-05-15 10:01:52,073] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-15 10:01:52,075] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
[2021-05-15 10:01:52,076] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (4e8a4a26f4b5, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,078] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-15 10:01:52,079] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (4e8a4a26f4b5, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,080] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 196 ms on 4e8a4a26f4b5 (executor driver) (26/87)
[2021-05-15 10:01:52,080] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 193 ms on 4e8a4a26f4b5 (executor driver) (27/87)
[2021-05-15 10:01:52,081] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-15 10:01:52,219] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1886 bytes result sent to driver
[2021-05-15 10:01:52,221] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (4e8a4a26f4b5, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,222] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 187 ms on 4e8a4a26f4b5 (executor driver) (28/87)
[2021-05-15 10:01:52,223] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-15 10:01:52,259] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1886 bytes result sent to driver
[2021-05-15 10:01:52,262] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (4e8a4a26f4b5, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,263] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-15 10:01:52,264] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 229 ms on 4e8a4a26f4b5 (executor driver) (29/87)
[2021-05-15 10:01:52,265] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1886 bytes result sent to driver
[2021-05-15 10:01:52,268] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1886 bytes result sent to driver
[2021-05-15 10:01:52,268] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (4e8a4a26f4b5, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,270] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-15 10:01:52,271] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (4e8a4a26f4b5, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,272] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 198 ms on 4e8a4a26f4b5 (executor driver) (30/87)
[2021-05-15 10:01:52,273] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 195 ms on 4e8a4a26f4b5 (executor driver) (31/87)
[2021-05-15 10:01:52,273] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-15 10:01:52,405] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-15 10:01:52,407] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (4e8a4a26f4b5, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,409] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-15 10:01:52,409] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 189 ms on 4e8a4a26f4b5 (executor driver) (32/87)
[2021-05-15 10:01:52,455] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-15 10:01:52,457] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (4e8a4a26f4b5, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,458] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-15 10:01:52,458] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-15 10:01:52,459] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (4e8a4a26f4b5, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,460] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-15 10:01:52,461] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-15 10:01:52,462] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 193 ms on 4e8a4a26f4b5 (executor driver) (33/87)
[2021-05-15 10:01:52,464] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 204 ms on 4e8a4a26f4b5 (executor driver) (34/87)
[2021-05-15 10:01:52,466] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (4e8a4a26f4b5, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,468] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
21/05/15 13:01:52 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 198 ms on 4e8a4a26f4b5 (executor driver) (35/87)
[2021-05-15 10:01:52,588] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-15 10:01:52,590] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (4e8a4a26f4b5, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,592] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-15 10:01:52,592] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 186 ms on 4e8a4a26f4b5 (executor driver) (36/87)
[2021-05-15 10:01:52,641] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-15 10:01:52,642] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (4e8a4a26f4b5, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,643] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 187 ms on 4e8a4a26f4b5 (executor driver) (37/87)
[2021-05-15 10:01:52,644] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-15 10:01:52,645] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
[2021-05-15 10:01:52,646] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-15 10:01:52,647] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (4e8a4a26f4b5, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,648] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-15 10:01:52,648] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (4e8a4a26f4b5, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,649] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-15 10:01:52,650] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 184 ms on 4e8a4a26f4b5 (executor driver) (38/87)
[2021-05-15 10:01:52,650] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 191 ms on 4e8a4a26f4b5 (executor driver) (39/87)
[2021-05-15 10:01:52,775] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1843 bytes result sent to driver
[2021-05-15 10:01:52,777] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (4e8a4a26f4b5, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,778] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-15 10:01:52,779] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 190 ms on 4e8a4a26f4b5 (executor driver) (40/87)
[2021-05-15 10:01:52,826] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-15 10:01:52,827] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
[2021-05-15 10:01:52,828] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (4e8a4a26f4b5, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:01:52 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-15 10:01:52,829] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-15 10:01:52,830] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 181 ms on 4e8a4a26f4b5 (executor driver) (41/87)
[2021-05-15 10:01:52,832] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (4e8a4a26f4b5, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,841] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (4e8a4a26f4b5, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,843] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 201 ms on 4e8a4a26f4b5 (executor driver) (42/87)
21/05/15 13:01:52 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-15 10:01:52,843] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 197 ms on 4e8a4a26f4b5 (executor driver) (43/87)
[2021-05-15 10:01:52,849] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-15 10:01:52,954] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1886 bytes result sent to driver
[2021-05-15 10:01:52,956] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (4e8a4a26f4b5, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:52,958] {docker.py:276} INFO - 21/05/15 13:01:52 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-15 10:01:52,958] {docker.py:276} INFO - 21/05/15 13:01:52 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 181 ms on 4e8a4a26f4b5 (executor driver) (44/87)
[2021-05-15 10:01:53,019] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1886 bytes result sent to driver
[2021-05-15 10:01:53,023] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (4e8a4a26f4b5, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,024] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-15 10:01:53,025] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 198 ms on 4e8a4a26f4b5 (executor driver) (45/87)
[2021-05-15 10:01:53,031] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-15 10:01:53,034] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-15 10:01:53,034] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (4e8a4a26f4b5, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,035] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-15 10:01:53,036] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (4e8a4a26f4b5, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,037] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 195 ms on 4e8a4a26f4b5 (executor driver) (46/87)
[2021-05-15 10:01:53,037] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
21/05/15 13:01:53 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 206 ms on 4e8a4a26f4b5 (executor driver) (47/87)
[2021-05-15 10:01:53,136] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-15 10:01:53,139] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (4e8a4a26f4b5, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,140] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 185 ms on 4e8a4a26f4b5 (executor driver) (48/87)
[2021-05-15 10:01:53,141] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-15 10:01:53,199] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-15 10:01:53,200] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (4e8a4a26f4b5, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,202] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-15 10:01:53,202] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 182 ms on 4e8a4a26f4b5 (executor driver) (49/87)
[2021-05-15 10:01:53,210] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-15 10:01:53,211] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (4e8a4a26f4b5, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,212] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-15 10:01:53,213] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 179 ms on 4e8a4a26f4b5 (executor driver) (50/87)
[2021-05-15 10:01:53,214] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-15 10:01:53,215] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (4e8a4a26f4b5, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,216] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-15 10:01:53,216] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 181 ms on 4e8a4a26f4b5 (executor driver) (51/87)
[2021-05-15 10:01:53,317] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-15 10:01:53,318] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (4e8a4a26f4b5, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,320] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 182 ms on 4e8a4a26f4b5 (executor driver) (52/87)
[2021-05-15 10:01:53,320] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-15 10:01:53,380] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-15 10:01:53,381] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (4e8a4a26f4b5, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,382] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 182 ms on 4e8a4a26f4b5 (executor driver) (53/87)
21/05/15 13:01:53 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-15 10:01:53,392] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1843 bytes result sent to driver
21/05/15 13:01:53 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1843 bytes result sent to driver
[2021-05-15 10:01:53,395] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (4e8a4a26f4b5, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:01:53 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (4e8a4a26f4b5, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,397] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 185 ms on 4e8a4a26f4b5 (executor driver) (54/87)
[2021-05-15 10:01:53,399] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
21/05/15 13:01:53 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 183 ms on 4e8a4a26f4b5 (executor driver) (55/87)
21/05/15 13:01:53 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-15 10:01:53,495] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
[2021-05-15 10:01:53,496] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (4e8a4a26f4b5, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,496] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 177 ms on 4e8a4a26f4b5 (executor driver) (56/87)
21/05/15 13:01:53 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-15 10:01:53,579] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1886 bytes result sent to driver
[2021-05-15 10:01:53,581] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1886 bytes result sent to driver
[2021-05-15 10:01:53,582] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1886 bytes result sent to driver
[2021-05-15 10:01:53,582] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (4e8a4a26f4b5, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,583] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 190 ms on 4e8a4a26f4b5 (executor driver) (57/87)
21/05/15 13:01:53 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-15 10:01:53,584] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 205 ms on 4e8a4a26f4b5 (executor driver) (58/87)
[2021-05-15 10:01:53,585] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (4e8a4a26f4b5, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,586] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-15 10:01:53,587] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (4e8a4a26f4b5, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,591] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 197 ms on 4e8a4a26f4b5 (executor driver) (59/87)
[2021-05-15 10:01:53,592] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-15 10:01:53,673] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1886 bytes result sent to driver
[2021-05-15 10:01:53,674] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (4e8a4a26f4b5, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,675] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 182 ms on 4e8a4a26f4b5 (executor driver) (60/87)
[2021-05-15 10:01:53,676] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-15 10:01:53,766] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-15 10:01:53,767] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-15 10:01:53,769] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-15 10:01:53,769] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (4e8a4a26f4b5, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,770] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 189 ms on 4e8a4a26f4b5 (executor driver) (61/87)
21/05/15 13:01:53 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-15 10:01:53,772] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (4e8a4a26f4b5, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,773] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 188 ms on 4e8a4a26f4b5 (executor driver) (62/87)
[2021-05-15 10:01:53,774] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-15 10:01:53,775] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (4e8a4a26f4b5, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,776] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-15 10:01:53,777] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 191 ms on 4e8a4a26f4b5 (executor driver) (63/87)
[2021-05-15 10:01:53,859] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-15 10:01:53,861] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (4e8a4a26f4b5, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,862] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 187 ms on 4e8a4a26f4b5 (executor driver) (64/87)
[2021-05-15 10:01:53,862] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-15 10:01:53,953] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
[2021-05-15 10:01:53,956] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-15 10:01:53,957] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1843 bytes result sent to driver
[2021-05-15 10:01:53,958] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (4e8a4a26f4b5, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,960] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-15 10:01:53,960] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (4e8a4a26f4b5, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,961] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-15 10:01:53,962] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (4e8a4a26f4b5, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:53,963] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 192 ms on 4e8a4a26f4b5 (executor driver) (65/87)
[2021-05-15 10:01:53,964] {docker.py:276} INFO - 21/05/15 13:01:53 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-15 10:01:53,964] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 196 ms on 4e8a4a26f4b5 (executor driver) (66/87)
[2021-05-15 10:01:53,965] {docker.py:276} INFO - 21/05/15 13:01:53 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 191 ms on 4e8a4a26f4b5 (executor driver) (67/87)
[2021-05-15 10:01:54,041] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-15 10:01:54,043] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (4e8a4a26f4b5, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,044] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 185 ms on 4e8a4a26f4b5 (executor driver) (68/87)
[2021-05-15 10:01:54,046] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-15 10:01:54,145] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-15 10:01:54,147] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (4e8a4a26f4b5, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,148] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 185 ms on 4e8a4a26f4b5 (executor driver) (69/87)
[2021-05-15 10:01:54,149] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-15 10:01:54,151] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-15 10:01:54,152] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-15 10:01:54,153] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (4e8a4a26f4b5, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,153] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 196 ms on 4e8a4a26f4b5 (executor driver) (70/87)
[2021-05-15 10:01:54,155] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-15 10:01:54,156] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (4e8a4a26f4b5, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,157] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 198 ms on 4e8a4a26f4b5 (executor driver) (71/87)
[2021-05-15 10:01:54,158] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-15 10:01:54,223] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1886 bytes result sent to driver
[2021-05-15 10:01:54,224] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (4e8a4a26f4b5, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,225] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 183 ms on 4e8a4a26f4b5 (executor driver) (72/87)
[2021-05-15 10:01:54,225] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-15 10:01:54,347] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1886 bytes result sent to driver
[2021-05-15 10:01:54,349] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (4e8a4a26f4b5, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,351] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 201 ms on 4e8a4a26f4b5 (executor driver) (73/87)
[2021-05-15 10:01:54,352] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-15 10:01:54,353] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1886 bytes result sent to driver
[2021-05-15 10:01:54,355] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (4e8a4a26f4b5, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,356] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-15 10:01:54,357] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 211 ms on 4e8a4a26f4b5 (executor driver) (74/87)
[2021-05-15 10:01:54,358] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1886 bytes result sent to driver
[2021-05-15 10:01:54,359] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (4e8a4a26f4b5, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,360] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 206 ms on 4e8a4a26f4b5 (executor driver) (75/87)
[2021-05-15 10:01:54,361] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-15 10:01:54,403] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1843 bytes result sent to driver
[2021-05-15 10:01:54,404] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (4e8a4a26f4b5, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,406] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 182 ms on 4e8a4a26f4b5 (executor driver) (76/87)
21/05/15 13:01:54 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-15 10:01:54,532] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-15 10:01:54,533] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (4e8a4a26f4b5, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,534] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 186 ms on 4e8a4a26f4b5 (executor driver) (77/87)
[2021-05-15 10:01:54,535] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-15 10:01:54,538] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (4e8a4a26f4b5, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,539] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-15 10:01:54,539] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 183 ms on 4e8a4a26f4b5 (executor driver) (78/87)
[2021-05-15 10:01:54,539] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-15 10:01:54,540] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-15 10:01:54,546] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (4e8a4a26f4b5, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:01:54 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 184 ms on 4e8a4a26f4b5 (executor driver) (79/87)
[2021-05-15 10:01:54,548] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-15 10:01:54,582] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-15 10:01:54,584] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (4e8a4a26f4b5, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,587] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 180 ms on 4e8a4a26f4b5 (executor driver) (80/87)
21/05/15 13:01:54 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-15 10:01:54,714] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-15 10:01:54,715] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (4e8a4a26f4b5, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,716] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 184 ms on 4e8a4a26f4b5 (executor driver) (81/87)
[2021-05-15 10:01:54,718] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-15 10:01:54,721] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-15 10:01:54,724] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (4e8a4a26f4b5, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:01:54 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
21/05/15 13:01:54 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (4e8a4a26f4b5, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:54,724] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 189 ms on 4e8a4a26f4b5 (executor driver) (82/87)
[2021-05-15 10:01:54,725] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 184 ms on 4e8a4a26f4b5 (executor driver) (83/87)
[2021-05-15 10:01:54,725] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-15 10:01:54,727] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-15 10:01:54,762] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-15 10:01:54,769] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 186 ms on 4e8a4a26f4b5 (executor driver) (84/87)
[2021-05-15 10:01:54,902] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
[2021-05-15 10:01:54,903] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-15 10:01:54,904] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 180 ms on 4e8a4a26f4b5 (executor driver) (85/87)
[2021-05-15 10:01:54,904] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 182 ms on 4e8a4a26f4b5 (executor driver) (86/87)
[2021-05-15 10:01:54,909] {docker.py:276} INFO - 21/05/15 13:01:54 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-15 10:01:54,910] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 195 ms on 4e8a4a26f4b5 (executor driver) (87/87)
[2021-05-15 10:01:54,913] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-15 10:01:54,914] {docker.py:276} INFO - 21/05/15 13:01:54 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 5.120 s
[2021-05-15 10:01:54,920] {docker.py:276} INFO - 21/05/15 13:01:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 10:01:54,921] {docker.py:276} INFO - 21/05/15 13:01:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-15 10:01:54,925] {docker.py:276} INFO - 21/05/15 13:01:54 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 5.191051 s
[2021-05-15 10:01:54,963] {docker.py:276} INFO - 21/05/15 13:01:54 INFO InMemoryFileIndex: It took 5746 ms to list leaf files for 87 paths.
[2021-05-15 10:01:55,083] {docker.py:276} INFO - 21/05/15 13:01:55 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 87 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621029665_to_1621031465.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621031465_to_1621033265.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621033265_to_1621035065.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621035065_to_1621036865.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621036865_to_1621038665.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621038665_to_1621040465.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621040465_to_1621042265.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621042265_to_1621044065.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621044065_to_1621045865.csv, s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621045865_to_1621047665.csv.
[2021-05-15 10:01:55,120] {docker.py:276} INFO - 21/05/15 13:01:55 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:01:55,123] {docker.py:276} INFO - 21/05/15 13:01:55 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 87 output partitions
21/05/15 13:01:55 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/15 13:01:55 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 10:01:55,124] {docker.py:276} INFO - 21/05/15 13:01:55 INFO DAGScheduler: Missing parents: List()
[2021-05-15 10:01:55,125] {docker.py:276} INFO - 21/05/15 13:01:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:01:55,140] {docker.py:276} INFO - 21/05/15 13:01:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-15 10:01:55,149] {docker.py:276} INFO - 21/05/15 13:01:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-15 10:01:55,150] {docker.py:276} INFO - 21/05/15 13:01:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4e8a4a26f4b5:43543 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-15 10:01:55,153] {docker.py:276} INFO - 21/05/15 13:01:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:01:55,156] {docker.py:276} INFO - 21/05/15 13:01:55 INFO DAGScheduler: Submitting 87 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/15 13:01:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 87 tasks resource profile 0
[2021-05-15 10:01:55,159] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 87) (4e8a4a26f4b5, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,159] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 88) (4e8a4a26f4b5, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,160] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 89) (4e8a4a26f4b5, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,161] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 90) (4e8a4a26f4b5, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,161] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 1.0 in stage 1.0 (TID 88)
[2021-05-15 10:01:55,162] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 3.0 in stage 1.0 (TID 90)
21/05/15 13:01:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 87)
[2021-05-15 10:01:55,163] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 2.0 in stage 1.0 (TID 89)
[2021-05-15 10:01:55,228] {docker.py:276} INFO - 21/05/15 13:01:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4e8a4a26f4b5:43543 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:01:55,338] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 87). 1843 bytes result sent to driver
[2021-05-15 10:01:55,339] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 91) (4e8a4a26f4b5, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,340] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 4.0 in stage 1.0 (TID 91)
[2021-05-15 10:01:55,349] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 3.0 in stage 1.0 (TID 90). 1843 bytes result sent to driver
[2021-05-15 10:01:55,349] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 2.0 in stage 1.0 (TID 89). 1843 bytes result sent to driver
[2021-05-15 10:01:55,350] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 87) in 183 ms on 4e8a4a26f4b5 (executor driver) (1/87)
[2021-05-15 10:01:55,351] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 92) (4e8a4a26f4b5, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,352] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 5.0 in stage 1.0 (TID 92)
[2021-05-15 10:01:55,352] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 93) (4e8a4a26f4b5, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,353] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 6.0 in stage 1.0 (TID 93)
[2021-05-15 10:01:55,353] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 90) in 193 ms on 4e8a4a26f4b5 (executor driver) (2/87)
[2021-05-15 10:01:55,354] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 89) in 194 ms on 4e8a4a26f4b5 (executor driver) (3/87)
[2021-05-15 10:01:55,358] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 1.0 in stage 1.0 (TID 88). 1843 bytes result sent to driver
[2021-05-15 10:01:55,359] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 94) (4e8a4a26f4b5, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,360] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 7.0 in stage 1.0 (TID 94)
[2021-05-15 10:01:55,360] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 88) in 202 ms on 4e8a4a26f4b5 (executor driver) (4/87)
[2021-05-15 10:01:55,515] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 4.0 in stage 1.0 (TID 91). 1843 bytes result sent to driver
[2021-05-15 10:01:55,516] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 95) (4e8a4a26f4b5, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,517] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 8.0 in stage 1.0 (TID 95)
[2021-05-15 10:01:55,517] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 91) in 143 ms on 4e8a4a26f4b5 (executor driver) (5/87)
[2021-05-15 10:01:55,533] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 7.0 in stage 1.0 (TID 94). 1843 bytes result sent to driver
[2021-05-15 10:01:55,533] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 96) (4e8a4a26f4b5, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,535] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 94) in 141 ms on 4e8a4a26f4b5 (executor driver) (6/87)
[2021-05-15 10:01:55,535] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 5.0 in stage 1.0 (TID 92). 1843 bytes result sent to driver
[2021-05-15 10:01:55,536] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 9.0 in stage 1.0 (TID 96)
[2021-05-15 10:01:55,537] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 97) (4e8a4a26f4b5, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,538] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 92) in 151 ms on 4e8a4a26f4b5 (executor driver) (7/87)
[2021-05-15 10:01:55,538] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 10.0 in stage 1.0 (TID 97)
[2021-05-15 10:01:55,539] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 6.0 in stage 1.0 (TID 93). 1843 bytes result sent to driver
[2021-05-15 10:01:55,539] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 98) (4e8a4a26f4b5, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,540] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 93) in 153 ms on 4e8a4a26f4b5 (executor driver) (8/87)
[2021-05-15 10:01:55,551] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 11.0 in stage 1.0 (TID 98)
[2021-05-15 10:01:55,692] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 8.0 in stage 1.0 (TID 95). 1886 bytes result sent to driver
[2021-05-15 10:01:55,694] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 99) (4e8a4a26f4b5, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,695] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 12.0 in stage 1.0 (TID 99)
[2021-05-15 10:01:55,696] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 95) in 180 ms on 4e8a4a26f4b5 (executor driver) (9/87)
[2021-05-15 10:01:55,735] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 9.0 in stage 1.0 (TID 96). 1886 bytes result sent to driver
[2021-05-15 10:01:55,737] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 10.0 in stage 1.0 (TID 97). 1886 bytes result sent to driver
[2021-05-15 10:01:55,738] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 100) (4e8a4a26f4b5, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,739] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 96) in 206 ms on 4e8a4a26f4b5 (executor driver) (10/87)
[2021-05-15 10:01:55,740] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 101) (4e8a4a26f4b5, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,741] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 14.0 in stage 1.0 (TID 101)
[2021-05-15 10:01:55,742] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 13.0 in stage 1.0 (TID 100)
[2021-05-15 10:01:55,742] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 97) in 206 ms on 4e8a4a26f4b5 (executor driver) (11/87)
[2021-05-15 10:01:55,743] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 11.0 in stage 1.0 (TID 98). 1843 bytes result sent to driver
[2021-05-15 10:01:55,744] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 102) (4e8a4a26f4b5, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,745] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 98) in 206 ms on 4e8a4a26f4b5 (executor driver) (12/87)
[2021-05-15 10:01:55,747] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 15.0 in stage 1.0 (TID 102)
[2021-05-15 10:01:55,872] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 12.0 in stage 1.0 (TID 99). 1843 bytes result sent to driver
[2021-05-15 10:01:55,874] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 103) (4e8a4a26f4b5, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,875] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 99) in 181 ms on 4e8a4a26f4b5 (executor driver) (13/87)
[2021-05-15 10:01:55,876] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 16.0 in stage 1.0 (TID 103)
[2021-05-15 10:01:55,918] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 14.0 in stage 1.0 (TID 101). 1843 bytes result sent to driver
[2021-05-15 10:01:55,919] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 15.0 in stage 1.0 (TID 102). 1843 bytes result sent to driver
21/05/15 13:01:55 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 104) (4e8a4a26f4b5, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,920] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Finished task 13.0 in stage 1.0 (TID 100). 1843 bytes result sent to driver
[2021-05-15 10:01:55,921] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 17.0 in stage 1.0 (TID 104)
[2021-05-15 10:01:55,921] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 105) (4e8a4a26f4b5, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,922] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 101) in 183 ms on 4e8a4a26f4b5 (executor driver) (14/87)
21/05/15 13:01:55 INFO Executor: Running task 18.0 in stage 1.0 (TID 105)
[2021-05-15 10:01:55,923] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 100) in 186 ms on 4e8a4a26f4b5 (executor driver) (15/87)
[2021-05-15 10:01:55,924] {docker.py:276} INFO - 21/05/15 13:01:55 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 106) (4e8a4a26f4b5, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:55,925] {docker.py:276} INFO - 21/05/15 13:01:55 INFO Executor: Running task 19.0 in stage 1.0 (TID 106)
21/05/15 13:01:55 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 102) in 182 ms on 4e8a4a26f4b5 (executor driver) (16/87)
[2021-05-15 10:01:56,054] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 16.0 in stage 1.0 (TID 103). 1843 bytes result sent to driver
[2021-05-15 10:01:56,056] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 107) (4e8a4a26f4b5, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,058] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 103) in 183 ms on 4e8a4a26f4b5 (executor driver) (17/87)
[2021-05-15 10:01:56,059] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 20.0 in stage 1.0 (TID 107)
[2021-05-15 10:01:56,100] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 17.0 in stage 1.0 (TID 104). 1843 bytes result sent to driver
[2021-05-15 10:01:56,101] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 19.0 in stage 1.0 (TID 106). 1843 bytes result sent to driver
[2021-05-15 10:01:56,103] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 108) (4e8a4a26f4b5, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,104] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 18.0 in stage 1.0 (TID 105). 1843 bytes result sent to driver
[2021-05-15 10:01:56,105] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 109) (4e8a4a26f4b5, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,106] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 21.0 in stage 1.0 (TID 108)
[2021-05-15 10:01:56,107] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 104) in 187 ms on 4e8a4a26f4b5 (executor driver) (18/87)
21/05/15 13:01:56 INFO Executor: Running task 22.0 in stage 1.0 (TID 109)
[2021-05-15 10:01:56,108] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 110) (4e8a4a26f4b5, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,110] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 106) in 185 ms on 4e8a4a26f4b5 (executor driver) (19/87)
[2021-05-15 10:01:56,117] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 105) in 190 ms on 4e8a4a26f4b5 (executor driver) (20/87)
[2021-05-15 10:01:56,118] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 23.0 in stage 1.0 (TID 110)
[2021-05-15 10:01:56,233] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 20.0 in stage 1.0 (TID 107). 1886 bytes result sent to driver
[2021-05-15 10:01:56,234] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 111) (4e8a4a26f4b5, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,236] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 107) in 181 ms on 4e8a4a26f4b5 (executor driver) (21/87)
21/05/15 13:01:56 INFO Executor: Running task 24.0 in stage 1.0 (TID 111)
[2021-05-15 10:01:56,295] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 23.0 in stage 1.0 (TID 110). 1843 bytes result sent to driver
[2021-05-15 10:01:56,297] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 112) (4e8a4a26f4b5, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,298] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 110) in 189 ms on 4e8a4a26f4b5 (executor driver) (22/87)
[2021-05-15 10:01:56,299] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 22.0 in stage 1.0 (TID 109). 1886 bytes result sent to driver
[2021-05-15 10:01:56,300] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 21.0 in stage 1.0 (TID 108). 1886 bytes result sent to driver
[2021-05-15 10:01:56,301] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 25.0 in stage 1.0 (TID 112)
[2021-05-15 10:01:56,302] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 113) (4e8a4a26f4b5, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,304] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 109) in 199 ms on 4e8a4a26f4b5 (executor driver) (23/87)
[2021-05-15 10:01:56,304] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 108) in 203 ms on 4e8a4a26f4b5 (executor driver) (24/87)
[2021-05-15 10:01:56,307] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 114) (4e8a4a26f4b5, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,308] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 26.0 in stage 1.0 (TID 113)
[2021-05-15 10:01:56,309] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 27.0 in stage 1.0 (TID 114)
[2021-05-15 10:01:56,410] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 24.0 in stage 1.0 (TID 111). 1843 bytes result sent to driver
[2021-05-15 10:01:56,412] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 115) (4e8a4a26f4b5, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,413] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 28.0 in stage 1.0 (TID 115)
[2021-05-15 10:01:56,413] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 111) in 179 ms on 4e8a4a26f4b5 (executor driver) (25/87)
[2021-05-15 10:01:56,478] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 25.0 in stage 1.0 (TID 112). 1843 bytes result sent to driver
[2021-05-15 10:01:56,479] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 112) in 183 ms on 4e8a4a26f4b5 (executor driver) (26/87)
[2021-05-15 10:01:56,480] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 27.0 in stage 1.0 (TID 114). 1843 bytes result sent to driver
[2021-05-15 10:01:56,482] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 116) (4e8a4a26f4b5, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,483] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 26.0 in stage 1.0 (TID 113). 1843 bytes result sent to driver
[2021-05-15 10:01:56,484] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 29.0 in stage 1.0 (TID 116)
[2021-05-15 10:01:56,485] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 117) (4e8a4a26f4b5, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,486] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 114) in 181 ms on 4e8a4a26f4b5 (executor driver) (27/87)
[2021-05-15 10:01:56,487] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 30.0 in stage 1.0 (TID 117)
[2021-05-15 10:01:56,488] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 118) (4e8a4a26f4b5, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,489] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 113) in 188 ms on 4e8a4a26f4b5 (executor driver) (28/87)
[2021-05-15 10:01:56,490] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 31.0 in stage 1.0 (TID 118)
[2021-05-15 10:01:56,585] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 28.0 in stage 1.0 (TID 115). 1843 bytes result sent to driver
[2021-05-15 10:01:56,586] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 119) (4e8a4a26f4b5, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,587] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 115) in 176 ms on 4e8a4a26f4b5 (executor driver) (29/87)
[2021-05-15 10:01:56,588] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 32.0 in stage 1.0 (TID 119)
[2021-05-15 10:01:56,658] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 29.0 in stage 1.0 (TID 116). 1843 bytes result sent to driver
[2021-05-15 10:01:56,659] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 31.0 in stage 1.0 (TID 118). 1843 bytes result sent to driver
[2021-05-15 10:01:56,660] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 120) (4e8a4a26f4b5, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,661] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 33.0 in stage 1.0 (TID 120)
[2021-05-15 10:01:56,662] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 121) (4e8a4a26f4b5, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,663] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 116) in 182 ms on 4e8a4a26f4b5 (executor driver) (30/87)
[2021-05-15 10:01:56,663] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 34.0 in stage 1.0 (TID 121)
[2021-05-15 10:01:56,664] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 118) in 176 ms on 4e8a4a26f4b5 (executor driver) (31/87)
[2021-05-15 10:01:56,673] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 30.0 in stage 1.0 (TID 117). 1886 bytes result sent to driver
[2021-05-15 10:01:56,674] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 122) (4e8a4a26f4b5, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,674] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 117) in 191 ms on 4e8a4a26f4b5 (executor driver) (32/87)
[2021-05-15 10:01:56,675] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 35.0 in stage 1.0 (TID 122)
[2021-05-15 10:01:56,760] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 32.0 in stage 1.0 (TID 119). 1886 bytes result sent to driver
[2021-05-15 10:01:56,761] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 123) (4e8a4a26f4b5, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,762] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 119) in 177 ms on 4e8a4a26f4b5 (executor driver) (33/87)
[2021-05-15 10:01:56,763] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 36.0 in stage 1.0 (TID 123)
[2021-05-15 10:01:56,845] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 35.0 in stage 1.0 (TID 122). 1843 bytes result sent to driver
[2021-05-15 10:01:56,846] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 124) (4e8a4a26f4b5, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,847] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 122) in 174 ms on 4e8a4a26f4b5 (executor driver) (34/87)
[2021-05-15 10:01:56,848] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 37.0 in stage 1.0 (TID 124)
[2021-05-15 10:01:56,849] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 34.0 in stage 1.0 (TID 121). 1886 bytes result sent to driver
[2021-05-15 10:01:56,850] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 33.0 in stage 1.0 (TID 120). 1886 bytes result sent to driver
[2021-05-15 10:01:56,851] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 125) (4e8a4a26f4b5, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,852] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 38.0 in stage 1.0 (TID 125)
[2021-05-15 10:01:56,853] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 126) (4e8a4a26f4b5, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,854] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 39.0 in stage 1.0 (TID 126)
[2021-05-15 10:01:56,855] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 121) in 194 ms on 4e8a4a26f4b5 (executor driver) (35/87)
[2021-05-15 10:01:56,855] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 120) in 197 ms on 4e8a4a26f4b5 (executor driver) (36/87)
[2021-05-15 10:01:56,934] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Finished task 36.0 in stage 1.0 (TID 123). 1843 bytes result sent to driver
[2021-05-15 10:01:56,935] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 123) in 174 ms on 4e8a4a26f4b5 (executor driver) (37/87)
[2021-05-15 10:01:56,937] {docker.py:276} INFO - 21/05/15 13:01:56 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 127) (4e8a4a26f4b5, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:56,939] {docker.py:276} INFO - 21/05/15 13:01:56 INFO Executor: Running task 40.0 in stage 1.0 (TID 127)
[2021-05-15 10:01:57,026] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 37.0 in stage 1.0 (TID 124). 1843 bytes result sent to driver
[2021-05-15 10:01:57,027] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 38.0 in stage 1.0 (TID 125). 1843 bytes result sent to driver
[2021-05-15 10:01:57,029] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 128) (4e8a4a26f4b5, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,029] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 39.0 in stage 1.0 (TID 126). 1843 bytes result sent to driver
21/05/15 13:01:57 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 124) in 184 ms on 4e8a4a26f4b5 (executor driver) (38/87)
[2021-05-15 10:01:57,031] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 125) in 180 ms on 4e8a4a26f4b5 (executor driver) (39/87)
[2021-05-15 10:01:57,031] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 41.0 in stage 1.0 (TID 128)
[2021-05-15 10:01:57,032] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 129) (4e8a4a26f4b5, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,033] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 42.0 in stage 1.0 (TID 129)
[2021-05-15 10:01:57,034] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 130) (4e8a4a26f4b5, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,035] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 126) in 183 ms on 4e8a4a26f4b5 (executor driver) (40/87)
[2021-05-15 10:01:57,035] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 43.0 in stage 1.0 (TID 130)
[2021-05-15 10:01:57,116] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 40.0 in stage 1.0 (TID 127). 1843 bytes result sent to driver
[2021-05-15 10:01:57,118] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 131) (4e8a4a26f4b5, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,119] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 44.0 in stage 1.0 (TID 131)
[2021-05-15 10:01:57,120] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 127) in 183 ms on 4e8a4a26f4b5 (executor driver) (41/87)
[2021-05-15 10:01:57,207] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 41.0 in stage 1.0 (TID 128). 1843 bytes result sent to driver
[2021-05-15 10:01:57,209] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 42.0 in stage 1.0 (TID 129). 1843 bytes result sent to driver
[2021-05-15 10:01:57,210] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 132) (4e8a4a26f4b5, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,211] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 128) in 183 ms on 4e8a4a26f4b5 (executor driver) (42/87)
[2021-05-15 10:01:57,212] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 45.0 in stage 1.0 (TID 132)
[2021-05-15 10:01:57,212] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 43.0 in stage 1.0 (TID 130). 1843 bytes result sent to driver
[2021-05-15 10:01:57,214] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 133) (4e8a4a26f4b5, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,215] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 129) in 184 ms on 4e8a4a26f4b5 (executor driver) (43/87)
[2021-05-15 10:01:57,216] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 46.0 in stage 1.0 (TID 133)
[2021-05-15 10:01:57,217] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 134) (4e8a4a26f4b5, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,218] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 47.0 in stage 1.0 (TID 134)
[2021-05-15 10:01:57,219] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 130) in 185 ms on 4e8a4a26f4b5 (executor driver) (44/87)
[2021-05-15 10:01:57,291] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 44.0 in stage 1.0 (TID 131). 1886 bytes result sent to driver
[2021-05-15 10:01:57,292] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 135) (4e8a4a26f4b5, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,293] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 48.0 in stage 1.0 (TID 135)
[2021-05-15 10:01:57,293] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 131) in 176 ms on 4e8a4a26f4b5 (executor driver) (45/87)
[2021-05-15 10:01:57,387] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 45.0 in stage 1.0 (TID 132). 1886 bytes result sent to driver
[2021-05-15 10:01:57,389] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 136) (4e8a4a26f4b5, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,390] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 132) in 181 ms on 4e8a4a26f4b5 (executor driver) (46/87)
[2021-05-15 10:01:57,391] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 49.0 in stage 1.0 (TID 136)
[2021-05-15 10:01:57,398] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 47.0 in stage 1.0 (TID 134). 1886 bytes result sent to driver
[2021-05-15 10:01:57,399] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 46.0 in stage 1.0 (TID 133). 1886 bytes result sent to driver
21/05/15 13:01:57 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 137) (4e8a4a26f4b5, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,400] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 50.0 in stage 1.0 (TID 137)
[2021-05-15 10:01:57,401] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 138) (4e8a4a26f4b5, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,401] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 134) in 186 ms on 4e8a4a26f4b5 (executor driver) (47/87)
[2021-05-15 10:01:57,402] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 51.0 in stage 1.0 (TID 138)
[2021-05-15 10:01:57,403] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 133) in 190 ms on 4e8a4a26f4b5 (executor driver) (48/87)
[2021-05-15 10:01:57,464] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 48.0 in stage 1.0 (TID 135). 1843 bytes result sent to driver
[2021-05-15 10:01:57,466] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 139) (4e8a4a26f4b5, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,467] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 52.0 in stage 1.0 (TID 139)
21/05/15 13:01:57 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 135) in 176 ms on 4e8a4a26f4b5 (executor driver) (49/87)
[2021-05-15 10:01:57,570] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 49.0 in stage 1.0 (TID 136). 1843 bytes result sent to driver
[2021-05-15 10:01:57,572] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 140) (4e8a4a26f4b5, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,573] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 136) in 185 ms on 4e8a4a26f4b5 (executor driver) (50/87)
[2021-05-15 10:01:57,575] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 53.0 in stage 1.0 (TID 140)
[2021-05-15 10:01:57,576] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 50.0 in stage 1.0 (TID 137). 1843 bytes result sent to driver
[2021-05-15 10:01:57,578] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 51.0 in stage 1.0 (TID 138). 1843 bytes result sent to driver
[2021-05-15 10:01:57,579] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 141) (4e8a4a26f4b5, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,580] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 137) in 181 ms on 4e8a4a26f4b5 (executor driver) (51/87)
21/05/15 13:01:57 INFO Executor: Running task 54.0 in stage 1.0 (TID 141)
[2021-05-15 10:01:57,581] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 138) in 180 ms on 4e8a4a26f4b5 (executor driver) (52/87)
[2021-05-15 10:01:57,582] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 142) (4e8a4a26f4b5, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,584] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 55.0 in stage 1.0 (TID 142)
[2021-05-15 10:01:57,642] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 52.0 in stage 1.0 (TID 139). 1843 bytes result sent to driver
[2021-05-15 10:01:57,643] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 143) (4e8a4a26f4b5, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,644] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 56.0 in stage 1.0 (TID 143)
21/05/15 13:01:57 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 139) in 178 ms on 4e8a4a26f4b5 (executor driver) (53/87)
[2021-05-15 10:01:57,756] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 53.0 in stage 1.0 (TID 140). 1843 bytes result sent to driver
[2021-05-15 10:01:57,758] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 144) (4e8a4a26f4b5, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,759] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 55.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-15 10:01:57,759] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 140) in 189 ms on 4e8a4a26f4b5 (executor driver) (54/87)
[2021-05-15 10:01:57,761] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 57.0 in stage 1.0 (TID 144)
[2021-05-15 10:01:57,762] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 145) (4e8a4a26f4b5, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,763] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 142) in 181 ms on 4e8a4a26f4b5 (executor driver) (55/87)
[2021-05-15 10:01:57,764] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 58.0 in stage 1.0 (TID 145)
[2021-05-15 10:01:57,765] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 54.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-15 10:01:57,766] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 146) (4e8a4a26f4b5, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,767] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 141) in 190 ms on 4e8a4a26f4b5 (executor driver) (56/87)
[2021-05-15 10:01:57,768] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 59.0 in stage 1.0 (TID 146)
[2021-05-15 10:01:57,816] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 56.0 in stage 1.0 (TID 143). 1886 bytes result sent to driver
[2021-05-15 10:01:57,817] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 147) (4e8a4a26f4b5, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,819] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 143) in 176 ms on 4e8a4a26f4b5 (executor driver) (57/87)
21/05/15 13:01:57 INFO Executor: Running task 60.0 in stage 1.0 (TID 147)
[2021-05-15 10:01:57,947] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 58.0 in stage 1.0 (TID 145). 1886 bytes result sent to driver
21/05/15 13:01:57 INFO Executor: Finished task 57.0 in stage 1.0 (TID 144). 1886 bytes result sent to driver
[2021-05-15 10:01:57,949] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 148) (4e8a4a26f4b5, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,950] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 145) in 188 ms on 4e8a4a26f4b5 (executor driver) (58/87)
[2021-05-15 10:01:57,952] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 149) (4e8a4a26f4b5, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,953] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 61.0 in stage 1.0 (TID 148)
[2021-05-15 10:01:57,953] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 59.0 in stage 1.0 (TID 146). 1886 bytes result sent to driver
[2021-05-15 10:01:57,954] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 144) in 195 ms on 4e8a4a26f4b5 (executor driver) (59/87)
[2021-05-15 10:01:57,954] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 62.0 in stage 1.0 (TID 149)
[2021-05-15 10:01:57,956] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 150) (4e8a4a26f4b5, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,957] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 146) in 191 ms on 4e8a4a26f4b5 (executor driver) (60/87)
[2021-05-15 10:01:57,959] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Running task 63.0 in stage 1.0 (TID 150)
[2021-05-15 10:01:57,988] {docker.py:276} INFO - 21/05/15 13:01:57 INFO Executor: Finished task 60.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-15 10:01:57,990] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 151) (4e8a4a26f4b5, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:57,990] {docker.py:276} INFO - 21/05/15 13:01:57 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 147) in 173 ms on 4e8a4a26f4b5 (executor driver) (61/87)
21/05/15 13:01:57 INFO Executor: Running task 64.0 in stage 1.0 (TID 151)
[2021-05-15 10:01:58,132] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 63.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
21/05/15 13:01:58 INFO Executor: Finished task 62.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
[2021-05-15 10:01:58,133] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 61.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-15 10:01:58,134] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 152) (4e8a4a26f4b5, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,136] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 150) in 181 ms on 4e8a4a26f4b5 (executor driver) (62/87)
[2021-05-15 10:01:58,137] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 149) in 185 ms on 4e8a4a26f4b5 (executor driver) (63/87)
[2021-05-15 10:01:58,138] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 153) (4e8a4a26f4b5, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,139] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 66.0 in stage 1.0 (TID 153)
[2021-05-15 10:01:58,140] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 154) (4e8a4a26f4b5, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,141] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 148) in 193 ms on 4e8a4a26f4b5 (executor driver) (64/87)
[2021-05-15 10:01:58,142] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 67.0 in stage 1.0 (TID 154)
[2021-05-15 10:01:58,142] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 65.0 in stage 1.0 (TID 152)
[2021-05-15 10:01:58,160] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 64.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-15 10:01:58,160] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 155) (4e8a4a26f4b5, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,161] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 151) in 172 ms on 4e8a4a26f4b5 (executor driver) (65/87)
[2021-05-15 10:01:58,163] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 68.0 in stage 1.0 (TID 155)
[2021-05-15 10:01:58,317] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 66.0 in stage 1.0 (TID 153). 1843 bytes result sent to driver
[2021-05-15 10:01:58,319] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 156) (4e8a4a26f4b5, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,321] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 153) in 183 ms on 4e8a4a26f4b5 (executor driver) (66/87)
[2021-05-15 10:01:58,322] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 69.0 in stage 1.0 (TID 156)
[2021-05-15 10:01:58,323] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 65.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-15 10:01:58,324] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 157) (4e8a4a26f4b5, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,325] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 152) in 192 ms on 4e8a4a26f4b5 (executor driver) (67/87)
[2021-05-15 10:01:58,326] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 67.0 in stage 1.0 (TID 154). 1843 bytes result sent to driver
[2021-05-15 10:01:58,327] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 70.0 in stage 1.0 (TID 157)
[2021-05-15 10:01:58,328] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 158) (4e8a4a26f4b5, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,329] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 71.0 in stage 1.0 (TID 158)
21/05/15 13:01:58 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 154) in 190 ms on 4e8a4a26f4b5 (executor driver) (68/87)
[2021-05-15 10:01:58,333] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 68.0 in stage 1.0 (TID 155). 1843 bytes result sent to driver
[2021-05-15 10:01:58,334] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 159) (4e8a4a26f4b5, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,335] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 72.0 in stage 1.0 (TID 159)
[2021-05-15 10:01:58,336] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 155) in 175 ms on 4e8a4a26f4b5 (executor driver) (69/87)
[2021-05-15 10:01:58,496] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 69.0 in stage 1.0 (TID 156). 1886 bytes result sent to driver
[2021-05-15 10:01:58,497] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 160) (4e8a4a26f4b5, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,498] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 156) in 180 ms on 4e8a4a26f4b5 (executor driver) (70/87)
[2021-05-15 10:01:58,499] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 73.0 in stage 1.0 (TID 160)
[2021-05-15 10:01:58,501] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 70.0 in stage 1.0 (TID 157). 1886 bytes result sent to driver
[2021-05-15 10:01:58,502] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 71.0 in stage 1.0 (TID 158). 1886 bytes result sent to driver
[2021-05-15 10:01:58,504] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 161) (4e8a4a26f4b5, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,505] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 157) in 181 ms on 4e8a4a26f4b5 (executor driver) (71/87)
[2021-05-15 10:01:58,505] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 74.0 in stage 1.0 (TID 161)
[2021-05-15 10:01:58,506] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 162) (4e8a4a26f4b5, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,507] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 158) in 181 ms on 4e8a4a26f4b5 (executor driver) (72/87)
[2021-05-15 10:01:58,508] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 75.0 in stage 1.0 (TID 162)
[2021-05-15 10:01:58,512] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 72.0 in stage 1.0 (TID 159). 1886 bytes result sent to driver
[2021-05-15 10:01:58,513] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 163) (4e8a4a26f4b5, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,513] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 159) in 180 ms on 4e8a4a26f4b5 (executor driver) (73/87)
[2021-05-15 10:01:58,514] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 76.0 in stage 1.0 (TID 163)
[2021-05-15 10:01:58,680] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 74.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-15 10:01:58,681] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 164) (4e8a4a26f4b5, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,683] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 73.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
[2021-05-15 10:01:58,683] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 161) in 180 ms on 4e8a4a26f4b5 (executor driver) (74/87)
[2021-05-15 10:01:58,685] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 77.0 in stage 1.0 (TID 164)
[2021-05-15 10:01:58,686] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 165) (4e8a4a26f4b5, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,687] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 160) in 190 ms on 4e8a4a26f4b5 (executor driver) (75/87)
[2021-05-15 10:01:58,687] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 78.0 in stage 1.0 (TID 165)
[2021-05-15 10:01:58,689] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 75.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-15 10:01:58,690] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 166) (4e8a4a26f4b5, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,691] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 162) in 185 ms on 4e8a4a26f4b5 (executor driver) (76/87)
[2021-05-15 10:01:58,691] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 79.0 in stage 1.0 (TID 166)
[2021-05-15 10:01:58,693] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 76.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-15 10:01:58,694] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 167) (4e8a4a26f4b5, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,695] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 163) in 183 ms on 4e8a4a26f4b5 (executor driver) (77/87)
[2021-05-15 10:01:58,695] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 80.0 in stage 1.0 (TID 167)
[2021-05-15 10:01:58,867] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 80.0 in stage 1.0 (TID 167). 1843 bytes result sent to driver
21/05/15 13:01:58 INFO Executor: Finished task 79.0 in stage 1.0 (TID 166). 1843 bytes result sent to driver
[2021-05-15 10:01:58,868] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 77.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-15 10:01:58,869] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Finished task 78.0 in stage 1.0 (TID 165). 1843 bytes result sent to driver
[2021-05-15 10:01:58,870] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 168) (4e8a4a26f4b5, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,871] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 81.0 in stage 1.0 (TID 168)
[2021-05-15 10:01:58,872] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 169) (4e8a4a26f4b5, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,872] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 167) in 179 ms on 4e8a4a26f4b5 (executor driver) (78/87)
[2021-05-15 10:01:58,873] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 166) in 183 ms on 4e8a4a26f4b5 (executor driver) (79/87)
[2021-05-15 10:01:58,874] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 82.0 in stage 1.0 (TID 169)
[2021-05-15 10:01:58,875] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 170) (4e8a4a26f4b5, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,877] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 83.0 in stage 1.0 (TID 170)
[2021-05-15 10:01:58,878] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 165) in 193 ms on 4e8a4a26f4b5 (executor driver) (80/87)
[2021-05-15 10:01:58,879] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 171) (4e8a4a26f4b5, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:58,880] {docker.py:276} INFO - 21/05/15 13:01:58 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 164) in 200 ms on 4e8a4a26f4b5 (executor driver) (81/87)
[2021-05-15 10:01:58,881] {docker.py:276} INFO - 21/05/15 13:01:58 INFO Executor: Running task 84.0 in stage 1.0 (TID 171)
[2021-05-15 10:01:59,052] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Finished task 83.0 in stage 1.0 (TID 170). 1843 bytes result sent to driver
[2021-05-15 10:01:59,053] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 172) (4e8a4a26f4b5, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:59,055] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 170) in 180 ms on 4e8a4a26f4b5 (executor driver) (82/87)
[2021-05-15 10:01:59,057] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Finished task 84.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-15 10:01:59,058] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Running task 85.0 in stage 1.0 (TID 172)
[2021-05-15 10:01:59,059] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 173) (4e8a4a26f4b5, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:01:59,060] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 171) in 181 ms on 4e8a4a26f4b5 (executor driver) (83/87)
[2021-05-15 10:01:59,061] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Running task 86.0 in stage 1.0 (TID 173)
[2021-05-15 10:01:59,061] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Finished task 82.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
[2021-05-15 10:01:59,062] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 169) in 191 ms on 4e8a4a26f4b5 (executor driver) (84/87)
[2021-05-15 10:01:59,071] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Finished task 81.0 in stage 1.0 (TID 168). 1886 bytes result sent to driver
[2021-05-15 10:01:59,071] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 168) in 205 ms on 4e8a4a26f4b5 (executor driver) (85/87)
[2021-05-15 10:01:59,241] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Finished task 86.0 in stage 1.0 (TID 173). 1886 bytes result sent to driver
[2021-05-15 10:01:59,243] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 173) in 184 ms on 4e8a4a26f4b5 (executor driver) (86/87)
[2021-05-15 10:01:59,244] {docker.py:276} INFO - 21/05/15 13:01:59 INFO Executor: Finished task 85.0 in stage 1.0 (TID 172). 1886 bytes result sent to driver
[2021-05-15 10:01:59,266] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 172) in 193 ms on 4e8a4a26f4b5 (executor driver) (87/87)
[2021-05-15 10:01:59,266] {docker.py:276} INFO - 21/05/15 13:01:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/05/15 13:01:59 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 4.085 s
[2021-05-15 10:01:59,286] {docker.py:276} INFO - 21/05/15 13:01:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 13:01:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-15 10:01:59,312] {docker.py:276} INFO - 21/05/15 13:01:59 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 4.132068 s
[2021-05-15 10:01:59,312] {docker.py:276} INFO - 21/05/15 13:01:59 INFO InMemoryFileIndex: It took 4184 ms to list leaf files for 87 paths.
[2021-05-15 10:01:59,673] {docker.py:276} INFO - 21/05/15 13:01:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4e8a4a26f4b5:43543 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:02:01,818] {docker.py:276} INFO - 21/05/15 13:02:01 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 10:02:01,824] {docker.py:276} INFO - 21/05/15 13:02:01 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-15 10:02:01,829] {docker.py:276} INFO - 21/05/15 13:02:01 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 10:02:02,383] {docker.py:276} INFO - 21/05/15 13:02:02 INFO CodeGenerator: Code generated in 292.8903 ms
[2021-05-15 10:02:02,398] {docker.py:276} INFO - 21/05/15 13:02:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-15 10:02:02,416] {docker.py:276} INFO - 21/05/15 13:02:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-15 10:02:02,417] {docker.py:276} INFO - 21/05/15 13:02:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4e8a4a26f4b5:43543 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 10:02:02,418] {docker.py:276} INFO - 21/05/15 13:02:02 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:02:02,438] {docker.py:276} INFO - 21/05/15 13:02:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 93551346 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 10:02:02,560] {docker.py:276} INFO - 21/05/15 13:02:02 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:02:02,561] {docker.py:276} INFO - 21/05/15 13:02:02 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/15 13:02:02 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 10:02:02,562] {docker.py:276} INFO - 21/05/15 13:02:02 INFO DAGScheduler: Parents of final stage: List()
21/05/15 13:02:02 INFO DAGScheduler: Missing parents: List()
[2021-05-15 10:02:02,562] {docker.py:276} INFO - 21/05/15 13:02:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:02:02,593] {docker.py:276} INFO - 21/05/15 13:02:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-15 10:02:02,611] {docker.py:276} INFO - 21/05/15 13:02:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-15 10:02:02,612] {docker.py:276} INFO - 21/05/15 13:02:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4e8a4a26f4b5:43543 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 10:02:02,613] {docker.py:276} INFO - 21/05/15 13:02:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:02:02,614] {docker.py:276} INFO - 21/05/15 13:02:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/15 13:02:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-15 10:02:02,618] {docker.py:276} INFO - 21/05/15 13:02:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 174) (4e8a4a26f4b5, executor driver, partition 0, PROCESS_LOCAL, 7215 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:02,619] {docker.py:276} INFO - 21/05/15 13:02:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 174)
[2021-05-15 10:02:02,719] {docker.py:276} INFO - 21/05/15 13:02:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621029665_to_1621031465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:02,750] {docker.py:276} INFO - 21/05/15 13:02:02 INFO CodeGenerator: Code generated in 21.894 ms
[2021-05-15 10:02:03,329] {docker.py:276} INFO - 21/05/15 13:02:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 174). 1564 bytes result sent to driver
[2021-05-15 10:02:03,331] {docker.py:276} INFO - 21/05/15 13:02:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 174) in 716 ms on 4e8a4a26f4b5 (executor driver) (1/1)
21/05/15 13:02:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-15 10:02:03,332] {docker.py:276} INFO - 21/05/15 13:02:03 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.766 s
[2021-05-15 10:02:03,333] {docker.py:276} INFO - 21/05/15 13:02:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 10:02:03,334] {docker.py:276} INFO - 21/05/15 13:02:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-15 10:02:03,334] {docker.py:276} INFO - 21/05/15 13:02:03 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.774400 s
[2021-05-15 10:02:03,370] {docker.py:276} INFO - 21/05/15 13:02:03 INFO CodeGenerator: Code generated in 16.8381 ms
[2021-05-15 10:02:03,453] {docker.py:276} INFO - 21/05/15 13:02:03 INFO FileSourceStrategy: Pushed Filters: 
21/05/15 13:02:03 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 10:02:03,453] {docker.py:276} INFO - 21/05/15 13:02:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 10:02:03,460] {docker.py:276} INFO - 21/05/15 13:02:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-15 10:02:03,478] {docker.py:276} INFO - 21/05/15 13:02:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-15 10:02:03,479] {docker.py:276} INFO - 21/05/15 13:02:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4e8a4a26f4b5:43543 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 10:02:03,485] {docker.py:276} INFO - 21/05/15 13:02:03 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:02:03,488] {docker.py:276} INFO - 21/05/15 13:02:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4e8a4a26f4b5:43543 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-15 10:02:03,493] {docker.py:276} INFO - 21/05/15 13:02:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 93551346 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 10:02:04,160] {docker.py:276} INFO - 21/05/15 13:02:04 INFO FileSourceStrategy: Pushed Filters: 
21/05/15 13:02:04 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 10:02:04,160] {docker.py:276} INFO - 21/05/15 13:02:04 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-15 10:02:05,166] {docker.py:276} INFO - 21/05/15 13:02:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:05,171] {docker.py:276} INFO - 21/05/15 13:02:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:05,172] {docker.py:276} INFO - 21/05/15 13:02:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058523257983960410204_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058523257983960410204_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058523257983960410204_0000}; taskId=attempt_202105151302058523257983960410204_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ba7fb06}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:05,173] {docker.py:276} INFO - 21/05/15 13:02:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:05,218] {docker.py:276} INFO - 21/05/15 13:02:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 10:02:05,324] {docker.py:276} INFO - 21/05/15 13:02:05 INFO CodeGenerator: Code generated in 69.7484 ms
[2021-05-15 10:02:05,327] {docker.py:276} INFO - 21/05/15 13:02:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 10:02:05,381] {docker.py:276} INFO - 21/05/15 13:02:05 INFO CodeGenerator: Code generated in 44.8973 ms
[2021-05-15 10:02:05,385] {docker.py:276} INFO - 21/05/15 13:02:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-15 10:02:05,407] {docker.py:276} INFO - 21/05/15 13:02:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-15 10:02:05,408] {docker.py:276} INFO - 21/05/15 13:02:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4e8a4a26f4b5:43543 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 10:02:05,410] {docker.py:276} INFO - 21/05/15 13:02:05 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:02:05,420] {docker.py:276} INFO - 21/05/15 13:02:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 93551346 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 10:02:05,491] {docker.py:276} INFO - 21/05/15 13:02:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 4e8a4a26f4b5:43543 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 10:02:05,568] {docker.py:276} INFO - 21/05/15 13:02:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:02:05,573] {docker.py:276} INFO - 21/05/15 13:02:05 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-15 10:02:05,577] {docker.py:276} INFO - 21/05/15 13:02:05 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/15 13:02:05 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 10:02:05,577] {docker.py:276} INFO - 21/05/15 13:02:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-15 10:02:05,579] {docker.py:276} INFO - 21/05/15 13:02:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-15 10:02:05,581] {docker.py:276} INFO - 21/05/15 13:02:05 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:02:05,597] {docker.py:276} INFO - 21/05/15 13:02:05 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-15 10:02:05,609] {docker.py:276} INFO - 21/05/15 13:02:05 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-15 10:02:05,610] {docker.py:276} INFO - 21/05/15 13:02:05 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4e8a4a26f4b5:43543 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-15 10:02:05,611] {docker.py:276} INFO - 21/05/15 13:02:05 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:02:05,614] {docker.py:276} INFO - 21/05/15 13:02:05 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21/05/15 13:02:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0
[2021-05-15 10:02:05,617] {docker.py:276} INFO - 21/05/15 13:02:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 175) (4e8a4a26f4b5, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:05,618] {docker.py:276} INFO - 21/05/15 13:02:05 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 176) (4e8a4a26f4b5, executor driver, partition 1, PROCESS_LOCAL, 7204 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:05,619] {docker.py:276} INFO - 21/05/15 13:02:05 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 177) (4e8a4a26f4b5, executor driver, partition 2, PROCESS_LOCAL, 7204 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:05,621] {docker.py:276} INFO - 21/05/15 13:02:05 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 178) (4e8a4a26f4b5, executor driver, partition 3, PROCESS_LOCAL, 7094 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:05,622] {docker.py:276} INFO - 21/05/15 13:02:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 175)
21/05/15 13:02:05 INFO Executor: Running task 2.0 in stage 3.0 (TID 177)
21/05/15 13:02:05 INFO Executor: Running task 1.0 in stage 3.0 (TID 176)
[2021-05-15 10:02:05,624] {docker.py:276} INFO - 21/05/15 13:02:05 INFO Executor: Running task 3.0 in stage 3.0 (TID 178)
[2021-05-15 10:02:05,751] {docker.py:276} INFO - 21/05/15 13:02:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4e8a4a26f4b5:43543 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 10:02:05,761] {docker.py:276} INFO - 21/05/15 13:02:05 INFO CodeGenerator: Code generated in 48.7685 ms
[2021-05-15 10:02:05,811] {docker.py:276} INFO - 21/05/15 13:02:05 INFO CodeGenerator: Code generated in 15.1295 ms
[2021-05-15 10:02:05,852] {docker.py:276} INFO - 21/05/15 13:02:05 INFO CodeGenerator: Code generated in 29.2259 ms
[2021-05-15 10:02:05,876] {docker.py:276} INFO - 21/05/15 13:02:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621069265_to_1621071065.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:05,876] {docker.py:276} INFO - 21/05/15 13:02:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621044065_to_1621045865.csv, range: 0-104506, partition values: [empty row]
21/05/15 13:02:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621056665_to_1621058465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:05,884] {docker.py:276} INFO - 21/05/15 13:02:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621029665_to_1621031465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:06,946] {docker.py:276} INFO - 21/05/15 13:02:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621058465_to_1621060265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:07,608] {docker.py:276} INFO - 21/05/15 13:02:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621060265_to_1621062065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:07,616] {docker.py:276} INFO - 21/05/15 13:02:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621045865_to_1621047665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:07,979] {docker.py:276} INFO - 21/05/15 13:02:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621071065_to_1621072865.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:07,979] {docker.py:276} INFO - 21/05/15 13:02:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621031465_to_1621033265.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:08,181] {docker.py:276} INFO - 21/05/15 13:02:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621062065_to_1621063865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:08,183] {docker.py:276} INFO - 21/05/15 13:02:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621047665_to_1621049465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:08,676] {docker.py:276} INFO - 21/05/15 13:02:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621072865_to_1621074665.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:08,682] {docker.py:276} INFO - 21/05/15 13:02:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621033265_to_1621035065.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:08,702] {docker.py:276} INFO - 21/05/15 13:02:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621063865_to_1621065665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:08,703] {docker.py:276} INFO - 21/05/15 13:02:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621049465_to_1621051265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:09,217] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621074665_to_1621076465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:09,222] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621065665_to_1621067465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:09,222] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621035065_to_1621036865.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:09,225] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621051265_to_1621053065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:09,728] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621036865_to_1621038665.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:09,744] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621067465_to_1621069265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:09,746] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621076465_to_1621078265.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:09,749] {docker.py:276} INFO - 21/05/15 13:02:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621053065_to_1621054865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:10,284] {docker.py:276} INFO - 21/05/15 13:02:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621069265_to_1621071065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:10,292] {docker.py:276} INFO - 21/05/15 13:02:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621078265_to_1621080065.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:10,600] {docker.py:276} INFO - 21/05/15 13:02:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621038665_to_1621040465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:10,788] {docker.py:276} INFO - 21/05/15 13:02:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621080065_to_1621081865.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:10,792] {docker.py:276} INFO - 21/05/15 13:02:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621071065_to_1621072865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:10,828] {docker.py:276} INFO - 21/05/15 13:02:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621054865_to_1621056665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,001] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621040465_to_1621042265.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:11,182] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621029665_to_1621031465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,183] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621056665_to_1621058465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,185] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621072865_to_1621074665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,368] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621042265_to_1621044065.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:11,657] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621058465_to_1621060265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,659] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621031465_to_1621033265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,660] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621074665_to_1621076465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:11,837] {docker.py:276} INFO - 21/05/15 13:02:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621044065_to_1621045865.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:12,050] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621060265_to_1621062065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,052] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621076465_to_1621078265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,054] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621033265_to_1621035065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,221] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621045865_to_1621047665.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:12,396] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621062065_to_1621063865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,405] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621078265_to_1621080065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,434] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621035065_to_1621036865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,703] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621047665_to_1621049465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:12,942] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621063865_to_1621065665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,945] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621036865_to_1621038665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:12,946] {docker.py:276} INFO - 21/05/15 13:02:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621080065_to_1621081865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:13,094] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621049465_to_1621051265.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:13,311] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621038665_to_1621040465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:13,313] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621029665_to_1621031465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:13,459] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621051265_to_1621053065.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:13,664] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621040465_to_1621042265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:13,666] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621031465_to_1621033265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:13,808] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621065665_to_1621067465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:13,811] {docker.py:276} INFO - 21/05/15 13:02:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621053065_to_1621054865.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:14,032] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621042265_to_1621044065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,060] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621033265_to_1621035065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,168] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621054865_to_1621056665.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:14,168] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621067465_to_1621069265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,390] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621044065_to_1621045865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,409] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621035065_to_1621036865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,510] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621056665_to_1621058465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:14,523] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621069265_to_1621071065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,766] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621036865_to_1621038665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,886] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621071065_to_1621072865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:14,887] {docker.py:276} INFO - 21/05/15 13:02:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621058465_to_1621060265.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:15,125] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621038665_to_1621040465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:15,236] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621045865_to_1621047665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:15,261] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621060265_to_1621062065.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:15,472] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621040465_to_1621042265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:15,594] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621047665_to_1621049465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:15,692] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621072865_to_1621074665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:15,730] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621062065_to_1621063865.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:15,826] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621042265_to_1621044065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:15,946] {docker.py:276} INFO - 21/05/15 13:02:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621049465_to_1621051265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:16,067] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621063865_to_1621065665.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:16,186] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621074665_to_1621076465.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:16,371] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621051265_to_1621053065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:16,399] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621065665_to_1621067465.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:16,467] {docker.py:276} INFO - 21/05/15 13:02:16 INFO Executor: Finished task 2.0 in stage 3.0 (TID 177). 2722 bytes result sent to driver
[2021-05-15 10:02:16,469] {docker.py:276} INFO - 21/05/15 13:02:16 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 177) in 10863 ms on 4e8a4a26f4b5 (executor driver) (1/4)
[2021-05-15 10:02:16,541] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621076465_to_1621078265.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:16,728] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621053065_to_1621054865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:16,729] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_01_05/from_1621067465_to_1621069265.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:02:16,911] {docker.py:276} INFO - 21/05/15 13:02:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621078265_to_1621080065.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:17,085] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_01_05/from_1621054865_to_1621056665.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:17,256] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_01_05/from_1621080065_to_1621081865.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:02:17,284] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 175). 2679 bytes result sent to driver
[2021-05-15 10:02:17,285] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 175) in 11683 ms on 4e8a4a26f4b5 (executor driver) (2/4)
[2021-05-15 10:02:17,607] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Finished task 1.0 in stage 3.0 (TID 176). 2679 bytes result sent to driver
[2021-05-15 10:02:17,608] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 176) in 12005 ms on 4e8a4a26f4b5 (executor driver) (3/4)
[2021-05-15 10:02:17,723] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Finished task 3.0 in stage 3.0 (TID 178). 2679 bytes result sent to driver
[2021-05-15 10:02:17,724] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 178) in 12118 ms on 4e8a4a26f4b5 (executor driver) (4/4)
21/05/15 13:02:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-15 10:02:17,724] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 12.151 s
[2021-05-15 10:02:17,725] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: looking for newly runnable stages
[2021-05-15 10:02:17,726] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: running: Set()
[2021-05-15 10:02:17,727] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-15 10:02:17,727] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: failed: Set()
[2021-05-15 10:02:17,732] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:02:17,781] {docker.py:276} INFO - 21/05/15 13:02:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-15 10:02:17,783] {docker.py:276} INFO - 21/05/15 13:02:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-15 10:02:17,784] {docker.py:276} INFO - 21/05/15 13:02:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4e8a4a26f4b5:43543 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-15 10:02:17,785] {docker.py:276} INFO - 21/05/15 13:02:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:02:17,787] {docker.py:276} INFO - 21/05/15 13:02:17 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/15 13:02:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-15 10:02:17,795] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 179) (4e8a4a26f4b5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:17,796] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 180) (4e8a4a26f4b5, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:17,797] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 181) (4e8a4a26f4b5, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:17,797] {docker.py:276} INFO - 21/05/15 13:02:17 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 182) (4e8a4a26f4b5, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:17,797] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 179)
[2021-05-15 10:02:17,799] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Running task 1.0 in stage 4.0 (TID 180)
[2021-05-15 10:02:17,806] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Running task 2.0 in stage 4.0 (TID 181)
[2021-05-15 10:02:17,812] {docker.py:276} INFO - 21/05/15 13:02:17 INFO Executor: Running task 3.0 in stage 4.0 (TID 182)
[2021-05-15 10:02:17,875] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:17,877] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:17,877] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:17,878] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-15 10:02:17,878] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-15 10:02:17,879] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-15 10:02:17,886] {docker.py:276} INFO - 21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2021-05-15 10:02:17,901] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:17,901] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056077750933826810507_0004_m_000000_179, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056077750933826810507_0004_m_000000_179}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056077750933826810507_0004}; taskId=attempt_202105151302056077750933826810507_0004_m_000000_179, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22f192bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:17,906] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:17,906] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:02:17,906] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:17,907] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:17,907] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051513020597654225323366513_0004_m_000002_181, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020597654225323366513_0004_m_000002_181}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051513020597654225323366513_0004}; taskId=attempt_2021051513020597654225323366513_0004_m_000002_181, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f0e2468}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:17,908] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:17,908] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056799643251518256206_0004_m_000003_182, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056799643251518256206_0004_m_000003_182}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056799643251518256206_0004}; taskId=attempt_202105151302056799643251518256206_0004_m_000003_182, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5158c9a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:17,909] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Starting: Task committer attempt_202105151302056077750933826810507_0004_m_000000_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056077750933826810507_0004_m_000000_179
[2021-05-15 10:02:17,910] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:17,911] {docker.py:276} INFO - 21/05/15 13:02:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:17,911] {docker.py:276} INFO - 21/05/15 13:02:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:17,912] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Starting: Task committer attempt_202105151302056799643251518256206_0004_m_000003_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056799643251518256206_0004_m_000003_182
[2021-05-15 10:02:17,912] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Starting: Task committer attempt_2021051513020597654225323366513_0004_m_000002_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020597654225323366513_0004_m_000002_181
[2021-05-15 10:02:17,914] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:17,914] {docker.py:276} INFO - 21/05/15 13:02:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055317960473675345394_0004_m_000001_180, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055317960473675345394_0004_m_000001_180}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055317960473675345394_0004}; taskId=attempt_202105151302055317960473675345394_0004_m_000001_180, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b990ec5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:17,915] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Starting: Task committer attempt_202105151302055317960473675345394_0004_m_000001_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055317960473675345394_0004_m_000001_180
[2021-05-15 10:02:17,939] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Task committer attempt_202105151302056799643251518256206_0004_m_000003_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056799643251518256206_0004_m_000003_182 : duration 0:00.028s
[2021-05-15 10:02:17,953] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Task committer attempt_2021051513020597654225323366513_0004_m_000002_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020597654225323366513_0004_m_000002_181 : duration 0:00.042s
[2021-05-15 10:02:17,954] {docker.py:276} INFO - 21/05/15 13:02:17 INFO StagingCommitter: Task committer attempt_202105151302056077750933826810507_0004_m_000000_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056077750933826810507_0004_m_000000_179 : duration 0:00.047s
[2021-05-15 10:02:17,975] {docker.py:276} INFO - 21/05/15 13:02:18 INFO StagingCommitter: Task committer attempt_202105151302055317960473675345394_0004_m_000001_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055317960473675345394_0004_m_000001_180 : duration 0:00.060s
[2021-05-15 10:02:19,937] {docker.py:276} INFO - 21/05/15 13:02:19 INFO StagingCommitter: Starting: Task committer attempt_202105151302056077750933826810507_0004_m_000000_179: needsTaskCommit() Task attempt_202105151302056077750933826810507_0004_m_000000_179
[2021-05-15 10:02:19,937] {docker.py:276} INFO - 21/05/15 13:02:19 INFO StagingCommitter: Task committer attempt_202105151302056077750933826810507_0004_m_000000_179: needsTaskCommit() Task attempt_202105151302056077750933826810507_0004_m_000000_179: duration 0:00.000s
[2021-05-15 10:02:19,938] {docker.py:276} INFO - 21/05/15 13:02:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056077750933826810507_0004_m_000000_179
[2021-05-15 10:02:19,941] {docker.py:276} INFO - 21/05/15 13:02:19 INFO StagingCommitter: Starting: Task committer attempt_202105151302056799643251518256206_0004_m_000003_182: needsTaskCommit() Task attempt_202105151302056799643251518256206_0004_m_000003_182
[2021-05-15 10:02:19,941] {docker.py:276} INFO - 21/05/15 13:02:19 INFO StagingCommitter: Task committer attempt_202105151302056799643251518256206_0004_m_000003_182: needsTaskCommit() Task attempt_202105151302056799643251518256206_0004_m_000003_182: duration 0:00.000s
[2021-05-15 10:02:19,942] {docker.py:276} INFO - 21/05/15 13:02:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056799643251518256206_0004_m_000003_182
[2021-05-15 10:02:19,946] {docker.py:276} INFO - 21/05/15 13:02:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 179). 4630 bytes result sent to driver
[2021-05-15 10:02:19,947] {docker.py:276} INFO - 21/05/15 13:02:19 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 183) (4e8a4a26f4b5, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:19,948] {docker.py:276} INFO - 21/05/15 13:02:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 179) in 2158 ms on 4e8a4a26f4b5 (executor driver) (1/200)
[2021-05-15 10:02:19,949] {docker.py:276} INFO - 21/05/15 13:02:19 INFO Executor: Finished task 3.0 in stage 4.0 (TID 182). 4630 bytes result sent to driver
[2021-05-15 10:02:19,950] {docker.py:276} INFO - 21/05/15 13:02:19 INFO Executor: Running task 4.0 in stage 4.0 (TID 183)
21/05/15 13:02:19 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 184) (4e8a4a26f4b5, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:19,951] {docker.py:276} INFO - 21/05/15 13:02:19 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 182) in 2158 ms on 4e8a4a26f4b5 (executor driver) (2/200)
[2021-05-15 10:02:19,951] {docker.py:276} INFO - 21/05/15 13:02:19 INFO Executor: Running task 5.0 in stage 4.0 (TID 184)
[2021-05-15 10:02:19,971] {docker.py:276} INFO - 21/05/15 13:02:19 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:19,972] {docker.py:276} INFO - 21/05/15 13:02:20 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:19,972] {docker.py:276} INFO - 21/05/15 13:02:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:19,974] {docker.py:276} INFO - 21/05/15 13:02:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:19,974] {docker.py:276} INFO - 21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054651101096098730040_0004_m_000004_183, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054651101096098730040_0004_m_000004_183}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054651101096098730040_0004}; taskId=attempt_202105151302054651101096098730040_0004_m_000004_183, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@263e9f19}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:19,974] {docker.py:276} INFO - 21/05/15 13:02:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:19,975] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302054651101096098730040_0004_m_000004_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054651101096098730040_0004_m_000004_183
[2021-05-15 10:02:19,975] {docker.py:276} INFO - 21/05/15 13:02:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:02:19,976] {docker.py:276} INFO - 21/05/15 13:02:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:19,976] {docker.py:276} INFO - 21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:19,976] {docker.py:276} INFO - 21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054389658643669391913_0004_m_000005_184, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054389658643669391913_0004_m_000005_184}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054389658643669391913_0004}; taskId=attempt_202105151302054389658643669391913_0004_m_000005_184, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37b4f71c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:19,977] {docker.py:276} INFO - 21/05/15 13:02:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:19,977] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302054389658643669391913_0004_m_000005_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054389658643669391913_0004_m_000005_184
[2021-05-15 10:02:19,983] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Task committer attempt_202105151302054651101096098730040_0004_m_000004_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054651101096098730040_0004_m_000004_183 : duration 0:00.007s
[2021-05-15 10:02:19,996] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Task committer attempt_202105151302054389658643669391913_0004_m_000005_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054389658643669391913_0004_m_000005_184 : duration 0:00.020s
[2021-05-15 10:02:20,077] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302055317960473675345394_0004_m_000001_180: needsTaskCommit() Task attempt_202105151302055317960473675345394_0004_m_000001_180
[2021-05-15 10:02:20,078] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Task committer attempt_202105151302055317960473675345394_0004_m_000001_180: needsTaskCommit() Task attempt_202105151302055317960473675345394_0004_m_000001_180: duration 0:00.000s
21/05/15 13:02:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055317960473675345394_0004_m_000001_180
[2021-05-15 10:02:20,080] {docker.py:276} INFO - 21/05/15 13:02:20 INFO Executor: Finished task 1.0 in stage 4.0 (TID 180). 4587 bytes result sent to driver
[2021-05-15 10:02:20,081] {docker.py:276} INFO - 21/05/15 13:02:20 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 185) (4e8a4a26f4b5, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:20,082] {docker.py:276} INFO - 21/05/15 13:02:20 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 180) in 2289 ms on 4e8a4a26f4b5 (executor driver) (3/200)
[2021-05-15 10:02:20,083] {docker.py:276} INFO - 21/05/15 13:02:20 INFO Executor: Running task 6.0 in stage 4.0 (TID 185)
[2021-05-15 10:02:20,100] {docker.py:276} INFO - 21/05/15 13:02:20 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:20,104] {docker.py:276} INFO - 21/05/15 13:02:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:20,105] {docker.py:276} INFO - 21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056594566712469479097_0004_m_000006_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056594566712469479097_0004_m_000006_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056594566712469479097_0004}; taskId=attempt_202105151302056594566712469479097_0004_m_000006_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48a37529}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302056594566712469479097_0004_m_000006_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056594566712469479097_0004_m_000006_185
[2021-05-15 10:02:20,109] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Task committer attempt_202105151302056594566712469479097_0004_m_000006_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056594566712469479097_0004_m_000006_185 : duration 0:00.005s
[2021-05-15 10:02:20,204] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Starting: Task committer attempt_2021051513020597654225323366513_0004_m_000002_181: needsTaskCommit() Task attempt_2021051513020597654225323366513_0004_m_000002_181
[2021-05-15 10:02:20,205] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Task committer attempt_2021051513020597654225323366513_0004_m_000002_181: needsTaskCommit() Task attempt_2021051513020597654225323366513_0004_m_000002_181: duration 0:00.001s
[2021-05-15 10:02:20,205] {docker.py:276} INFO - 21/05/15 13:02:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051513020597654225323366513_0004_m_000002_181
[2021-05-15 10:02:20,207] {docker.py:276} INFO - 21/05/15 13:02:20 INFO Executor: Finished task 2.0 in stage 4.0 (TID 181). 4587 bytes result sent to driver
[2021-05-15 10:02:20,208] {docker.py:276} INFO - 21/05/15 13:02:20 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 186) (4e8a4a26f4b5, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:20,208] {docker.py:276} INFO - 21/05/15 13:02:20 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 181) in 2415 ms on 4e8a4a26f4b5 (executor driver) (4/200)
[2021-05-15 10:02:20,210] {docker.py:276} INFO - 21/05/15 13:02:20 INFO Executor: Running task 7.0 in stage 4.0 (TID 186)
[2021-05-15 10:02:20,218] {docker.py:276} INFO - 21/05/15 13:02:20 INFO ShuffleBlockFetcherIterator: Getting 4 (12.9 KiB) non-empty blocks including 4 (12.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:20,218] {docker.py:276} INFO - 21/05/15 13:02:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:20,221] {docker.py:276} INFO - 21/05/15 13:02:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:20,221] {docker.py:276} INFO - 21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:20,221] {docker.py:276} INFO - 21/05/15 13:02:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056620797817935297891_0004_m_000007_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056620797817935297891_0004_m_000007_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056620797817935297891_0004}; taskId=attempt_202105151302056620797817935297891_0004_m_000007_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@462e70c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:20,222] {docker.py:276} INFO - 21/05/15 13:02:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:20,222] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302056620797817935297891_0004_m_000007_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056620797817935297891_0004_m_000007_186
[2021-05-15 10:02:20,226] {docker.py:276} INFO - 21/05/15 13:02:20 INFO StagingCommitter: Task committer attempt_202105151302056620797817935297891_0004_m_000007_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056620797817935297891_0004_m_000007_186 : duration 0:00.004s
[2021-05-15 10:02:22,142] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302054389658643669391913_0004_m_000005_184: needsTaskCommit() Task attempt_202105151302054389658643669391913_0004_m_000005_184
[2021-05-15 10:02:22,142] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302054389658643669391913_0004_m_000005_184: needsTaskCommit() Task attempt_202105151302054389658643669391913_0004_m_000005_184: duration 0:00.002s
21/05/15 13:02:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054389658643669391913_0004_m_000005_184
[2021-05-15 10:02:22,143] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Finished task 5.0 in stage 4.0 (TID 184). 4587 bytes result sent to driver
[2021-05-15 10:02:22,144] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 187) (4e8a4a26f4b5, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:22,145] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Running task 8.0 in stage 4.0 (TID 187)
[2021-05-15 10:02:22,146] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 184) in 2198 ms on 4e8a4a26f4b5 (executor driver) (5/200)
[2021-05-15 10:02:22,154] {docker.py:276} INFO - 21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:22,157] {docker.py:276} INFO - 21/05/15 13:02:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:22,157] {docker.py:276} INFO - 21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058004739091432375710_0004_m_000008_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058004739091432375710_0004_m_000008_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058004739091432375710_0004}; taskId=attempt_202105151302058004739091432375710_0004_m_000008_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@442a024c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:22,158] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302058004739091432375710_0004_m_000008_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058004739091432375710_0004_m_000008_187
[2021-05-15 10:02:22,162] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302058004739091432375710_0004_m_000008_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058004739091432375710_0004_m_000008_187 : duration 0:00.004s
[2021-05-15 10:02:22,458] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302054651101096098730040_0004_m_000004_183: needsTaskCommit() Task attempt_202105151302054651101096098730040_0004_m_000004_183
[2021-05-15 10:02:22,460] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302054651101096098730040_0004_m_000004_183: needsTaskCommit() Task attempt_202105151302054651101096098730040_0004_m_000004_183: duration 0:00.004s
21/05/15 13:02:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054651101096098730040_0004_m_000004_183
[2021-05-15 10:02:22,462] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Finished task 4.0 in stage 4.0 (TID 183). 4587 bytes result sent to driver
[2021-05-15 10:02:22,463] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 188) (4e8a4a26f4b5, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:22,464] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 183) in 2520 ms on 4e8a4a26f4b5 (executor driver) (6/200)
[2021-05-15 10:02:22,465] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Running task 9.0 in stage 4.0 (TID 188)
[2021-05-15 10:02:22,476] {docker.py:276} INFO - 21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:22,478] {docker.py:276} INFO - 21/05/15 13:02:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:22,479] {docker.py:276} INFO - 21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057668766470823630536_0004_m_000009_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057668766470823630536_0004_m_000009_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057668766470823630536_0004}; taskId=attempt_202105151302057668766470823630536_0004_m_000009_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64621878}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:22,480] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302057668766470823630536_0004_m_000009_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057668766470823630536_0004_m_000009_188
[2021-05-15 10:02:22,485] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302057668766470823630536_0004_m_000009_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057668766470823630536_0004_m_000009_188 : duration 0:00.006s
[2021-05-15 10:02:22,661] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302056620797817935297891_0004_m_000007_186: needsTaskCommit() Task attempt_202105151302056620797817935297891_0004_m_000007_186
[2021-05-15 10:02:22,661] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302056620797817935297891_0004_m_000007_186: needsTaskCommit() Task attempt_202105151302056620797817935297891_0004_m_000007_186: duration 0:00.001s
21/05/15 13:02:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056620797817935297891_0004_m_000007_186
[2021-05-15 10:02:22,663] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Finished task 7.0 in stage 4.0 (TID 186). 4544 bytes result sent to driver
[2021-05-15 10:02:22,664] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 189) (4e8a4a26f4b5, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:22,665] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 186) in 2460 ms on 4e8a4a26f4b5 (executor driver) (7/200)
[2021-05-15 10:02:22,666] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Running task 10.0 in stage 4.0 (TID 189)
[2021-05-15 10:02:22,670] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302056594566712469479097_0004_m_000006_185: needsTaskCommit() Task attempt_202105151302056594566712469479097_0004_m_000006_185
[2021-05-15 10:02:22,670] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302056594566712469479097_0004_m_000006_185: needsTaskCommit() Task attempt_202105151302056594566712469479097_0004_m_000006_185: duration 0:00.000s
21/05/15 13:02:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056594566712469479097_0004_m_000006_185
[2021-05-15 10:02:22,671] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Finished task 6.0 in stage 4.0 (TID 185). 4587 bytes result sent to driver
[2021-05-15 10:02:22,672] {docker.py:276} INFO - 21/05/15 13:02:22 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 190) (4e8a4a26f4b5, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:22,673] {docker.py:276} INFO - 21/05/15 13:02:22 INFO Executor: Running task 11.0 in stage 4.0 (TID 190)
21/05/15 13:02:22 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 185) in 2595 ms on 4e8a4a26f4b5 (executor driver) (8/200)
[2021-05-15 10:02:22,688] {docker.py:276} INFO - 21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:22,690] {docker.py:276} INFO - 21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:22,693] {docker.py:276} INFO - 21/05/15 13:02:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:22,694] {docker.py:276} INFO - 21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:22,694] {docker.py:276} INFO - 21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058675257357462933652_0004_m_000011_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058675257357462933652_0004_m_000011_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058675257357462933652_0004}; taskId=attempt_202105151302058675257357462933652_0004_m_000011_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fbe7a67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:22,695] {docker.py:276} INFO - 21/05/15 13:02:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:22,695] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302058675257357462933652_0004_m_000011_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058675257357462933652_0004_m_000011_190
[2021-05-15 10:02:22,700] {docker.py:276} INFO - 21/05/15 13:02:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:22,701] {docker.py:276} INFO - 21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:22,701] {docker.py:276} INFO - 21/05/15 13:02:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053484815894373420142_0004_m_000010_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053484815894373420142_0004_m_000010_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053484815894373420142_0004}; taskId=attempt_202105151302053484815894373420142_0004_m_000010_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26f80318}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:22,702] {docker.py:276} INFO - 21/05/15 13:02:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:22 INFO StagingCommitter: Starting: Task committer attempt_202105151302053484815894373420142_0004_m_000010_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053484815894373420142_0004_m_000010_189
[2021-05-15 10:02:22,703] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302058675257357462933652_0004_m_000011_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058675257357462933652_0004_m_000011_190 : duration 0:00.007s
[2021-05-15 10:02:22,710] {docker.py:276} INFO - 21/05/15 13:02:22 INFO StagingCommitter: Task committer attempt_202105151302053484815894373420142_0004_m_000010_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053484815894373420142_0004_m_000010_189 : duration 0:00.008s
[2021-05-15 10:02:24,701] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302058004739091432375710_0004_m_000008_187: needsTaskCommit() Task attempt_202105151302058004739091432375710_0004_m_000008_187
[2021-05-15 10:02:24,702] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Task committer attempt_202105151302058004739091432375710_0004_m_000008_187: needsTaskCommit() Task attempt_202105151302058004739091432375710_0004_m_000008_187: duration 0:00.001s
[2021-05-15 10:02:24,703] {docker.py:276} INFO - 21/05/15 13:02:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058004739091432375710_0004_m_000008_187
[2021-05-15 10:02:24,704] {docker.py:276} INFO - 21/05/15 13:02:24 INFO Executor: Finished task 8.0 in stage 4.0 (TID 187). 4544 bytes result sent to driver
[2021-05-15 10:02:24,705] {docker.py:276} INFO - 21/05/15 13:02:24 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 191) (4e8a4a26f4b5, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:24,706] {docker.py:276} INFO - 21/05/15 13:02:24 INFO Executor: Running task 12.0 in stage 4.0 (TID 191)
[2021-05-15 10:02:24,707] {docker.py:276} INFO - 21/05/15 13:02:24 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 187) in 2565 ms on 4e8a4a26f4b5 (executor driver) (9/200)
[2021-05-15 10:02:24,718] {docker.py:276} INFO - 21/05/15 13:02:24 INFO ShuffleBlockFetcherIterator: Getting 4 (11.3 KiB) non-empty blocks including 4 (11.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:24,719] {docker.py:276} INFO - 21/05/15 13:02:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:02:24,721] {docker.py:276} INFO - 21/05/15 13:02:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:02:24,722] {docker.py:276} INFO - 21/05/15 13:02:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:24,722] {docker.py:276} INFO - 21/05/15 13:02:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:24,723] {docker.py:276} INFO - 21/05/15 13:02:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058584903413714254329_0004_m_000012_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058584903413714254329_0004_m_000012_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058584903413714254329_0004}; taskId=attempt_202105151302058584903413714254329_0004_m_000012_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c56fb3d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:24,723] {docker.py:276} INFO - 21/05/15 13:02:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:24,723] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302058584903413714254329_0004_m_000012_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058584903413714254329_0004_m_000012_191
[2021-05-15 10:02:24,727] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Task committer attempt_202105151302058584903413714254329_0004_m_000012_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058584903413714254329_0004_m_000012_191 : duration 0:00.005s
[2021-05-15 10:02:24,888] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302057668766470823630536_0004_m_000009_188: needsTaskCommit() Task attempt_202105151302057668766470823630536_0004_m_000009_188
[2021-05-15 10:02:24,889] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Task committer attempt_202105151302057668766470823630536_0004_m_000009_188: needsTaskCommit() Task attempt_202105151302057668766470823630536_0004_m_000009_188: duration 0:00.003s
[2021-05-15 10:02:24,889] {docker.py:276} INFO - 21/05/15 13:02:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057668766470823630536_0004_m_000009_188
[2021-05-15 10:02:24,890] {docker.py:276} INFO - 21/05/15 13:02:24 INFO Executor: Finished task 9.0 in stage 4.0 (TID 188). 4544 bytes result sent to driver
[2021-05-15 10:02:24,892] {docker.py:276} INFO - 21/05/15 13:02:24 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 192) (4e8a4a26f4b5, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:24,893] {docker.py:276} INFO - 21/05/15 13:02:24 INFO Executor: Running task 13.0 in stage 4.0 (TID 192)
[2021-05-15 10:02:24,894] {docker.py:276} INFO - 21/05/15 13:02:24 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 188) in 2433 ms on 4e8a4a26f4b5 (executor driver) (10/200)
[2021-05-15 10:02:24,907] {docker.py:276} INFO - 21/05/15 13:02:24 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:24,909] {docker.py:276} INFO - 21/05/15 13:02:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:24,910] {docker.py:276} INFO - 21/05/15 13:02:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:24,910] {docker.py:276} INFO - 21/05/15 13:02:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052893595698274035144_0004_m_000013_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052893595698274035144_0004_m_000013_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052893595698274035144_0004}; taskId=attempt_202105151302052893595698274035144_0004_m_000013_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@273f5274}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:24,911] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302052893595698274035144_0004_m_000013_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052893595698274035144_0004_m_000013_192
[2021-05-15 10:02:24,915] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Task committer attempt_202105151302052893595698274035144_0004_m_000013_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052893595698274035144_0004_m_000013_192 : duration 0:00.005s
[2021-05-15 10:02:24,963] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302053484815894373420142_0004_m_000010_189: needsTaskCommit() Task attempt_202105151302053484815894373420142_0004_m_000010_189
[2021-05-15 10:02:24,963] {docker.py:276} INFO - 21/05/15 13:02:24 INFO StagingCommitter: Task committer attempt_202105151302053484815894373420142_0004_m_000010_189: needsTaskCommit() Task attempt_202105151302053484815894373420142_0004_m_000010_189: duration 0:00.002s
[2021-05-15 10:02:24,964] {docker.py:276} INFO - 21/05/15 13:02:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053484815894373420142_0004_m_000010_189
[2021-05-15 10:02:24,965] {docker.py:276} INFO - 21/05/15 13:02:24 INFO Executor: Finished task 10.0 in stage 4.0 (TID 189). 4544 bytes result sent to driver
[2021-05-15 10:02:24,965] {docker.py:276} INFO - 21/05/15 13:02:25 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 193) (4e8a4a26f4b5, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:24,966] {docker.py:276} INFO - 21/05/15 13:02:25 INFO Executor: Running task 14.0 in stage 4.0 (TID 193)
[2021-05-15 10:02:24,967] {docker.py:276} INFO - 21/05/15 13:02:25 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 189) in 2305 ms on 4e8a4a26f4b5 (executor driver) (11/200)
[2021-05-15 10:02:24,977] {docker.py:276} INFO - 21/05/15 13:02:25 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:24,980] {docker.py:276} INFO - 21/05/15 13:02:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054588165818411252529_0004_m_000014_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054588165818411252529_0004_m_000014_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054588165818411252529_0004}; taskId=attempt_202105151302054588165818411252529_0004_m_000014_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d186d52}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:24,980] {docker.py:276} INFO - 21/05/15 13:02:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105151302054588165818411252529_0004_m_000014_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054588165818411252529_0004_m_000014_193
[2021-05-15 10:02:24,984] {docker.py:276} INFO - 21/05/15 13:02:25 INFO StagingCommitter: Task committer attempt_202105151302054588165818411252529_0004_m_000014_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054588165818411252529_0004_m_000014_193 : duration 0:00.004s
[2021-05-15 10:02:25,150] {docker.py:276} INFO - 21/05/15 13:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105151302058675257357462933652_0004_m_000011_190: needsTaskCommit() Task attempt_202105151302058675257357462933652_0004_m_000011_190
[2021-05-15 10:02:25,151] {docker.py:276} INFO - 21/05/15 13:02:25 INFO StagingCommitter: Task committer attempt_202105151302058675257357462933652_0004_m_000011_190: needsTaskCommit() Task attempt_202105151302058675257357462933652_0004_m_000011_190: duration 0:00.002s
21/05/15 13:02:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058675257357462933652_0004_m_000011_190
[2021-05-15 10:02:25,153] {docker.py:276} INFO - 21/05/15 13:02:25 INFO Executor: Finished task 11.0 in stage 4.0 (TID 190). 4544 bytes result sent to driver
[2021-05-15 10:02:25,155] {docker.py:276} INFO - 21/05/15 13:02:25 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 194) (4e8a4a26f4b5, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:25,156] {docker.py:276} INFO - 21/05/15 13:02:25 INFO Executor: Running task 15.0 in stage 4.0 (TID 194)
[2021-05-15 10:02:25,157] {docker.py:276} INFO - 21/05/15 13:02:25 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 190) in 2488 ms on 4e8a4a26f4b5 (executor driver) (12/200)
[2021-05-15 10:02:25,175] {docker.py:276} INFO - 21/05/15 13:02:25 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:25,177] {docker.py:276} INFO - 21/05/15 13:02:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:25,177] {docker.py:276} INFO - 21/05/15 13:02:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053766924134082208157_0004_m_000015_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053766924134082208157_0004_m_000015_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053766924134082208157_0004}; taskId=attempt_202105151302053766924134082208157_0004_m_000015_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43cfbcec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:25,178] {docker.py:276} INFO - 21/05/15 13:02:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:25 INFO StagingCommitter: Starting: Task committer attempt_202105151302053766924134082208157_0004_m_000015_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053766924134082208157_0004_m_000015_194
[2021-05-15 10:02:25,182] {docker.py:276} INFO - 21/05/15 13:02:25 INFO StagingCommitter: Task committer attempt_202105151302053766924134082208157_0004_m_000015_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053766924134082208157_0004_m_000015_194 : duration 0:00.004s
[2021-05-15 10:02:27,289] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302058584903413714254329_0004_m_000012_191: needsTaskCommit() Task attempt_202105151302058584903413714254329_0004_m_000012_191
[2021-05-15 10:02:27,289] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302058584903413714254329_0004_m_000012_191: needsTaskCommit() Task attempt_202105151302058584903413714254329_0004_m_000012_191: duration 0:00.004s
[2021-05-15 10:02:27,290] {docker.py:276} INFO - 21/05/15 13:02:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058584903413714254329_0004_m_000012_191
[2021-05-15 10:02:27,292] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Finished task 12.0 in stage 4.0 (TID 191). 4587 bytes result sent to driver
[2021-05-15 10:02:27,294] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 195) (4e8a4a26f4b5, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:27,295] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 191) in 2558 ms on 4e8a4a26f4b5 (executor driver) (13/200)
21/05/15 13:02:27 INFO Executor: Running task 16.0 in stage 4.0 (TID 195)
[2021-05-15 10:02:27,300] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302052893595698274035144_0004_m_000013_192: needsTaskCommit() Task attempt_202105151302052893595698274035144_0004_m_000013_192
[2021-05-15 10:02:27,301] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302052893595698274035144_0004_m_000013_192: needsTaskCommit() Task attempt_202105151302052893595698274035144_0004_m_000013_192: duration 0:00.000s
21/05/15 13:02:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052893595698274035144_0004_m_000013_192
[2021-05-15 10:02:27,302] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Finished task 13.0 in stage 4.0 (TID 192). 4587 bytes result sent to driver
[2021-05-15 10:02:27,304] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 196) (4e8a4a26f4b5, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:27,305] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 192) in 2381 ms on 4e8a4a26f4b5 (executor driver) (14/200)
[2021-05-15 10:02:27,305] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Running task 17.0 in stage 4.0 (TID 196)
[2021-05-15 10:02:27,311] {docker.py:276} INFO - 21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:27,313] {docker.py:276} INFO - 21/05/15 13:02:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:27,314] {docker.py:276} INFO - 21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055221104746934622164_0004_m_000016_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055221104746934622164_0004_m_000016_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055221104746934622164_0004}; taskId=attempt_202105151302055221104746934622164_0004_m_000016_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74f2b336}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:27,314] {docker.py:276} INFO - 21/05/15 13:02:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302055221104746934622164_0004_m_000016_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055221104746934622164_0004_m_000016_195
[2021-05-15 10:02:27,315] {docker.py:276} INFO - 21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:27,316] {docker.py:276} INFO - 21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:02:27,319] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302055221104746934622164_0004_m_000016_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055221104746934622164_0004_m_000016_195 : duration 0:00.005s
[2021-05-15 10:02:27,320] {docker.py:276} INFO - 21/05/15 13:02:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:27,320] {docker.py:276} INFO - 21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:27,321] {docker.py:276} INFO - 21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058661601843309085255_0004_m_000017_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058661601843309085255_0004_m_000017_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058661601843309085255_0004}; taskId=attempt_202105151302058661601843309085255_0004_m_000017_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d4df438}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:27,321] {docker.py:276} INFO - 21/05/15 13:02:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:27,322] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302058661601843309085255_0004_m_000017_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058661601843309085255_0004_m_000017_196
[2021-05-15 10:02:27,326] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302058661601843309085255_0004_m_000017_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058661601843309085255_0004_m_000017_196 : duration 0:00.005s
[2021-05-15 10:02:27,472] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302054588165818411252529_0004_m_000014_193: needsTaskCommit() Task attempt_202105151302054588165818411252529_0004_m_000014_193
[2021-05-15 10:02:27,473] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302054588165818411252529_0004_m_000014_193: needsTaskCommit() Task attempt_202105151302054588165818411252529_0004_m_000014_193: duration 0:00.002s
21/05/15 13:02:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054588165818411252529_0004_m_000014_193
[2021-05-15 10:02:27,475] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Finished task 14.0 in stage 4.0 (TID 193). 4587 bytes result sent to driver
[2021-05-15 10:02:27,476] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 197) (4e8a4a26f4b5, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:27,477] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Running task 18.0 in stage 4.0 (TID 197)
[2021-05-15 10:02:27,478] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 193) in 2480 ms on 4e8a4a26f4b5 (executor driver) (15/200)
[2021-05-15 10:02:27,488] {docker.py:276} INFO - 21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:27,488] {docker.py:276} INFO - 21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:27,490] {docker.py:276} INFO - 21/05/15 13:02:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:27,491] {docker.py:276} INFO - 21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:27,491] {docker.py:276} INFO - 21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055891786716797521335_0004_m_000018_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055891786716797521335_0004_m_000018_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055891786716797521335_0004}; taskId=attempt_202105151302055891786716797521335_0004_m_000018_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@220ce95f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:27,491] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302055891786716797521335_0004_m_000018_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055891786716797521335_0004_m_000018_197
[2021-05-15 10:02:27,496] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302055891786716797521335_0004_m_000018_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055891786716797521335_0004_m_000018_197 : duration 0:00.005s
[2021-05-15 10:02:27,705] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302053766924134082208157_0004_m_000015_194: needsTaskCommit() Task attempt_202105151302053766924134082208157_0004_m_000015_194
[2021-05-15 10:02:27,706] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302053766924134082208157_0004_m_000015_194: needsTaskCommit() Task attempt_202105151302053766924134082208157_0004_m_000015_194: duration 0:00.003s
21/05/15 13:02:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053766924134082208157_0004_m_000015_194
[2021-05-15 10:02:27,708] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Finished task 15.0 in stage 4.0 (TID 194). 4587 bytes result sent to driver
[2021-05-15 10:02:27,709] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 198) (4e8a4a26f4b5, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:27,710] {docker.py:276} INFO - 21/05/15 13:02:27 INFO Executor: Running task 19.0 in stage 4.0 (TID 198)
[2021-05-15 10:02:27,711] {docker.py:276} INFO - 21/05/15 13:02:27 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 194) in 2525 ms on 4e8a4a26f4b5 (executor driver) (16/200)
[2021-05-15 10:02:27,721] {docker.py:276} INFO - 21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:27,723] {docker.py:276} INFO - 21/05/15 13:02:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054890837564361118556_0004_m_000019_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054890837564361118556_0004_m_000019_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054890837564361118556_0004}; taskId=attempt_202105151302054890837564361118556_0004_m_000019_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48cf8145}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:27,724] {docker.py:276} INFO - 21/05/15 13:02:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:27,724] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Starting: Task committer attempt_202105151302054890837564361118556_0004_m_000019_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054890837564361118556_0004_m_000019_198
[2021-05-15 10:02:27,729] {docker.py:276} INFO - 21/05/15 13:02:27 INFO StagingCommitter: Task committer attempt_202105151302054890837564361118556_0004_m_000019_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054890837564361118556_0004_m_000019_198 : duration 0:00.005s
[2021-05-15 10:02:29,741] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Starting: Task committer attempt_202105151302058661601843309085255_0004_m_000017_196: needsTaskCommit() Task attempt_202105151302058661601843309085255_0004_m_000017_196
[2021-05-15 10:02:29,742] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Task committer attempt_202105151302058661601843309085255_0004_m_000017_196: needsTaskCommit() Task attempt_202105151302058661601843309085255_0004_m_000017_196: duration 0:00.003s
21/05/15 13:02:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058661601843309085255_0004_m_000017_196
[2021-05-15 10:02:29,746] {docker.py:276} INFO - 21/05/15 13:02:29 INFO Executor: Finished task 17.0 in stage 4.0 (TID 196). 4544 bytes result sent to driver
[2021-05-15 10:02:29,747] {docker.py:276} INFO - 21/05/15 13:02:29 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 199) (4e8a4a26f4b5, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:29,748] {docker.py:276} INFO - 21/05/15 13:02:29 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 196) in 2447 ms on 4e8a4a26f4b5 (executor driver) (17/200)
[2021-05-15 10:02:29,749] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Starting: Task committer attempt_202105151302055221104746934622164_0004_m_000016_195: needsTaskCommit() Task attempt_202105151302055221104746934622164_0004_m_000016_195
[2021-05-15 10:02:29,750] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Task committer attempt_202105151302055221104746934622164_0004_m_000016_195: needsTaskCommit() Task attempt_202105151302055221104746934622164_0004_m_000016_195: duration 0:00.001s
21/05/15 13:02:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055221104746934622164_0004_m_000016_195
[2021-05-15 10:02:29,752] {docker.py:276} INFO - 21/05/15 13:02:29 INFO Executor: Running task 20.0 in stage 4.0 (TID 199)
[2021-05-15 10:02:29,752] {docker.py:276} INFO - 21/05/15 13:02:29 INFO Executor: Finished task 16.0 in stage 4.0 (TID 195). 4544 bytes result sent to driver
[2021-05-15 10:02:29,753] {docker.py:276} INFO - 21/05/15 13:02:29 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 200) (4e8a4a26f4b5, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:29,754] {docker.py:276} INFO - 21/05/15 13:02:29 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 195) in 2464 ms on 4e8a4a26f4b5 (executor driver) (18/200)
21/05/15 13:02:29 INFO Executor: Running task 21.0 in stage 4.0 (TID 200)
[2021-05-15 10:02:29,763] {docker.py:276} INFO - 21/05/15 13:02:29 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:29 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:29,764] {docker.py:276} INFO - 21/05/15 13:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:29,766] {docker.py:276} INFO - 21/05/15 13:02:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:29,766] {docker.py:276} INFO - 21/05/15 13:02:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055196677687891807430_0004_m_000021_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055196677687891807430_0004_m_000021_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055196677687891807430_0004}; taskId=attempt_202105151302055196677687891807430_0004_m_000021_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31727114}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:29,767] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Starting: Task committer attempt_202105151302055196677687891807430_0004_m_000021_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055196677687891807430_0004_m_000021_200
[2021-05-15 10:02:29,770] {docker.py:276} INFO - 21/05/15 13:02:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:29,771] {docker.py:276} INFO - 21/05/15 13:02:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:29,774] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Task committer attempt_202105151302055196677687891807430_0004_m_000021_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055196677687891807430_0004_m_000021_200 : duration 0:00.009s
[2021-05-15 10:02:29,775] {docker.py:276} INFO - 21/05/15 13:02:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056651253127869325047_0004_m_000020_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056651253127869325047_0004_m_000020_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056651253127869325047_0004}; taskId=attempt_202105151302056651253127869325047_0004_m_000020_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57f4c2cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:29,775] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Starting: Task committer attempt_202105151302056651253127869325047_0004_m_000020_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056651253127869325047_0004_m_000020_199
[2021-05-15 10:02:29,782] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Task committer attempt_202105151302056651253127869325047_0004_m_000020_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056651253127869325047_0004_m_000020_199 : duration 0:00.007s
[2021-05-15 10:02:29,859] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Starting: Task committer attempt_202105151302055891786716797521335_0004_m_000018_197: needsTaskCommit() Task attempt_202105151302055891786716797521335_0004_m_000018_197
[2021-05-15 10:02:29,859] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Task committer attempt_202105151302055891786716797521335_0004_m_000018_197: needsTaskCommit() Task attempt_202105151302055891786716797521335_0004_m_000018_197: duration 0:00.001s
21/05/15 13:02:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055891786716797521335_0004_m_000018_197
[2021-05-15 10:02:29,861] {docker.py:276} INFO - 21/05/15 13:02:29 INFO Executor: Finished task 18.0 in stage 4.0 (TID 197). 4544 bytes result sent to driver
[2021-05-15 10:02:29,862] {docker.py:276} INFO - 21/05/15 13:02:29 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 201) (4e8a4a26f4b5, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:29,862] {docker.py:276} INFO - 21/05/15 13:02:29 INFO Executor: Running task 22.0 in stage 4.0 (TID 201)
21/05/15 13:02:29 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 197) in 2390 ms on 4e8a4a26f4b5 (executor driver) (19/200)
[2021-05-15 10:02:29,870] {docker.py:276} INFO - 21/05/15 13:02:29 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:29,871] {docker.py:276} INFO - 21/05/15 13:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:29,873] {docker.py:276} INFO - 21/05/15 13:02:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:29,873] {docker.py:276} INFO - 21/05/15 13:02:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:29,873] {docker.py:276} INFO - 21/05/15 13:02:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055000824175736585977_0004_m_000022_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055000824175736585977_0004_m_000022_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055000824175736585977_0004}; taskId=attempt_202105151302055000824175736585977_0004_m_000022_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56b5989c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:29,874] {docker.py:276} INFO - 21/05/15 13:02:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:29,874] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Starting: Task committer attempt_202105151302055000824175736585977_0004_m_000022_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055000824175736585977_0004_m_000022_201
[2021-05-15 10:02:29,882] {docker.py:276} INFO - 21/05/15 13:02:29 INFO StagingCommitter: Task committer attempt_202105151302055000824175736585977_0004_m_000022_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055000824175736585977_0004_m_000022_201 : duration 0:00.009s
[2021-05-15 10:02:30,252] {docker.py:276} INFO - 21/05/15 13:02:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302054890837564361118556_0004_m_000019_198: needsTaskCommit() Task attempt_202105151302054890837564361118556_0004_m_000019_198
[2021-05-15 10:02:30,253] {docker.py:276} INFO - 21/05/15 13:02:30 INFO StagingCommitter: Task committer attempt_202105151302054890837564361118556_0004_m_000019_198: needsTaskCommit() Task attempt_202105151302054890837564361118556_0004_m_000019_198: duration 0:00.002s
[2021-05-15 10:02:30,253] {docker.py:276} INFO - 21/05/15 13:02:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054890837564361118556_0004_m_000019_198
[2021-05-15 10:02:30,255] {docker.py:276} INFO - 21/05/15 13:02:30 INFO Executor: Finished task 19.0 in stage 4.0 (TID 198). 4544 bytes result sent to driver
[2021-05-15 10:02:30,256] {docker.py:276} INFO - 21/05/15 13:02:30 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 202) (4e8a4a26f4b5, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:30,257] {docker.py:276} INFO - 21/05/15 13:02:30 INFO Executor: Running task 23.0 in stage 4.0 (TID 202)
[2021-05-15 10:02:30,257] {docker.py:276} INFO - 21/05/15 13:02:30 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 198) in 2552 ms on 4e8a4a26f4b5 (executor driver) (20/200)
[2021-05-15 10:02:30,265] {docker.py:276} INFO - 21/05/15 13:02:30 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:30,266] {docker.py:276} INFO - 21/05/15 13:02:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:30,267] {docker.py:276} INFO - 21/05/15 13:02:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:30,267] {docker.py:276} INFO - 21/05/15 13:02:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058306221594230453790_0004_m_000023_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058306221594230453790_0004_m_000023_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058306221594230453790_0004}; taskId=attempt_202105151302058306221594230453790_0004_m_000023_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19600a2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:30,267] {docker.py:276} INFO - 21/05/15 13:02:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302058306221594230453790_0004_m_000023_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058306221594230453790_0004_m_000023_202
[2021-05-15 10:02:30,270] {docker.py:276} INFO - 21/05/15 13:02:30 INFO StagingCommitter: Task committer attempt_202105151302058306221594230453790_0004_m_000023_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058306221594230453790_0004_m_000023_202 : duration 0:00.003s
[2021-05-15 10:02:32,272] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105151302055000824175736585977_0004_m_000022_201: needsTaskCommit() Task attempt_202105151302055000824175736585977_0004_m_000022_201
[2021-05-15 10:02:32,272] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_202105151302055000824175736585977_0004_m_000022_201: needsTaskCommit() Task attempt_202105151302055000824175736585977_0004_m_000022_201: duration 0:00.001s
[2021-05-15 10:02:32,272] {docker.py:276} INFO - 21/05/15 13:02:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055000824175736585977_0004_m_000022_201
[2021-05-15 10:02:32,273] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Finished task 22.0 in stage 4.0 (TID 201). 4587 bytes result sent to driver
[2021-05-15 10:02:32,274] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 203) (4e8a4a26f4b5, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:32,275] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 201) in 2416 ms on 4e8a4a26f4b5 (executor driver) (21/200)
[2021-05-15 10:02:32,275] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Running task 24.0 in stage 4.0 (TID 203)
[2021-05-15 10:02:32,283] {docker.py:276} INFO - 21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:32,283] {docker.py:276} INFO - 21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:32,285] {docker.py:276} INFO - 21/05/15 13:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:32,286] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:32,286] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055985335504423418594_0004_m_000024_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055985335504423418594_0004_m_000024_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055985335504423418594_0004}; taskId=attempt_202105151302055985335504423418594_0004_m_000024_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b3320d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:32,286] {docker.py:276} INFO - 21/05/15 13:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:32,287] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105151302055985335504423418594_0004_m_000024_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055985335504423418594_0004_m_000024_203
[2021-05-15 10:02:32,290] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_202105151302055985335504423418594_0004_m_000024_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055985335504423418594_0004_m_000024_203 : duration 0:00.004s
[2021-05-15 10:02:32,298] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105151302055196677687891807430_0004_m_000021_200: needsTaskCommit() Task attempt_202105151302055196677687891807430_0004_m_000021_200
[2021-05-15 10:02:32,299] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_202105151302055196677687891807430_0004_m_000021_200: needsTaskCommit() Task attempt_202105151302055196677687891807430_0004_m_000021_200: duration 0:00.002s
[2021-05-15 10:02:32,300] {docker.py:276} INFO - 21/05/15 13:02:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055196677687891807430_0004_m_000021_200
[2021-05-15 10:02:32,300] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105151302056651253127869325047_0004_m_000020_199: needsTaskCommit() Task attempt_202105151302056651253127869325047_0004_m_000020_199
[2021-05-15 10:02:32,301] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_202105151302056651253127869325047_0004_m_000020_199: needsTaskCommit() Task attempt_202105151302056651253127869325047_0004_m_000020_199: duration 0:00.000s
21/05/15 13:02:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056651253127869325047_0004_m_000020_199
21/05/15 13:02:32 INFO Executor: Finished task 21.0 in stage 4.0 (TID 200). 4587 bytes result sent to driver
[2021-05-15 10:02:32,301] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Finished task 20.0 in stage 4.0 (TID 199). 4587 bytes result sent to driver
[2021-05-15 10:02:32,302] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 204) (4e8a4a26f4b5, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:32,302] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Running task 25.0 in stage 4.0 (TID 204)
[2021-05-15 10:02:32,303] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 205) (4e8a4a26f4b5, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:32,303] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Running task 26.0 in stage 4.0 (TID 205)
[2021-05-15 10:02:32,304] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 200) in 2554 ms on 4e8a4a26f4b5 (executor driver) (22/200)
[2021-05-15 10:02:32,304] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 199) in 2560 ms on 4e8a4a26f4b5 (executor driver) (23/200)
[2021-05-15 10:02:32,315] {docker.py:276} INFO - 21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:32,316] {docker.py:276} INFO - 21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:32,317] {docker.py:276} INFO - 21/05/15 13:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:32,318] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:32,318] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205550514180741038323_0004_m_000025_204, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205550514180741038323_0004_m_000025_204}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205550514180741038323_0004}; taskId=attempt_20210515130205550514180741038323_0004_m_000025_204, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29351a10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:32,319] {docker.py:276} INFO - 21/05/15 13:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:32,320] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_20210515130205550514180741038323_0004_m_000025_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205550514180741038323_0004_m_000025_204
[2021-05-15 10:02:32,320] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:32,320] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056968337963240104253_0004_m_000026_205, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056968337963240104253_0004_m_000026_205}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056968337963240104253_0004}; taskId=attempt_202105151302056968337963240104253_0004_m_000026_205, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9ebbbab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:32,320] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105151302056968337963240104253_0004_m_000026_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056968337963240104253_0004_m_000026_205
[2021-05-15 10:02:32,323] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_202105151302056968337963240104253_0004_m_000026_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056968337963240104253_0004_m_000026_205 : duration 0:00.003s
[2021-05-15 10:02:32,325] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_20210515130205550514180741038323_0004_m_000025_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205550514180741038323_0004_m_000025_204 : duration 0:00.006s
[2021-05-15 10:02:32,771] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_202105151302058306221594230453790_0004_m_000023_202: needsTaskCommit() Task attempt_202105151302058306221594230453790_0004_m_000023_202
[2021-05-15 10:02:32,772] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_202105151302058306221594230453790_0004_m_000023_202: needsTaskCommit() Task attempt_202105151302058306221594230453790_0004_m_000023_202: duration 0:00.001s
21/05/15 13:02:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058306221594230453790_0004_m_000023_202
[2021-05-15 10:02:32,773] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Finished task 23.0 in stage 4.0 (TID 202). 4587 bytes result sent to driver
[2021-05-15 10:02:32,775] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 206) (4e8a4a26f4b5, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:32,776] {docker.py:276} INFO - 21/05/15 13:02:32 INFO Executor: Running task 27.0 in stage 4.0 (TID 206)
[2021-05-15 10:02:32,776] {docker.py:276} INFO - 21/05/15 13:02:32 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 202) in 2522 ms on 4e8a4a26f4b5 (executor driver) (24/200)
[2021-05-15 10:02:32,787] {docker.py:276} INFO - 21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:32,789] {docker.py:276} INFO - 21/05/15 13:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:32,790] {docker.py:276} INFO - 21/05/15 13:02:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205977350713224635951_0004_m_000027_206, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205977350713224635951_0004_m_000027_206}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205977350713224635951_0004}; taskId=attempt_20210515130205977350713224635951_0004_m_000027_206, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4209d5fa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:32 INFO StagingCommitter: Starting: Task committer attempt_20210515130205977350713224635951_0004_m_000027_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205977350713224635951_0004_m_000027_206
[2021-05-15 10:02:32,793] {docker.py:276} INFO - 21/05/15 13:02:32 INFO StagingCommitter: Task committer attempt_20210515130205977350713224635951_0004_m_000027_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205977350713224635951_0004_m_000027_206 : duration 0:00.003s
[2021-05-15 10:02:34,635] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Starting: Task committer attempt_20210515130205550514180741038323_0004_m_000025_204: needsTaskCommit() Task attempt_20210515130205550514180741038323_0004_m_000025_204
[2021-05-15 10:02:34,636] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Task committer attempt_20210515130205550514180741038323_0004_m_000025_204: needsTaskCommit() Task attempt_20210515130205550514180741038323_0004_m_000025_204: duration 0:00.002s
[2021-05-15 10:02:34,637] {docker.py:276} INFO - 21/05/15 13:02:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205550514180741038323_0004_m_000025_204
[2021-05-15 10:02:34,638] {docker.py:276} INFO - 21/05/15 13:02:34 INFO Executor: Finished task 25.0 in stage 4.0 (TID 204). 4544 bytes result sent to driver
[2021-05-15 10:02:34,639] {docker.py:276} INFO - 21/05/15 13:02:34 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 207) (4e8a4a26f4b5, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:34,640] {docker.py:276} INFO - 21/05/15 13:02:34 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 204) in 2342 ms on 4e8a4a26f4b5 (executor driver) (25/200)
[2021-05-15 10:02:34,641] {docker.py:276} INFO - 21/05/15 13:02:34 INFO Executor: Running task 28.0 in stage 4.0 (TID 207)
[2021-05-15 10:02:34,658] {docker.py:276} INFO - 21/05/15 13:02:34 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:34,658] {docker.py:276} INFO - 21/05/15 13:02:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:34,660] {docker.py:276} INFO - 21/05/15 13:02:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:34,660] {docker.py:276} INFO - 21/05/15 13:02:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:34,661] {docker.py:276} INFO - 21/05/15 13:02:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054831774172258931751_0004_m_000028_207, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054831774172258931751_0004_m_000028_207}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054831774172258931751_0004}; taskId=attempt_202105151302054831774172258931751_0004_m_000028_207, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42a8ebec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:34,661] {docker.py:276} INFO - 21/05/15 13:02:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:34,661] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302054831774172258931751_0004_m_000028_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054831774172258931751_0004_m_000028_207
[2021-05-15 10:02:34,664] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Task committer attempt_202105151302054831774172258931751_0004_m_000028_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054831774172258931751_0004_m_000028_207 : duration 0:00.003s
[2021-05-15 10:02:34,707] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302056968337963240104253_0004_m_000026_205: needsTaskCommit() Task attempt_202105151302056968337963240104253_0004_m_000026_205
[2021-05-15 10:02:34,708] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Task committer attempt_202105151302056968337963240104253_0004_m_000026_205: needsTaskCommit() Task attempt_202105151302056968337963240104253_0004_m_000026_205: duration 0:00.001s
21/05/15 13:02:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056968337963240104253_0004_m_000026_205
[2021-05-15 10:02:34,709] {docker.py:276} INFO - 21/05/15 13:02:34 INFO Executor: Finished task 26.0 in stage 4.0 (TID 205). 4544 bytes result sent to driver
[2021-05-15 10:02:34,709] {docker.py:276} INFO - 21/05/15 13:02:34 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 208) (4e8a4a26f4b5, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:34,710] {docker.py:276} INFO - 21/05/15 13:02:34 INFO Executor: Running task 29.0 in stage 4.0 (TID 208)
[2021-05-15 10:02:34,711] {docker.py:276} INFO - 21/05/15 13:02:34 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 205) in 2412 ms on 4e8a4a26f4b5 (executor driver) (26/200)
[2021-05-15 10:02:34,718] {docker.py:276} INFO - 21/05/15 13:02:34 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:34,721] {docker.py:276} INFO - 21/05/15 13:02:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059019821112920716400_0004_m_000029_208, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059019821112920716400_0004_m_000029_208}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059019821112920716400_0004}; taskId=attempt_202105151302059019821112920716400_0004_m_000029_208, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58af8e1e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:34,721] {docker.py:276} INFO - 21/05/15 13:02:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302059019821112920716400_0004_m_000029_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059019821112920716400_0004_m_000029_208
[2021-05-15 10:02:34,724] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Task committer attempt_202105151302059019821112920716400_0004_m_000029_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059019821112920716400_0004_m_000029_208 : duration 0:00.003s
[2021-05-15 10:02:34,807] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302055985335504423418594_0004_m_000024_203: needsTaskCommit() Task attempt_202105151302055985335504423418594_0004_m_000024_203
[2021-05-15 10:02:34,807] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Task committer attempt_202105151302055985335504423418594_0004_m_000024_203: needsTaskCommit() Task attempt_202105151302055985335504423418594_0004_m_000024_203: duration 0:00.002s
21/05/15 13:02:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055985335504423418594_0004_m_000024_203
[2021-05-15 10:02:34,808] {docker.py:276} INFO - 21/05/15 13:02:34 INFO Executor: Finished task 24.0 in stage 4.0 (TID 203). 4544 bytes result sent to driver
[2021-05-15 10:02:34,809] {docker.py:276} INFO - 21/05/15 13:02:34 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 203) in 2539 ms on 4e8a4a26f4b5 (executor driver) (27/200)
[2021-05-15 10:02:34,810] {docker.py:276} INFO - 21/05/15 13:02:34 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 209) (4e8a4a26f4b5, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:34,811] {docker.py:276} INFO - 21/05/15 13:02:34 INFO Executor: Running task 30.0 in stage 4.0 (TID 209)
[2021-05-15 10:02:34,820] {docker.py:276} INFO - 21/05/15 13:02:34 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:34,822] {docker.py:276} INFO - 21/05/15 13:02:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:34,823] {docker.py:276} INFO - 21/05/15 13:02:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:34,823] {docker.py:276} INFO - 21/05/15 13:02:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054297440399285933522_0004_m_000030_209, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054297440399285933522_0004_m_000030_209}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054297440399285933522_0004}; taskId=attempt_202105151302054297440399285933522_0004_m_000030_209, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56a38e8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302054297440399285933522_0004_m_000030_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054297440399285933522_0004_m_000030_209
[2021-05-15 10:02:34,825] {docker.py:276} INFO - 21/05/15 13:02:34 INFO StagingCommitter: Task committer attempt_202105151302054297440399285933522_0004_m_000030_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054297440399285933522_0004_m_000030_209 : duration 0:00.003s
[2021-05-15 10:02:35,255] {docker.py:276} INFO - 21/05/15 13:02:35 INFO StagingCommitter: Starting: Task committer attempt_20210515130205977350713224635951_0004_m_000027_206: needsTaskCommit() Task attempt_20210515130205977350713224635951_0004_m_000027_206
[2021-05-15 10:02:35,256] {docker.py:276} INFO - 21/05/15 13:02:35 INFO StagingCommitter: Task committer attempt_20210515130205977350713224635951_0004_m_000027_206: needsTaskCommit() Task attempt_20210515130205977350713224635951_0004_m_000027_206: duration 0:00.003s
21/05/15 13:02:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205977350713224635951_0004_m_000027_206
[2021-05-15 10:02:35,257] {docker.py:276} INFO - 21/05/15 13:02:35 INFO Executor: Finished task 27.0 in stage 4.0 (TID 206). 4544 bytes result sent to driver
[2021-05-15 10:02:35,258] {docker.py:276} INFO - 21/05/15 13:02:35 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 210) (4e8a4a26f4b5, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:35,260] {docker.py:276} INFO - 21/05/15 13:02:35 INFO Executor: Running task 31.0 in stage 4.0 (TID 210)
21/05/15 13:02:35 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 206) in 2489 ms on 4e8a4a26f4b5 (executor driver) (28/200)
[2021-05-15 10:02:35,271] {docker.py:276} INFO - 21/05/15 13:02:35 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:35,273] {docker.py:276} INFO - 21/05/15 13:02:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:35,274] {docker.py:276} INFO - 21/05/15 13:02:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:35,274] {docker.py:276} INFO - 21/05/15 13:02:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056580601863434650240_0004_m_000031_210, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056580601863434650240_0004_m_000031_210}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056580601863434650240_0004}; taskId=attempt_202105151302056580601863434650240_0004_m_000031_210, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c1161fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:35,274] {docker.py:276} INFO - 21/05/15 13:02:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:35,275] {docker.py:276} INFO - 21/05/15 13:02:35 INFO StagingCommitter: Starting: Task committer attempt_202105151302056580601863434650240_0004_m_000031_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056580601863434650240_0004_m_000031_210
[2021-05-15 10:02:35,277] {docker.py:276} INFO - 21/05/15 13:02:35 INFO StagingCommitter: Task committer attempt_202105151302056580601863434650240_0004_m_000031_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056580601863434650240_0004_m_000031_210 : duration 0:00.003s
[2021-05-15 10:02:37,007] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302054831774172258931751_0004_m_000028_207: needsTaskCommit() Task attempt_202105151302054831774172258931751_0004_m_000028_207
[2021-05-15 10:02:37,008] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302054831774172258931751_0004_m_000028_207: needsTaskCommit() Task attempt_202105151302054831774172258931751_0004_m_000028_207: duration 0:00.003s
21/05/15 13:02:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054831774172258931751_0004_m_000028_207
[2021-05-15 10:02:37,010] {docker.py:276} INFO - 21/05/15 13:02:37 INFO Executor: Finished task 28.0 in stage 4.0 (TID 207). 4544 bytes result sent to driver
[2021-05-15 10:02:37,013] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 211) (4e8a4a26f4b5, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:02:37 INFO Executor: Running task 32.0 in stage 4.0 (TID 211)
21/05/15 13:02:37 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 207) in 2375 ms on 4e8a4a26f4b5 (executor driver) (29/200)
[2021-05-15 10:02:37,022] {docker.py:276} INFO - 21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:37,024] {docker.py:276} INFO - 21/05/15 13:02:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:37,025] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:37,025] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051483224407069480871_0004_m_000032_211, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051483224407069480871_0004_m_000032_211}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051483224407069480871_0004}; taskId=attempt_202105151302051483224407069480871_0004_m_000032_211, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@626408c0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:37,025] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302051483224407069480871_0004_m_000032_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051483224407069480871_0004_m_000032_211
[2021-05-15 10:02:37,028] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302051483224407069480871_0004_m_000032_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051483224407069480871_0004_m_000032_211 : duration 0:00.003s
[2021-05-15 10:02:37,233] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302059019821112920716400_0004_m_000029_208: needsTaskCommit() Task attempt_202105151302059019821112920716400_0004_m_000029_208
[2021-05-15 10:02:37,234] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302059019821112920716400_0004_m_000029_208: needsTaskCommit() Task attempt_202105151302059019821112920716400_0004_m_000029_208: duration 0:00.003s
[2021-05-15 10:02:37,234] {docker.py:276} INFO - 21/05/15 13:02:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059019821112920716400_0004_m_000029_208
[2021-05-15 10:02:37,236] {docker.py:276} INFO - 21/05/15 13:02:37 INFO Executor: Finished task 29.0 in stage 4.0 (TID 208). 4544 bytes result sent to driver
[2021-05-15 10:02:37,238] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 212) (4e8a4a26f4b5, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:37,238] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 208) in 2532 ms on 4e8a4a26f4b5 (executor driver) (30/200)
[2021-05-15 10:02:37,239] {docker.py:276} INFO - 21/05/15 13:02:37 INFO Executor: Running task 33.0 in stage 4.0 (TID 212)
[2021-05-15 10:02:37,250] {docker.py:276} INFO - 21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:37,250] {docker.py:276} INFO - 21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:37,252] {docker.py:276} INFO - 21/05/15 13:02:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:37,253] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:37,253] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055381355003948090764_0004_m_000033_212, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055381355003948090764_0004_m_000033_212}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055381355003948090764_0004}; taskId=attempt_202105151302055381355003948090764_0004_m_000033_212, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d22a261}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302055381355003948090764_0004_m_000033_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055381355003948090764_0004_m_000033_212
[2021-05-15 10:02:37,255] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302055381355003948090764_0004_m_000033_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055381355003948090764_0004_m_000033_212 : duration 0:00.003s
[2021-05-15 10:02:37,282] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302054297440399285933522_0004_m_000030_209: needsTaskCommit() Task attempt_202105151302054297440399285933522_0004_m_000030_209
[2021-05-15 10:02:37,282] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302054297440399285933522_0004_m_000030_209: needsTaskCommit() Task attempt_202105151302054297440399285933522_0004_m_000030_209: duration 0:00.001s
21/05/15 13:02:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054297440399285933522_0004_m_000030_209
[2021-05-15 10:02:37,283] {docker.py:276} INFO - 21/05/15 13:02:37 INFO Executor: Finished task 30.0 in stage 4.0 (TID 209). 4544 bytes result sent to driver
[2021-05-15 10:02:37,284] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 213) (4e8a4a26f4b5, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:37,285] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 209) in 2478 ms on 4e8a4a26f4b5 (executor driver) (31/200)
21/05/15 13:02:37 INFO Executor: Running task 34.0 in stage 4.0 (TID 213)
[2021-05-15 10:02:37,292] {docker.py:276} INFO - 21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:37,295] {docker.py:276} INFO - 21/05/15 13:02:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:37,295] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:37,296] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053938414381468786976_0004_m_000034_213, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053938414381468786976_0004_m_000034_213}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053938414381468786976_0004}; taskId=attempt_202105151302053938414381468786976_0004_m_000034_213, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59e0a9dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302053938414381468786976_0004_m_000034_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053938414381468786976_0004_m_000034_213
[2021-05-15 10:02:37,298] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302053938414381468786976_0004_m_000034_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053938414381468786976_0004_m_000034_213 : duration 0:00.003s
[2021-05-15 10:02:37,598] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302056580601863434650240_0004_m_000031_210: needsTaskCommit() Task attempt_202105151302056580601863434650240_0004_m_000031_210
[2021-05-15 10:02:37,599] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302056580601863434650240_0004_m_000031_210: needsTaskCommit() Task attempt_202105151302056580601863434650240_0004_m_000031_210: duration 0:00.003s
[2021-05-15 10:02:37,600] {docker.py:276} INFO - 21/05/15 13:02:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056580601863434650240_0004_m_000031_210
[2021-05-15 10:02:37,601] {docker.py:276} INFO - 21/05/15 13:02:37 INFO Executor: Finished task 31.0 in stage 4.0 (TID 210). 4544 bytes result sent to driver
[2021-05-15 10:02:37,603] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 214) (4e8a4a26f4b5, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:37,604] {docker.py:276} INFO - 21/05/15 13:02:37 INFO Executor: Running task 35.0 in stage 4.0 (TID 214)
[2021-05-15 10:02:37,605] {docker.py:276} INFO - 21/05/15 13:02:37 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 210) in 2350 ms on 4e8a4a26f4b5 (executor driver) (32/200)
[2021-05-15 10:02:37,615] {docker.py:276} INFO - 21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:37,617] {docker.py:276} INFO - 21/05/15 13:02:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:37,617] {docker.py:276} INFO - 21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051826624833397841308_0004_m_000035_214, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051826624833397841308_0004_m_000035_214}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051826624833397841308_0004}; taskId=attempt_202105151302051826624833397841308_0004_m_000035_214, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75668348}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:37,618] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302051826624833397841308_0004_m_000035_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051826624833397841308_0004_m_000035_214
[2021-05-15 10:02:37,621] {docker.py:276} INFO - 21/05/15 13:02:37 INFO StagingCommitter: Task committer attempt_202105151302051826624833397841308_0004_m_000035_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051826624833397841308_0004_m_000035_214 : duration 0:00.003s
[2021-05-15 10:02:39,547] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Starting: Task committer attempt_202105151302051483224407069480871_0004_m_000032_211: needsTaskCommit() Task attempt_202105151302051483224407069480871_0004_m_000032_211
21/05/15 13:02:39 INFO StagingCommitter: Task committer attempt_202105151302051483224407069480871_0004_m_000032_211: needsTaskCommit() Task attempt_202105151302051483224407069480871_0004_m_000032_211: duration 0:00.001s
21/05/15 13:02:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051483224407069480871_0004_m_000032_211
[2021-05-15 10:02:39,550] {docker.py:276} INFO - 21/05/15 13:02:39 INFO Executor: Finished task 32.0 in stage 4.0 (TID 211). 4587 bytes result sent to driver
[2021-05-15 10:02:39,552] {docker.py:276} INFO - 21/05/15 13:02:39 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 215) (4e8a4a26f4b5, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:39,553] {docker.py:276} INFO - 21/05/15 13:02:39 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 211) in 2545 ms on 4e8a4a26f4b5 (executor driver) (33/200)
[2021-05-15 10:02:39,554] {docker.py:276} INFO - 21/05/15 13:02:39 INFO Executor: Running task 36.0 in stage 4.0 (TID 215)
[2021-05-15 10:02:39,565] {docker.py:276} INFO - 21/05/15 13:02:39 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:39,567] {docker.py:276} INFO - 21/05/15 13:02:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:39,567] {docker.py:276} INFO - 21/05/15 13:02:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205970689815159551135_0004_m_000036_215, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205970689815159551135_0004_m_000036_215}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205970689815159551135_0004}; taskId=attempt_20210515130205970689815159551135_0004_m_000036_215, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b047b20}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:39 INFO StagingCommitter: Starting: Task committer attempt_20210515130205970689815159551135_0004_m_000036_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205970689815159551135_0004_m_000036_215
[2021-05-15 10:02:39,570] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Task committer attempt_20210515130205970689815159551135_0004_m_000036_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205970689815159551135_0004_m_000036_215 : duration 0:00.003s
[2021-05-15 10:02:39,693] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Starting: Task committer attempt_202105151302055381355003948090764_0004_m_000033_212: needsTaskCommit() Task attempt_202105151302055381355003948090764_0004_m_000033_212
[2021-05-15 10:02:39,694] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Task committer attempt_202105151302055381355003948090764_0004_m_000033_212: needsTaskCommit() Task attempt_202105151302055381355003948090764_0004_m_000033_212: duration 0:00.001s
21/05/15 13:02:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055381355003948090764_0004_m_000033_212
[2021-05-15 10:02:39,698] {docker.py:276} INFO - 21/05/15 13:02:39 INFO Executor: Finished task 33.0 in stage 4.0 (TID 212). 4587 bytes result sent to driver
[2021-05-15 10:02:39,699] {docker.py:276} INFO - 21/05/15 13:02:39 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 216) (4e8a4a26f4b5, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:39,700] {docker.py:276} INFO - 21/05/15 13:02:39 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 212) in 2466 ms on 4e8a4a26f4b5 (executor driver) (34/200)
[2021-05-15 10:02:39,701] {docker.py:276} INFO - 21/05/15 13:02:39 INFO Executor: Running task 37.0 in stage 4.0 (TID 216)
[2021-05-15 10:02:39,709] {docker.py:276} INFO - 21/05/15 13:02:39 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:39,712] {docker.py:276} INFO - 21/05/15 13:02:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051513020520352656637890363_0004_m_000037_216, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020520352656637890363_0004_m_000037_216}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051513020520352656637890363_0004}; taskId=attempt_2021051513020520352656637890363_0004_m_000037_216, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bef50bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:39,712] {docker.py:276} INFO - 21/05/15 13:02:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:39,713] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Starting: Task committer attempt_2021051513020520352656637890363_0004_m_000037_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020520352656637890363_0004_m_000037_216
[2021-05-15 10:02:39,716] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Task committer attempt_2021051513020520352656637890363_0004_m_000037_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020520352656637890363_0004_m_000037_216 : duration 0:00.004s
[2021-05-15 10:02:39,819] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Starting: Task committer attempt_202105151302053938414381468786976_0004_m_000034_213: needsTaskCommit() Task attempt_202105151302053938414381468786976_0004_m_000034_213
[2021-05-15 10:02:39,819] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Task committer attempt_202105151302053938414381468786976_0004_m_000034_213: needsTaskCommit() Task attempt_202105151302053938414381468786976_0004_m_000034_213: duration 0:00.000s
21/05/15 13:02:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053938414381468786976_0004_m_000034_213
[2021-05-15 10:02:39,823] {docker.py:276} INFO - 21/05/15 13:02:39 INFO Executor: Finished task 34.0 in stage 4.0 (TID 213). 4587 bytes result sent to driver
[2021-05-15 10:02:39,824] {docker.py:276} INFO - 21/05/15 13:02:39 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 217) (4e8a4a26f4b5, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:39,824] {docker.py:276} INFO - 21/05/15 13:02:39 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 213) in 2543 ms on 4e8a4a26f4b5 (executor driver) (35/200)
21/05/15 13:02:39 INFO Executor: Running task 38.0 in stage 4.0 (TID 217)
[2021-05-15 10:02:39,834] {docker.py:276} INFO - 21/05/15 13:02:39 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:39,836] {docker.py:276} INFO - 21/05/15 13:02:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:39,836] {docker.py:276} INFO - 21/05/15 13:02:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205525270132185362651_0004_m_000038_217, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205525270132185362651_0004_m_000038_217}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205525270132185362651_0004}; taskId=attempt_20210515130205525270132185362651_0004_m_000038_217, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79f6bcd2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:39 INFO StagingCommitter: Starting: Task committer attempt_20210515130205525270132185362651_0004_m_000038_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205525270132185362651_0004_m_000038_217
[2021-05-15 10:02:39,840] {docker.py:276} INFO - 21/05/15 13:02:39 INFO StagingCommitter: Task committer attempt_20210515130205525270132185362651_0004_m_000038_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205525270132185362651_0004_m_000038_217 : duration 0:00.002s
[2021-05-15 10:02:40,080] {docker.py:276} INFO - 21/05/15 13:02:40 INFO StagingCommitter: Starting: Task committer attempt_202105151302051826624833397841308_0004_m_000035_214: needsTaskCommit() Task attempt_202105151302051826624833397841308_0004_m_000035_214
21/05/15 13:02:40 INFO StagingCommitter: Task committer attempt_202105151302051826624833397841308_0004_m_000035_214: needsTaskCommit() Task attempt_202105151302051826624833397841308_0004_m_000035_214: duration 0:00.001s
21/05/15 13:02:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051826624833397841308_0004_m_000035_214
[2021-05-15 10:02:40,081] {docker.py:276} INFO - 21/05/15 13:02:40 INFO Executor: Finished task 35.0 in stage 4.0 (TID 214). 4587 bytes result sent to driver
[2021-05-15 10:02:40,083] {docker.py:276} INFO - 21/05/15 13:02:40 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 218) (4e8a4a26f4b5, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:40,084] {docker.py:276} INFO - 21/05/15 13:02:40 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 214) in 2484 ms on 4e8a4a26f4b5 (executor driver) (36/200)
[2021-05-15 10:02:40,085] {docker.py:276} INFO - 21/05/15 13:02:40 INFO Executor: Running task 39.0 in stage 4.0 (TID 218)
[2021-05-15 10:02:40,095] {docker.py:276} INFO - 21/05/15 13:02:40 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:40,097] {docker.py:276} INFO - 21/05/15 13:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054046857534040946054_0004_m_000039_218, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054046857534040946054_0004_m_000039_218}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054046857534040946054_0004}; taskId=attempt_202105151302054046857534040946054_0004_m_000039_218, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@774c60dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:40,098] {docker.py:276} INFO - 21/05/15 13:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:40 INFO StagingCommitter: Starting: Task committer attempt_202105151302054046857534040946054_0004_m_000039_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054046857534040946054_0004_m_000039_218
[2021-05-15 10:02:40,101] {docker.py:276} INFO - 21/05/15 13:02:40 INFO StagingCommitter: Task committer attempt_202105151302054046857534040946054_0004_m_000039_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054046857534040946054_0004_m_000039_218 : duration 0:00.003s
[2021-05-15 10:02:42,106] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_20210515130205970689815159551135_0004_m_000036_215: needsTaskCommit() Task attempt_20210515130205970689815159551135_0004_m_000036_215
21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_20210515130205970689815159551135_0004_m_000036_215: needsTaskCommit() Task attempt_20210515130205970689815159551135_0004_m_000036_215: duration 0:00.001s
21/05/15 13:02:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205970689815159551135_0004_m_000036_215
[2021-05-15 10:02:42,107] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Finished task 36.0 in stage 4.0 (TID 215). 4544 bytes result sent to driver
[2021-05-15 10:02:42,110] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 219) (4e8a4a26f4b5, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:42,111] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 215) in 2562 ms on 4e8a4a26f4b5 (executor driver) (37/200)
21/05/15 13:02:42 INFO Executor: Running task 40.0 in stage 4.0 (TID 219)
[2021-05-15 10:02:42,121] {docker.py:276} INFO - 21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:42,123] {docker.py:276} INFO - 21/05/15 13:02:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:42,124] {docker.py:276} INFO - 21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:42,124] {docker.py:276} INFO - 21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054476810575125105813_0004_m_000040_219, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054476810575125105813_0004_m_000040_219}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054476810575125105813_0004}; taskId=attempt_202105151302054476810575125105813_0004_m_000040_219, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@553d150e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:42,125] {docker.py:276} INFO - 21/05/15 13:02:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302054476810575125105813_0004_m_000040_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054476810575125105813_0004_m_000040_219
[2021-05-15 10:02:42,127] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_202105151302054476810575125105813_0004_m_000040_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054476810575125105813_0004_m_000040_219 : duration 0:00.003s
[2021-05-15 10:02:42,180] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_2021051513020520352656637890363_0004_m_000037_216: needsTaskCommit() Task attempt_2021051513020520352656637890363_0004_m_000037_216
[2021-05-15 10:02:42,181] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_2021051513020520352656637890363_0004_m_000037_216: needsTaskCommit() Task attempt_2021051513020520352656637890363_0004_m_000037_216: duration 0:00.000s
21/05/15 13:02:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051513020520352656637890363_0004_m_000037_216
[2021-05-15 10:02:42,181] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Finished task 37.0 in stage 4.0 (TID 216). 4544 bytes result sent to driver
[2021-05-15 10:02:42,182] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 220) (4e8a4a26f4b5, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:42,183] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Running task 41.0 in stage 4.0 (TID 220)
[2021-05-15 10:02:42,183] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 216) in 2487 ms on 4e8a4a26f4b5 (executor driver) (38/200)
[2021-05-15 10:02:42,191] {docker.py:276} INFO - 21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:42,193] {docker.py:276} INFO - 21/05/15 13:02:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:42,194] {docker.py:276} INFO - 21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051865376200300819731_0004_m_000041_220, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051865376200300819731_0004_m_000041_220}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051865376200300819731_0004}; taskId=attempt_202105151302051865376200300819731_0004_m_000041_220, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56644bf6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302051865376200300819731_0004_m_000041_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051865376200300819731_0004_m_000041_220
[2021-05-15 10:02:42,197] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_202105151302051865376200300819731_0004_m_000041_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051865376200300819731_0004_m_000041_220 : duration 0:00.003s
[2021-05-15 10:02:42,340] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_20210515130205525270132185362651_0004_m_000038_217: needsTaskCommit() Task attempt_20210515130205525270132185362651_0004_m_000038_217
[2021-05-15 10:02:42,341] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_20210515130205525270132185362651_0004_m_000038_217: needsTaskCommit() Task attempt_20210515130205525270132185362651_0004_m_000038_217: duration 0:00.001s
21/05/15 13:02:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205525270132185362651_0004_m_000038_217
[2021-05-15 10:02:42,343] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Finished task 38.0 in stage 4.0 (TID 217). 4544 bytes result sent to driver
[2021-05-15 10:02:42,345] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 221) (4e8a4a26f4b5, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:42,346] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Running task 42.0 in stage 4.0 (TID 221)
[2021-05-15 10:02:42,347] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 217) in 2525 ms on 4e8a4a26f4b5 (executor driver) (39/200)
[2021-05-15 10:02:42,356] {docker.py:276} INFO - 21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:42,358] {docker.py:276} INFO - 21/05/15 13:02:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:42,358] {docker.py:276} INFO - 21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052000066004282276489_0004_m_000042_221, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052000066004282276489_0004_m_000042_221}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052000066004282276489_0004}; taskId=attempt_202105151302052000066004282276489_0004_m_000042_221, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c575d61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:42,358] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302052000066004282276489_0004_m_000042_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052000066004282276489_0004_m_000042_221
[2021-05-15 10:02:42,361] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_202105151302052000066004282276489_0004_m_000042_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052000066004282276489_0004_m_000042_221 : duration 0:00.003s
[2021-05-15 10:02:42,649] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302054046857534040946054_0004_m_000039_218: needsTaskCommit() Task attempt_202105151302054046857534040946054_0004_m_000039_218
[2021-05-15 10:02:42,649] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_202105151302054046857534040946054_0004_m_000039_218: needsTaskCommit() Task attempt_202105151302054046857534040946054_0004_m_000039_218: duration 0:00.001s
[2021-05-15 10:02:42,650] {docker.py:276} INFO - 21/05/15 13:02:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054046857534040946054_0004_m_000039_218
[2021-05-15 10:02:42,651] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Finished task 39.0 in stage 4.0 (TID 218). 4544 bytes result sent to driver
[2021-05-15 10:02:42,652] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 222) (4e8a4a26f4b5, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:42,653] {docker.py:276} INFO - 21/05/15 13:02:42 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 218) in 2574 ms on 4e8a4a26f4b5 (executor driver) (40/200)
[2021-05-15 10:02:42,654] {docker.py:276} INFO - 21/05/15 13:02:42 INFO Executor: Running task 43.0 in stage 4.0 (TID 222)
[2021-05-15 10:02:42,662] {docker.py:276} INFO - 21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:42,665] {docker.py:276} INFO - 21/05/15 13:02:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:42,665] {docker.py:276} INFO - 21/05/15 13:02:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053462046261262585136_0004_m_000043_222, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053462046261262585136_0004_m_000043_222}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053462046261262585136_0004}; taskId=attempt_202105151302053462046261262585136_0004_m_000043_222, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@334ca607}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302053462046261262585136_0004_m_000043_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053462046261262585136_0004_m_000043_222
[2021-05-15 10:02:42,668] {docker.py:276} INFO - 21/05/15 13:02:42 INFO StagingCommitter: Task committer attempt_202105151302053462046261262585136_0004_m_000043_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053462046261262585136_0004_m_000043_222 : duration 0:00.002s
[2021-05-15 10:02:43,985] {docker.py:276} INFO - 21/05/15 13:02:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302054476810575125105813_0004_m_000040_219: needsTaskCommit() Task attempt_202105151302054476810575125105813_0004_m_000040_219
[2021-05-15 10:02:43,985] {docker.py:276} INFO - 21/05/15 13:02:44 INFO StagingCommitter: Task committer attempt_202105151302054476810575125105813_0004_m_000040_219: needsTaskCommit() Task attempt_202105151302054476810575125105813_0004_m_000040_219: duration 0:00.001s
21/05/15 13:02:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054476810575125105813_0004_m_000040_219
[2021-05-15 10:02:43,987] {docker.py:276} INFO - 21/05/15 13:02:44 INFO Executor: Finished task 40.0 in stage 4.0 (TID 219). 4544 bytes result sent to driver
[2021-05-15 10:02:43,988] {docker.py:276} INFO - 21/05/15 13:02:44 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 223) (4e8a4a26f4b5, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:43,989] {docker.py:276} INFO - 21/05/15 13:02:44 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 219) in 1881 ms on 4e8a4a26f4b5 (executor driver) (41/200)
[2021-05-15 10:02:43,990] {docker.py:276} INFO - 21/05/15 13:02:44 INFO Executor: Running task 44.0 in stage 4.0 (TID 223)
[2021-05-15 10:02:43,999] {docker.py:276} INFO - 21/05/15 13:02:44 INFO ShuffleBlockFetcherIterator: Getting 4 (11.6 KiB) non-empty blocks including 4 (11.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:44,001] {docker.py:276} INFO - 21/05/15 13:02:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:44,002] {docker.py:276} INFO - 21/05/15 13:02:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051043213537173680771_0004_m_000044_223, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051043213537173680771_0004_m_000044_223}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051043213537173680771_0004}; taskId=attempt_202105151302051043213537173680771_0004_m_000044_223, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b7fe253}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302051043213537173680771_0004_m_000044_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051043213537173680771_0004_m_000044_223
[2021-05-15 10:02:44,005] {docker.py:276} INFO - 21/05/15 13:02:44 INFO StagingCommitter: Task committer attempt_202105151302051043213537173680771_0004_m_000044_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051043213537173680771_0004_m_000044_223 : duration 0:00.003s
[2021-05-15 10:02:44,165] {docker.py:276} INFO - 21/05/15 13:02:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302052000066004282276489_0004_m_000042_221: needsTaskCommit() Task attempt_202105151302052000066004282276489_0004_m_000042_221
[2021-05-15 10:02:44,166] {docker.py:276} INFO - 21/05/15 13:02:44 INFO StagingCommitter: Task committer attempt_202105151302052000066004282276489_0004_m_000042_221: needsTaskCommit() Task attempt_202105151302052000066004282276489_0004_m_000042_221: duration 0:00.002s
21/05/15 13:02:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052000066004282276489_0004_m_000042_221
[2021-05-15 10:02:44,169] {docker.py:276} INFO - 21/05/15 13:02:44 INFO Executor: Finished task 42.0 in stage 4.0 (TID 221). 4544 bytes result sent to driver
[2021-05-15 10:02:44,170] {docker.py:276} INFO - 21/05/15 13:02:44 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 224) (4e8a4a26f4b5, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:44,171] {docker.py:276} INFO - 21/05/15 13:02:44 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 221) in 1830 ms on 4e8a4a26f4b5 (executor driver) (42/200)
21/05/15 13:02:44 INFO Executor: Running task 45.0 in stage 4.0 (TID 224)
[2021-05-15 10:02:44,181] {docker.py:276} INFO - 21/05/15 13:02:44 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:44,184] {docker.py:276} INFO - 21/05/15 13:02:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051779457274915803181_0004_m_000045_224, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051779457274915803181_0004_m_000045_224}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051779457274915803181_0004}; taskId=attempt_202105151302051779457274915803181_0004_m_000045_224, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@509ecde9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:44,184] {docker.py:276} INFO - 21/05/15 13:02:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302051779457274915803181_0004_m_000045_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051779457274915803181_0004_m_000045_224
[2021-05-15 10:02:44,187] {docker.py:276} INFO - 21/05/15 13:02:44 INFO StagingCommitter: Task committer attempt_202105151302051779457274915803181_0004_m_000045_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051779457274915803181_0004_m_000045_224 : duration 0:00.003s
[2021-05-15 10:02:45,130] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105151302051865376200300819731_0004_m_000041_220: needsTaskCommit() Task attempt_202105151302051865376200300819731_0004_m_000041_220
[2021-05-15 10:02:45,131] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Task committer attempt_202105151302051865376200300819731_0004_m_000041_220: needsTaskCommit() Task attempt_202105151302051865376200300819731_0004_m_000041_220: duration 0:00.001s
21/05/15 13:02:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051865376200300819731_0004_m_000041_220
[2021-05-15 10:02:45,132] {docker.py:276} INFO - 21/05/15 13:02:45 INFO Executor: Finished task 41.0 in stage 4.0 (TID 220). 4544 bytes result sent to driver
[2021-05-15 10:02:45,133] {docker.py:276} INFO - 21/05/15 13:02:45 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 225) (4e8a4a26f4b5, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:45,133] {docker.py:276} INFO - 21/05/15 13:02:45 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 220) in 2954 ms on 4e8a4a26f4b5 (executor driver) (43/200)
[2021-05-15 10:02:45,134] {docker.py:276} INFO - 21/05/15 13:02:45 INFO Executor: Running task 46.0 in stage 4.0 (TID 225)
[2021-05-15 10:02:45,151] {docker.py:276} INFO - 21/05/15 13:02:45 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:02:45,152] {docker.py:276} INFO - 21/05/15 13:02:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:45,153] {docker.py:276} INFO - 21/05/15 13:02:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205415727928879033771_0004_m_000046_225, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205415727928879033771_0004_m_000046_225}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205415727928879033771_0004}; taskId=attempt_20210515130205415727928879033771_0004_m_000046_225, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40c625b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:45 INFO StagingCommitter: Starting: Task committer attempt_20210515130205415727928879033771_0004_m_000046_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205415727928879033771_0004_m_000046_225
[2021-05-15 10:02:45,156] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Task committer attempt_20210515130205415727928879033771_0004_m_000046_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205415727928879033771_0004_m_000046_225 : duration 0:00.003s
[2021-05-15 10:02:45,192] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105151302053462046261262585136_0004_m_000043_222: needsTaskCommit() Task attempt_202105151302053462046261262585136_0004_m_000043_222
[2021-05-15 10:02:45,193] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Task committer attempt_202105151302053462046261262585136_0004_m_000043_222: needsTaskCommit() Task attempt_202105151302053462046261262585136_0004_m_000043_222: duration 0:00.000s
21/05/15 13:02:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053462046261262585136_0004_m_000043_222
[2021-05-15 10:02:45,194] {docker.py:276} INFO - 21/05/15 13:02:45 INFO Executor: Finished task 43.0 in stage 4.0 (TID 222). 4587 bytes result sent to driver
[2021-05-15 10:02:45,195] {docker.py:276} INFO - 21/05/15 13:02:45 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 226) (4e8a4a26f4b5, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:45,196] {docker.py:276} INFO - 21/05/15 13:02:45 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 222) in 2546 ms on 4e8a4a26f4b5 (executor driver) (44/200)
[2021-05-15 10:02:45,196] {docker.py:276} INFO - 21/05/15 13:02:45 INFO Executor: Running task 47.0 in stage 4.0 (TID 226)
[2021-05-15 10:02:45,205] {docker.py:276} INFO - 21/05/15 13:02:45 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:45,206] {docker.py:276} INFO - 21/05/15 13:02:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:45,207] {docker.py:276} INFO - 21/05/15 13:02:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055648140476807836476_0004_m_000047_226, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055648140476807836476_0004_m_000047_226}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055648140476807836476_0004}; taskId=attempt_202105151302055648140476807836476_0004_m_000047_226, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75ab6082}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:45,207] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Starting: Task committer attempt_202105151302055648140476807836476_0004_m_000047_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055648140476807836476_0004_m_000047_226
[2021-05-15 10:02:45,210] {docker.py:276} INFO - 21/05/15 13:02:45 INFO StagingCommitter: Task committer attempt_202105151302055648140476807836476_0004_m_000047_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055648140476807836476_0004_m_000047_226 : duration 0:00.003s
[2021-05-15 10:02:46,172] {docker.py:276} INFO - 21/05/15 13:02:46 INFO StagingCommitter: Starting: Task committer attempt_202105151302051779457274915803181_0004_m_000045_224: needsTaskCommit() Task attempt_202105151302051779457274915803181_0004_m_000045_224
[2021-05-15 10:02:46,172] {docker.py:276} INFO - 21/05/15 13:02:46 INFO StagingCommitter: Task committer attempt_202105151302051779457274915803181_0004_m_000045_224: needsTaskCommit() Task attempt_202105151302051779457274915803181_0004_m_000045_224: duration 0:00.001s
[2021-05-15 10:02:46,173] {docker.py:276} INFO - 21/05/15 13:02:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051779457274915803181_0004_m_000045_224
[2021-05-15 10:02:46,175] {docker.py:276} INFO - 21/05/15 13:02:46 INFO Executor: Finished task 45.0 in stage 4.0 (TID 224). 4587 bytes result sent to driver
[2021-05-15 10:02:46,176] {docker.py:276} INFO - 21/05/15 13:02:46 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 227) (4e8a4a26f4b5, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:46,178] {docker.py:276} INFO - 21/05/15 13:02:46 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 224) in 2010 ms on 4e8a4a26f4b5 (executor driver) (45/200)
21/05/15 13:02:46 INFO Executor: Running task 48.0 in stage 4.0 (TID 227)
[2021-05-15 10:02:46,187] {docker.py:276} INFO - 21/05/15 13:02:46 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:46,190] {docker.py:276} INFO - 21/05/15 13:02:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057105337173573568303_0004_m_000048_227, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057105337173573568303_0004_m_000048_227}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057105337173573568303_0004}; taskId=attempt_202105151302057105337173573568303_0004_m_000048_227, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@732a9d7f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:46,190] {docker.py:276} INFO - 21/05/15 13:02:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:46 INFO StagingCommitter: Starting: Task committer attempt_202105151302057105337173573568303_0004_m_000048_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057105337173573568303_0004_m_000048_227
[2021-05-15 10:02:46,193] {docker.py:276} INFO - 21/05/15 13:02:46 INFO StagingCommitter: Task committer attempt_202105151302057105337173573568303_0004_m_000048_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057105337173573568303_0004_m_000048_227 : duration 0:00.003s
[2021-05-15 10:02:47,073] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302051043213537173680771_0004_m_000044_223: needsTaskCommit() Task attempt_202105151302051043213537173680771_0004_m_000044_223
21/05/15 13:02:47 INFO StagingCommitter: Task committer attempt_202105151302051043213537173680771_0004_m_000044_223: needsTaskCommit() Task attempt_202105151302051043213537173680771_0004_m_000044_223: duration 0:00.000s
21/05/15 13:02:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051043213537173680771_0004_m_000044_223
[2021-05-15 10:02:47,073] {docker.py:276} INFO - 21/05/15 13:02:47 INFO Executor: Finished task 44.0 in stage 4.0 (TID 223). 4587 bytes result sent to driver
[2021-05-15 10:02:47,074] {docker.py:276} INFO - 21/05/15 13:02:47 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 228) (4e8a4a26f4b5, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:47,075] {docker.py:276} INFO - 21/05/15 13:02:47 INFO Executor: Running task 49.0 in stage 4.0 (TID 228)
[2021-05-15 10:02:47,077] {docker.py:276} INFO - 21/05/15 13:02:47 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 223) in 3092 ms on 4e8a4a26f4b5 (executor driver) (46/200)
[2021-05-15 10:02:47,087] {docker.py:276} INFO - 21/05/15 13:02:47 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:47,089] {docker.py:276} INFO - 21/05/15 13:02:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058234527241583331640_0004_m_000049_228, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058234527241583331640_0004_m_000049_228}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058234527241583331640_0004}; taskId=attempt_202105151302058234527241583331640_0004_m_000049_228, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a4167f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:47,089] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302058234527241583331640_0004_m_000049_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058234527241583331640_0004_m_000049_228
[2021-05-15 10:02:47,092] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Task committer attempt_202105151302058234527241583331640_0004_m_000049_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058234527241583331640_0004_m_000049_228 : duration 0:00.004s
[2021-05-15 10:02:47,420] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302055648140476807836476_0004_m_000047_226: needsTaskCommit() Task attempt_202105151302055648140476807836476_0004_m_000047_226
21/05/15 13:02:47 INFO StagingCommitter: Task committer attempt_202105151302055648140476807836476_0004_m_000047_226: needsTaskCommit() Task attempt_202105151302055648140476807836476_0004_m_000047_226: duration 0:00.001s
21/05/15 13:02:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055648140476807836476_0004_m_000047_226
[2021-05-15 10:02:47,421] {docker.py:276} INFO - 21/05/15 13:02:47 INFO Executor: Finished task 47.0 in stage 4.0 (TID 226). 4544 bytes result sent to driver
[2021-05-15 10:02:47,422] {docker.py:276} INFO - 21/05/15 13:02:47 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 229) (4e8a4a26f4b5, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:47,422] {docker.py:276} INFO - 21/05/15 13:02:47 INFO Executor: Running task 50.0 in stage 4.0 (TID 229)
[2021-05-15 10:02:47,423] {docker.py:276} INFO - 21/05/15 13:02:47 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 226) in 2231 ms on 4e8a4a26f4b5 (executor driver) (47/200)
[2021-05-15 10:02:47,431] {docker.py:276} INFO - 21/05/15 13:02:47 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:47,432] {docker.py:276} INFO - 21/05/15 13:02:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:47,433] {docker.py:276} INFO - 21/05/15 13:02:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:47,434] {docker.py:276} INFO - 21/05/15 13:02:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:47,434] {docker.py:276} INFO - 21/05/15 13:02:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059128643670289467854_0004_m_000050_229, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059128643670289467854_0004_m_000050_229}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059128643670289467854_0004}; taskId=attempt_202105151302059128643670289467854_0004_m_000050_229, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b909934}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:47,434] {docker.py:276} INFO - 21/05/15 13:02:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302059128643670289467854_0004_m_000050_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059128643670289467854_0004_m_000050_229
[2021-05-15 10:02:47,436] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Task committer attempt_202105151302059128643670289467854_0004_m_000050_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059128643670289467854_0004_m_000050_229 : duration 0:00.002s
[2021-05-15 10:02:47,618] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Starting: Task committer attempt_20210515130205415727928879033771_0004_m_000046_225: needsTaskCommit() Task attempt_20210515130205415727928879033771_0004_m_000046_225
[2021-05-15 10:02:47,619] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Task committer attempt_20210515130205415727928879033771_0004_m_000046_225: needsTaskCommit() Task attempt_20210515130205415727928879033771_0004_m_000046_225: duration 0:00.000s
[2021-05-15 10:02:47,619] {docker.py:276} INFO - 21/05/15 13:02:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205415727928879033771_0004_m_000046_225
[2021-05-15 10:02:47,620] {docker.py:276} INFO - 21/05/15 13:02:47 INFO Executor: Finished task 46.0 in stage 4.0 (TID 225). 4587 bytes result sent to driver
[2021-05-15 10:02:47,621] {docker.py:276} INFO - 21/05/15 13:02:47 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 230) (4e8a4a26f4b5, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:47,621] {docker.py:276} INFO - 21/05/15 13:02:47 INFO Executor: Running task 51.0 in stage 4.0 (TID 230)
[2021-05-15 10:02:47,623] {docker.py:276} INFO - 21/05/15 13:02:47 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 225) in 2493 ms on 4e8a4a26f4b5 (executor driver) (48/200)
[2021-05-15 10:02:47,632] {docker.py:276} INFO - 21/05/15 13:02:47 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:47,632] {docker.py:276} INFO - 21/05/15 13:02:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:47,634] {docker.py:276} INFO - 21/05/15 13:02:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:02:47,635] {docker.py:276} INFO - 21/05/15 13:02:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:47,635] {docker.py:276} INFO - 21/05/15 13:02:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:47,636] {docker.py:276} INFO - 21/05/15 13:02:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055211402154614952893_0004_m_000051_230, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055211402154614952893_0004_m_000051_230}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055211402154614952893_0004}; taskId=attempt_202105151302055211402154614952893_0004_m_000051_230, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60dc4d72}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:47,636] {docker.py:276} INFO - 21/05/15 13:02:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:47,637] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302055211402154614952893_0004_m_000051_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055211402154614952893_0004_m_000051_230
[2021-05-15 10:02:47,639] {docker.py:276} INFO - 21/05/15 13:02:47 INFO StagingCommitter: Task committer attempt_202105151302055211402154614952893_0004_m_000051_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055211402154614952893_0004_m_000051_230 : duration 0:00.003s
[2021-05-15 10:02:48,540] {docker.py:276} INFO - 21/05/15 13:02:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302057105337173573568303_0004_m_000048_227: needsTaskCommit() Task attempt_202105151302057105337173573568303_0004_m_000048_227
[2021-05-15 10:02:48,541] {docker.py:276} INFO - 21/05/15 13:02:48 INFO StagingCommitter: Task committer attempt_202105151302057105337173573568303_0004_m_000048_227: needsTaskCommit() Task attempt_202105151302057105337173573568303_0004_m_000048_227: duration 0:00.001s
21/05/15 13:02:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057105337173573568303_0004_m_000048_227
[2021-05-15 10:02:48,543] {docker.py:276} INFO - 21/05/15 13:02:48 INFO Executor: Finished task 48.0 in stage 4.0 (TID 227). 4544 bytes result sent to driver
[2021-05-15 10:02:48,544] {docker.py:276} INFO - 21/05/15 13:02:48 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 231) (4e8a4a26f4b5, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:48,545] {docker.py:276} INFO - 21/05/15 13:02:48 INFO Executor: Running task 52.0 in stage 4.0 (TID 231)
[2021-05-15 10:02:48,546] {docker.py:276} INFO - 21/05/15 13:02:48 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 227) in 2373 ms on 4e8a4a26f4b5 (executor driver) (49/200)
[2021-05-15 10:02:48,556] {docker.py:276} INFO - 21/05/15 13:02:48 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:48,558] {docker.py:276} INFO - 21/05/15 13:02:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052307788372022966974_0004_m_000052_231, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052307788372022966974_0004_m_000052_231}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052307788372022966974_0004}; taskId=attempt_202105151302052307788372022966974_0004_m_000052_231, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2115bdf1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302052307788372022966974_0004_m_000052_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052307788372022966974_0004_m_000052_231
[2021-05-15 10:02:48,561] {docker.py:276} INFO - 21/05/15 13:02:48 INFO StagingCommitter: Task committer attempt_202105151302052307788372022966974_0004_m_000052_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052307788372022966974_0004_m_000052_231 : duration 0:00.003s
[2021-05-15 10:02:49,718] {docker.py:276} INFO - 21/05/15 13:02:49 INFO StagingCommitter: Starting: Task committer attempt_202105151302059128643670289467854_0004_m_000050_229: needsTaskCommit() Task attempt_202105151302059128643670289467854_0004_m_000050_229
21/05/15 13:02:49 INFO StagingCommitter: Task committer attempt_202105151302059128643670289467854_0004_m_000050_229: needsTaskCommit() Task attempt_202105151302059128643670289467854_0004_m_000050_229: duration 0:00.001s
[2021-05-15 10:02:49,719] {docker.py:276} INFO - 21/05/15 13:02:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059128643670289467854_0004_m_000050_229
[2021-05-15 10:02:49,722] {docker.py:276} INFO - 21/05/15 13:02:49 INFO Executor: Finished task 50.0 in stage 4.0 (TID 229). 4544 bytes result sent to driver
[2021-05-15 10:02:49,724] {docker.py:276} INFO - 21/05/15 13:02:49 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 232) (4e8a4a26f4b5, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:49,725] {docker.py:276} INFO - 21/05/15 13:02:49 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 229) in 2307 ms on 4e8a4a26f4b5 (executor driver) (50/200)
[2021-05-15 10:02:49,726] {docker.py:276} INFO - 21/05/15 13:02:49 INFO Executor: Running task 53.0 in stage 4.0 (TID 232)
[2021-05-15 10:02:49,736] {docker.py:276} INFO - 21/05/15 13:02:49 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:49,737] {docker.py:276} INFO - 21/05/15 13:02:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056631144988794766138_0004_m_000053_232, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056631144988794766138_0004_m_000053_232}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056631144988794766138_0004}; taskId=attempt_202105151302056631144988794766138_0004_m_000053_232, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@590e10fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:49,738] {docker.py:276} INFO - 21/05/15 13:02:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:49 INFO StagingCommitter: Starting: Task committer attempt_202105151302056631144988794766138_0004_m_000053_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056631144988794766138_0004_m_000053_232
[2021-05-15 10:02:49,740] {docker.py:276} INFO - 21/05/15 13:02:49 INFO StagingCommitter: Task committer attempt_202105151302056631144988794766138_0004_m_000053_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056631144988794766138_0004_m_000053_232 : duration 0:00.002s
[2021-05-15 10:02:50,034] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302055211402154614952893_0004_m_000051_230: needsTaskCommit() Task attempt_202105151302055211402154614952893_0004_m_000051_230
[2021-05-15 10:02:50,035] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Task committer attempt_202105151302055211402154614952893_0004_m_000051_230: needsTaskCommit() Task attempt_202105151302055211402154614952893_0004_m_000051_230: duration 0:00.001s
[2021-05-15 10:02:50,036] {docker.py:276} INFO - 21/05/15 13:02:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055211402154614952893_0004_m_000051_230
[2021-05-15 10:02:50,037] {docker.py:276} INFO - 21/05/15 13:02:50 INFO Executor: Finished task 51.0 in stage 4.0 (TID 230). 4544 bytes result sent to driver
[2021-05-15 10:02:50,038] {docker.py:276} INFO - 21/05/15 13:02:50 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 233) (4e8a4a26f4b5, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:50,039] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302058234527241583331640_0004_m_000049_228: needsTaskCommit() Task attempt_202105151302058234527241583331640_0004_m_000049_228
[2021-05-15 10:02:50,040] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Task committer attempt_202105151302058234527241583331640_0004_m_000049_228: needsTaskCommit() Task attempt_202105151302058234527241583331640_0004_m_000049_228: duration 0:00.001s
21/05/15 13:02:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058234527241583331640_0004_m_000049_228
[2021-05-15 10:02:50,041] {docker.py:276} INFO - 21/05/15 13:02:50 INFO Executor: Running task 54.0 in stage 4.0 (TID 233)
[2021-05-15 10:02:50,042] {docker.py:276} INFO - 21/05/15 13:02:50 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 230) in 2423 ms on 4e8a4a26f4b5 (executor driver) (51/200)
[2021-05-15 10:02:50,042] {docker.py:276} INFO - 21/05/15 13:02:50 INFO Executor: Finished task 49.0 in stage 4.0 (TID 228). 4544 bytes result sent to driver
[2021-05-15 10:02:50,043] {docker.py:276} INFO - 21/05/15 13:02:50 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 234) (4e8a4a26f4b5, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:50,044] {docker.py:276} INFO - 21/05/15 13:02:50 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 228) in 2974 ms on 4e8a4a26f4b5 (executor driver) (52/200)
[2021-05-15 10:02:50,044] {docker.py:276} INFO - 21/05/15 13:02:50 INFO Executor: Running task 55.0 in stage 4.0 (TID 234)
[2021-05-15 10:02:50,052] {docker.py:276} INFO - 21/05/15 13:02:50 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:50,052] {docker.py:276} INFO - 21/05/15 13:02:50 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:50,053] {docker.py:276} INFO - 21/05/15 13:02:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:50,054] {docker.py:276} INFO - 21/05/15 13:02:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:50,055] {docker.py:276} INFO - 21/05/15 13:02:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052410071132183474120_0004_m_000055_234, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052410071132183474120_0004_m_000055_234}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052410071132183474120_0004}; taskId=attempt_202105151302052410071132183474120_0004_m_000055_234, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59c95b8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:50,055] {docker.py:276} INFO - 21/05/15 13:02:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:50,056] {docker.py:276} INFO - 21/05/15 13:02:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:50,056] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302052410071132183474120_0004_m_000055_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052410071132183474120_0004_m_000055_234
[2021-05-15 10:02:50,056] {docker.py:276} INFO - 21/05/15 13:02:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:50,056] {docker.py:276} INFO - 21/05/15 13:02:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052500047997662747659_0004_m_000054_233, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052500047997662747659_0004_m_000054_233}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052500047997662747659_0004}; taskId=attempt_202105151302052500047997662747659_0004_m_000054_233, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ef43f61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:50,057] {docker.py:276} INFO - 21/05/15 13:02:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:50,057] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302052500047997662747659_0004_m_000054_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052500047997662747659_0004_m_000054_233
[2021-05-15 10:02:50,060] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Task committer attempt_202105151302052500047997662747659_0004_m_000054_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052500047997662747659_0004_m_000054_233 : duration 0:00.004s
[2021-05-15 10:02:50,062] {docker.py:276} INFO - 21/05/15 13:02:50 INFO StagingCommitter: Task committer attempt_202105151302052410071132183474120_0004_m_000055_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052410071132183474120_0004_m_000055_234 : duration 0:00.007s
[2021-05-15 10:02:51,098] {docker.py:276} INFO - 21/05/15 13:02:51 INFO StagingCommitter: Starting: Task committer attempt_202105151302052307788372022966974_0004_m_000052_231: needsTaskCommit() Task attempt_202105151302052307788372022966974_0004_m_000052_231
[2021-05-15 10:02:51,100] {docker.py:276} INFO - 21/05/15 13:02:51 INFO StagingCommitter: Task committer attempt_202105151302052307788372022966974_0004_m_000052_231: needsTaskCommit() Task attempt_202105151302052307788372022966974_0004_m_000052_231: duration 0:00.001s
21/05/15 13:02:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052307788372022966974_0004_m_000052_231
[2021-05-15 10:02:51,101] {docker.py:276} INFO - 21/05/15 13:02:51 INFO Executor: Finished task 52.0 in stage 4.0 (TID 231). 4544 bytes result sent to driver
[2021-05-15 10:02:51,103] {docker.py:276} INFO - 21/05/15 13:02:51 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 235) (4e8a4a26f4b5, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:51,104] {docker.py:276} INFO - 21/05/15 13:02:51 INFO Executor: Running task 56.0 in stage 4.0 (TID 235)
21/05/15 13:02:51 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 231) in 2563 ms on 4e8a4a26f4b5 (executor driver) (53/200)
[2021-05-15 10:02:51,117] {docker.py:276} INFO - 21/05/15 13:02:51 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:02:51,121] {docker.py:276} INFO - 21/05/15 13:02:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:02:51,122] {docker.py:276} INFO - 21/05/15 13:02:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:51,122] {docker.py:276} INFO - 21/05/15 13:02:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:51,123] {docker.py:276} INFO - 21/05/15 13:02:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054570729664196151041_0004_m_000056_235, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054570729664196151041_0004_m_000056_235}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054570729664196151041_0004}; taskId=attempt_202105151302054570729664196151041_0004_m_000056_235, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7578daa3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:51,123] {docker.py:276} INFO - 21/05/15 13:02:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:51,124] {docker.py:276} INFO - 21/05/15 13:02:51 INFO StagingCommitter: Starting: Task committer attempt_202105151302054570729664196151041_0004_m_000056_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054570729664196151041_0004_m_000056_235
[2021-05-15 10:02:51,126] {docker.py:276} INFO - 21/05/15 13:02:51 INFO StagingCommitter: Task committer attempt_202105151302054570729664196151041_0004_m_000056_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054570729664196151041_0004_m_000056_235 : duration 0:00.004s
[2021-05-15 10:02:52,230] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105151302056631144988794766138_0004_m_000053_232: needsTaskCommit() Task attempt_202105151302056631144988794766138_0004_m_000053_232
21/05/15 13:02:52 INFO StagingCommitter: Task committer attempt_202105151302056631144988794766138_0004_m_000053_232: needsTaskCommit() Task attempt_202105151302056631144988794766138_0004_m_000053_232: duration 0:00.001s
[2021-05-15 10:02:52,231] {docker.py:276} INFO - 21/05/15 13:02:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056631144988794766138_0004_m_000053_232
[2021-05-15 10:02:52,232] {docker.py:276} INFO - 21/05/15 13:02:52 INFO Executor: Finished task 53.0 in stage 4.0 (TID 232). 4544 bytes result sent to driver
[2021-05-15 10:02:52,234] {docker.py:276} INFO - 21/05/15 13:02:52 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 236) (4e8a4a26f4b5, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:52,235] {docker.py:276} INFO - 21/05/15 13:02:52 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 232) in 2514 ms on 4e8a4a26f4b5 (executor driver) (54/200)
[2021-05-15 10:02:52,235] {docker.py:276} INFO - 21/05/15 13:02:52 INFO Executor: Running task 57.0 in stage 4.0 (TID 236)
[2021-05-15 10:02:52,245] {docker.py:276} INFO - 21/05/15 13:02:52 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:52,247] {docker.py:276} INFO - 21/05/15 13:02:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054517538416993172434_0004_m_000057_236, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054517538416993172434_0004_m_000057_236}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054517538416993172434_0004}; taskId=attempt_202105151302054517538416993172434_0004_m_000057_236, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b4e7ed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:52,247] {docker.py:276} INFO - 21/05/15 13:02:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105151302054517538416993172434_0004_m_000057_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054517538416993172434_0004_m_000057_236
[2021-05-15 10:02:52,250] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Task committer attempt_202105151302054517538416993172434_0004_m_000057_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054517538416993172434_0004_m_000057_236 : duration 0:00.003s
[2021-05-15 10:02:52,392] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105151302052410071132183474120_0004_m_000055_234: needsTaskCommit() Task attempt_202105151302052410071132183474120_0004_m_000055_234
[2021-05-15 10:02:52,393] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Task committer attempt_202105151302052410071132183474120_0004_m_000055_234: needsTaskCommit() Task attempt_202105151302052410071132183474120_0004_m_000055_234: duration 0:00.001s
21/05/15 13:02:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052410071132183474120_0004_m_000055_234
[2021-05-15 10:02:52,395] {docker.py:276} INFO - 21/05/15 13:02:52 INFO Executor: Finished task 55.0 in stage 4.0 (TID 234). 4544 bytes result sent to driver
[2021-05-15 10:02:52,408] {docker.py:276} INFO - 21/05/15 13:02:52 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 237) (4e8a4a26f4b5, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:52,409] {docker.py:276} INFO - 21/05/15 13:02:52 INFO Executor: Running task 58.0 in stage 4.0 (TID 237)
[2021-05-15 10:02:52,409] {docker.py:276} INFO - 21/05/15 13:02:52 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 234) in 2369 ms on 4e8a4a26f4b5 (executor driver) (55/200)
[2021-05-15 10:02:52,417] {docker.py:276} INFO - 21/05/15 13:02:52 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:52,417] {docker.py:276} INFO - 21/05/15 13:02:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:02:52,419] {docker.py:276} INFO - 21/05/15 13:02:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:02:52,420] {docker.py:276} INFO - 21/05/15 13:02:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:52,420] {docker.py:276} INFO - 21/05/15 13:02:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:52,421] {docker.py:276} INFO - 21/05/15 13:02:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205609375808282497274_0004_m_000058_237, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205609375808282497274_0004_m_000058_237}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205609375808282497274_0004}; taskId=attempt_20210515130205609375808282497274_0004_m_000058_237, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@98bcbe2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:52,421] {docker.py:276} INFO - 21/05/15 13:02:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:52,422] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Starting: Task committer attempt_20210515130205609375808282497274_0004_m_000058_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205609375808282497274_0004_m_000058_237
[2021-05-15 10:02:52,425] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Task committer attempt_20210515130205609375808282497274_0004_m_000058_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205609375808282497274_0004_m_000058_237 : duration 0:00.003s
[2021-05-15 10:02:52,461] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105151302052500047997662747659_0004_m_000054_233: needsTaskCommit() Task attempt_202105151302052500047997662747659_0004_m_000054_233
[2021-05-15 10:02:52,462] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Task committer attempt_202105151302052500047997662747659_0004_m_000054_233: needsTaskCommit() Task attempt_202105151302052500047997662747659_0004_m_000054_233: duration 0:00.001s
21/05/15 13:02:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052500047997662747659_0004_m_000054_233
[2021-05-15 10:02:52,464] {docker.py:276} INFO - 21/05/15 13:02:52 INFO Executor: Finished task 54.0 in stage 4.0 (TID 233). 4587 bytes result sent to driver
[2021-05-15 10:02:52,465] {docker.py:276} INFO - 21/05/15 13:02:52 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 238) (4e8a4a26f4b5, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:52,467] {docker.py:276} INFO - 21/05/15 13:02:52 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 233) in 2431 ms on 4e8a4a26f4b5 (executor driver) (56/200)
[2021-05-15 10:02:52,468] {docker.py:276} INFO - 21/05/15 13:02:52 INFO Executor: Running task 59.0 in stage 4.0 (TID 238)
[2021-05-15 10:02:52,477] {docker.py:276} INFO - 21/05/15 13:02:52 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:52,479] {docker.py:276} INFO - 21/05/15 13:02:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:52,480] {docker.py:276} INFO - 21/05/15 13:02:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:52,480] {docker.py:276} INFO - 21/05/15 13:02:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058981791686021542132_0004_m_000059_238, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058981791686021542132_0004_m_000059_238}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058981791686021542132_0004}; taskId=attempt_202105151302058981791686021542132_0004_m_000059_238, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4da6a442}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:52,481] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Starting: Task committer attempt_202105151302058981791686021542132_0004_m_000059_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058981791686021542132_0004_m_000059_238
[2021-05-15 10:02:52,484] {docker.py:276} INFO - 21/05/15 13:02:52 INFO StagingCommitter: Task committer attempt_202105151302058981791686021542132_0004_m_000059_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058981791686021542132_0004_m_000059_238 : duration 0:00.004s
[2021-05-15 10:02:53,555] {docker.py:276} INFO - 21/05/15 13:02:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302054570729664196151041_0004_m_000056_235: needsTaskCommit() Task attempt_202105151302054570729664196151041_0004_m_000056_235
[2021-05-15 10:02:53,556] {docker.py:276} INFO - 21/05/15 13:02:53 INFO StagingCommitter: Task committer attempt_202105151302054570729664196151041_0004_m_000056_235: needsTaskCommit() Task attempt_202105151302054570729664196151041_0004_m_000056_235: duration 0:00.001s
[2021-05-15 10:02:53,557] {docker.py:276} INFO - 21/05/15 13:02:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054570729664196151041_0004_m_000056_235
[2021-05-15 10:02:53,557] {docker.py:276} INFO - 21/05/15 13:02:53 INFO Executor: Finished task 56.0 in stage 4.0 (TID 235). 4587 bytes result sent to driver
[2021-05-15 10:02:53,559] {docker.py:276} INFO - 21/05/15 13:02:53 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 239) (4e8a4a26f4b5, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:53,561] {docker.py:276} INFO - 21/05/15 13:02:53 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 235) in 2462 ms on 4e8a4a26f4b5 (executor driver) (57/200)
[2021-05-15 10:02:53,561] {docker.py:276} INFO - 21/05/15 13:02:53 INFO Executor: Running task 60.0 in stage 4.0 (TID 239)
[2021-05-15 10:02:53,571] {docker.py:276} INFO - 21/05/15 13:02:53 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:53,571] {docker.py:276} INFO - 21/05/15 13:02:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:53,573] {docker.py:276} INFO - 21/05/15 13:02:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:53,573] {docker.py:276} INFO - 21/05/15 13:02:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056449702967157333301_0004_m_000060_239, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056449702967157333301_0004_m_000060_239}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056449702967157333301_0004}; taskId=attempt_202105151302056449702967157333301_0004_m_000060_239, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5fa4ced2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:53,574] {docker.py:276} INFO - 21/05/15 13:02:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:53,574] {docker.py:276} INFO - 21/05/15 13:02:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302056449702967157333301_0004_m_000060_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056449702967157333301_0004_m_000060_239
[2021-05-15 10:02:53,577] {docker.py:276} INFO - 21/05/15 13:02:53 INFO StagingCommitter: Task committer attempt_202105151302056449702967157333301_0004_m_000060_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056449702967157333301_0004_m_000060_239 : duration 0:00.003s
[2021-05-15 10:02:54,757] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Starting: Task committer attempt_202105151302054517538416993172434_0004_m_000057_236: needsTaskCommit() Task attempt_202105151302054517538416993172434_0004_m_000057_236
[2021-05-15 10:02:54,758] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Task committer attempt_202105151302054517538416993172434_0004_m_000057_236: needsTaskCommit() Task attempt_202105151302054517538416993172434_0004_m_000057_236: duration 0:00.000s
21/05/15 13:02:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054517538416993172434_0004_m_000057_236
[2021-05-15 10:02:54,759] {docker.py:276} INFO - 21/05/15 13:02:54 INFO Executor: Finished task 57.0 in stage 4.0 (TID 236). 4587 bytes result sent to driver
[2021-05-15 10:02:54,760] {docker.py:276} INFO - 21/05/15 13:02:54 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 240) (4e8a4a26f4b5, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:54,761] {docker.py:276} INFO - 21/05/15 13:02:54 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 236) in 2531 ms on 4e8a4a26f4b5 (executor driver) (58/200)
21/05/15 13:02:54 INFO Executor: Running task 61.0 in stage 4.0 (TID 240)
[2021-05-15 10:02:54,770] {docker.py:276} INFO - 21/05/15 13:02:54 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:54,772] {docker.py:276} INFO - 21/05/15 13:02:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053266580281465971288_0004_m_000061_240, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053266580281465971288_0004_m_000061_240}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053266580281465971288_0004}; taskId=attempt_202105151302053266580281465971288_0004_m_000061_240, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7aac546e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:54,772] {docker.py:276} INFO - 21/05/15 13:02:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:54 INFO StagingCommitter: Starting: Task committer attempt_202105151302053266580281465971288_0004_m_000061_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053266580281465971288_0004_m_000061_240
[2021-05-15 10:02:54,775] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Task committer attempt_202105151302053266580281465971288_0004_m_000061_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053266580281465971288_0004_m_000061_240 : duration 0:00.003s
[2021-05-15 10:02:54,897] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Starting: Task committer attempt_20210515130205609375808282497274_0004_m_000058_237: needsTaskCommit() Task attempt_20210515130205609375808282497274_0004_m_000058_237
[2021-05-15 10:02:54,897] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Task committer attempt_20210515130205609375808282497274_0004_m_000058_237: needsTaskCommit() Task attempt_20210515130205609375808282497274_0004_m_000058_237: duration 0:00.000s
21/05/15 13:02:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205609375808282497274_0004_m_000058_237
[2021-05-15 10:02:54,898] {docker.py:276} INFO - 21/05/15 13:02:54 INFO Executor: Finished task 58.0 in stage 4.0 (TID 237). 4544 bytes result sent to driver
[2021-05-15 10:02:54,900] {docker.py:276} INFO - 21/05/15 13:02:54 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 241) (4e8a4a26f4b5, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:54,901] {docker.py:276} INFO - 21/05/15 13:02:54 INFO Executor: Running task 62.0 in stage 4.0 (TID 241)
[2021-05-15 10:02:54,902] {docker.py:276} INFO - 21/05/15 13:02:54 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 237) in 2495 ms on 4e8a4a26f4b5 (executor driver) (59/200)
[2021-05-15 10:02:54,910] {docker.py:276} INFO - 21/05/15 13:02:54 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:54,913] {docker.py:276} INFO - 21/05/15 13:02:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:54,913] {docker.py:276} INFO - 21/05/15 13:02:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:54,914] {docker.py:276} INFO - 21/05/15 13:02:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053735435868893073412_0004_m_000062_241, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053735435868893073412_0004_m_000062_241}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053735435868893073412_0004}; taskId=attempt_202105151302053735435868893073412_0004_m_000062_241, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6291b61d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:54,914] {docker.py:276} INFO - 21/05/15 13:02:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:54,914] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Starting: Task committer attempt_202105151302053735435868893073412_0004_m_000062_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053735435868893073412_0004_m_000062_241
[2021-05-15 10:02:54,917] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Task committer attempt_202105151302053735435868893073412_0004_m_000062_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053735435868893073412_0004_m_000062_241 : duration 0:00.003s
[2021-05-15 10:02:54,923] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Starting: Task committer attempt_202105151302058981791686021542132_0004_m_000059_238: needsTaskCommit() Task attempt_202105151302058981791686021542132_0004_m_000059_238
[2021-05-15 10:02:54,924] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Task committer attempt_202105151302058981791686021542132_0004_m_000059_238: needsTaskCommit() Task attempt_202105151302058981791686021542132_0004_m_000059_238: duration 0:00.001s
[2021-05-15 10:02:54,924] {docker.py:276} INFO - 21/05/15 13:02:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058981791686021542132_0004_m_000059_238
[2021-05-15 10:02:54,925] {docker.py:276} INFO - 21/05/15 13:02:54 INFO Executor: Finished task 59.0 in stage 4.0 (TID 238). 4544 bytes result sent to driver
[2021-05-15 10:02:54,926] {docker.py:276} INFO - 21/05/15 13:02:54 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 242) (4e8a4a26f4b5, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:54,927] {docker.py:276} INFO - 21/05/15 13:02:54 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 238) in 2465 ms on 4e8a4a26f4b5 (executor driver) (60/200)
[2021-05-15 10:02:54,928] {docker.py:276} INFO - 21/05/15 13:02:54 INFO Executor: Running task 63.0 in stage 4.0 (TID 242)
[2021-05-15 10:02:54,935] {docker.py:276} INFO - 21/05/15 13:02:54 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:54,937] {docker.py:276} INFO - 21/05/15 13:02:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205462186251238067426_0004_m_000063_242, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205462186251238067426_0004_m_000063_242}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205462186251238067426_0004}; taskId=attempt_20210515130205462186251238067426_0004_m_000063_242, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c1d652}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:54,937] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Starting: Task committer attempt_20210515130205462186251238067426_0004_m_000063_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205462186251238067426_0004_m_000063_242
[2021-05-15 10:02:54,941] {docker.py:276} INFO - 21/05/15 13:02:54 INFO StagingCommitter: Task committer attempt_20210515130205462186251238067426_0004_m_000063_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205462186251238067426_0004_m_000063_242 : duration 0:00.004s
[2021-05-15 10:02:55,900] {docker.py:276} INFO - 21/05/15 13:02:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302056449702967157333301_0004_m_000060_239: needsTaskCommit() Task attempt_202105151302056449702967157333301_0004_m_000060_239
21/05/15 13:02:55 INFO StagingCommitter: Task committer attempt_202105151302056449702967157333301_0004_m_000060_239: needsTaskCommit() Task attempt_202105151302056449702967157333301_0004_m_000060_239: duration 0:00.001s
21/05/15 13:02:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056449702967157333301_0004_m_000060_239
[2021-05-15 10:02:55,901] {docker.py:276} INFO - 21/05/15 13:02:55 INFO Executor: Finished task 60.0 in stage 4.0 (TID 239). 4544 bytes result sent to driver
[2021-05-15 10:02:55,905] {docker.py:276} INFO - 21/05/15 13:02:55 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 243) (4e8a4a26f4b5, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:02:55 INFO Executor: Running task 64.0 in stage 4.0 (TID 243)
21/05/15 13:02:55 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 239) in 2312 ms on 4e8a4a26f4b5 (executor driver) (61/200)
[2021-05-15 10:02:55,913] {docker.py:276} INFO - 21/05/15 13:02:55 INFO ShuffleBlockFetcherIterator: Getting 4 (10.4 KiB) non-empty blocks including 4 (10.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:55,916] {docker.py:276} INFO - 21/05/15 13:02:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055847283583110668681_0004_m_000064_243, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055847283583110668681_0004_m_000064_243}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055847283583110668681_0004}; taskId=attempt_202105151302055847283583110668681_0004_m_000064_243, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34281c00}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:55,916] {docker.py:276} INFO - 21/05/15 13:02:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302055847283583110668681_0004_m_000064_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055847283583110668681_0004_m_000064_243
[2021-05-15 10:02:55,919] {docker.py:276} INFO - 21/05/15 13:02:55 INFO StagingCommitter: Task committer attempt_202105151302055847283583110668681_0004_m_000064_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055847283583110668681_0004_m_000064_243 : duration 0:00.002s
[2021-05-15 10:02:56,964] {docker.py:276} INFO - 21/05/15 13:02:56 INFO StagingCommitter: Starting: Task committer attempt_202105151302053735435868893073412_0004_m_000062_241: needsTaskCommit() Task attempt_202105151302053735435868893073412_0004_m_000062_241
21/05/15 13:02:56 INFO StagingCommitter: Task committer attempt_202105151302053735435868893073412_0004_m_000062_241: needsTaskCommit() Task attempt_202105151302053735435868893073412_0004_m_000062_241: duration 0:00.000s
21/05/15 13:02:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053735435868893073412_0004_m_000062_241
[2021-05-15 10:02:56,965] {docker.py:276} INFO - 21/05/15 13:02:56 INFO Executor: Finished task 62.0 in stage 4.0 (TID 241). 4544 bytes result sent to driver
[2021-05-15 10:02:56,968] {docker.py:276} INFO - 21/05/15 13:02:56 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 244) (4e8a4a26f4b5, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:02:56 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 241) in 2034 ms on 4e8a4a26f4b5 (executor driver) (62/200)
21/05/15 13:02:56 INFO Executor: Running task 65.0 in stage 4.0 (TID 244)
[2021-05-15 10:02:56,975] {docker.py:276} INFO - 21/05/15 13:02:56 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:56,978] {docker.py:276} INFO - 21/05/15 13:02:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205260114604142934276_0004_m_000065_244, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205260114604142934276_0004_m_000065_244}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205260114604142934276_0004}; taskId=attempt_20210515130205260114604142934276_0004_m_000065_244, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39102f4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:56,978] {docker.py:276} INFO - 21/05/15 13:02:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:56,978] {docker.py:276} INFO - 21/05/15 13:02:56 INFO StagingCommitter: Starting: Task committer attempt_20210515130205260114604142934276_0004_m_000065_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205260114604142934276_0004_m_000065_244
[2021-05-15 10:02:56,981] {docker.py:276} INFO - 21/05/15 13:02:56 INFO StagingCommitter: Task committer attempt_20210515130205260114604142934276_0004_m_000065_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205260114604142934276_0004_m_000065_244 : duration 0:00.003s
[2021-05-15 10:02:57,178] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Starting: Task committer attempt_202105151302053266580281465971288_0004_m_000061_240: needsTaskCommit() Task attempt_202105151302053266580281465971288_0004_m_000061_240
[2021-05-15 10:02:57,178] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Task committer attempt_202105151302053266580281465971288_0004_m_000061_240: needsTaskCommit() Task attempt_202105151302053266580281465971288_0004_m_000061_240: duration 0:00.001s
21/05/15 13:02:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053266580281465971288_0004_m_000061_240
[2021-05-15 10:02:57,180] {docker.py:276} INFO - 21/05/15 13:02:57 INFO Executor: Finished task 61.0 in stage 4.0 (TID 240). 4544 bytes result sent to driver
[2021-05-15 10:02:57,182] {docker.py:276} INFO - 21/05/15 13:02:57 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 245) (4e8a4a26f4b5, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:57,183] {docker.py:276} INFO - 21/05/15 13:02:57 INFO Executor: Running task 66.0 in stage 4.0 (TID 245)
[2021-05-15 10:02:57,183] {docker.py:276} INFO - 21/05/15 13:02:57 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 240) in 2391 ms on 4e8a4a26f4b5 (executor driver) (63/200)
[2021-05-15 10:02:57,192] {docker.py:276} INFO - 21/05/15 13:02:57 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:02:57,193] {docker.py:276} INFO - 21/05/15 13:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:57,195] {docker.py:276} INFO - 21/05/15 13:02:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:57,195] {docker.py:276} INFO - 21/05/15 13:02:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051778291722861706761_0004_m_000066_245, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051778291722861706761_0004_m_000066_245}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051778291722861706761_0004}; taskId=attempt_202105151302051778291722861706761_0004_m_000066_245, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@425c49c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:57,196] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Starting: Task committer attempt_202105151302051778291722861706761_0004_m_000066_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051778291722861706761_0004_m_000066_245
[2021-05-15 10:02:57,199] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Task committer attempt_202105151302051778291722861706761_0004_m_000066_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051778291722861706761_0004_m_000066_245 : duration 0:00.003s
[2021-05-15 10:02:57,448] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Starting: Task committer attempt_20210515130205462186251238067426_0004_m_000063_242: needsTaskCommit() Task attempt_20210515130205462186251238067426_0004_m_000063_242
[2021-05-15 10:02:57,448] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Task committer attempt_20210515130205462186251238067426_0004_m_000063_242: needsTaskCommit() Task attempt_20210515130205462186251238067426_0004_m_000063_242: duration 0:00.000s
[2021-05-15 10:02:57,449] {docker.py:276} INFO - 21/05/15 13:02:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205462186251238067426_0004_m_000063_242
[2021-05-15 10:02:57,450] {docker.py:276} INFO - 21/05/15 13:02:57 INFO Executor: Finished task 63.0 in stage 4.0 (TID 242). 4544 bytes result sent to driver
[2021-05-15 10:02:57,452] {docker.py:276} INFO - 21/05/15 13:02:57 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 246) (4e8a4a26f4b5, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:02:57 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 242) in 2493 ms on 4e8a4a26f4b5 (executor driver) (64/200)
21/05/15 13:02:57 INFO Executor: Running task 67.0 in stage 4.0 (TID 246)
[2021-05-15 10:02:57,462] {docker.py:276} INFO - 21/05/15 13:02:57 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:57,464] {docker.py:276} INFO - 21/05/15 13:02:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054491938900454900038_0004_m_000067_246, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054491938900454900038_0004_m_000067_246}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054491938900454900038_0004}; taskId=attempt_202105151302054491938900454900038_0004_m_000067_246, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26037bc0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:57,464] {docker.py:276} INFO - 21/05/15 13:02:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:57 INFO StagingCommitter: Starting: Task committer attempt_202105151302054491938900454900038_0004_m_000067_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054491938900454900038_0004_m_000067_246
[2021-05-15 10:02:57,468] {docker.py:276} INFO - 21/05/15 13:02:57 INFO StagingCommitter: Task committer attempt_202105151302054491938900454900038_0004_m_000067_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054491938900454900038_0004_m_000067_246 : duration 0:00.003s
[2021-05-15 10:02:58,187] {docker.py:276} INFO - 21/05/15 13:02:58 INFO StagingCommitter: Starting: Task committer attempt_202105151302055847283583110668681_0004_m_000064_243: needsTaskCommit() Task attempt_202105151302055847283583110668681_0004_m_000064_243
21/05/15 13:02:58 INFO StagingCommitter: Task committer attempt_202105151302055847283583110668681_0004_m_000064_243: needsTaskCommit() Task attempt_202105151302055847283583110668681_0004_m_000064_243: duration 0:00.000s
21/05/15 13:02:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055847283583110668681_0004_m_000064_243
[2021-05-15 10:02:58,188] {docker.py:276} INFO - 21/05/15 13:02:58 INFO Executor: Finished task 64.0 in stage 4.0 (TID 243). 4544 bytes result sent to driver
[2021-05-15 10:02:58,189] {docker.py:276} INFO - 21/05/15 13:02:58 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 247) (4e8a4a26f4b5, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:58,191] {docker.py:276} INFO - 21/05/15 13:02:58 INFO Executor: Running task 68.0 in stage 4.0 (TID 247)
21/05/15 13:02:58 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 243) in 2292 ms on 4e8a4a26f4b5 (executor driver) (65/200)
[2021-05-15 10:02:58,200] {docker.py:276} INFO - 21/05/15 13:02:58 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:58,202] {docker.py:276} INFO - 21/05/15 13:02:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:58,202] {docker.py:276} INFO - 21/05/15 13:02:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053361803256606593121_0004_m_000068_247, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053361803256606593121_0004_m_000068_247}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053361803256606593121_0004}; taskId=attempt_202105151302053361803256606593121_0004_m_000068_247, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@400525d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:02:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:02:58 INFO StagingCommitter: Starting: Task committer attempt_202105151302053361803256606593121_0004_m_000068_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053361803256606593121_0004_m_000068_247
[2021-05-15 10:02:58,205] {docker.py:276} INFO - 21/05/15 13:02:58 INFO StagingCommitter: Task committer attempt_202105151302053361803256606593121_0004_m_000068_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053361803256606593121_0004_m_000068_247 : duration 0:00.003s
[2021-05-15 10:02:59,497] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Starting: Task committer attempt_20210515130205260114604142934276_0004_m_000065_244: needsTaskCommit() Task attempt_20210515130205260114604142934276_0004_m_000065_244
[2021-05-15 10:02:59,498] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Task committer attempt_20210515130205260114604142934276_0004_m_000065_244: needsTaskCommit() Task attempt_20210515130205260114604142934276_0004_m_000065_244: duration 0:00.000s
21/05/15 13:02:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205260114604142934276_0004_m_000065_244
[2021-05-15 10:02:59,500] {docker.py:276} INFO - 21/05/15 13:02:59 INFO Executor: Finished task 65.0 in stage 4.0 (TID 244). 4544 bytes result sent to driver
[2021-05-15 10:02:59,501] {docker.py:276} INFO - 21/05/15 13:02:59 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 248) (4e8a4a26f4b5, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:59,502] {docker.py:276} INFO - 21/05/15 13:02:59 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 244) in 2539 ms on 4e8a4a26f4b5 (executor driver) (66/200)
[2021-05-15 10:02:59,503] {docker.py:276} INFO - 21/05/15 13:02:59 INFO Executor: Running task 69.0 in stage 4.0 (TID 248)
[2021-05-15 10:02:59,512] {docker.py:276} INFO - 21/05/15 13:02:59 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:59,514] {docker.py:276} INFO - 21/05/15 13:02:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:59,515] {docker.py:276} INFO - 21/05/15 13:02:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:59,515] {docker.py:276} INFO - 21/05/15 13:02:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205846274223156714061_0004_m_000069_248, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205846274223156714061_0004_m_000069_248}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205846274223156714061_0004}; taskId=attempt_20210515130205846274223156714061_0004_m_000069_248, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4593f1c0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:59,515] {docker.py:276} INFO - 21/05/15 13:02:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:59,516] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Starting: Task committer attempt_20210515130205846274223156714061_0004_m_000069_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205846274223156714061_0004_m_000069_248
[2021-05-15 10:02:59,518] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Task committer attempt_20210515130205846274223156714061_0004_m_000069_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205846274223156714061_0004_m_000069_248 : duration 0:00.003s
[2021-05-15 10:02:59,700] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Starting: Task committer attempt_202105151302051778291722861706761_0004_m_000066_245: needsTaskCommit() Task attempt_202105151302051778291722861706761_0004_m_000066_245
[2021-05-15 10:02:59,701] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Task committer attempt_202105151302051778291722861706761_0004_m_000066_245: needsTaskCommit() Task attempt_202105151302051778291722861706761_0004_m_000066_245: duration 0:00.001s
21/05/15 13:02:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051778291722861706761_0004_m_000066_245
[2021-05-15 10:02:59,703] {docker.py:276} INFO - 21/05/15 13:02:59 INFO Executor: Finished task 66.0 in stage 4.0 (TID 245). 4544 bytes result sent to driver
[2021-05-15 10:02:59,715] {docker.py:276} INFO - 21/05/15 13:02:59 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 249) (4e8a4a26f4b5, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:59,715] {docker.py:276} INFO - 21/05/15 13:02:59 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 245) in 2538 ms on 4e8a4a26f4b5 (executor driver) (67/200)
21/05/15 13:02:59 INFO Executor: Running task 70.0 in stage 4.0 (TID 249)
[2021-05-15 10:02:59,723] {docker.py:276} INFO - 21/05/15 13:02:59 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:59,725] {docker.py:276} INFO - 21/05/15 13:02:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:02:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056050434724810519008_0004_m_000070_249, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056050434724810519008_0004_m_000070_249}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056050434724810519008_0004}; taskId=attempt_202105151302056050434724810519008_0004_m_000070_249, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6aefe3ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:59,725] {docker.py:276} INFO - 21/05/15 13:02:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:59,726] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Starting: Task committer attempt_202105151302056050434724810519008_0004_m_000070_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056050434724810519008_0004_m_000070_249
[2021-05-15 10:02:59,728] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Task committer attempt_202105151302056050434724810519008_0004_m_000070_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056050434724810519008_0004_m_000070_249 : duration 0:00.003s
[2021-05-15 10:02:59,793] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Starting: Task committer attempt_202105151302054491938900454900038_0004_m_000067_246: needsTaskCommit() Task attempt_202105151302054491938900454900038_0004_m_000067_246
[2021-05-15 10:02:59,794] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Task committer attempt_202105151302054491938900454900038_0004_m_000067_246: needsTaskCommit() Task attempt_202105151302054491938900454900038_0004_m_000067_246: duration 0:00.001s
[2021-05-15 10:02:59,795] {docker.py:276} INFO - 21/05/15 13:02:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054491938900454900038_0004_m_000067_246
[2021-05-15 10:02:59,797] {docker.py:276} INFO - 21/05/15 13:02:59 INFO Executor: Finished task 67.0 in stage 4.0 (TID 246). 4587 bytes result sent to driver
[2021-05-15 10:02:59,798] {docker.py:276} INFO - 21/05/15 13:02:59 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 250) (4e8a4a26f4b5, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:02:59,799] {docker.py:276} INFO - 21/05/15 13:02:59 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 246) in 2352 ms on 4e8a4a26f4b5 (executor driver) (68/200)
[2021-05-15 10:02:59,800] {docker.py:276} INFO - 21/05/15 13:02:59 INFO Executor: Running task 71.0 in stage 4.0 (TID 250)
[2021-05-15 10:02:59,809] {docker.py:276} INFO - 21/05/15 13:02:59 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:02:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:02:59,811] {docker.py:276} INFO - 21/05/15 13:02:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:02:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:02:59,811] {docker.py:276} INFO - 21/05/15 13:02:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:02:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055633734702936570848_0004_m_000071_250, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055633734702936570848_0004_m_000071_250}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055633734702936570848_0004}; taskId=attempt_202105151302055633734702936570848_0004_m_000071_250, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b027430}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:02:59,812] {docker.py:276} INFO - 21/05/15 13:02:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:02:59,812] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Starting: Task committer attempt_202105151302055633734702936570848_0004_m_000071_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055633734702936570848_0004_m_000071_250
[2021-05-15 10:02:59,815] {docker.py:276} INFO - 21/05/15 13:02:59 INFO StagingCommitter: Task committer attempt_202105151302055633734702936570848_0004_m_000071_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055633734702936570848_0004_m_000071_250 : duration 0:00.003s
[2021-05-15 10:03:00,895] {docker.py:276} INFO - 21/05/15 13:03:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302053361803256606593121_0004_m_000068_247: needsTaskCommit() Task attempt_202105151302053361803256606593121_0004_m_000068_247
21/05/15 13:03:00 INFO StagingCommitter: Task committer attempt_202105151302053361803256606593121_0004_m_000068_247: needsTaskCommit() Task attempt_202105151302053361803256606593121_0004_m_000068_247: duration 0:00.001s
[2021-05-15 10:03:00,896] {docker.py:276} INFO - 21/05/15 13:03:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053361803256606593121_0004_m_000068_247
[2021-05-15 10:03:00,898] {docker.py:276} INFO - 21/05/15 13:03:00 INFO Executor: Finished task 68.0 in stage 4.0 (TID 247). 4587 bytes result sent to driver
[2021-05-15 10:03:00,899] {docker.py:276} INFO - 21/05/15 13:03:00 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 251) (4e8a4a26f4b5, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:00,900] {docker.py:276} INFO - 21/05/15 13:03:00 INFO Executor: Running task 72.0 in stage 4.0 (TID 251)
21/05/15 13:03:00 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 247) in 2714 ms on 4e8a4a26f4b5 (executor driver) (69/200)
[2021-05-15 10:03:00,910] {docker.py:276} INFO - 21/05/15 13:03:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:00,912] {docker.py:276} INFO - 21/05/15 13:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205897397834070282603_0004_m_000072_251, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205897397834070282603_0004_m_000072_251}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205897397834070282603_0004}; taskId=attempt_20210515130205897397834070282603_0004_m_000072_251, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65d33e75}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:00,912] {docker.py:276} INFO - 21/05/15 13:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:00 INFO StagingCommitter: Starting: Task committer attempt_20210515130205897397834070282603_0004_m_000072_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205897397834070282603_0004_m_000072_251
[2021-05-15 10:03:00,915] {docker.py:276} INFO - 21/05/15 13:03:00 INFO StagingCommitter: Task committer attempt_20210515130205897397834070282603_0004_m_000072_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205897397834070282603_0004_m_000072_251 : duration 0:00.004s
[2021-05-15 10:03:01,955] {docker.py:276} INFO - 21/05/15 13:03:01 INFO StagingCommitter: Starting: Task committer attempt_20210515130205846274223156714061_0004_m_000069_248: needsTaskCommit() Task attempt_20210515130205846274223156714061_0004_m_000069_248
[2021-05-15 10:03:01,956] {docker.py:276} INFO - 21/05/15 13:03:01 INFO StagingCommitter: Task committer attempt_20210515130205846274223156714061_0004_m_000069_248: needsTaskCommit() Task attempt_20210515130205846274223156714061_0004_m_000069_248: duration 0:00.000s
[2021-05-15 10:03:01,957] {docker.py:276} INFO - 21/05/15 13:03:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205846274223156714061_0004_m_000069_248
[2021-05-15 10:03:01,959] {docker.py:276} INFO - 21/05/15 13:03:01 INFO Executor: Finished task 69.0 in stage 4.0 (TID 248). 4587 bytes result sent to driver
[2021-05-15 10:03:01,960] {docker.py:276} INFO - 21/05/15 13:03:01 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 252) (4e8a4a26f4b5, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:01,961] {docker.py:276} INFO - 21/05/15 13:03:01 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 248) in 2462 ms on 4e8a4a26f4b5 (executor driver) (70/200)
21/05/15 13:03:01 INFO Executor: Running task 73.0 in stage 4.0 (TID 252)
[2021-05-15 10:03:01,971] {docker.py:276} INFO - 21/05/15 13:03:01 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:01,973] {docker.py:276} INFO - 21/05/15 13:03:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:01,973] {docker.py:276} INFO - 21/05/15 13:03:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205280185452257795252_0004_m_000073_252, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205280185452257795252_0004_m_000073_252}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205280185452257795252_0004}; taskId=attempt_20210515130205280185452257795252_0004_m_000073_252, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4378e119}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:01,974] {docker.py:276} INFO - 21/05/15 13:03:01 INFO StagingCommitter: Starting: Task committer attempt_20210515130205280185452257795252_0004_m_000073_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205280185452257795252_0004_m_000073_252
[2021-05-15 10:03:01,977] {docker.py:276} INFO - 21/05/15 13:03:01 INFO StagingCommitter: Task committer attempt_20210515130205280185452257795252_0004_m_000073_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205280185452257795252_0004_m_000073_252 : duration 0:00.004s
[2021-05-15 10:03:02,267] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Starting: Task committer attempt_202105151302056050434724810519008_0004_m_000070_249: needsTaskCommit() Task attempt_202105151302056050434724810519008_0004_m_000070_249
[2021-05-15 10:03:02,268] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Task committer attempt_202105151302056050434724810519008_0004_m_000070_249: needsTaskCommit() Task attempt_202105151302056050434724810519008_0004_m_000070_249: duration 0:00.001s
21/05/15 13:03:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056050434724810519008_0004_m_000070_249
[2021-05-15 10:03:02,269] {docker.py:276} INFO - 21/05/15 13:03:02 INFO Executor: Finished task 70.0 in stage 4.0 (TID 249). 4544 bytes result sent to driver
[2021-05-15 10:03:02,271] {docker.py:276} INFO - 21/05/15 13:03:02 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 253) (4e8a4a26f4b5, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:02,272] {docker.py:276} INFO - 21/05/15 13:03:02 INFO Executor: Running task 74.0 in stage 4.0 (TID 253)
21/05/15 13:03:02 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 249) in 2560 ms on 4e8a4a26f4b5 (executor driver) (71/200)
[2021-05-15 10:03:02,282] {docker.py:276} INFO - 21/05/15 13:03:02 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:02,284] {docker.py:276} INFO - 21/05/15 13:03:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052526324197888119186_0004_m_000074_253, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052526324197888119186_0004_m_000074_253}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052526324197888119186_0004}; taskId=attempt_202105151302052526324197888119186_0004_m_000074_253, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61cdf94a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:02,284] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Starting: Task committer attempt_202105151302052526324197888119186_0004_m_000074_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052526324197888119186_0004_m_000074_253
[2021-05-15 10:03:02,287] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Task committer attempt_202105151302052526324197888119186_0004_m_000074_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052526324197888119186_0004_m_000074_253 : duration 0:00.003s
[2021-05-15 10:03:02,334] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Starting: Task committer attempt_202105151302055633734702936570848_0004_m_000071_250: needsTaskCommit() Task attempt_202105151302055633734702936570848_0004_m_000071_250
[2021-05-15 10:03:02,336] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Task committer attempt_202105151302055633734702936570848_0004_m_000071_250: needsTaskCommit() Task attempt_202105151302055633734702936570848_0004_m_000071_250: duration 0:00.000s
21/05/15 13:03:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055633734702936570848_0004_m_000071_250
[2021-05-15 10:03:02,337] {docker.py:276} INFO - 21/05/15 13:03:02 INFO Executor: Finished task 71.0 in stage 4.0 (TID 250). 4544 bytes result sent to driver
[2021-05-15 10:03:02,338] {docker.py:276} INFO - 21/05/15 13:03:02 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 250) in 2542 ms on 4e8a4a26f4b5 (executor driver) (72/200)
[2021-05-15 10:03:02,340] {docker.py:276} INFO - 21/05/15 13:03:02 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 254) (4e8a4a26f4b5, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:02,341] {docker.py:276} INFO - 21/05/15 13:03:02 INFO Executor: Running task 75.0 in stage 4.0 (TID 254)
[2021-05-15 10:03:02,350] {docker.py:276} INFO - 21/05/15 13:03:02 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:02,351] {docker.py:276} INFO - 21/05/15 13:03:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:02,353] {docker.py:276} INFO - 21/05/15 13:03:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058922138168552624580_0004_m_000075_254, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058922138168552624580_0004_m_000075_254}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058922138168552624580_0004}; taskId=attempt_202105151302058922138168552624580_0004_m_000075_254, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2da59ee5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:02,353] {docker.py:276} INFO - 21/05/15 13:03:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:02 INFO StagingCommitter: Starting: Task committer attempt_202105151302058922138168552624580_0004_m_000075_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058922138168552624580_0004_m_000075_254
[2021-05-15 10:03:02,356] {docker.py:276} INFO - 21/05/15 13:03:02 INFO StagingCommitter: Task committer attempt_202105151302058922138168552624580_0004_m_000075_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058922138168552624580_0004_m_000075_254 : duration 0:00.003s
[2021-05-15 10:03:03,357] {docker.py:276} INFO - 21/05/15 13:03:03 INFO StagingCommitter: Starting: Task committer attempt_20210515130205897397834070282603_0004_m_000072_251: needsTaskCommit() Task attempt_20210515130205897397834070282603_0004_m_000072_251
21/05/15 13:03:03 INFO StagingCommitter: Task committer attempt_20210515130205897397834070282603_0004_m_000072_251: needsTaskCommit() Task attempt_20210515130205897397834070282603_0004_m_000072_251: duration 0:00.001s
21/05/15 13:03:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205897397834070282603_0004_m_000072_251
[2021-05-15 10:03:03,359] {docker.py:276} INFO - 21/05/15 13:03:03 INFO Executor: Finished task 72.0 in stage 4.0 (TID 251). 4544 bytes result sent to driver
[2021-05-15 10:03:03,362] {docker.py:276} INFO - 21/05/15 13:03:03 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 255) (4e8a4a26f4b5, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:03,363] {docker.py:276} INFO - 21/05/15 13:03:03 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 251) in 2468 ms on 4e8a4a26f4b5 (executor driver) (73/200)
[2021-05-15 10:03:03,364] {docker.py:276} INFO - 21/05/15 13:03:03 INFO Executor: Running task 76.0 in stage 4.0 (TID 255)
[2021-05-15 10:03:03,373] {docker.py:276} INFO - 21/05/15 13:03:03 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:03,375] {docker.py:276} INFO - 21/05/15 13:03:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205693671234541521274_0004_m_000076_255, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205693671234541521274_0004_m_000076_255}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205693671234541521274_0004}; taskId=attempt_20210515130205693671234541521274_0004_m_000076_255, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36541dd3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:03,376] {docker.py:276} INFO - 21/05/15 13:03:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:03,376] {docker.py:276} INFO - 21/05/15 13:03:03 INFO StagingCommitter: Starting: Task committer attempt_20210515130205693671234541521274_0004_m_000076_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205693671234541521274_0004_m_000076_255
[2021-05-15 10:03:03,379] {docker.py:276} INFO - 21/05/15 13:03:03 INFO StagingCommitter: Task committer attempt_20210515130205693671234541521274_0004_m_000076_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205693671234541521274_0004_m_000076_255 : duration 0:00.004s
[2021-05-15 10:03:04,426] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Starting: Task committer attempt_20210515130205280185452257795252_0004_m_000073_252: needsTaskCommit() Task attempt_20210515130205280185452257795252_0004_m_000073_252
[2021-05-15 10:03:04,427] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Task committer attempt_20210515130205280185452257795252_0004_m_000073_252: needsTaskCommit() Task attempt_20210515130205280185452257795252_0004_m_000073_252: duration 0:00.001s
[2021-05-15 10:03:04,428] {docker.py:276} INFO - 21/05/15 13:03:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205280185452257795252_0004_m_000073_252
[2021-05-15 10:03:04,429] {docker.py:276} INFO - 21/05/15 13:03:04 INFO Executor: Finished task 73.0 in stage 4.0 (TID 252). 4544 bytes result sent to driver
[2021-05-15 10:03:04,431] {docker.py:276} INFO - 21/05/15 13:03:04 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 256) (4e8a4a26f4b5, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:04,431] {docker.py:276} INFO - 21/05/15 13:03:04 INFO Executor: Running task 77.0 in stage 4.0 (TID 256)
[2021-05-15 10:03:04,432] {docker.py:276} INFO - 21/05/15 13:03:04 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 252) in 2476 ms on 4e8a4a26f4b5 (executor driver) (74/200)
[2021-05-15 10:03:04,442] {docker.py:276} INFO - 21/05/15 13:03:04 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:04,444] {docker.py:276} INFO - 21/05/15 13:03:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056095085118190288375_0004_m_000077_256, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056095085118190288375_0004_m_000077_256}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056095085118190288375_0004}; taskId=attempt_202105151302056095085118190288375_0004_m_000077_256, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4aa5e228}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:04,444] {docker.py:276} INFO - 21/05/15 13:03:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:04 INFO StagingCommitter: Starting: Task committer attempt_202105151302056095085118190288375_0004_m_000077_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056095085118190288375_0004_m_000077_256
[2021-05-15 10:03:04,446] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Task committer attempt_202105151302056095085118190288375_0004_m_000077_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056095085118190288375_0004_m_000077_256 : duration 0:00.002s
[2021-05-15 10:03:04,662] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Starting: Task committer attempt_202105151302058922138168552624580_0004_m_000075_254: needsTaskCommit() Task attempt_202105151302058922138168552624580_0004_m_000075_254
21/05/15 13:03:04 INFO StagingCommitter: Task committer attempt_202105151302058922138168552624580_0004_m_000075_254: needsTaskCommit() Task attempt_202105151302058922138168552624580_0004_m_000075_254: duration 0:00.001s
[2021-05-15 10:03:04,663] {docker.py:276} INFO - 21/05/15 13:03:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058922138168552624580_0004_m_000075_254
[2021-05-15 10:03:04,665] {docker.py:276} INFO - 21/05/15 13:03:04 INFO Executor: Finished task 75.0 in stage 4.0 (TID 254). 4544 bytes result sent to driver
[2021-05-15 10:03:04,666] {docker.py:276} INFO - 21/05/15 13:03:04 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 257) (4e8a4a26f4b5, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:04,668] {docker.py:276} INFO - 21/05/15 13:03:04 INFO Executor: Running task 78.0 in stage 4.0 (TID 257)
[2021-05-15 10:03:04,669] {docker.py:276} INFO - 21/05/15 13:03:04 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 254) in 2332 ms on 4e8a4a26f4b5 (executor driver) (75/200)
[2021-05-15 10:03:04,673] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Starting: Task committer attempt_202105151302052526324197888119186_0004_m_000074_253: needsTaskCommit() Task attempt_202105151302052526324197888119186_0004_m_000074_253
[2021-05-15 10:03:04,674] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Task committer attempt_202105151302052526324197888119186_0004_m_000074_253: needsTaskCommit() Task attempt_202105151302052526324197888119186_0004_m_000074_253: duration 0:00.001s
[2021-05-15 10:03:04,674] {docker.py:276} INFO - 21/05/15 13:03:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052526324197888119186_0004_m_000074_253
[2021-05-15 10:03:04,675] {docker.py:276} INFO - 21/05/15 13:03:04 INFO Executor: Finished task 74.0 in stage 4.0 (TID 253). 4544 bytes result sent to driver
[2021-05-15 10:03:04,677] {docker.py:276} INFO - 21/05/15 13:03:04 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 258) (4e8a4a26f4b5, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:04,678] {docker.py:276} INFO - 21/05/15 13:03:04 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 253) in 2411 ms on 4e8a4a26f4b5 (executor driver) (76/200)
21/05/15 13:03:04 INFO Executor: Running task 79.0 in stage 4.0 (TID 258)
[2021-05-15 10:03:04,681] {docker.py:276} INFO - 21/05/15 13:03:04 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:04,683] {docker.py:276} INFO - 21/05/15 13:03:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052396294293221421530_0004_m_000078_257, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052396294293221421530_0004_m_000078_257}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052396294293221421530_0004}; taskId=attempt_202105151302052396294293221421530_0004_m_000078_257, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6128f1fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:04 INFO StagingCommitter: Starting: Task committer attempt_202105151302052396294293221421530_0004_m_000078_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052396294293221421530_0004_m_000078_257
[2021-05-15 10:03:04,686] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Task committer attempt_202105151302052396294293221421530_0004_m_000078_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052396294293221421530_0004_m_000078_257 : duration 0:00.003s
[2021-05-15 10:03:04,687] {docker.py:276} INFO - 21/05/15 13:03:04 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:04,687] {docker.py:276} INFO - 21/05/15 13:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:03:04,689] {docker.py:276} INFO - 21/05/15 13:03:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:03:04,689] {docker.py:276} INFO - 21/05/15 13:03:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:04,690] {docker.py:276} INFO - 21/05/15 13:03:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:04,690] {docker.py:276} INFO - 21/05/15 13:03:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053281524149382867119_0004_m_000079_258, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053281524149382867119_0004_m_000079_258}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053281524149382867119_0004}; taskId=attempt_202105151302053281524149382867119_0004_m_000079_258, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3babd372}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:04,690] {docker.py:276} INFO - 21/05/15 13:03:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:04,691] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Starting: Task committer attempt_202105151302053281524149382867119_0004_m_000079_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053281524149382867119_0004_m_000079_258
[2021-05-15 10:03:04,694] {docker.py:276} INFO - 21/05/15 13:03:04 INFO StagingCommitter: Task committer attempt_202105151302053281524149382867119_0004_m_000079_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053281524149382867119_0004_m_000079_258 : duration 0:00.003s
[2021-05-15 10:03:05,843] {docker.py:276} INFO - 21/05/15 13:03:05 INFO StagingCommitter: Starting: Task committer attempt_20210515130205693671234541521274_0004_m_000076_255: needsTaskCommit() Task attempt_20210515130205693671234541521274_0004_m_000076_255
21/05/15 13:03:05 INFO StagingCommitter: Task committer attempt_20210515130205693671234541521274_0004_m_000076_255: needsTaskCommit() Task attempt_20210515130205693671234541521274_0004_m_000076_255: duration 0:00.000s
21/05/15 13:03:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205693671234541521274_0004_m_000076_255
[2021-05-15 10:03:05,844] {docker.py:276} INFO - 21/05/15 13:03:05 INFO Executor: Finished task 76.0 in stage 4.0 (TID 255). 4544 bytes result sent to driver
[2021-05-15 10:03:05,846] {docker.py:276} INFO - 21/05/15 13:03:05 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 259) (4e8a4a26f4b5, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:05,848] {docker.py:276} INFO - 21/05/15 13:03:05 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 255) in 2491 ms on 4e8a4a26f4b5 (executor driver) (77/200)
[2021-05-15 10:03:05,848] {docker.py:276} INFO - 21/05/15 13:03:05 INFO Executor: Running task 80.0 in stage 4.0 (TID 259)
[2021-05-15 10:03:05,859] {docker.py:276} INFO - 21/05/15 13:03:05 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:05,861] {docker.py:276} INFO - 21/05/15 13:03:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205604323134061655049_0004_m_000080_259, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205604323134061655049_0004_m_000080_259}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205604323134061655049_0004}; taskId=attempt_20210515130205604323134061655049_0004_m_000080_259, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78c65e4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:05,861] {docker.py:276} INFO - 21/05/15 13:03:05 INFO StagingCommitter: Starting: Task committer attempt_20210515130205604323134061655049_0004_m_000080_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205604323134061655049_0004_m_000080_259
[2021-05-15 10:03:05,865] {docker.py:276} INFO - 21/05/15 13:03:05 INFO StagingCommitter: Task committer attempt_20210515130205604323134061655049_0004_m_000080_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205604323134061655049_0004_m_000080_259 : duration 0:00.003s
[2021-05-15 10:03:06,756] {docker.py:276} INFO - 21/05/15 13:03:06 INFO StagingCommitter: Starting: Task committer attempt_202105151302052396294293221421530_0004_m_000078_257: needsTaskCommit() Task attempt_202105151302052396294293221421530_0004_m_000078_257
21/05/15 13:03:06 INFO StagingCommitter: Task committer attempt_202105151302052396294293221421530_0004_m_000078_257: needsTaskCommit() Task attempt_202105151302052396294293221421530_0004_m_000078_257: duration 0:00.001s
[2021-05-15 10:03:06,757] {docker.py:276} INFO - 21/05/15 13:03:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052396294293221421530_0004_m_000078_257
[2021-05-15 10:03:06,759] {docker.py:276} INFO - 21/05/15 13:03:06 INFO Executor: Finished task 78.0 in stage 4.0 (TID 257). 4544 bytes result sent to driver
[2021-05-15 10:03:06,761] {docker.py:276} INFO - 21/05/15 13:03:06 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 260) (4e8a4a26f4b5, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:06,762] {docker.py:276} INFO - 21/05/15 13:03:06 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 257) in 2098 ms on 4e8a4a26f4b5 (executor driver) (78/200)
[2021-05-15 10:03:06,763] {docker.py:276} INFO - 21/05/15 13:03:06 INFO Executor: Running task 81.0 in stage 4.0 (TID 260)
[2021-05-15 10:03:06,772] {docker.py:276} INFO - 21/05/15 13:03:06 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:06,774] {docker.py:276} INFO - 21/05/15 13:03:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:06,775] {docker.py:276} INFO - 21/05/15 13:03:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058653773029949975737_0004_m_000081_260, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058653773029949975737_0004_m_000081_260}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058653773029949975737_0004}; taskId=attempt_202105151302058653773029949975737_0004_m_000081_260, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@540f439e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:06,775] {docker.py:276} INFO - 21/05/15 13:03:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:06,776] {docker.py:276} INFO - 21/05/15 13:03:06 INFO StagingCommitter: Starting: Task committer attempt_202105151302058653773029949975737_0004_m_000081_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058653773029949975737_0004_m_000081_260
[2021-05-15 10:03:06,779] {docker.py:276} INFO - 21/05/15 13:03:06 INFO StagingCommitter: Task committer attempt_202105151302058653773029949975737_0004_m_000081_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058653773029949975737_0004_m_000081_260 : duration 0:00.003s
[2021-05-15 10:03:06,983] {docker.py:276} INFO - 21/05/15 13:03:06 INFO StagingCommitter: Starting: Task committer attempt_202105151302056095085118190288375_0004_m_000077_256: needsTaskCommit() Task attempt_202105151302056095085118190288375_0004_m_000077_256
[2021-05-15 10:03:06,983] {docker.py:276} INFO - 21/05/15 13:03:06 INFO StagingCommitter: Task committer attempt_202105151302056095085118190288375_0004_m_000077_256: needsTaskCommit() Task attempt_202105151302056095085118190288375_0004_m_000077_256: duration 0:00.001s
21/05/15 13:03:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056095085118190288375_0004_m_000077_256
[2021-05-15 10:03:06,984] {docker.py:276} INFO - 21/05/15 13:03:06 INFO Executor: Finished task 77.0 in stage 4.0 (TID 256). 4544 bytes result sent to driver
[2021-05-15 10:03:06,986] {docker.py:276} INFO - 21/05/15 13:03:06 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 261) (4e8a4a26f4b5, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:06,987] {docker.py:276} INFO - 21/05/15 13:03:07 INFO Executor: Running task 82.0 in stage 4.0 (TID 261)
[2021-05-15 10:03:06,988] {docker.py:276} INFO - 21/05/15 13:03:07 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 256) in 2561 ms on 4e8a4a26f4b5 (executor driver) (79/200)
[2021-05-15 10:03:07,006] {docker.py:276} INFO - 21/05/15 13:03:07 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:07,007] {docker.py:276} INFO - 21/05/15 13:03:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:07,008] {docker.py:276} INFO - 21/05/15 13:03:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:07,009] {docker.py:276} INFO - 21/05/15 13:03:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051848435013220578421_0004_m_000082_261, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051848435013220578421_0004_m_000082_261}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051848435013220578421_0004}; taskId=attempt_202105151302051848435013220578421_0004_m_000082_261, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3969bc72}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:07,010] {docker.py:276} INFO - 21/05/15 13:03:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:07 INFO StagingCommitter: Starting: Task committer attempt_202105151302051848435013220578421_0004_m_000082_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051848435013220578421_0004_m_000082_261
[2021-05-15 10:03:07,012] {docker.py:276} INFO - 21/05/15 13:03:07 INFO StagingCommitter: Task committer attempt_202105151302051848435013220578421_0004_m_000082_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051848435013220578421_0004_m_000082_261 : duration 0:00.003s
[2021-05-15 10:03:07,166] {docker.py:276} INFO - 21/05/15 13:03:07 INFO StagingCommitter: Starting: Task committer attempt_202105151302053281524149382867119_0004_m_000079_258: needsTaskCommit() Task attempt_202105151302053281524149382867119_0004_m_000079_258
[2021-05-15 10:03:07,167] {docker.py:276} INFO - 21/05/15 13:03:07 INFO StagingCommitter: Task committer attempt_202105151302053281524149382867119_0004_m_000079_258: needsTaskCommit() Task attempt_202105151302053281524149382867119_0004_m_000079_258: duration 0:00.000s
[2021-05-15 10:03:07,167] {docker.py:276} INFO - 21/05/15 13:03:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053281524149382867119_0004_m_000079_258
[2021-05-15 10:03:07,168] {docker.py:276} INFO - 21/05/15 13:03:07 INFO Executor: Finished task 79.0 in stage 4.0 (TID 258). 4587 bytes result sent to driver
[2021-05-15 10:03:07,169] {docker.py:276} INFO - 21/05/15 13:03:07 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 262) (4e8a4a26f4b5, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:07,170] {docker.py:276} INFO - 21/05/15 13:03:07 INFO Executor: Running task 83.0 in stage 4.0 (TID 262)
21/05/15 13:03:07 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 258) in 2496 ms on 4e8a4a26f4b5 (executor driver) (80/200)
[2021-05-15 10:03:07,179] {docker.py:276} INFO - 21/05/15 13:03:07 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:07,181] {docker.py:276} INFO - 21/05/15 13:03:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:07,181] {docker.py:276} INFO - 21/05/15 13:03:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053312346720192768576_0004_m_000083_262, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053312346720192768576_0004_m_000083_262}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053312346720192768576_0004}; taskId=attempt_202105151302053312346720192768576_0004_m_000083_262, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31c4819e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:07 INFO StagingCommitter: Starting: Task committer attempt_202105151302053312346720192768576_0004_m_000083_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053312346720192768576_0004_m_000083_262
[2021-05-15 10:03:07,184] {docker.py:276} INFO - 21/05/15 13:03:07 INFO StagingCommitter: Task committer attempt_202105151302053312346720192768576_0004_m_000083_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053312346720192768576_0004_m_000083_262 : duration 0:00.003s
[2021-05-15 10:03:08,428] {docker.py:276} INFO - 21/05/15 13:03:08 INFO StagingCommitter: Starting: Task committer attempt_20210515130205604323134061655049_0004_m_000080_259: needsTaskCommit() Task attempt_20210515130205604323134061655049_0004_m_000080_259
[2021-05-15 10:03:08,429] {docker.py:276} INFO - 21/05/15 13:03:08 INFO StagingCommitter: Task committer attempt_20210515130205604323134061655049_0004_m_000080_259: needsTaskCommit() Task attempt_20210515130205604323134061655049_0004_m_000080_259: duration 0:00.000s
21/05/15 13:03:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205604323134061655049_0004_m_000080_259
[2021-05-15 10:03:08,430] {docker.py:276} INFO - 21/05/15 13:03:08 INFO Executor: Finished task 80.0 in stage 4.0 (TID 259). 4587 bytes result sent to driver
[2021-05-15 10:03:08,431] {docker.py:276} INFO - 21/05/15 13:03:08 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 263) (4e8a4a26f4b5, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:08,432] {docker.py:276} INFO - 21/05/15 13:03:08 INFO Executor: Running task 84.0 in stage 4.0 (TID 263)
21/05/15 13:03:08 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 259) in 2589 ms on 4e8a4a26f4b5 (executor driver) (81/200)
[2021-05-15 10:03:08,441] {docker.py:276} INFO - 21/05/15 13:03:08 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:08,443] {docker.py:276} INFO - 21/05/15 13:03:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205172305565089856433_0004_m_000084_263, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205172305565089856433_0004_m_000084_263}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205172305565089856433_0004}; taskId=attempt_20210515130205172305565089856433_0004_m_000084_263, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6617fcca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:08,443] {docker.py:276} INFO - 21/05/15 13:03:08 INFO StagingCommitter: Starting: Task committer attempt_20210515130205172305565089856433_0004_m_000084_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205172305565089856433_0004_m_000084_263
[2021-05-15 10:03:08,445] {docker.py:276} INFO - 21/05/15 13:03:08 INFO StagingCommitter: Task committer attempt_20210515130205172305565089856433_0004_m_000084_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205172305565089856433_0004_m_000084_263 : duration 0:00.003s
[2021-05-15 10:03:09,242] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Starting: Task committer attempt_202105151302058653773029949975737_0004_m_000081_260: needsTaskCommit() Task attempt_202105151302058653773029949975737_0004_m_000081_260
[2021-05-15 10:03:09,243] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Task committer attempt_202105151302058653773029949975737_0004_m_000081_260: needsTaskCommit() Task attempt_202105151302058653773029949975737_0004_m_000081_260: duration 0:00.001s
21/05/15 13:03:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058653773029949975737_0004_m_000081_260
[2021-05-15 10:03:09,245] {docker.py:276} INFO - 21/05/15 13:03:09 INFO Executor: Finished task 81.0 in stage 4.0 (TID 260). 4587 bytes result sent to driver
[2021-05-15 10:03:09,246] {docker.py:276} INFO - 21/05/15 13:03:09 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 264) (4e8a4a26f4b5, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:09,247] {docker.py:276} INFO - 21/05/15 13:03:09 INFO Executor: Running task 85.0 in stage 4.0 (TID 264)
[2021-05-15 10:03:09,248] {docker.py:276} INFO - 21/05/15 13:03:09 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 260) in 2490 ms on 4e8a4a26f4b5 (executor driver) (82/200)
[2021-05-15 10:03:09,258] {docker.py:276} INFO - 21/05/15 13:03:09 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:09,259] {docker.py:276} INFO - 21/05/15 13:03:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:09,260] {docker.py:276} INFO - 21/05/15 13:03:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205777150512389133814_0004_m_000085_264, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205777150512389133814_0004_m_000085_264}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205777150512389133814_0004}; taskId=attempt_20210515130205777150512389133814_0004_m_000085_264, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a91c903}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:09,260] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Starting: Task committer attempt_20210515130205777150512389133814_0004_m_000085_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205777150512389133814_0004_m_000085_264
[2021-05-15 10:03:09,262] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Task committer attempt_20210515130205777150512389133814_0004_m_000085_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205777150512389133814_0004_m_000085_264 : duration 0:00.003s
[2021-05-15 10:03:09,454] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Starting: Task committer attempt_202105151302051848435013220578421_0004_m_000082_261: needsTaskCommit() Task attempt_202105151302051848435013220578421_0004_m_000082_261
[2021-05-15 10:03:09,455] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Task committer attempt_202105151302051848435013220578421_0004_m_000082_261: needsTaskCommit() Task attempt_202105151302051848435013220578421_0004_m_000082_261: duration 0:00.002s
[2021-05-15 10:03:09,456] {docker.py:276} INFO - 21/05/15 13:03:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051848435013220578421_0004_m_000082_261
[2021-05-15 10:03:09,458] {docker.py:276} INFO - 21/05/15 13:03:09 INFO Executor: Finished task 82.0 in stage 4.0 (TID 261). 4587 bytes result sent to driver
[2021-05-15 10:03:09,459] {docker.py:276} INFO - 21/05/15 13:03:09 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 265) (4e8a4a26f4b5, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:09,460] {docker.py:276} INFO - 21/05/15 13:03:09 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 261) in 2478 ms on 4e8a4a26f4b5 (executor driver) (83/200)
[2021-05-15 10:03:09,461] {docker.py:276} INFO - 21/05/15 13:03:09 INFO Executor: Running task 86.0 in stage 4.0 (TID 265)
[2021-05-15 10:03:09,469] {docker.py:276} INFO - 21/05/15 13:03:09 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:09,470] {docker.py:276} INFO - 21/05/15 13:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:09,471] {docker.py:276} INFO - 21/05/15 13:03:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:09,472] {docker.py:276} INFO - 21/05/15 13:03:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:09,472] {docker.py:276} INFO - 21/05/15 13:03:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058844220367626806688_0004_m_000086_265, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058844220367626806688_0004_m_000086_265}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058844220367626806688_0004}; taskId=attempt_202105151302058844220367626806688_0004_m_000086_265, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47241cbe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:09,472] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Starting: Task committer attempt_202105151302058844220367626806688_0004_m_000086_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058844220367626806688_0004_m_000086_265
[2021-05-15 10:03:09,474] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Task committer attempt_202105151302058844220367626806688_0004_m_000086_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058844220367626806688_0004_m_000086_265 : duration 0:00.002s
[2021-05-15 10:03:09,676] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Starting: Task committer attempt_202105151302053312346720192768576_0004_m_000083_262: needsTaskCommit() Task attempt_202105151302053312346720192768576_0004_m_000083_262
[2021-05-15 10:03:09,677] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Task committer attempt_202105151302053312346720192768576_0004_m_000083_262: needsTaskCommit() Task attempt_202105151302053312346720192768576_0004_m_000083_262: duration 0:00.002s
21/05/15 13:03:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053312346720192768576_0004_m_000083_262
[2021-05-15 10:03:09,680] {docker.py:276} INFO - 21/05/15 13:03:09 INFO Executor: Finished task 83.0 in stage 4.0 (TID 262). 4544 bytes result sent to driver
[2021-05-15 10:03:09,681] {docker.py:276} INFO - 21/05/15 13:03:09 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 266) (4e8a4a26f4b5, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:09,682] {docker.py:276} INFO - 21/05/15 13:03:09 INFO Executor: Running task 87.0 in stage 4.0 (TID 266)
[2021-05-15 10:03:09,683] {docker.py:276} INFO - 21/05/15 13:03:09 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 262) in 2517 ms on 4e8a4a26f4b5 (executor driver) (84/200)
[2021-05-15 10:03:09,692] {docker.py:276} INFO - 21/05/15 13:03:09 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:09,694] {docker.py:276} INFO - 21/05/15 13:03:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058173762255134710277_0004_m_000087_266, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058173762255134710277_0004_m_000087_266}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058173762255134710277_0004}; taskId=attempt_202105151302058173762255134710277_0004_m_000087_266, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41edd2e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:09,695] {docker.py:276} INFO - 21/05/15 13:03:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:09,695] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Starting: Task committer attempt_202105151302058173762255134710277_0004_m_000087_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058173762255134710277_0004_m_000087_266
[2021-05-15 10:03:09,698] {docker.py:276} INFO - 21/05/15 13:03:09 INFO StagingCommitter: Task committer attempt_202105151302058173762255134710277_0004_m_000087_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058173762255134710277_0004_m_000087_266 : duration 0:00.003s
[2021-05-15 10:03:10,870] {docker.py:276} INFO - 21/05/15 13:03:10 INFO StagingCommitter: Starting: Task committer attempt_20210515130205172305565089856433_0004_m_000084_263: needsTaskCommit() Task attempt_20210515130205172305565089856433_0004_m_000084_263
[2021-05-15 10:03:10,871] {docker.py:276} INFO - 21/05/15 13:03:10 INFO StagingCommitter: Task committer attempt_20210515130205172305565089856433_0004_m_000084_263: needsTaskCommit() Task attempt_20210515130205172305565089856433_0004_m_000084_263: duration 0:00.003s
21/05/15 13:03:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205172305565089856433_0004_m_000084_263
[2021-05-15 10:03:10,873] {docker.py:276} INFO - 21/05/15 13:03:10 INFO Executor: Finished task 84.0 in stage 4.0 (TID 263). 4544 bytes result sent to driver
[2021-05-15 10:03:10,874] {docker.py:276} INFO - 21/05/15 13:03:10 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 267) (4e8a4a26f4b5, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:10,876] {docker.py:276} INFO - 21/05/15 13:03:10 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 263) in 2449 ms on 4e8a4a26f4b5 (executor driver) (85/200)
[2021-05-15 10:03:10,877] {docker.py:276} INFO - 21/05/15 13:03:10 INFO Executor: Running task 88.0 in stage 4.0 (TID 267)
[2021-05-15 10:03:10,886] {docker.py:276} INFO - 21/05/15 13:03:10 INFO ShuffleBlockFetcherIterator: Getting 4 (11.3 KiB) non-empty blocks including 4 (11.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:10,888] {docker.py:276} INFO - 21/05/15 13:03:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:10,889] {docker.py:276} INFO - 21/05/15 13:03:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051329851742959209869_0004_m_000088_267, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051329851742959209869_0004_m_000088_267}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051329851742959209869_0004}; taskId=attempt_202105151302051329851742959209869_0004_m_000088_267, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ef3e36a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:10 INFO StagingCommitter: Starting: Task committer attempt_202105151302051329851742959209869_0004_m_000088_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051329851742959209869_0004_m_000088_267
[2021-05-15 10:03:10,891] {docker.py:276} INFO - 21/05/15 13:03:10 INFO StagingCommitter: Task committer attempt_202105151302051329851742959209869_0004_m_000088_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051329851742959209869_0004_m_000088_267 : duration 0:00.003s
[2021-05-15 10:03:11,593] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Starting: Task committer attempt_20210515130205777150512389133814_0004_m_000085_264: needsTaskCommit() Task attempt_20210515130205777150512389133814_0004_m_000085_264
[2021-05-15 10:03:11,594] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Task committer attempt_20210515130205777150512389133814_0004_m_000085_264: needsTaskCommit() Task attempt_20210515130205777150512389133814_0004_m_000085_264: duration 0:00.002s
21/05/15 13:03:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205777150512389133814_0004_m_000085_264
[2021-05-15 10:03:11,596] {docker.py:276} INFO - 21/05/15 13:03:11 INFO Executor: Finished task 85.0 in stage 4.0 (TID 264). 4544 bytes result sent to driver
[2021-05-15 10:03:11,598] {docker.py:276} INFO - 21/05/15 13:03:11 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 268) (4e8a4a26f4b5, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:11,599] {docker.py:276} INFO - 21/05/15 13:03:11 INFO Executor: Running task 89.0 in stage 4.0 (TID 268)
[2021-05-15 10:03:11,599] {docker.py:276} INFO - 21/05/15 13:03:11 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 264) in 2356 ms on 4e8a4a26f4b5 (executor driver) (86/200)
[2021-05-15 10:03:11,608] {docker.py:276} INFO - 21/05/15 13:03:11 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:11,610] {docker.py:276} INFO - 21/05/15 13:03:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:11,611] {docker.py:276} INFO - 21/05/15 13:03:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056071252671377632118_0004_m_000089_268, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056071252671377632118_0004_m_000089_268}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056071252671377632118_0004}; taskId=attempt_202105151302056071252671377632118_0004_m_000089_268, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57a1685f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:11,611] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Starting: Task committer attempt_202105151302056071252671377632118_0004_m_000089_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056071252671377632118_0004_m_000089_268
[2021-05-15 10:03:11,614] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Task committer attempt_202105151302056071252671377632118_0004_m_000089_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056071252671377632118_0004_m_000089_268 : duration 0:00.003s
[2021-05-15 10:03:11,839] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Starting: Task committer attempt_202105151302058844220367626806688_0004_m_000086_265: needsTaskCommit() Task attempt_202105151302058844220367626806688_0004_m_000086_265
[2021-05-15 10:03:11,840] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Task committer attempt_202105151302058844220367626806688_0004_m_000086_265: needsTaskCommit() Task attempt_202105151302058844220367626806688_0004_m_000086_265: duration 0:00.001s
21/05/15 13:03:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058844220367626806688_0004_m_000086_265
[2021-05-15 10:03:11,841] {docker.py:276} INFO - 21/05/15 13:03:11 INFO Executor: Finished task 86.0 in stage 4.0 (TID 265). 4544 bytes result sent to driver
[2021-05-15 10:03:11,843] {docker.py:276} INFO - 21/05/15 13:03:11 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 269) (4e8a4a26f4b5, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:11,844] {docker.py:276} INFO - 21/05/15 13:03:11 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 265) in 2387 ms on 4e8a4a26f4b5 (executor driver) (87/200)
[2021-05-15 10:03:11,845] {docker.py:276} INFO - 21/05/15 13:03:11 INFO Executor: Running task 90.0 in stage 4.0 (TID 269)
[2021-05-15 10:03:11,854] {docker.py:276} INFO - 21/05/15 13:03:11 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:11,856] {docker.py:276} INFO - 21/05/15 13:03:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059209802047452502921_0004_m_000090_269, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059209802047452502921_0004_m_000090_269}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059209802047452502921_0004}; taskId=attempt_202105151302059209802047452502921_0004_m_000090_269, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@740169b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:11,856] {docker.py:276} INFO - 21/05/15 13:03:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:11 INFO StagingCommitter: Starting: Task committer attempt_202105151302059209802047452502921_0004_m_000090_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059209802047452502921_0004_m_000090_269
[2021-05-15 10:03:11,859] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Task committer attempt_202105151302059209802047452502921_0004_m_000090_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059209802047452502921_0004_m_000090_269 : duration 0:00.003s
[2021-05-15 10:03:11,979] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Starting: Task committer attempt_202105151302058173762255134710277_0004_m_000087_266: needsTaskCommit() Task attempt_202105151302058173762255134710277_0004_m_000087_266
[2021-05-15 10:03:11,979] {docker.py:276} INFO - 21/05/15 13:03:11 INFO StagingCommitter: Task committer attempt_202105151302058173762255134710277_0004_m_000087_266: needsTaskCommit() Task attempt_202105151302058173762255134710277_0004_m_000087_266: duration 0:00.001s
21/05/15 13:03:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058173762255134710277_0004_m_000087_266
[2021-05-15 10:03:11,980] {docker.py:276} INFO - 21/05/15 13:03:11 INFO Executor: Finished task 87.0 in stage 4.0 (TID 266). 4544 bytes result sent to driver
[2021-05-15 10:03:11,981] {docker.py:276} INFO - 21/05/15 13:03:12 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 270) (4e8a4a26f4b5, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:11,983] {docker.py:276} INFO - 21/05/15 13:03:12 INFO Executor: Running task 91.0 in stage 4.0 (TID 270)
21/05/15 13:03:12 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 266) in 2306 ms on 4e8a4a26f4b5 (executor driver) (88/200)
[2021-05-15 10:03:11,990] {docker.py:276} INFO - 21/05/15 13:03:12 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:11,992] {docker.py:276} INFO - 21/05/15 13:03:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:11,992] {docker.py:276} INFO - 21/05/15 13:03:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054709957873975789878_0004_m_000091_270, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054709957873975789878_0004_m_000091_270}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054709957873975789878_0004}; taskId=attempt_202105151302054709957873975789878_0004_m_000091_270, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ba09f0e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:12 INFO StagingCommitter: Starting: Task committer attempt_202105151302054709957873975789878_0004_m_000091_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054709957873975789878_0004_m_000091_270
[2021-05-15 10:03:11,995] {docker.py:276} INFO - 21/05/15 13:03:12 INFO StagingCommitter: Task committer attempt_202105151302054709957873975789878_0004_m_000091_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054709957873975789878_0004_m_000091_270 : duration 0:00.003s
[2021-05-15 10:03:13,266] {docker.py:276} INFO - 21/05/15 13:03:13 INFO StagingCommitter: Starting: Task committer attempt_202105151302051329851742959209869_0004_m_000088_267: needsTaskCommit() Task attempt_202105151302051329851742959209869_0004_m_000088_267
[2021-05-15 10:03:13,267] {docker.py:276} INFO - 21/05/15 13:03:13 INFO StagingCommitter: Task committer attempt_202105151302051329851742959209869_0004_m_000088_267: needsTaskCommit() Task attempt_202105151302051329851742959209869_0004_m_000088_267: duration 0:00.000s
21/05/15 13:03:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051329851742959209869_0004_m_000088_267
[2021-05-15 10:03:13,269] {docker.py:276} INFO - 21/05/15 13:03:13 INFO Executor: Finished task 88.0 in stage 4.0 (TID 267). 4544 bytes result sent to driver
[2021-05-15 10:03:13,271] {docker.py:276} INFO - 21/05/15 13:03:13 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 271) (4e8a4a26f4b5, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:13,272] {docker.py:276} INFO - 21/05/15 13:03:13 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 267) in 2402 ms on 4e8a4a26f4b5 (executor driver) (89/200)
[2021-05-15 10:03:13,273] {docker.py:276} INFO - 21/05/15 13:03:13 INFO Executor: Running task 92.0 in stage 4.0 (TID 271)
[2021-05-15 10:03:13,284] {docker.py:276} INFO - 21/05/15 13:03:13 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:13,286] {docker.py:276} INFO - 21/05/15 13:03:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:13,287] {docker.py:276} INFO - 21/05/15 13:03:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051695663472706308263_0004_m_000092_271, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051695663472706308263_0004_m_000092_271}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051695663472706308263_0004}; taskId=attempt_202105151302051695663472706308263_0004_m_000092_271, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d22df8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:13 INFO StagingCommitter: Starting: Task committer attempt_202105151302051695663472706308263_0004_m_000092_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051695663472706308263_0004_m_000092_271
[2021-05-15 10:03:13,290] {docker.py:276} INFO - 21/05/15 13:03:13 INFO StagingCommitter: Task committer attempt_202105151302051695663472706308263_0004_m_000092_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051695663472706308263_0004_m_000092_271 : duration 0:00.003s
[2021-05-15 10:03:14,054] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105151302056071252671377632118_0004_m_000089_268: needsTaskCommit() Task attempt_202105151302056071252671377632118_0004_m_000089_268
[2021-05-15 10:03:14,055] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Task committer attempt_202105151302056071252671377632118_0004_m_000089_268: needsTaskCommit() Task attempt_202105151302056071252671377632118_0004_m_000089_268: duration 0:00.001s
21/05/15 13:03:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056071252671377632118_0004_m_000089_268
[2021-05-15 10:03:14,056] {docker.py:276} INFO - 21/05/15 13:03:14 INFO Executor: Finished task 89.0 in stage 4.0 (TID 268). 4544 bytes result sent to driver
[2021-05-15 10:03:14,057] {docker.py:276} INFO - 21/05/15 13:03:14 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 272) (4e8a4a26f4b5, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:14,059] {docker.py:276} INFO - 21/05/15 13:03:14 INFO Executor: Running task 93.0 in stage 4.0 (TID 272)
[2021-05-15 10:03:14,060] {docker.py:276} INFO - 21/05/15 13:03:14 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 268) in 2464 ms on 4e8a4a26f4b5 (executor driver) (90/200)
[2021-05-15 10:03:14,069] {docker.py:276} INFO - 21/05/15 13:03:14 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:14,071] {docker.py:276} INFO - 21/05/15 13:03:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:14,072] {docker.py:276} INFO - 21/05/15 13:03:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056319920678798954665_0004_m_000093_272, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056319920678798954665_0004_m_000093_272}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056319920678798954665_0004}; taskId=attempt_202105151302056319920678798954665_0004_m_000093_272, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17f29464}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105151302056319920678798954665_0004_m_000093_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056319920678798954665_0004_m_000093_272
[2021-05-15 10:03:14,075] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Task committer attempt_202105151302056319920678798954665_0004_m_000093_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056319920678798954665_0004_m_000093_272 : duration 0:00.003s
[2021-05-15 10:03:14,185] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105151302059209802047452502921_0004_m_000090_269: needsTaskCommit() Task attempt_202105151302059209802047452502921_0004_m_000090_269
[2021-05-15 10:03:14,186] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Task committer attempt_202105151302059209802047452502921_0004_m_000090_269: needsTaskCommit() Task attempt_202105151302059209802047452502921_0004_m_000090_269: duration 0:00.001s
[2021-05-15 10:03:14,187] {docker.py:276} INFO - 21/05/15 13:03:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059209802047452502921_0004_m_000090_269
[2021-05-15 10:03:14,188] {docker.py:276} INFO - 21/05/15 13:03:14 INFO Executor: Finished task 90.0 in stage 4.0 (TID 269). 4544 bytes result sent to driver
[2021-05-15 10:03:14,189] {docker.py:276} INFO - 21/05/15 13:03:14 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 273) (4e8a4a26f4b5, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:14,190] {docker.py:276} INFO - 21/05/15 13:03:14 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 269) in 2351 ms on 4e8a4a26f4b5 (executor driver) (91/200)
[2021-05-15 10:03:14,191] {docker.py:276} INFO - 21/05/15 13:03:14 INFO Executor: Running task 94.0 in stage 4.0 (TID 273)
[2021-05-15 10:03:14,201] {docker.py:276} INFO - 21/05/15 13:03:14 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:14,203] {docker.py:276} INFO - 21/05/15 13:03:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:14,203] {docker.py:276} INFO - 21/05/15 13:03:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:14,204] {docker.py:276} INFO - 21/05/15 13:03:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052415588051568460457_0004_m_000094_273, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052415588051568460457_0004_m_000094_273}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052415588051568460457_0004}; taskId=attempt_202105151302052415588051568460457_0004_m_000094_273, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@421f2ea5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:14,204] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105151302052415588051568460457_0004_m_000094_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052415588051568460457_0004_m_000094_273
[2021-05-15 10:03:14,207] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Task committer attempt_202105151302052415588051568460457_0004_m_000094_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052415588051568460457_0004_m_000094_273 : duration 0:00.003s
[2021-05-15 10:03:14,392] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105151302054709957873975789878_0004_m_000091_270: needsTaskCommit() Task attempt_202105151302054709957873975789878_0004_m_000091_270
21/05/15 13:03:14 INFO StagingCommitter: Task committer attempt_202105151302054709957873975789878_0004_m_000091_270: needsTaskCommit() Task attempt_202105151302054709957873975789878_0004_m_000091_270: duration 0:00.000s
[2021-05-15 10:03:14,393] {docker.py:276} INFO - 21/05/15 13:03:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054709957873975789878_0004_m_000091_270
[2021-05-15 10:03:14,394] {docker.py:276} INFO - 21/05/15 13:03:14 INFO Executor: Finished task 91.0 in stage 4.0 (TID 270). 4544 bytes result sent to driver
[2021-05-15 10:03:14,408] {docker.py:276} INFO - 21/05/15 13:03:14 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 274) (4e8a4a26f4b5, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:14,409] {docker.py:276} INFO - 21/05/15 13:03:14 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 270) in 2430 ms on 4e8a4a26f4b5 (executor driver) (92/200)
[2021-05-15 10:03:14,410] {docker.py:276} INFO - 21/05/15 13:03:14 INFO Executor: Running task 95.0 in stage 4.0 (TID 274)
[2021-05-15 10:03:14,417] {docker.py:276} INFO - 21/05/15 13:03:14 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:14,420] {docker.py:276} INFO - 21/05/15 13:03:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:14,420] {docker.py:276} INFO - 21/05/15 13:03:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057759077836326871434_0004_m_000095_274, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057759077836326871434_0004_m_000095_274}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057759077836326871434_0004}; taskId=attempt_202105151302057759077836326871434_0004_m_000095_274, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72b8a8eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:14,420] {docker.py:276} INFO - 21/05/15 13:03:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:14 INFO StagingCommitter: Starting: Task committer attempt_202105151302057759077836326871434_0004_m_000095_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057759077836326871434_0004_m_000095_274
[2021-05-15 10:03:14,423] {docker.py:276} INFO - 21/05/15 13:03:14 INFO StagingCommitter: Task committer attempt_202105151302057759077836326871434_0004_m_000095_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057759077836326871434_0004_m_000095_274 : duration 0:00.004s
[2021-05-15 10:03:15,603] {docker.py:276} INFO - 21/05/15 13:03:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302051695663472706308263_0004_m_000092_271: needsTaskCommit() Task attempt_202105151302051695663472706308263_0004_m_000092_271
[2021-05-15 10:03:15,604] {docker.py:276} INFO - 21/05/15 13:03:15 INFO StagingCommitter: Task committer attempt_202105151302051695663472706308263_0004_m_000092_271: needsTaskCommit() Task attempt_202105151302051695663472706308263_0004_m_000092_271: duration 0:00.001s
21/05/15 13:03:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051695663472706308263_0004_m_000092_271
[2021-05-15 10:03:15,605] {docker.py:276} INFO - 21/05/15 13:03:15 INFO Executor: Finished task 92.0 in stage 4.0 (TID 271). 4587 bytes result sent to driver
[2021-05-15 10:03:15,609] {docker.py:276} INFO - 21/05/15 13:03:15 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 275) (4e8a4a26f4b5, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:15 INFO Executor: Running task 96.0 in stage 4.0 (TID 275)
21/05/15 13:03:15 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 271) in 2341 ms on 4e8a4a26f4b5 (executor driver) (93/200)
[2021-05-15 10:03:15,619] {docker.py:276} INFO - 21/05/15 13:03:15 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:15,620] {docker.py:276} INFO - 21/05/15 13:03:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054923880614187372167_0004_m_000096_275, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054923880614187372167_0004_m_000096_275}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054923880614187372167_0004}; taskId=attempt_202105151302054923880614187372167_0004_m_000096_275, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43055ad3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:15,621] {docker.py:276} INFO - 21/05/15 13:03:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302054923880614187372167_0004_m_000096_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054923880614187372167_0004_m_000096_275
[2021-05-15 10:03:15,624] {docker.py:276} INFO - 21/05/15 13:03:15 INFO StagingCommitter: Task committer attempt_202105151302054923880614187372167_0004_m_000096_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054923880614187372167_0004_m_000096_275 : duration 0:00.003s
[2021-05-15 10:03:16,418] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302056319920678798954665_0004_m_000093_272: needsTaskCommit() Task attempt_202105151302056319920678798954665_0004_m_000093_272
[2021-05-15 10:03:16,418] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Task committer attempt_202105151302056319920678798954665_0004_m_000093_272: needsTaskCommit() Task attempt_202105151302056319920678798954665_0004_m_000093_272: duration 0:00.000s
21/05/15 13:03:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056319920678798954665_0004_m_000093_272
[2021-05-15 10:03:16,419] {docker.py:276} INFO - 21/05/15 13:03:16 INFO Executor: Finished task 93.0 in stage 4.0 (TID 272). 4587 bytes result sent to driver
[2021-05-15 10:03:16,420] {docker.py:276} INFO - 21/05/15 13:03:16 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 276) (4e8a4a26f4b5, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:16,420] {docker.py:276} INFO - 21/05/15 13:03:16 INFO Executor: Running task 97.0 in stage 4.0 (TID 276)
[2021-05-15 10:03:16,421] {docker.py:276} INFO - 21/05/15 13:03:16 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 272) in 2367 ms on 4e8a4a26f4b5 (executor driver) (94/200)
[2021-05-15 10:03:16,428] {docker.py:276} INFO - 21/05/15 13:03:16 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:16,428] {docker.py:276} INFO - 21/05/15 13:03:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:16,430] {docker.py:276} INFO - 21/05/15 13:03:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:16,430] {docker.py:276} INFO - 21/05/15 13:03:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:16,431] {docker.py:276} INFO - 21/05/15 13:03:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058907537906147668122_0004_m_000097_276, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058907537906147668122_0004_m_000097_276}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058907537906147668122_0004}; taskId=attempt_202105151302058907537906147668122_0004_m_000097_276, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60b554cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:16,431] {docker.py:276} INFO - 21/05/15 13:03:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:16,431] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302058907537906147668122_0004_m_000097_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058907537906147668122_0004_m_000097_276
[2021-05-15 10:03:16,434] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Task committer attempt_202105151302058907537906147668122_0004_m_000097_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058907537906147668122_0004_m_000097_276 : duration 0:00.003s
[2021-05-15 10:03:16,670] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302052415588051568460457_0004_m_000094_273: needsTaskCommit() Task attempt_202105151302052415588051568460457_0004_m_000094_273
[2021-05-15 10:03:16,671] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Task committer attempt_202105151302052415588051568460457_0004_m_000094_273: needsTaskCommit() Task attempt_202105151302052415588051568460457_0004_m_000094_273: duration 0:00.002s
[2021-05-15 10:03:16,672] {docker.py:276} INFO - 21/05/15 13:03:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052415588051568460457_0004_m_000094_273
[2021-05-15 10:03:16,673] {docker.py:276} INFO - 21/05/15 13:03:16 INFO Executor: Finished task 94.0 in stage 4.0 (TID 273). 4587 bytes result sent to driver
[2021-05-15 10:03:16,675] {docker.py:276} INFO - 21/05/15 13:03:16 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 277) (4e8a4a26f4b5, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:16,676] {docker.py:276} INFO - 21/05/15 13:03:16 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 273) in 2490 ms on 4e8a4a26f4b5 (executor driver) (95/200)
21/05/15 13:03:16 INFO Executor: Running task 98.0 in stage 4.0 (TID 277)
[2021-05-15 10:03:16,686] {docker.py:276} INFO - 21/05/15 13:03:16 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:16,687] {docker.py:276} INFO - 21/05/15 13:03:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:16,688] {docker.py:276} INFO - 21/05/15 13:03:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:16,689] {docker.py:276} INFO - 21/05/15 13:03:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:16,690] {docker.py:276} INFO - 21/05/15 13:03:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051695614499804150794_0004_m_000098_277, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051695614499804150794_0004_m_000098_277}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051695614499804150794_0004}; taskId=attempt_202105151302051695614499804150794_0004_m_000098_277, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@453fcdfc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:16,690] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302051695614499804150794_0004_m_000098_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051695614499804150794_0004_m_000098_277
[2021-05-15 10:03:16,694] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Task committer attempt_202105151302051695614499804150794_0004_m_000098_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051695614499804150794_0004_m_000098_277 : duration 0:00.004s
[2021-05-15 10:03:16,749] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302057759077836326871434_0004_m_000095_274: needsTaskCommit() Task attempt_202105151302057759077836326871434_0004_m_000095_274
[2021-05-15 10:03:16,749] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Task committer attempt_202105151302057759077836326871434_0004_m_000095_274: needsTaskCommit() Task attempt_202105151302057759077836326871434_0004_m_000095_274: duration 0:00.002s
21/05/15 13:03:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057759077836326871434_0004_m_000095_274
[2021-05-15 10:03:16,751] {docker.py:276} INFO - 21/05/15 13:03:16 INFO Executor: Finished task 95.0 in stage 4.0 (TID 274). 4544 bytes result sent to driver
[2021-05-15 10:03:16,752] {docker.py:276} INFO - 21/05/15 13:03:16 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 278) (4e8a4a26f4b5, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:16,754] {docker.py:276} INFO - 21/05/15 13:03:16 INFO Executor: Running task 99.0 in stage 4.0 (TID 278)
[2021-05-15 10:03:16,754] {docker.py:276} INFO - 21/05/15 13:03:16 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 274) in 2349 ms on 4e8a4a26f4b5 (executor driver) (96/200)
[2021-05-15 10:03:16,767] {docker.py:276} INFO - 21/05/15 13:03:16 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:03:16,769] {docker.py:276} INFO - 21/05/15 13:03:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205570758728797463730_0004_m_000099_278, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205570758728797463730_0004_m_000099_278}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205570758728797463730_0004}; taskId=attempt_20210515130205570758728797463730_0004_m_000099_278, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65f5b8a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:16,769] {docker.py:276} INFO - 21/05/15 13:03:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:16 INFO StagingCommitter: Starting: Task committer attempt_20210515130205570758728797463730_0004_m_000099_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205570758728797463730_0004_m_000099_278
[2021-05-15 10:03:16,772] {docker.py:276} INFO - 21/05/15 13:03:16 INFO StagingCommitter: Task committer attempt_20210515130205570758728797463730_0004_m_000099_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205570758728797463730_0004_m_000099_278 : duration 0:00.003s
[2021-05-15 10:03:18,264] {docker.py:276} INFO - 21/05/15 13:03:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302054923880614187372167_0004_m_000096_275: needsTaskCommit() Task attempt_202105151302054923880614187372167_0004_m_000096_275
[2021-05-15 10:03:18,265] {docker.py:276} INFO - 21/05/15 13:03:18 INFO StagingCommitter: Task committer attempt_202105151302054923880614187372167_0004_m_000096_275: needsTaskCommit() Task attempt_202105151302054923880614187372167_0004_m_000096_275: duration 0:00.000s
21/05/15 13:03:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054923880614187372167_0004_m_000096_275
[2021-05-15 10:03:18,266] {docker.py:276} INFO - 21/05/15 13:03:18 INFO Executor: Finished task 96.0 in stage 4.0 (TID 275). 4544 bytes result sent to driver
[2021-05-15 10:03:18,268] {docker.py:276} INFO - 21/05/15 13:03:18 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 279) (4e8a4a26f4b5, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:18,269] {docker.py:276} INFO - 21/05/15 13:03:18 INFO Executor: Running task 100.0 in stage 4.0 (TID 279)
[2021-05-15 10:03:18,270] {docker.py:276} INFO - 21/05/15 13:03:18 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 275) in 2666 ms on 4e8a4a26f4b5 (executor driver) (97/200)
[2021-05-15 10:03:18,279] {docker.py:276} INFO - 21/05/15 13:03:18 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:18,281] {docker.py:276} INFO - 21/05/15 13:03:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:18,282] {docker.py:276} INFO - 21/05/15 13:03:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058153171698965801296_0004_m_000100_279, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058153171698965801296_0004_m_000100_279}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058153171698965801296_0004}; taskId=attempt_202105151302058153171698965801296_0004_m_000100_279, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70336cf9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:18,282] {docker.py:276} INFO - 21/05/15 13:03:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:18,282] {docker.py:276} INFO - 21/05/15 13:03:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302058153171698965801296_0004_m_000100_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058153171698965801296_0004_m_000100_279
[2021-05-15 10:03:18,285] {docker.py:276} INFO - 21/05/15 13:03:18 INFO StagingCommitter: Task committer attempt_202105151302058153171698965801296_0004_m_000100_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058153171698965801296_0004_m_000100_279 : duration 0:00.003s
[2021-05-15 10:03:18,429] {docker.py:276} INFO - 21/05/15 13:03:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302058907537906147668122_0004_m_000097_276: needsTaskCommit() Task attempt_202105151302058907537906147668122_0004_m_000097_276
21/05/15 13:03:18 INFO StagingCommitter: Task committer attempt_202105151302058907537906147668122_0004_m_000097_276: needsTaskCommit() Task attempt_202105151302058907537906147668122_0004_m_000097_276: duration 0:00.000s
21/05/15 13:03:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058907537906147668122_0004_m_000097_276
[2021-05-15 10:03:18,430] {docker.py:276} INFO - 21/05/15 13:03:18 INFO Executor: Finished task 97.0 in stage 4.0 (TID 276). 4544 bytes result sent to driver
[2021-05-15 10:03:18,431] {docker.py:276} INFO - 21/05/15 13:03:18 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 280) (4e8a4a26f4b5, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:18,433] {docker.py:276} INFO - 21/05/15 13:03:18 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 276) in 2015 ms on 4e8a4a26f4b5 (executor driver) (98/200)
[2021-05-15 10:03:18,434] {docker.py:276} INFO - 21/05/15 13:03:18 INFO Executor: Running task 101.0 in stage 4.0 (TID 280)
[2021-05-15 10:03:18,445] {docker.py:276} INFO - 21/05/15 13:03:18 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:18,447] {docker.py:276} INFO - 21/05/15 13:03:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055833739164340080082_0004_m_000101_280, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055833739164340080082_0004_m_000101_280}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055833739164340080082_0004}; taskId=attempt_202105151302055833739164340080082_0004_m_000101_280, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3736de70}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302055833739164340080082_0004_m_000101_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055833739164340080082_0004_m_000101_280
[2021-05-15 10:03:18,450] {docker.py:276} INFO - 21/05/15 13:03:18 INFO StagingCommitter: Task committer attempt_202105151302055833739164340080082_0004_m_000101_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055833739164340080082_0004_m_000101_280 : duration 0:00.003s
[2021-05-15 10:03:19,018] {docker.py:276} INFO - 21/05/15 13:03:19 INFO StagingCommitter: Starting: Task committer attempt_202105151302051695614499804150794_0004_m_000098_277: needsTaskCommit() Task attempt_202105151302051695614499804150794_0004_m_000098_277
21/05/15 13:03:19 INFO StagingCommitter: Task committer attempt_202105151302051695614499804150794_0004_m_000098_277: needsTaskCommit() Task attempt_202105151302051695614499804150794_0004_m_000098_277: duration 0:00.000s
21/05/15 13:03:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051695614499804150794_0004_m_000098_277
[2021-05-15 10:03:19,022] {docker.py:276} INFO - 21/05/15 13:03:19 INFO Executor: Finished task 98.0 in stage 4.0 (TID 277). 4544 bytes result sent to driver
[2021-05-15 10:03:19,023] {docker.py:276} INFO - 21/05/15 13:03:19 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 281) (4e8a4a26f4b5, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:19,025] {docker.py:276} INFO - 21/05/15 13:03:19 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 277) in 2353 ms on 4e8a4a26f4b5 (executor driver) (99/200)
21/05/15 13:03:19 INFO Executor: Running task 102.0 in stage 4.0 (TID 281)
[2021-05-15 10:03:19,033] {docker.py:276} INFO - 21/05/15 13:03:19 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:19,035] {docker.py:276} INFO - 21/05/15 13:03:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054439048987426795157_0004_m_000102_281, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054439048987426795157_0004_m_000102_281}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054439048987426795157_0004}; taskId=attempt_202105151302054439048987426795157_0004_m_000102_281, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62e43c0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:19,035] {docker.py:276} INFO - 21/05/15 13:03:19 INFO StagingCommitter: Starting: Task committer attempt_202105151302054439048987426795157_0004_m_000102_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054439048987426795157_0004_m_000102_281
[2021-05-15 10:03:19,039] {docker.py:276} INFO - 21/05/15 13:03:19 INFO StagingCommitter: Task committer attempt_202105151302054439048987426795157_0004_m_000102_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054439048987426795157_0004_m_000102_281 : duration 0:00.004s
[2021-05-15 10:03:19,248] {docker.py:276} INFO - 21/05/15 13:03:19 INFO StagingCommitter: Starting: Task committer attempt_20210515130205570758728797463730_0004_m_000099_278: needsTaskCommit() Task attempt_20210515130205570758728797463730_0004_m_000099_278
[2021-05-15 10:03:19,249] {docker.py:276} INFO - 21/05/15 13:03:19 INFO StagingCommitter: Task committer attempt_20210515130205570758728797463730_0004_m_000099_278: needsTaskCommit() Task attempt_20210515130205570758728797463730_0004_m_000099_278: duration 0:00.001s
21/05/15 13:03:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205570758728797463730_0004_m_000099_278
[2021-05-15 10:03:19,251] {docker.py:276} INFO - 21/05/15 13:03:19 INFO Executor: Finished task 99.0 in stage 4.0 (TID 278). 4544 bytes result sent to driver
[2021-05-15 10:03:19,252] {docker.py:276} INFO - 21/05/15 13:03:19 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 282) (4e8a4a26f4b5, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:19,253] {docker.py:276} INFO - 21/05/15 13:03:19 INFO Executor: Running task 103.0 in stage 4.0 (TID 282)
[2021-05-15 10:03:19,253] {docker.py:276} INFO - 21/05/15 13:03:19 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 278) in 2504 ms on 4e8a4a26f4b5 (executor driver) (100/200)
[2021-05-15 10:03:19,262] {docker.py:276} INFO - 21/05/15 13:03:19 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:19,264] {docker.py:276} INFO - 21/05/15 13:03:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053700359380647605487_0004_m_000103_282, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053700359380647605487_0004_m_000103_282}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053700359380647605487_0004}; taskId=attempt_202105151302053700359380647605487_0004_m_000103_282, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c6ef708}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:19 INFO StagingCommitter: Starting: Task committer attempt_202105151302053700359380647605487_0004_m_000103_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053700359380647605487_0004_m_000103_282
[2021-05-15 10:03:19,267] {docker.py:276} INFO - 21/05/15 13:03:19 INFO StagingCommitter: Task committer attempt_202105151302053700359380647605487_0004_m_000103_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053700359380647605487_0004_m_000103_282 : duration 0:00.004s
[2021-05-15 10:03:20,744] {docker.py:276} INFO - 21/05/15 13:03:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302055833739164340080082_0004_m_000101_280: needsTaskCommit() Task attempt_202105151302055833739164340080082_0004_m_000101_280
[2021-05-15 10:03:20,745] {docker.py:276} INFO - 21/05/15 13:03:20 INFO StagingCommitter: Task committer attempt_202105151302055833739164340080082_0004_m_000101_280: needsTaskCommit() Task attempt_202105151302055833739164340080082_0004_m_000101_280: duration 0:00.000s
21/05/15 13:03:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055833739164340080082_0004_m_000101_280
[2021-05-15 10:03:20,748] {docker.py:276} INFO - 21/05/15 13:03:20 INFO Executor: Finished task 101.0 in stage 4.0 (TID 280). 4544 bytes result sent to driver
21/05/15 13:03:20 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 283) (4e8a4a26f4b5, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:20,749] {docker.py:276} INFO - 21/05/15 13:03:20 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 280) in 2319 ms on 4e8a4a26f4b5 (executor driver) (101/200)
[2021-05-15 10:03:20,750] {docker.py:276} INFO - 21/05/15 13:03:20 INFO Executor: Running task 104.0 in stage 4.0 (TID 283)
[2021-05-15 10:03:20,758] {docker.py:276} INFO - 21/05/15 13:03:20 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:20,759] {docker.py:276} INFO - 21/05/15 13:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:20,760] {docker.py:276} INFO - 21/05/15 13:03:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:20,760] {docker.py:276} INFO - 21/05/15 13:03:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058696911286320640544_0004_m_000104_283, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058696911286320640544_0004_m_000104_283}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058696911286320640544_0004}; taskId=attempt_202105151302058696911286320640544_0004_m_000104_283, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58f920b1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:20,761] {docker.py:276} INFO - 21/05/15 13:03:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302058696911286320640544_0004_m_000104_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058696911286320640544_0004_m_000104_283
[2021-05-15 10:03:20,763] {docker.py:276} INFO - 21/05/15 13:03:20 INFO StagingCommitter: Task committer attempt_202105151302058696911286320640544_0004_m_000104_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058696911286320640544_0004_m_000104_283 : duration 0:00.003s
[2021-05-15 10:03:21,235] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105151302058153171698965801296_0004_m_000100_279: needsTaskCommit() Task attempt_202105151302058153171698965801296_0004_m_000100_279
[2021-05-15 10:03:21,236] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Task committer attempt_202105151302058153171698965801296_0004_m_000100_279: needsTaskCommit() Task attempt_202105151302058153171698965801296_0004_m_000100_279: duration 0:00.001s
[2021-05-15 10:03:21,236] {docker.py:276} INFO - 21/05/15 13:03:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058153171698965801296_0004_m_000100_279
[2021-05-15 10:03:21,239] {docker.py:276} INFO - 21/05/15 13:03:21 INFO Executor: Finished task 100.0 in stage 4.0 (TID 279). 4544 bytes result sent to driver
[2021-05-15 10:03:21,241] {docker.py:276} INFO - 21/05/15 13:03:21 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 284) (4e8a4a26f4b5, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:21 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 279) in 2977 ms on 4e8a4a26f4b5 (executor driver) (102/200)
[2021-05-15 10:03:21,241] {docker.py:276} INFO - 21/05/15 13:03:21 INFO Executor: Running task 105.0 in stage 4.0 (TID 284)
[2021-05-15 10:03:21,250] {docker.py:276} INFO - 21/05/15 13:03:21 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:21,251] {docker.py:276} INFO - 21/05/15 13:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:21,252] {docker.py:276} INFO - 21/05/15 13:03:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:21,252] {docker.py:276} INFO - 21/05/15 13:03:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:21,253] {docker.py:276} INFO - 21/05/15 13:03:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056190148365124363852_0004_m_000105_284, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056190148365124363852_0004_m_000105_284}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056190148365124363852_0004}; taskId=attempt_202105151302056190148365124363852_0004_m_000105_284, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ab17d9c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105151302056190148365124363852_0004_m_000105_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056190148365124363852_0004_m_000105_284
[2021-05-15 10:03:21,255] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Task committer attempt_202105151302056190148365124363852_0004_m_000105_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056190148365124363852_0004_m_000105_284 : duration 0:00.003s
[2021-05-15 10:03:21,379] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105151302054439048987426795157_0004_m_000102_281: needsTaskCommit() Task attempt_202105151302054439048987426795157_0004_m_000102_281
21/05/15 13:03:21 INFO StagingCommitter: Task committer attempt_202105151302054439048987426795157_0004_m_000102_281: needsTaskCommit() Task attempt_202105151302054439048987426795157_0004_m_000102_281: duration 0:00.000s
21/05/15 13:03:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054439048987426795157_0004_m_000102_281
[2021-05-15 10:03:21,386] {docker.py:276} INFO - 21/05/15 13:03:21 INFO Executor: Finished task 102.0 in stage 4.0 (TID 281). 4544 bytes result sent to driver
[2021-05-15 10:03:21,387] {docker.py:276} INFO - 21/05/15 13:03:21 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 285) (4e8a4a26f4b5, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:21,388] {docker.py:276} INFO - 21/05/15 13:03:21 INFO Executor: Running task 106.0 in stage 4.0 (TID 285)
21/05/15 13:03:21 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 281) in 2369 ms on 4e8a4a26f4b5 (executor driver) (103/200)
[2021-05-15 10:03:21,397] {docker.py:276} INFO - 21/05/15 13:03:21 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:21,399] {docker.py:276} INFO - 21/05/15 13:03:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057121615505657956134_0004_m_000106_285, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057121615505657956134_0004_m_000106_285}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057121615505657956134_0004}; taskId=attempt_202105151302057121615505657956134_0004_m_000106_285, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28ab828e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:21,399] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105151302057121615505657956134_0004_m_000106_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057121615505657956134_0004_m_000106_285
[2021-05-15 10:03:21,402] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Task committer attempt_202105151302057121615505657956134_0004_m_000106_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057121615505657956134_0004_m_000106_285 : duration 0:00.003s
[2021-05-15 10:03:21,729] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105151302053700359380647605487_0004_m_000103_282: needsTaskCommit() Task attempt_202105151302053700359380647605487_0004_m_000103_282
[2021-05-15 10:03:21,751] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Task committer attempt_202105151302053700359380647605487_0004_m_000103_282: needsTaskCommit() Task attempt_202105151302053700359380647605487_0004_m_000103_282: duration 0:00.002s
21/05/15 13:03:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053700359380647605487_0004_m_000103_282
[2021-05-15 10:03:21,751] {docker.py:276} INFO - 21/05/15 13:03:21 INFO Executor: Finished task 103.0 in stage 4.0 (TID 282). 4544 bytes result sent to driver
[2021-05-15 10:03:21,752] {docker.py:276} INFO - 21/05/15 13:03:21 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 286) (4e8a4a26f4b5, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:21,752] {docker.py:276} INFO - 21/05/15 13:03:21 INFO Executor: Running task 107.0 in stage 4.0 (TID 286)
[2021-05-15 10:03:21,752] {docker.py:276} INFO - 21/05/15 13:03:21 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 282) in 2485 ms on 4e8a4a26f4b5 (executor driver) (104/200)
[2021-05-15 10:03:21,753] {docker.py:276} INFO - 21/05/15 13:03:21 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:21,754] {docker.py:276} INFO - 21/05/15 13:03:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053416461047830160779_0004_m_000107_286, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053416461047830160779_0004_m_000107_286}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053416461047830160779_0004}; taskId=attempt_202105151302053416461047830160779_0004_m_000107_286, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41d3ffb0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:21 INFO StagingCommitter: Starting: Task committer attempt_202105151302053416461047830160779_0004_m_000107_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053416461047830160779_0004_m_000107_286
[2021-05-15 10:03:21,757] {docker.py:276} INFO - 21/05/15 13:03:21 INFO StagingCommitter: Task committer attempt_202105151302053416461047830160779_0004_m_000107_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053416461047830160779_0004_m_000107_286 : duration 0:00.003s
[2021-05-15 10:03:23,243] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105151302058696911286320640544_0004_m_000104_283: needsTaskCommit() Task attempt_202105151302058696911286320640544_0004_m_000104_283
[2021-05-15 10:03:23,244] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Task committer attempt_202105151302058696911286320640544_0004_m_000104_283: needsTaskCommit() Task attempt_202105151302058696911286320640544_0004_m_000104_283: duration 0:00.000s
21/05/15 13:03:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058696911286320640544_0004_m_000104_283
[2021-05-15 10:03:23,245] {docker.py:276} INFO - 21/05/15 13:03:23 INFO Executor: Finished task 104.0 in stage 4.0 (TID 283). 4587 bytes result sent to driver
[2021-05-15 10:03:23,247] {docker.py:276} INFO - 21/05/15 13:03:23 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 287) (4e8a4a26f4b5, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:23,248] {docker.py:276} INFO - 21/05/15 13:03:23 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 283) in 2504 ms on 4e8a4a26f4b5 (executor driver) (105/200)
[2021-05-15 10:03:23,249] {docker.py:276} INFO - 21/05/15 13:03:23 INFO Executor: Running task 108.0 in stage 4.0 (TID 287)
[2021-05-15 10:03:23,258] {docker.py:276} INFO - 21/05/15 13:03:23 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:23,260] {docker.py:276} INFO - 21/05/15 13:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052659439857176445230_0004_m_000108_287, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052659439857176445230_0004_m_000108_287}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052659439857176445230_0004}; taskId=attempt_202105151302052659439857176445230_0004_m_000108_287, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f52e11}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:23,260] {docker.py:276} INFO - 21/05/15 13:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105151302052659439857176445230_0004_m_000108_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052659439857176445230_0004_m_000108_287
[2021-05-15 10:03:23,263] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Task committer attempt_202105151302052659439857176445230_0004_m_000108_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052659439857176445230_0004_m_000108_287 : duration 0:00.003s
[2021-05-15 10:03:23,746] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105151302056190148365124363852_0004_m_000105_284: needsTaskCommit() Task attempt_202105151302056190148365124363852_0004_m_000105_284
[2021-05-15 10:03:23,746] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Task committer attempt_202105151302056190148365124363852_0004_m_000105_284: needsTaskCommit() Task attempt_202105151302056190148365124363852_0004_m_000105_284: duration 0:00.001s
[2021-05-15 10:03:23,747] {docker.py:276} INFO - 21/05/15 13:03:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056190148365124363852_0004_m_000105_284
[2021-05-15 10:03:23,748] {docker.py:276} INFO - 21/05/15 13:03:23 INFO Executor: Finished task 105.0 in stage 4.0 (TID 284). 4587 bytes result sent to driver
[2021-05-15 10:03:23,749] {docker.py:276} INFO - 21/05/15 13:03:23 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 288) (4e8a4a26f4b5, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:23,751] {docker.py:276} INFO - 21/05/15 13:03:23 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 284) in 2515 ms on 4e8a4a26f4b5 (executor driver) (106/200)
[2021-05-15 10:03:23,752] {docker.py:276} INFO - 21/05/15 13:03:23 INFO Executor: Running task 109.0 in stage 4.0 (TID 288)
[2021-05-15 10:03:23,760] {docker.py:276} INFO - 21/05/15 13:03:23 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:23,761] {docker.py:276} INFO - 21/05/15 13:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058123372832987776101_0004_m_000109_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058123372832987776101_0004_m_000109_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058123372832987776101_0004}; taskId=attempt_202105151302058123372832987776101_0004_m_000109_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14be323a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:23,762] {docker.py:276} INFO - 21/05/15 13:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105151302058123372832987776101_0004_m_000109_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058123372832987776101_0004_m_000109_288
[2021-05-15 10:03:23,764] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Task committer attempt_202105151302058123372832987776101_0004_m_000109_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058123372832987776101_0004_m_000109_288 : duration 0:00.003s
[2021-05-15 10:03:23,777] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105151302057121615505657956134_0004_m_000106_285: needsTaskCommit() Task attempt_202105151302057121615505657956134_0004_m_000106_285
[2021-05-15 10:03:23,778] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Task committer attempt_202105151302057121615505657956134_0004_m_000106_285: needsTaskCommit() Task attempt_202105151302057121615505657956134_0004_m_000106_285: duration 0:00.000s
21/05/15 13:03:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057121615505657956134_0004_m_000106_285
[2021-05-15 10:03:23,779] {docker.py:276} INFO - 21/05/15 13:03:23 INFO Executor: Finished task 106.0 in stage 4.0 (TID 285). 4587 bytes result sent to driver
[2021-05-15 10:03:23,779] {docker.py:276} INFO - 21/05/15 13:03:23 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 289) (4e8a4a26f4b5, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:23,780] {docker.py:276} INFO - 21/05/15 13:03:23 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 285) in 2397 ms on 4e8a4a26f4b5 (executor driver) (107/200)
[2021-05-15 10:03:23,781] {docker.py:276} INFO - 21/05/15 13:03:23 INFO Executor: Running task 110.0 in stage 4.0 (TID 289)
[2021-05-15 10:03:23,792] {docker.py:276} INFO - 21/05/15 13:03:23 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:23,794] {docker.py:276} INFO - 21/05/15 13:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:23,794] {docker.py:276} INFO - 21/05/15 13:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058423387360970705183_0004_m_000110_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058423387360970705183_0004_m_000110_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058423387360970705183_0004}; taskId=attempt_202105151302058423387360970705183_0004_m_000110_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e7546f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105151302058423387360970705183_0004_m_000110_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058423387360970705183_0004_m_000110_289
[2021-05-15 10:03:23,797] {docker.py:276} INFO - 21/05/15 13:03:23 INFO StagingCommitter: Task committer attempt_202105151302058423387360970705183_0004_m_000110_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058423387360970705183_0004_m_000110_289 : duration 0:00.002s
[2021-05-15 10:03:24,078] {docker.py:276} INFO - 21/05/15 13:03:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302053416461047830160779_0004_m_000107_286: needsTaskCommit() Task attempt_202105151302053416461047830160779_0004_m_000107_286
[2021-05-15 10:03:24,079] {docker.py:276} INFO - 21/05/15 13:03:24 INFO StagingCommitter: Task committer attempt_202105151302053416461047830160779_0004_m_000107_286: needsTaskCommit() Task attempt_202105151302053416461047830160779_0004_m_000107_286: duration 0:00.001s
21/05/15 13:03:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053416461047830160779_0004_m_000107_286
[2021-05-15 10:03:24,080] {docker.py:276} INFO - 21/05/15 13:03:24 INFO Executor: Finished task 107.0 in stage 4.0 (TID 286). 4587 bytes result sent to driver
[2021-05-15 10:03:24,082] {docker.py:276} INFO - 21/05/15 13:03:24 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 290) (4e8a4a26f4b5, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:24,083] {docker.py:276} INFO - 21/05/15 13:03:24 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 286) in 2352 ms on 4e8a4a26f4b5 (executor driver) (108/200)
[2021-05-15 10:03:24,084] {docker.py:276} INFO - 21/05/15 13:03:24 INFO Executor: Running task 111.0 in stage 4.0 (TID 290)
[2021-05-15 10:03:24,093] {docker.py:276} INFO - 21/05/15 13:03:24 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:24,095] {docker.py:276} INFO - 21/05/15 13:03:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057205296871197129987_0004_m_000111_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057205296871197129987_0004_m_000111_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057205296871197129987_0004}; taskId=attempt_202105151302057205296871197129987_0004_m_000111_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54e825b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:24,096] {docker.py:276} INFO - 21/05/15 13:03:24 INFO StagingCommitter: Starting: Task committer attempt_202105151302057205296871197129987_0004_m_000111_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057205296871197129987_0004_m_000111_290
[2021-05-15 10:03:24,099] {docker.py:276} INFO - 21/05/15 13:03:24 INFO StagingCommitter: Task committer attempt_202105151302057205296871197129987_0004_m_000111_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057205296871197129987_0004_m_000111_290 : duration 0:00.004s
[2021-05-15 10:03:25,925] {docker.py:276} INFO - 21/05/15 13:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105151302052659439857176445230_0004_m_000108_287: needsTaskCommit() Task attempt_202105151302052659439857176445230_0004_m_000108_287
[2021-05-15 10:03:25,925] {docker.py:276} INFO - 21/05/15 13:03:25 INFO StagingCommitter: Task committer attempt_202105151302052659439857176445230_0004_m_000108_287: needsTaskCommit() Task attempt_202105151302052659439857176445230_0004_m_000108_287: duration 0:00.001s
[2021-05-15 10:03:25,926] {docker.py:276} INFO - 21/05/15 13:03:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052659439857176445230_0004_m_000108_287
[2021-05-15 10:03:25,926] {docker.py:276} INFO - 21/05/15 13:03:25 INFO Executor: Finished task 108.0 in stage 4.0 (TID 287). 4544 bytes result sent to driver
[2021-05-15 10:03:25,927] {docker.py:276} INFO - 21/05/15 13:03:25 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 291) (4e8a4a26f4b5, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:25,928] {docker.py:276} INFO - 21/05/15 13:03:25 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 287) in 2650 ms on 4e8a4a26f4b5 (executor driver) (109/200)
[2021-05-15 10:03:25,929] {docker.py:276} INFO - 21/05/15 13:03:25 INFO Executor: Running task 112.0 in stage 4.0 (TID 291)
[2021-05-15 10:03:25,936] {docker.py:276} INFO - 21/05/15 13:03:25 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:25,936] {docker.py:276} INFO - 21/05/15 13:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:25,938] {docker.py:276} INFO - 21/05/15 13:03:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:25,938] {docker.py:276} INFO - 21/05/15 13:03:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057473532855773820099_0004_m_000112_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057473532855773820099_0004_m_000112_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057473532855773820099_0004}; taskId=attempt_202105151302057473532855773820099_0004_m_000112_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70d50982}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:25,938] {docker.py:276} INFO - 21/05/15 13:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105151302057473532855773820099_0004_m_000112_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057473532855773820099_0004_m_000112_291
[2021-05-15 10:03:25,942] {docker.py:276} INFO - 21/05/15 13:03:25 INFO StagingCommitter: Task committer attempt_202105151302057473532855773820099_0004_m_000112_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057473532855773820099_0004_m_000112_291 : duration 0:00.003s
[2021-05-15 10:03:26,086] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Starting: Task committer attempt_202105151302058123372832987776101_0004_m_000109_288: needsTaskCommit() Task attempt_202105151302058123372832987776101_0004_m_000109_288
[2021-05-15 10:03:26,086] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Task committer attempt_202105151302058123372832987776101_0004_m_000109_288: needsTaskCommit() Task attempt_202105151302058123372832987776101_0004_m_000109_288: duration 0:00.000s
21/05/15 13:03:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058123372832987776101_0004_m_000109_288
[2021-05-15 10:03:26,087] {docker.py:276} INFO - 21/05/15 13:03:26 INFO Executor: Finished task 109.0 in stage 4.0 (TID 288). 4544 bytes result sent to driver
[2021-05-15 10:03:26,088] {docker.py:276} INFO - 21/05/15 13:03:26 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 292) (4e8a4a26f4b5, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:26,089] {docker.py:276} INFO - 21/05/15 13:03:26 INFO Executor: Running task 113.0 in stage 4.0 (TID 292)
21/05/15 13:03:26 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 288) in 2308 ms on 4e8a4a26f4b5 (executor driver) (110/200)
[2021-05-15 10:03:26,096] {docker.py:276} INFO - 21/05/15 13:03:26 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:26,098] {docker.py:276} INFO - 21/05/15 13:03:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058188229374968537083_0004_m_000113_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058188229374968537083_0004_m_000113_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058188229374968537083_0004}; taskId=attempt_202105151302058188229374968537083_0004_m_000113_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b32391a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:26,099] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Starting: Task committer attempt_202105151302058188229374968537083_0004_m_000113_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058188229374968537083_0004_m_000113_292
[2021-05-15 10:03:26,102] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Task committer attempt_202105151302058188229374968537083_0004_m_000113_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058188229374968537083_0004_m_000113_292 : duration 0:00.004s
[2021-05-15 10:03:26,286] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Starting: Task committer attempt_202105151302058423387360970705183_0004_m_000110_289: needsTaskCommit() Task attempt_202105151302058423387360970705183_0004_m_000110_289
[2021-05-15 10:03:26,287] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Task committer attempt_202105151302058423387360970705183_0004_m_000110_289: needsTaskCommit() Task attempt_202105151302058423387360970705183_0004_m_000110_289: duration 0:00.000s
21/05/15 13:03:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058423387360970705183_0004_m_000110_289
[2021-05-15 10:03:26,288] {docker.py:276} INFO - 21/05/15 13:03:26 INFO Executor: Finished task 110.0 in stage 4.0 (TID 289). 4544 bytes result sent to driver
[2021-05-15 10:03:26,289] {docker.py:276} INFO - 21/05/15 13:03:26 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 293) (4e8a4a26f4b5, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:26,290] {docker.py:276} INFO - 21/05/15 13:03:26 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 289) in 2478 ms on 4e8a4a26f4b5 (executor driver) (111/200)
[2021-05-15 10:03:26,291] {docker.py:276} INFO - 21/05/15 13:03:26 INFO Executor: Running task 114.0 in stage 4.0 (TID 293)
[2021-05-15 10:03:26,301] {docker.py:276} INFO - 21/05/15 13:03:26 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:26,302] {docker.py:276} INFO - 21/05/15 13:03:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205286021727464035647_0004_m_000114_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205286021727464035647_0004_m_000114_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205286021727464035647_0004}; taskId=attempt_20210515130205286021727464035647_0004_m_000114_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6452d409}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:26,303] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Starting: Task committer attempt_20210515130205286021727464035647_0004_m_000114_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205286021727464035647_0004_m_000114_293
[2021-05-15 10:03:26,305] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Task committer attempt_20210515130205286021727464035647_0004_m_000114_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205286021727464035647_0004_m_000114_293 : duration 0:00.002s
[2021-05-15 10:03:26,428] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Starting: Task committer attempt_202105151302057205296871197129987_0004_m_000111_290: needsTaskCommit() Task attempt_202105151302057205296871197129987_0004_m_000111_290
[2021-05-15 10:03:26,429] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Task committer attempt_202105151302057205296871197129987_0004_m_000111_290: needsTaskCommit() Task attempt_202105151302057205296871197129987_0004_m_000111_290: duration 0:00.001s
21/05/15 13:03:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057205296871197129987_0004_m_000111_290
[2021-05-15 10:03:26,430] {docker.py:276} INFO - 21/05/15 13:03:26 INFO Executor: Finished task 111.0 in stage 4.0 (TID 290). 4544 bytes result sent to driver
[2021-05-15 10:03:26,432] {docker.py:276} INFO - 21/05/15 13:03:26 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 294) (4e8a4a26f4b5, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:26,433] {docker.py:276} INFO - 21/05/15 13:03:26 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 290) in 2318 ms on 4e8a4a26f4b5 (executor driver) (112/200)
[2021-05-15 10:03:26,434] {docker.py:276} INFO - 21/05/15 13:03:26 INFO Executor: Running task 115.0 in stage 4.0 (TID 294)
[2021-05-15 10:03:26,445] {docker.py:276} INFO - 21/05/15 13:03:26 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:26,447] {docker.py:276} INFO - 21/05/15 13:03:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205302025704772625829_0004_m_000115_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205302025704772625829_0004_m_000115_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205302025704772625829_0004}; taskId=attempt_20210515130205302025704772625829_0004_m_000115_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14ffa045}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:26,447] {docker.py:276} INFO - 21/05/15 13:03:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:26 INFO StagingCommitter: Starting: Task committer attempt_20210515130205302025704772625829_0004_m_000115_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205302025704772625829_0004_m_000115_294
[2021-05-15 10:03:26,450] {docker.py:276} INFO - 21/05/15 13:03:26 INFO StagingCommitter: Task committer attempt_20210515130205302025704772625829_0004_m_000115_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205302025704772625829_0004_m_000115_294 : duration 0:00.002s
[2021-05-15 10:03:28,400] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105151302057473532855773820099_0004_m_000112_291: needsTaskCommit() Task attempt_202105151302057473532855773820099_0004_m_000112_291
21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_202105151302057473532855773820099_0004_m_000112_291: needsTaskCommit() Task attempt_202105151302057473532855773820099_0004_m_000112_291: duration 0:00.001s
21/05/15 13:03:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057473532855773820099_0004_m_000112_291
[2021-05-15 10:03:28,401] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Finished task 112.0 in stage 4.0 (TID 291). 4544 bytes result sent to driver
[2021-05-15 10:03:28,402] {docker.py:276} INFO - 21/05/15 13:03:28 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 295) (4e8a4a26f4b5, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:28,404] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Running task 116.0 in stage 4.0 (TID 295)
21/05/15 13:03:28 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 291) in 2479 ms on 4e8a4a26f4b5 (executor driver) (113/200)
[2021-05-15 10:03:28,414] {docker.py:276} INFO - 21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:28,415] {docker.py:276} INFO - 21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:28,417] {docker.py:276} INFO - 21/05/15 13:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:28,417] {docker.py:276} INFO - 21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057106056199958686164_0004_m_000116_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057106056199958686164_0004_m_000116_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057106056199958686164_0004}; taskId=attempt_202105151302057106056199958686164_0004_m_000116_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20495c92}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:28,418] {docker.py:276} INFO - 21/05/15 13:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:28,418] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105151302057106056199958686164_0004_m_000116_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057106056199958686164_0004_m_000116_295
[2021-05-15 10:03:28,420] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_202105151302057106056199958686164_0004_m_000116_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057106056199958686164_0004_m_000116_295 : duration 0:00.003s
[2021-05-15 10:03:28,497] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105151302058188229374968537083_0004_m_000113_292: needsTaskCommit() Task attempt_202105151302058188229374968537083_0004_m_000113_292
[2021-05-15 10:03:28,497] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_202105151302058188229374968537083_0004_m_000113_292: needsTaskCommit() Task attempt_202105151302058188229374968537083_0004_m_000113_292: duration 0:00.001s
21/05/15 13:03:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058188229374968537083_0004_m_000113_292
[2021-05-15 10:03:28,499] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Finished task 113.0 in stage 4.0 (TID 292). 4544 bytes result sent to driver
[2021-05-15 10:03:28,501] {docker.py:276} INFO - 21/05/15 13:03:28 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 296) (4e8a4a26f4b5, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:28,502] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Running task 117.0 in stage 4.0 (TID 296)
21/05/15 13:03:28 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 292) in 2416 ms on 4e8a4a26f4b5 (executor driver) (114/200)
[2021-05-15 10:03:28,512] {docker.py:276} INFO - 21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:28,514] {docker.py:276} INFO - 21/05/15 13:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056631370847744243987_0004_m_000117_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056631370847744243987_0004_m_000117_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056631370847744243987_0004}; taskId=attempt_202105151302056631370847744243987_0004_m_000117_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7dda9561}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:28,514] {docker.py:276} INFO - 21/05/15 13:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105151302056631370847744243987_0004_m_000117_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056631370847744243987_0004_m_000117_296
[2021-05-15 10:03:28,517] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_202105151302056631370847744243987_0004_m_000117_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056631370847744243987_0004_m_000117_296 : duration 0:00.003s
[2021-05-15 10:03:28,803] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_20210515130205286021727464035647_0004_m_000114_293: needsTaskCommit() Task attempt_20210515130205286021727464035647_0004_m_000114_293
[2021-05-15 10:03:28,804] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_20210515130205286021727464035647_0004_m_000114_293: needsTaskCommit() Task attempt_20210515130205286021727464035647_0004_m_000114_293: duration 0:00.001s
21/05/15 13:03:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205286021727464035647_0004_m_000114_293
[2021-05-15 10:03:28,806] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Finished task 114.0 in stage 4.0 (TID 293). 4544 bytes result sent to driver
[2021-05-15 10:03:28,808] {docker.py:276} INFO - 21/05/15 13:03:28 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 297) (4e8a4a26f4b5, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:28,808] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Running task 118.0 in stage 4.0 (TID 297)
[2021-05-15 10:03:28,810] {docker.py:276} INFO - 21/05/15 13:03:28 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 293) in 2524 ms on 4e8a4a26f4b5 (executor driver) (115/200)
[2021-05-15 10:03:28,814] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_20210515130205302025704772625829_0004_m_000115_294: needsTaskCommit() Task attempt_20210515130205302025704772625829_0004_m_000115_294
[2021-05-15 10:03:28,814] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_20210515130205302025704772625829_0004_m_000115_294: needsTaskCommit() Task attempt_20210515130205302025704772625829_0004_m_000115_294: duration 0:00.001s
[2021-05-15 10:03:28,815] {docker.py:276} INFO - 21/05/15 13:03:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205302025704772625829_0004_m_000115_294
[2021-05-15 10:03:28,816] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Finished task 115.0 in stage 4.0 (TID 294). 4544 bytes result sent to driver
[2021-05-15 10:03:28,816] {docker.py:276} INFO - 21/05/15 13:03:28 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 298) (4e8a4a26f4b5, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:28,817] {docker.py:276} INFO - 21/05/15 13:03:28 INFO Executor: Running task 119.0 in stage 4.0 (TID 298)
21/05/15 13:03:28 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 294) in 2388 ms on 4e8a4a26f4b5 (executor driver) (116/200)
[2021-05-15 10:03:28,820] {docker.py:276} INFO - 21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:28,821] {docker.py:276} INFO - 21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:28,823] {docker.py:276} INFO - 21/05/15 13:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205173246047591823355_0004_m_000118_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205173246047591823355_0004_m_000118_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205173246047591823355_0004}; taskId=attempt_20210515130205173246047591823355_0004_m_000118_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@257abf25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_20210515130205173246047591823355_0004_m_000118_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205173246047591823355_0004_m_000118_297
[2021-05-15 10:03:28,826] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_20210515130205173246047591823355_0004_m_000118_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205173246047591823355_0004_m_000118_297 : duration 0:00.003s
[2021-05-15 10:03:28,827] {docker.py:276} INFO - 21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:28,829] {docker.py:276} INFO - 21/05/15 13:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:03:28,830] {docker.py:276} INFO - 21/05/15 13:03:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:28,830] {docker.py:276} INFO - 21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:28,831] {docker.py:276} INFO - 21/05/15 13:03:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058393194397376754775_0004_m_000119_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058393194397376754775_0004_m_000119_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058393194397376754775_0004}; taskId=attempt_202105151302058393194397376754775_0004_m_000119_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ab61eca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:28,831] {docker.py:276} INFO - 21/05/15 13:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105151302058393194397376754775_0004_m_000119_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058393194397376754775_0004_m_000119_298
[2021-05-15 10:03:28,835] {docker.py:276} INFO - 21/05/15 13:03:28 INFO StagingCommitter: Task committer attempt_202105151302058393194397376754775_0004_m_000119_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058393194397376754775_0004_m_000119_298 : duration 0:00.004s
[2021-05-15 10:03:30,707] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302057106056199958686164_0004_m_000116_295: needsTaskCommit() Task attempt_202105151302057106056199958686164_0004_m_000116_295
[2021-05-15 10:03:30,708] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Task committer attempt_202105151302057106056199958686164_0004_m_000116_295: needsTaskCommit() Task attempt_202105151302057106056199958686164_0004_m_000116_295: duration 0:00.001s
21/05/15 13:03:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057106056199958686164_0004_m_000116_295
[2021-05-15 10:03:30,710] {docker.py:276} INFO - 21/05/15 13:03:30 INFO Executor: Finished task 116.0 in stage 4.0 (TID 295). 4587 bytes result sent to driver
[2021-05-15 10:03:30,711] {docker.py:276} INFO - 21/05/15 13:03:30 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 299) (4e8a4a26f4b5, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:30,713] {docker.py:276} INFO - 21/05/15 13:03:30 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 295) in 2313 ms on 4e8a4a26f4b5 (executor driver) (117/200)
[2021-05-15 10:03:30,713] {docker.py:276} INFO - 21/05/15 13:03:30 INFO Executor: Running task 120.0 in stage 4.0 (TID 299)
[2021-05-15 10:03:30,723] {docker.py:276} INFO - 21/05/15 13:03:30 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:30,724] {docker.py:276} INFO - 21/05/15 13:03:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:30,725] {docker.py:276} INFO - 21/05/15 13:03:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053499204987933504645_0004_m_000120_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053499204987933504645_0004_m_000120_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053499204987933504645_0004}; taskId=attempt_202105151302053499204987933504645_0004_m_000120_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75c40aae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:30,725] {docker.py:276} INFO - 21/05/15 13:03:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:30,725] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302053499204987933504645_0004_m_000120_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053499204987933504645_0004_m_000120_299
[2021-05-15 10:03:30,728] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Task committer attempt_202105151302053499204987933504645_0004_m_000120_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053499204987933504645_0004_m_000120_299 : duration 0:00.003s
[2021-05-15 10:03:30,827] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302056631370847744243987_0004_m_000117_296: needsTaskCommit() Task attempt_202105151302056631370847744243987_0004_m_000117_296
21/05/15 13:03:30 INFO StagingCommitter: Task committer attempt_202105151302056631370847744243987_0004_m_000117_296: needsTaskCommit() Task attempt_202105151302056631370847744243987_0004_m_000117_296: duration 0:00.001s
[2021-05-15 10:03:30,828] {docker.py:276} INFO - 21/05/15 13:03:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056631370847744243987_0004_m_000117_296
[2021-05-15 10:03:30,830] {docker.py:276} INFO - 21/05/15 13:03:30 INFO Executor: Finished task 117.0 in stage 4.0 (TID 296). 4587 bytes result sent to driver
[2021-05-15 10:03:30,831] {docker.py:276} INFO - 21/05/15 13:03:30 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 300) (4e8a4a26f4b5, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:30,832] {docker.py:276} INFO - 21/05/15 13:03:30 INFO Executor: Running task 121.0 in stage 4.0 (TID 300)
[2021-05-15 10:03:30,833] {docker.py:276} INFO - 21/05/15 13:03:30 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 296) in 2335 ms on 4e8a4a26f4b5 (executor driver) (118/200)
[2021-05-15 10:03:30,842] {docker.py:276} INFO - 21/05/15 13:03:30 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:30,844] {docker.py:276} INFO - 21/05/15 13:03:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:30,844] {docker.py:276} INFO - 21/05/15 13:03:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057202039598410789107_0004_m_000121_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057202039598410789107_0004_m_000121_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057202039598410789107_0004}; taskId=attempt_202105151302057202039598410789107_0004_m_000121_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9ea0112}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:30,845] {docker.py:276} INFO - 21/05/15 13:03:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302057202039598410789107_0004_m_000121_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057202039598410789107_0004_m_000121_300
[2021-05-15 10:03:30,847] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Task committer attempt_202105151302057202039598410789107_0004_m_000121_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057202039598410789107_0004_m_000121_300 : duration 0:00.003s
[2021-05-15 10:03:30,877] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302058393194397376754775_0004_m_000119_298: needsTaskCommit() Task attempt_202105151302058393194397376754775_0004_m_000119_298
[2021-05-15 10:03:30,879] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Task committer attempt_202105151302058393194397376754775_0004_m_000119_298: needsTaskCommit() Task attempt_202105151302058393194397376754775_0004_m_000119_298: duration 0:00.001s
21/05/15 13:03:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058393194397376754775_0004_m_000119_298
[2021-05-15 10:03:30,881] {docker.py:276} INFO - 21/05/15 13:03:30 INFO Executor: Finished task 119.0 in stage 4.0 (TID 298). 4587 bytes result sent to driver
[2021-05-15 10:03:30,882] {docker.py:276} INFO - 21/05/15 13:03:30 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 301) (4e8a4a26f4b5, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:30,884] {docker.py:276} INFO - 21/05/15 13:03:30 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 298) in 2070 ms on 4e8a4a26f4b5 (executor driver) (119/200)
[2021-05-15 10:03:30,884] {docker.py:276} INFO - 21/05/15 13:03:30 INFO Executor: Running task 122.0 in stage 4.0 (TID 301)
[2021-05-15 10:03:30,895] {docker.py:276} INFO - 21/05/15 13:03:30 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:30,896] {docker.py:276} INFO - 21/05/15 13:03:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053554500179063730353_0004_m_000122_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053554500179063730353_0004_m_000122_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053554500179063730353_0004}; taskId=attempt_202105151302053554500179063730353_0004_m_000122_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@393f6a58}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:30,897] {docker.py:276} INFO - 21/05/15 13:03:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:30 INFO StagingCommitter: Starting: Task committer attempt_202105151302053554500179063730353_0004_m_000122_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053554500179063730353_0004_m_000122_301
[2021-05-15 10:03:30,900] {docker.py:276} INFO - 21/05/15 13:03:30 INFO StagingCommitter: Task committer attempt_202105151302053554500179063730353_0004_m_000122_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053554500179063730353_0004_m_000122_301 : duration 0:00.003s
[2021-05-15 10:03:31,213] {docker.py:276} INFO - 21/05/15 13:03:31 INFO StagingCommitter: Starting: Task committer attempt_20210515130205173246047591823355_0004_m_000118_297: needsTaskCommit() Task attempt_20210515130205173246047591823355_0004_m_000118_297
[2021-05-15 10:03:31,214] {docker.py:276} INFO - 21/05/15 13:03:31 INFO StagingCommitter: Task committer attempt_20210515130205173246047591823355_0004_m_000118_297: needsTaskCommit() Task attempt_20210515130205173246047591823355_0004_m_000118_297: duration 0:00.000s
21/05/15 13:03:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205173246047591823355_0004_m_000118_297
[2021-05-15 10:03:31,215] {docker.py:276} INFO - 21/05/15 13:03:31 INFO Executor: Finished task 118.0 in stage 4.0 (TID 297). 4587 bytes result sent to driver
[2021-05-15 10:03:31,217] {docker.py:276} INFO - 21/05/15 13:03:31 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 302) (4e8a4a26f4b5, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:31,218] {docker.py:276} INFO - 21/05/15 13:03:31 INFO Executor: Running task 123.0 in stage 4.0 (TID 302)
[2021-05-15 10:03:31,219] {docker.py:276} INFO - 21/05/15 13:03:31 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 297) in 2414 ms on 4e8a4a26f4b5 (executor driver) (120/200)
[2021-05-15 10:03:31,228] {docker.py:276} INFO - 21/05/15 13:03:31 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:31,229] {docker.py:276} INFO - 21/05/15 13:03:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:31,230] {docker.py:276} INFO - 21/05/15 13:03:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056203835025110693999_0004_m_000123_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056203835025110693999_0004_m_000123_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056203835025110693999_0004}; taskId=attempt_202105151302056203835025110693999_0004_m_000123_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@440d85f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:31,230] {docker.py:276} INFO - 21/05/15 13:03:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:31,231] {docker.py:276} INFO - 21/05/15 13:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105151302056203835025110693999_0004_m_000123_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056203835025110693999_0004_m_000123_302
[2021-05-15 10:03:31,233] {docker.py:276} INFO - 21/05/15 13:03:31 INFO StagingCommitter: Task committer attempt_202105151302056203835025110693999_0004_m_000123_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056203835025110693999_0004_m_000123_302 : duration 0:00.003s
[2021-05-15 10:03:33,032] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Starting: Task committer attempt_202105151302057202039598410789107_0004_m_000121_300: needsTaskCommit() Task attempt_202105151302057202039598410789107_0004_m_000121_300
[2021-05-15 10:03:33,032] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Task committer attempt_202105151302057202039598410789107_0004_m_000121_300: needsTaskCommit() Task attempt_202105151302057202039598410789107_0004_m_000121_300: duration 0:00.001s
21/05/15 13:03:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057202039598410789107_0004_m_000121_300
[2021-05-15 10:03:33,034] {docker.py:276} INFO - 21/05/15 13:03:33 INFO Executor: Finished task 121.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
[2021-05-15 10:03:33,035] {docker.py:276} INFO - 21/05/15 13:03:33 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 303) (4e8a4a26f4b5, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:33,035] {docker.py:276} INFO - 21/05/15 13:03:33 INFO Executor: Running task 124.0 in stage 4.0 (TID 303)
[2021-05-15 10:03:33,036] {docker.py:276} INFO - 21/05/15 13:03:33 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 300) in 2208 ms on 4e8a4a26f4b5 (executor driver) (121/200)
[2021-05-15 10:03:33,044] {docker.py:276} INFO - 21/05/15 13:03:33 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:33,046] {docker.py:276} INFO - 21/05/15 13:03:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058323400401685512253_0004_m_000124_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058323400401685512253_0004_m_000124_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058323400401685512253_0004}; taskId=attempt_202105151302058323400401685512253_0004_m_000124_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e3c91ed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:33,046] {docker.py:276} INFO - 21/05/15 13:03:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:33 INFO StagingCommitter: Starting: Task committer attempt_202105151302058323400401685512253_0004_m_000124_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058323400401685512253_0004_m_000124_303
[2021-05-15 10:03:33,049] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Task committer attempt_202105151302058323400401685512253_0004_m_000124_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058323400401685512253_0004_m_000124_303 : duration 0:00.004s
[2021-05-15 10:03:33,198] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Starting: Task committer attempt_202105151302053554500179063730353_0004_m_000122_301: needsTaskCommit() Task attempt_202105151302053554500179063730353_0004_m_000122_301
[2021-05-15 10:03:33,200] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Task committer attempt_202105151302053554500179063730353_0004_m_000122_301: needsTaskCommit() Task attempt_202105151302053554500179063730353_0004_m_000122_301: duration 0:00.001s
21/05/15 13:03:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053554500179063730353_0004_m_000122_301
[2021-05-15 10:03:33,202] {docker.py:276} INFO - 21/05/15 13:03:33 INFO Executor: Finished task 122.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-15 10:03:33,203] {docker.py:276} INFO - 21/05/15 13:03:33 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 304) (4e8a4a26f4b5, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:33,204] {docker.py:276} INFO - 21/05/15 13:03:33 INFO Executor: Running task 125.0 in stage 4.0 (TID 304)
[2021-05-15 10:03:33,205] {docker.py:276} INFO - 21/05/15 13:03:33 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 301) in 2326 ms on 4e8a4a26f4b5 (executor driver) (122/200)
[2021-05-15 10:03:33,214] {docker.py:276} INFO - 21/05/15 13:03:33 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:33,216] {docker.py:276} INFO - 21/05/15 13:03:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052752346993506574116_0004_m_000125_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052752346993506574116_0004_m_000125_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052752346993506574116_0004}; taskId=attempt_202105151302052752346993506574116_0004_m_000125_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e54927b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:33,217] {docker.py:276} INFO - 21/05/15 13:03:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:33 INFO StagingCommitter: Starting: Task committer attempt_202105151302052752346993506574116_0004_m_000125_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052752346993506574116_0004_m_000125_304
[2021-05-15 10:03:33,219] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Task committer attempt_202105151302052752346993506574116_0004_m_000125_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052752346993506574116_0004_m_000125_304 : duration 0:00.003s
[2021-05-15 10:03:33,635] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Starting: Task committer attempt_202105151302056203835025110693999_0004_m_000123_302: needsTaskCommit() Task attempt_202105151302056203835025110693999_0004_m_000123_302
[2021-05-15 10:03:33,636] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Task committer attempt_202105151302056203835025110693999_0004_m_000123_302: needsTaskCommit() Task attempt_202105151302056203835025110693999_0004_m_000123_302: duration 0:00.000s
21/05/15 13:03:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056203835025110693999_0004_m_000123_302
[2021-05-15 10:03:33,636] {docker.py:276} INFO - 21/05/15 13:03:33 INFO Executor: Finished task 123.0 in stage 4.0 (TID 302). 4544 bytes result sent to driver
[2021-05-15 10:03:33,638] {docker.py:276} INFO - 21/05/15 13:03:33 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 305) (4e8a4a26f4b5, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:33 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 302) in 2423 ms on 4e8a4a26f4b5 (executor driver) (123/200)
[2021-05-15 10:03:33,638] {docker.py:276} INFO - 21/05/15 13:03:33 INFO Executor: Running task 126.0 in stage 4.0 (TID 305)
[2021-05-15 10:03:33,645] {docker.py:276} INFO - 21/05/15 13:03:33 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:33,647] {docker.py:276} INFO - 21/05/15 13:03:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059026247720312858614_0004_m_000126_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059026247720312858614_0004_m_000126_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059026247720312858614_0004}; taskId=attempt_202105151302059026247720312858614_0004_m_000126_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30b6dc82}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:33 INFO StagingCommitter: Starting: Task committer attempt_202105151302059026247720312858614_0004_m_000126_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059026247720312858614_0004_m_000126_305
[2021-05-15 10:03:33,650] {docker.py:276} INFO - 21/05/15 13:03:33 INFO StagingCommitter: Task committer attempt_202105151302059026247720312858614_0004_m_000126_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059026247720312858614_0004_m_000126_305 : duration 0:00.003s
[2021-05-15 10:03:34,146] {docker.py:276} INFO - 21/05/15 13:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302053499204987933504645_0004_m_000120_299: needsTaskCommit() Task attempt_202105151302053499204987933504645_0004_m_000120_299
[2021-05-15 10:03:34,147] {docker.py:276} INFO - 21/05/15 13:03:34 INFO StagingCommitter: Task committer attempt_202105151302053499204987933504645_0004_m_000120_299: needsTaskCommit() Task attempt_202105151302053499204987933504645_0004_m_000120_299: duration 0:00.002s
21/05/15 13:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053499204987933504645_0004_m_000120_299
[2021-05-15 10:03:34,149] {docker.py:276} INFO - 21/05/15 13:03:34 INFO Executor: Finished task 120.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-15 10:03:34,150] {docker.py:276} INFO - 21/05/15 13:03:34 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 306) (4e8a4a26f4b5, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:34,151] {docker.py:276} INFO - 21/05/15 13:03:34 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 299) in 3444 ms on 4e8a4a26f4b5 (executor driver) (124/200)
[2021-05-15 10:03:34,152] {docker.py:276} INFO - 21/05/15 13:03:34 INFO Executor: Running task 127.0 in stage 4.0 (TID 306)
[2021-05-15 10:03:34,163] {docker.py:276} INFO - 21/05/15 13:03:34 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:34,165] {docker.py:276} INFO - 21/05/15 13:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057941956171421296841_0004_m_000127_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057941956171421296841_0004_m_000127_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057941956171421296841_0004}; taskId=attempt_202105151302057941956171421296841_0004_m_000127_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11b730a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105151302057941956171421296841_0004_m_000127_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057941956171421296841_0004_m_000127_306
[2021-05-15 10:03:34,168] {docker.py:276} INFO - 21/05/15 13:03:34 INFO StagingCommitter: Task committer attempt_202105151302057941956171421296841_0004_m_000127_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057941956171421296841_0004_m_000127_306 : duration 0:00.003s
[2021-05-15 10:03:35,025] {docker.py:276} INFO - 21/05/15 13:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105151302058323400401685512253_0004_m_000124_303: needsTaskCommit() Task attempt_202105151302058323400401685512253_0004_m_000124_303
[2021-05-15 10:03:35,026] {docker.py:276} INFO - 21/05/15 13:03:35 INFO StagingCommitter: Task committer attempt_202105151302058323400401685512253_0004_m_000124_303: needsTaskCommit() Task attempt_202105151302058323400401685512253_0004_m_000124_303: duration 0:00.000s
21/05/15 13:03:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058323400401685512253_0004_m_000124_303
[2021-05-15 10:03:35,027] {docker.py:276} INFO - 21/05/15 13:03:35 INFO Executor: Finished task 124.0 in stage 4.0 (TID 303). 4544 bytes result sent to driver
[2021-05-15 10:03:35,028] {docker.py:276} INFO - 21/05/15 13:03:35 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 307) (4e8a4a26f4b5, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:35,028] {docker.py:276} INFO - 21/05/15 13:03:35 INFO Executor: Running task 128.0 in stage 4.0 (TID 307)
[2021-05-15 10:03:35,030] {docker.py:276} INFO - 21/05/15 13:03:35 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 303) in 1998 ms on 4e8a4a26f4b5 (executor driver) (125/200)
[2021-05-15 10:03:35,037] {docker.py:276} INFO - 21/05/15 13:03:35 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:35,039] {docker.py:276} INFO - 21/05/15 13:03:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054754667414392979359_0004_m_000128_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054754667414392979359_0004_m_000128_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054754667414392979359_0004}; taskId=attempt_202105151302054754667414392979359_0004_m_000128_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@381fab3f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:35,039] {docker.py:276} INFO - 21/05/15 13:03:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105151302054754667414392979359_0004_m_000128_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054754667414392979359_0004_m_000128_307
[2021-05-15 10:03:35,042] {docker.py:276} INFO - 21/05/15 13:03:35 INFO StagingCommitter: Task committer attempt_202105151302054754667414392979359_0004_m_000128_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054754667414392979359_0004_m_000128_307 : duration 0:00.004s
[2021-05-15 10:03:35,644] {docker.py:276} INFO - 21/05/15 13:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105151302052752346993506574116_0004_m_000125_304: needsTaskCommit() Task attempt_202105151302052752346993506574116_0004_m_000125_304
21/05/15 13:03:35 INFO StagingCommitter: Task committer attempt_202105151302052752346993506574116_0004_m_000125_304: needsTaskCommit() Task attempt_202105151302052752346993506574116_0004_m_000125_304: duration 0:00.001s
21/05/15 13:03:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052752346993506574116_0004_m_000125_304
[2021-05-15 10:03:35,646] {docker.py:276} INFO - 21/05/15 13:03:35 INFO Executor: Finished task 125.0 in stage 4.0 (TID 304). 4544 bytes result sent to driver
[2021-05-15 10:03:35,647] {docker.py:276} INFO - 21/05/15 13:03:35 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 308) (4e8a4a26f4b5, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:35,648] {docker.py:276} INFO - 21/05/15 13:03:35 INFO Executor: Running task 129.0 in stage 4.0 (TID 308)
[2021-05-15 10:03:35,648] {docker.py:276} INFO - 21/05/15 13:03:35 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 304) in 2449 ms on 4e8a4a26f4b5 (executor driver) (126/200)
[2021-05-15 10:03:35,657] {docker.py:276} INFO - 21/05/15 13:03:35 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:35,659] {docker.py:276} INFO - 21/05/15 13:03:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057244954947793785999_0004_m_000129_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057244954947793785999_0004_m_000129_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057244954947793785999_0004}; taskId=attempt_202105151302057244954947793785999_0004_m_000129_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60929253}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105151302057244954947793785999_0004_m_000129_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057244954947793785999_0004_m_000129_308
[2021-05-15 10:03:35,663] {docker.py:276} INFO - 21/05/15 13:03:35 INFO StagingCommitter: Task committer attempt_202105151302057244954947793785999_0004_m_000129_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057244954947793785999_0004_m_000129_308 : duration 0:00.004s
[2021-05-15 10:03:35,998] {docker.py:276} INFO - 21/05/15 13:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105151302059026247720312858614_0004_m_000126_305: needsTaskCommit() Task attempt_202105151302059026247720312858614_0004_m_000126_305
21/05/15 13:03:36 INFO StagingCommitter: Task committer attempt_202105151302059026247720312858614_0004_m_000126_305: needsTaskCommit() Task attempt_202105151302059026247720312858614_0004_m_000126_305: duration 0:00.001s
21/05/15 13:03:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059026247720312858614_0004_m_000126_305
[2021-05-15 10:03:36,000] {docker.py:276} INFO - 21/05/15 13:03:36 INFO Executor: Finished task 126.0 in stage 4.0 (TID 305). 4544 bytes result sent to driver
[2021-05-15 10:03:36,001] {docker.py:276} INFO - 21/05/15 13:03:36 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 309) (4e8a4a26f4b5, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:36,002] {docker.py:276} INFO - 21/05/15 13:03:36 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 305) in 2368 ms on 4e8a4a26f4b5 (executor driver) (127/200)
[2021-05-15 10:03:36,003] {docker.py:276} INFO - 21/05/15 13:03:36 INFO Executor: Running task 130.0 in stage 4.0 (TID 309)
[2021-05-15 10:03:36,012] {docker.py:276} INFO - 21/05/15 13:03:36 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:36,014] {docker.py:276} INFO - 21/05/15 13:03:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056923013690332174562_0004_m_000130_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056923013690332174562_0004_m_000130_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056923013690332174562_0004}; taskId=attempt_202105151302056923013690332174562_0004_m_000130_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6018bc9b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:36,015] {docker.py:276} INFO - 21/05/15 13:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105151302056923013690332174562_0004_m_000130_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056923013690332174562_0004_m_000130_309
[2021-05-15 10:03:36,018] {docker.py:276} INFO - 21/05/15 13:03:36 INFO StagingCommitter: Task committer attempt_202105151302056923013690332174562_0004_m_000130_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056923013690332174562_0004_m_000130_309 : duration 0:00.004s
[2021-05-15 10:03:36,467] {docker.py:276} INFO - 21/05/15 13:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105151302057941956171421296841_0004_m_000127_306: needsTaskCommit() Task attempt_202105151302057941956171421296841_0004_m_000127_306
[2021-05-15 10:03:36,468] {docker.py:276} INFO - 21/05/15 13:03:36 INFO StagingCommitter: Task committer attempt_202105151302057941956171421296841_0004_m_000127_306: needsTaskCommit() Task attempt_202105151302057941956171421296841_0004_m_000127_306: duration 0:00.001s
[2021-05-15 10:03:36,469] {docker.py:276} INFO - 21/05/15 13:03:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057941956171421296841_0004_m_000127_306
[2021-05-15 10:03:36,470] {docker.py:276} INFO - 21/05/15 13:03:36 INFO Executor: Finished task 127.0 in stage 4.0 (TID 306). 4544 bytes result sent to driver
[2021-05-15 10:03:36,472] {docker.py:276} INFO - 21/05/15 13:03:36 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 310) (4e8a4a26f4b5, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:36,473] {docker.py:276} INFO - 21/05/15 13:03:36 INFO Executor: Running task 131.0 in stage 4.0 (TID 310)
[2021-05-15 10:03:36,473] {docker.py:276} INFO - 21/05/15 13:03:36 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 306) in 2327 ms on 4e8a4a26f4b5 (executor driver) (128/200)
[2021-05-15 10:03:36,483] {docker.py:276} INFO - 21/05/15 13:03:36 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:36,483] {docker.py:276} INFO - 21/05/15 13:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:36,484] {docker.py:276} INFO - 21/05/15 13:03:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:36,485] {docker.py:276} INFO - 21/05/15 13:03:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056233772336818883551_0004_m_000131_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056233772336818883551_0004_m_000131_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056233772336818883551_0004}; taskId=attempt_202105151302056233772336818883551_0004_m_000131_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@205b751c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:36,485] {docker.py:276} INFO - 21/05/15 13:03:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105151302056233772336818883551_0004_m_000131_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056233772336818883551_0004_m_000131_310
[2021-05-15 10:03:36,488] {docker.py:276} INFO - 21/05/15 13:03:36 INFO StagingCommitter: Task committer attempt_202105151302056233772336818883551_0004_m_000131_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056233772336818883551_0004_m_000131_310 : duration 0:00.003s
[2021-05-15 10:03:37,501] {docker.py:276} INFO - 21/05/15 13:03:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302054754667414392979359_0004_m_000128_307: needsTaskCommit() Task attempt_202105151302054754667414392979359_0004_m_000128_307
[2021-05-15 10:03:37,502] {docker.py:276} INFO - 21/05/15 13:03:37 INFO StagingCommitter: Task committer attempt_202105151302054754667414392979359_0004_m_000128_307: needsTaskCommit() Task attempt_202105151302054754667414392979359_0004_m_000128_307: duration 0:00.001s
21/05/15 13:03:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054754667414392979359_0004_m_000128_307
[2021-05-15 10:03:37,504] {docker.py:276} INFO - 21/05/15 13:03:37 INFO Executor: Finished task 128.0 in stage 4.0 (TID 307). 4544 bytes result sent to driver
[2021-05-15 10:03:37,506] {docker.py:276} INFO - 21/05/15 13:03:37 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 311) (4e8a4a26f4b5, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:37,507] {docker.py:276} INFO - 21/05/15 13:03:37 INFO Executor: Running task 132.0 in stage 4.0 (TID 311)
[2021-05-15 10:03:37,507] {docker.py:276} INFO - 21/05/15 13:03:37 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 307) in 2482 ms on 4e8a4a26f4b5 (executor driver) (129/200)
[2021-05-15 10:03:37,527] {docker.py:276} INFO - 21/05/15 13:03:37 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:37,528] {docker.py:276} INFO - 21/05/15 13:03:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053809481088870488660_0004_m_000132_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053809481088870488660_0004_m_000132_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053809481088870488660_0004}; taskId=attempt_202105151302053809481088870488660_0004_m_000132_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e8a106b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302053809481088870488660_0004_m_000132_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053809481088870488660_0004_m_000132_311
[2021-05-15 10:03:37,532] {docker.py:276} INFO - 21/05/15 13:03:37 INFO StagingCommitter: Task committer attempt_202105151302053809481088870488660_0004_m_000132_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053809481088870488660_0004_m_000132_311 : duration 0:00.003s
[2021-05-15 10:03:37,702] {docker.py:276} INFO - 21/05/15 13:03:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302057244954947793785999_0004_m_000129_308: needsTaskCommit() Task attempt_202105151302057244954947793785999_0004_m_000129_308
21/05/15 13:03:37 INFO StagingCommitter: Task committer attempt_202105151302057244954947793785999_0004_m_000129_308: needsTaskCommit() Task attempt_202105151302057244954947793785999_0004_m_000129_308: duration 0:00.000s
21/05/15 13:03:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057244954947793785999_0004_m_000129_308
[2021-05-15 10:03:37,703] {docker.py:276} INFO - 21/05/15 13:03:37 INFO Executor: Finished task 129.0 in stage 4.0 (TID 308). 4587 bytes result sent to driver
[2021-05-15 10:03:37,704] {docker.py:276} INFO - 21/05/15 13:03:37 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 312) (4e8a4a26f4b5, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:37,706] {docker.py:276} INFO - 21/05/15 13:03:37 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 308) in 2062 ms on 4e8a4a26f4b5 (executor driver) (130/200)
[2021-05-15 10:03:37,707] {docker.py:276} INFO - 21/05/15 13:03:37 INFO Executor: Running task 133.0 in stage 4.0 (TID 312)
[2021-05-15 10:03:37,719] {docker.py:276} INFO - 21/05/15 13:03:37 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-15 10:03:37,720] {docker.py:276} INFO - 21/05/15 13:03:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:37,721] {docker.py:276} INFO - 21/05/15 13:03:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057579806395248101418_0004_m_000133_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057579806395248101418_0004_m_000133_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057579806395248101418_0004}; taskId=attempt_202105151302057579806395248101418_0004_m_000133_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76502335}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:37 INFO StagingCommitter: Starting: Task committer attempt_202105151302057579806395248101418_0004_m_000133_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057579806395248101418_0004_m_000133_312
[2021-05-15 10:03:37,724] {docker.py:276} INFO - 21/05/15 13:03:37 INFO StagingCommitter: Task committer attempt_202105151302057579806395248101418_0004_m_000133_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057579806395248101418_0004_m_000133_312 : duration 0:00.004s
[2021-05-15 10:03:38,457] {docker.py:276} INFO - 21/05/15 13:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105151302056923013690332174562_0004_m_000130_309: needsTaskCommit() Task attempt_202105151302056923013690332174562_0004_m_000130_309
[2021-05-15 10:03:38,457] {docker.py:276} INFO - 21/05/15 13:03:38 INFO StagingCommitter: Task committer attempt_202105151302056923013690332174562_0004_m_000130_309: needsTaskCommit() Task attempt_202105151302056923013690332174562_0004_m_000130_309: duration 0:00.001s
21/05/15 13:03:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056923013690332174562_0004_m_000130_309
[2021-05-15 10:03:38,458] {docker.py:276} INFO - 21/05/15 13:03:38 INFO Executor: Finished task 130.0 in stage 4.0 (TID 309). 4587 bytes result sent to driver
[2021-05-15 10:03:38,459] {docker.py:276} INFO - 21/05/15 13:03:38 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 313) (4e8a4a26f4b5, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:38,459] {docker.py:276} INFO - 21/05/15 13:03:38 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 309) in 2463 ms on 4e8a4a26f4b5 (executor driver) (131/200)
21/05/15 13:03:38 INFO Executor: Running task 134.0 in stage 4.0 (TID 313)
[2021-05-15 10:03:38,467] {docker.py:276} INFO - 21/05/15 13:03:38 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:38,469] {docker.py:276} INFO - 21/05/15 13:03:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051056179084336886074_0004_m_000134_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051056179084336886074_0004_m_000134_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051056179084336886074_0004}; taskId=attempt_202105151302051056179084336886074_0004_m_000134_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11423f1c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:38,469] {docker.py:276} INFO - 21/05/15 13:03:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105151302051056179084336886074_0004_m_000134_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051056179084336886074_0004_m_000134_313
[2021-05-15 10:03:38,473] {docker.py:276} INFO - 21/05/15 13:03:38 INFO StagingCommitter: Task committer attempt_202105151302051056179084336886074_0004_m_000134_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051056179084336886074_0004_m_000134_313 : duration 0:00.003s
[2021-05-15 10:03:38,984] {docker.py:276} INFO - 21/05/15 13:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105151302056233772336818883551_0004_m_000131_310: needsTaskCommit() Task attempt_202105151302056233772336818883551_0004_m_000131_310
21/05/15 13:03:38 INFO StagingCommitter: Task committer attempt_202105151302056233772336818883551_0004_m_000131_310: needsTaskCommit() Task attempt_202105151302056233772336818883551_0004_m_000131_310: duration 0:00.000s
21/05/15 13:03:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056233772336818883551_0004_m_000131_310
[2021-05-15 10:03:38,986] {docker.py:276} INFO - 21/05/15 13:03:39 INFO Executor: Finished task 131.0 in stage 4.0 (TID 310). 4587 bytes result sent to driver
[2021-05-15 10:03:38,987] {docker.py:276} INFO - 21/05/15 13:03:39 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 314) (4e8a4a26f4b5, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:38,988] {docker.py:276} INFO - 21/05/15 13:03:39 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 310) in 2520 ms on 4e8a4a26f4b5 (executor driver) (132/200)
[2021-05-15 10:03:38,991] {docker.py:276} INFO - 21/05/15 13:03:39 INFO Executor: Running task 135.0 in stage 4.0 (TID 314)
[2021-05-15 10:03:39,000] {docker.py:276} INFO - 21/05/15 13:03:39 INFO ShuffleBlockFetcherIterator: Getting 4 (11.6 KiB) non-empty blocks including 4 (11.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:39,002] {docker.py:276} INFO - 21/05/15 13:03:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057892502895012139956_0004_m_000135_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057892502895012139956_0004_m_000135_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057892502895012139956_0004}; taskId=attempt_202105151302057892502895012139956_0004_m_000135_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cdbf9ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105151302057892502895012139956_0004_m_000135_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057892502895012139956_0004_m_000135_314
[2021-05-15 10:03:39,005] {docker.py:276} INFO - 21/05/15 13:03:39 INFO StagingCommitter: Task committer attempt_202105151302057892502895012139956_0004_m_000135_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057892502895012139956_0004_m_000135_314 : duration 0:00.002s
[2021-05-15 10:03:39,345] {docker.py:276} INFO - 21/05/15 13:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105151302057579806395248101418_0004_m_000133_312: needsTaskCommit() Task attempt_202105151302057579806395248101418_0004_m_000133_312
[2021-05-15 10:03:39,346] {docker.py:276} INFO - 21/05/15 13:03:39 INFO StagingCommitter: Task committer attempt_202105151302057579806395248101418_0004_m_000133_312: needsTaskCommit() Task attempt_202105151302057579806395248101418_0004_m_000133_312: duration 0:00.001s
[2021-05-15 10:03:39,346] {docker.py:276} INFO - 21/05/15 13:03:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057579806395248101418_0004_m_000133_312
[2021-05-15 10:03:39,348] {docker.py:276} INFO - 21/05/15 13:03:39 INFO Executor: Finished task 133.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-15 10:03:39,349] {docker.py:276} INFO - 21/05/15 13:03:39 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 315) (4e8a4a26f4b5, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:39,350] {docker.py:276} INFO - 21/05/15 13:03:39 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 312) in 1648 ms on 4e8a4a26f4b5 (executor driver) (133/200)
[2021-05-15 10:03:39,350] {docker.py:276} INFO - 21/05/15 13:03:39 INFO Executor: Running task 136.0 in stage 4.0 (TID 315)
[2021-05-15 10:03:39,358] {docker.py:276} INFO - 21/05/15 13:03:39 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:39,360] {docker.py:276} INFO - 21/05/15 13:03:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051357742723051214602_0004_m_000136_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051357742723051214602_0004_m_000136_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051357742723051214602_0004}; taskId=attempt_202105151302051357742723051214602_0004_m_000136_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11fe12f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:39,360] {docker.py:276} INFO - 21/05/15 13:03:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105151302051357742723051214602_0004_m_000136_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051357742723051214602_0004_m_000136_315
[2021-05-15 10:03:39,363] {docker.py:276} INFO - 21/05/15 13:03:39 INFO StagingCommitter: Task committer attempt_202105151302051357742723051214602_0004_m_000136_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051357742723051214602_0004_m_000136_315 : duration 0:00.002s
[2021-05-15 10:03:40,537] {docker.py:276} INFO - 21/05/15 13:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105151302053809481088870488660_0004_m_000132_311: needsTaskCommit() Task attempt_202105151302053809481088870488660_0004_m_000132_311
[2021-05-15 10:03:40,538] {docker.py:276} INFO - 21/05/15 13:03:40 INFO StagingCommitter: Task committer attempt_202105151302053809481088870488660_0004_m_000132_311: needsTaskCommit() Task attempt_202105151302053809481088870488660_0004_m_000132_311: duration 0:00.001s
21/05/15 13:03:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053809481088870488660_0004_m_000132_311
[2021-05-15 10:03:40,541] {docker.py:276} INFO - 21/05/15 13:03:40 INFO Executor: Finished task 132.0 in stage 4.0 (TID 311). 4587 bytes result sent to driver
[2021-05-15 10:03:40,542] {docker.py:276} INFO - 21/05/15 13:03:40 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 316) (4e8a4a26f4b5, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:40,543] {docker.py:276} INFO - 21/05/15 13:03:40 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 311) in 3042 ms on 4e8a4a26f4b5 (executor driver) (134/200)
[2021-05-15 10:03:40,545] {docker.py:276} INFO - 21/05/15 13:03:40 INFO Executor: Running task 137.0 in stage 4.0 (TID 316)
[2021-05-15 10:03:40,554] {docker.py:276} INFO - 21/05/15 13:03:40 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:40,556] {docker.py:276} INFO - 21/05/15 13:03:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205812933982937501834_0004_m_000137_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205812933982937501834_0004_m_000137_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205812933982937501834_0004}; taskId=attempt_20210515130205812933982937501834_0004_m_000137_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4767ed8d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:40 INFO StagingCommitter: Starting: Task committer attempt_20210515130205812933982937501834_0004_m_000137_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205812933982937501834_0004_m_000137_316
[2021-05-15 10:03:40,559] {docker.py:276} INFO - 21/05/15 13:03:40 INFO StagingCommitter: Task committer attempt_20210515130205812933982937501834_0004_m_000137_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205812933982937501834_0004_m_000137_316 : duration 0:00.002s
[2021-05-15 10:03:40,832] {docker.py:276} INFO - 21/05/15 13:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105151302051056179084336886074_0004_m_000134_313: needsTaskCommit() Task attempt_202105151302051056179084336886074_0004_m_000134_313
[2021-05-15 10:03:40,833] {docker.py:276} INFO - 21/05/15 13:03:40 INFO StagingCommitter: Task committer attempt_202105151302051056179084336886074_0004_m_000134_313: needsTaskCommit() Task attempt_202105151302051056179084336886074_0004_m_000134_313: duration 0:00.001s
21/05/15 13:03:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051056179084336886074_0004_m_000134_313
[2021-05-15 10:03:40,834] {docker.py:276} INFO - 21/05/15 13:03:40 INFO Executor: Finished task 134.0 in stage 4.0 (TID 313). 4544 bytes result sent to driver
[2021-05-15 10:03:40,836] {docker.py:276} INFO - 21/05/15 13:03:40 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 317) (4e8a4a26f4b5, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:40,837] {docker.py:276} INFO - 21/05/15 13:03:40 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 313) in 2381 ms on 4e8a4a26f4b5 (executor driver) (135/200)
[2021-05-15 10:03:40,838] {docker.py:276} INFO - 21/05/15 13:03:40 INFO Executor: Running task 138.0 in stage 4.0 (TID 317)
[2021-05-15 10:03:40,846] {docker.py:276} INFO - 21/05/15 13:03:40 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:40,848] {docker.py:276} INFO - 21/05/15 13:03:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059174480173604305009_0004_m_000138_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059174480173604305009_0004_m_000138_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059174480173604305009_0004}; taskId=attempt_202105151302059174480173604305009_0004_m_000138_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44bd1c55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:40,848] {docker.py:276} INFO - 21/05/15 13:03:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105151302059174480173604305009_0004_m_000138_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059174480173604305009_0004_m_000138_317
[2021-05-15 10:03:40,851] {docker.py:276} INFO - 21/05/15 13:03:40 INFO StagingCommitter: Task committer attempt_202105151302059174480173604305009_0004_m_000138_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059174480173604305009_0004_m_000138_317 : duration 0:00.003s
[2021-05-15 10:03:41,680] {docker.py:276} INFO - 21/05/15 13:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105151302051357742723051214602_0004_m_000136_315: needsTaskCommit() Task attempt_202105151302051357742723051214602_0004_m_000136_315
[2021-05-15 10:03:41,681] {docker.py:276} INFO - 21/05/15 13:03:41 INFO StagingCommitter: Task committer attempt_202105151302051357742723051214602_0004_m_000136_315: needsTaskCommit() Task attempt_202105151302051357742723051214602_0004_m_000136_315: duration 0:00.001s
21/05/15 13:03:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051357742723051214602_0004_m_000136_315
[2021-05-15 10:03:41,682] {docker.py:276} INFO - 21/05/15 13:03:41 INFO Executor: Finished task 136.0 in stage 4.0 (TID 315). 4544 bytes result sent to driver
[2021-05-15 10:03:41,683] {docker.py:276} INFO - 21/05/15 13:03:41 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 318) (4e8a4a26f4b5, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:41,684] {docker.py:276} INFO - 21/05/15 13:03:41 INFO Executor: Running task 139.0 in stage 4.0 (TID 318)
[2021-05-15 10:03:41,684] {docker.py:276} INFO - 21/05/15 13:03:41 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 315) in 2339 ms on 4e8a4a26f4b5 (executor driver) (136/200)
[2021-05-15 10:03:41,694] {docker.py:276} INFO - 21/05/15 13:03:41 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:41,695] {docker.py:276} INFO - 21/05/15 13:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:03:41,697] {docker.py:276} INFO - 21/05/15 13:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052263036822422164892_0004_m_000139_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052263036822422164892_0004_m_000139_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052263036822422164892_0004}; taskId=attempt_202105151302052263036822422164892_0004_m_000139_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5387069b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105151302052263036822422164892_0004_m_000139_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052263036822422164892_0004_m_000139_318
[2021-05-15 10:03:41,700] {docker.py:276} INFO - 21/05/15 13:03:41 INFO StagingCommitter: Task committer attempt_202105151302052263036822422164892_0004_m_000139_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052263036822422164892_0004_m_000139_318 : duration 0:00.003s
[2021-05-15 10:03:41,983] {docker.py:276} INFO - 21/05/15 13:03:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302057892502895012139956_0004_m_000135_314: needsTaskCommit() Task attempt_202105151302057892502895012139956_0004_m_000135_314
[2021-05-15 10:03:41,983] {docker.py:276} INFO - 21/05/15 13:03:42 INFO StagingCommitter: Task committer attempt_202105151302057892502895012139956_0004_m_000135_314: needsTaskCommit() Task attempt_202105151302057892502895012139956_0004_m_000135_314: duration 0:00.001s
21/05/15 13:03:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057892502895012139956_0004_m_000135_314
[2021-05-15 10:03:41,986] {docker.py:276} INFO - 21/05/15 13:03:42 INFO Executor: Finished task 135.0 in stage 4.0 (TID 314). 4544 bytes result sent to driver
[2021-05-15 10:03:41,987] {docker.py:276} INFO - 21/05/15 13:03:42 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 319) (4e8a4a26f4b5, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:41,988] {docker.py:276} INFO - 21/05/15 13:03:42 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 314) in 3005 ms on 4e8a4a26f4b5 (executor driver) (137/200)
[2021-05-15 10:03:41,988] {docker.py:276} INFO - 21/05/15 13:03:42 INFO Executor: Running task 140.0 in stage 4.0 (TID 319)
[2021-05-15 10:03:41,998] {docker.py:276} INFO - 21/05/15 13:03:42 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:41,999] {docker.py:276} INFO - 21/05/15 13:03:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:42,000] {docker.py:276} INFO - 21/05/15 13:03:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054052020438887510049_0004_m_000140_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054052020438887510049_0004_m_000140_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054052020438887510049_0004}; taskId=attempt_202105151302054052020438887510049_0004_m_000140_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71e0f091}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:42 INFO StagingCommitter: Starting: Task committer attempt_202105151302054052020438887510049_0004_m_000140_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054052020438887510049_0004_m_000140_319
[2021-05-15 10:03:42,003] {docker.py:276} INFO - 21/05/15 13:03:42 INFO StagingCommitter: Task committer attempt_202105151302054052020438887510049_0004_m_000140_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054052020438887510049_0004_m_000140_319 : duration 0:00.003s
[2021-05-15 10:03:42,972] {docker.py:276} INFO - 21/05/15 13:03:42 INFO StagingCommitter: Starting: Task committer attempt_20210515130205812933982937501834_0004_m_000137_316: needsTaskCommit() Task attempt_20210515130205812933982937501834_0004_m_000137_316
[2021-05-15 10:03:42,973] {docker.py:276} INFO - 21/05/15 13:03:42 INFO StagingCommitter: Task committer attempt_20210515130205812933982937501834_0004_m_000137_316: needsTaskCommit() Task attempt_20210515130205812933982937501834_0004_m_000137_316: duration 0:00.001s
21/05/15 13:03:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205812933982937501834_0004_m_000137_316
[2021-05-15 10:03:42,975] {docker.py:276} INFO - 21/05/15 13:03:42 INFO Executor: Finished task 137.0 in stage 4.0 (TID 316). 4544 bytes result sent to driver
[2021-05-15 10:03:42,977] {docker.py:276} INFO - 21/05/15 13:03:42 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 320) (4e8a4a26f4b5, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:42,977] {docker.py:276} INFO - 21/05/15 13:03:42 INFO Executor: Running task 141.0 in stage 4.0 (TID 320)
21/05/15 13:03:42 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 316) in 2439 ms on 4e8a4a26f4b5 (executor driver) (138/200)
[2021-05-15 10:03:42,988] {docker.py:276} INFO - 21/05/15 13:03:43 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:42,991] {docker.py:276} INFO - 21/05/15 13:03:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057264087855370304803_0004_m_000141_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057264087855370304803_0004_m_000141_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057264087855370304803_0004}; taskId=attempt_202105151302057264087855370304803_0004_m_000141_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40ad2cca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:42,991] {docker.py:276} INFO - 21/05/15 13:03:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105151302057264087855370304803_0004_m_000141_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057264087855370304803_0004_m_000141_320
[2021-05-15 10:03:42,994] {docker.py:276} INFO - 21/05/15 13:03:43 INFO StagingCommitter: Task committer attempt_202105151302057264087855370304803_0004_m_000141_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057264087855370304803_0004_m_000141_320 : duration 0:00.003s
[2021-05-15 10:03:43,417] {docker.py:276} INFO - 21/05/15 13:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105151302059174480173604305009_0004_m_000138_317: needsTaskCommit() Task attempt_202105151302059174480173604305009_0004_m_000138_317
[2021-05-15 10:03:43,418] {docker.py:276} INFO - 21/05/15 13:03:43 INFO StagingCommitter: Task committer attempt_202105151302059174480173604305009_0004_m_000138_317: needsTaskCommit() Task attempt_202105151302059174480173604305009_0004_m_000138_317: duration 0:00.002s
21/05/15 13:03:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059174480173604305009_0004_m_000138_317
[2021-05-15 10:03:43,420] {docker.py:276} INFO - 21/05/15 13:03:43 INFO Executor: Finished task 138.0 in stage 4.0 (TID 317). 4544 bytes result sent to driver
[2021-05-15 10:03:43,422] {docker.py:276} INFO - 21/05/15 13:03:43 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 321) (4e8a4a26f4b5, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:43,423] {docker.py:276} INFO - 21/05/15 13:03:43 INFO Executor: Running task 142.0 in stage 4.0 (TID 321)
[2021-05-15 10:03:43,424] {docker.py:276} INFO - 21/05/15 13:03:43 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 317) in 2589 ms on 4e8a4a26f4b5 (executor driver) (139/200)
[2021-05-15 10:03:43,433] {docker.py:276} INFO - 21/05/15 13:03:43 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:43,435] {docker.py:276} INFO - 21/05/15 13:03:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205827195627572166116_0004_m_000142_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205827195627572166116_0004_m_000142_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205827195627572166116_0004}; taskId=attempt_20210515130205827195627572166116_0004_m_000142_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cd9caa1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:43,435] {docker.py:276} INFO - 21/05/15 13:03:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:43 INFO StagingCommitter: Starting: Task committer attempt_20210515130205827195627572166116_0004_m_000142_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205827195627572166116_0004_m_000142_321
[2021-05-15 10:03:43,438] {docker.py:276} INFO - 21/05/15 13:03:43 INFO StagingCommitter: Task committer attempt_20210515130205827195627572166116_0004_m_000142_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205827195627572166116_0004_m_000142_321 : duration 0:00.002s
[2021-05-15 10:03:44,037] {docker.py:276} INFO - 21/05/15 13:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302052263036822422164892_0004_m_000139_318: needsTaskCommit() Task attempt_202105151302052263036822422164892_0004_m_000139_318
[2021-05-15 10:03:44,038] {docker.py:276} INFO - 21/05/15 13:03:44 INFO StagingCommitter: Task committer attempt_202105151302052263036822422164892_0004_m_000139_318: needsTaskCommit() Task attempt_202105151302052263036822422164892_0004_m_000139_318: duration 0:00.000s
21/05/15 13:03:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052263036822422164892_0004_m_000139_318
[2021-05-15 10:03:44,040] {docker.py:276} INFO - 21/05/15 13:03:44 INFO Executor: Finished task 139.0 in stage 4.0 (TID 318). 4544 bytes result sent to driver
[2021-05-15 10:03:44,041] {docker.py:276} INFO - 21/05/15 13:03:44 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 322) (4e8a4a26f4b5, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:44,042] {docker.py:276} INFO - 21/05/15 13:03:44 INFO Executor: Running task 143.0 in stage 4.0 (TID 322)
21/05/15 13:03:44 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 318) in 2361 ms on 4e8a4a26f4b5 (executor driver) (140/200)
[2021-05-15 10:03:44,052] {docker.py:276} INFO - 21/05/15 13:03:44 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:44,054] {docker.py:276} INFO - 21/05/15 13:03:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058405866039696197220_0004_m_000143_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058405866039696197220_0004_m_000143_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058405866039696197220_0004}; taskId=attempt_202105151302058405866039696197220_0004_m_000143_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@431ee1e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:44,054] {docker.py:276} INFO - 21/05/15 13:03:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302058405866039696197220_0004_m_000143_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058405866039696197220_0004_m_000143_322
[2021-05-15 10:03:44,057] {docker.py:276} INFO - 21/05/15 13:03:44 INFO StagingCommitter: Task committer attempt_202105151302058405866039696197220_0004_m_000143_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058405866039696197220_0004_m_000143_322 : duration 0:00.003s
[2021-05-15 10:03:44,515] {docker.py:276} INFO - 21/05/15 13:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302054052020438887510049_0004_m_000140_319: needsTaskCommit() Task attempt_202105151302054052020438887510049_0004_m_000140_319
[2021-05-15 10:03:44,516] {docker.py:276} INFO - 21/05/15 13:03:44 INFO StagingCommitter: Task committer attempt_202105151302054052020438887510049_0004_m_000140_319: needsTaskCommit() Task attempt_202105151302054052020438887510049_0004_m_000140_319: duration 0:00.002s
21/05/15 13:03:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054052020438887510049_0004_m_000140_319
[2021-05-15 10:03:44,518] {docker.py:276} INFO - 21/05/15 13:03:44 INFO Executor: Finished task 140.0 in stage 4.0 (TID 319). 4544 bytes result sent to driver
[2021-05-15 10:03:44,519] {docker.py:276} INFO - 21/05/15 13:03:44 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 323) (4e8a4a26f4b5, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:44,521] {docker.py:276} INFO - 21/05/15 13:03:44 INFO Executor: Running task 144.0 in stage 4.0 (TID 323)
[2021-05-15 10:03:44,521] {docker.py:276} INFO - 21/05/15 13:03:44 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 319) in 2537 ms on 4e8a4a26f4b5 (executor driver) (141/200)
[2021-05-15 10:03:44,530] {docker.py:276} INFO - 21/05/15 13:03:44 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:44,531] {docker.py:276} INFO - 21/05/15 13:03:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:44,532] {docker.py:276} INFO - 21/05/15 13:03:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053085660014183923668_0004_m_000144_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053085660014183923668_0004_m_000144_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053085660014183923668_0004}; taskId=attempt_202105151302053085660014183923668_0004_m_000144_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a10c83d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105151302053085660014183923668_0004_m_000144_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053085660014183923668_0004_m_000144_323
[2021-05-15 10:03:44,535] {docker.py:276} INFO - 21/05/15 13:03:44 INFO StagingCommitter: Task committer attempt_202105151302053085660014183923668_0004_m_000144_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053085660014183923668_0004_m_000144_323 : duration 0:00.003s
[2021-05-15 10:03:45,509] {docker.py:276} INFO - 21/05/15 13:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105151302057264087855370304803_0004_m_000141_320: needsTaskCommit() Task attempt_202105151302057264087855370304803_0004_m_000141_320
[2021-05-15 10:03:45,510] {docker.py:276} INFO - 21/05/15 13:03:45 INFO StagingCommitter: Task committer attempt_202105151302057264087855370304803_0004_m_000141_320: needsTaskCommit() Task attempt_202105151302057264087855370304803_0004_m_000141_320: duration 0:00.001s
21/05/15 13:03:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057264087855370304803_0004_m_000141_320
[2021-05-15 10:03:45,511] {docker.py:276} INFO - 21/05/15 13:03:45 INFO Executor: Finished task 141.0 in stage 4.0 (TID 320). 4587 bytes result sent to driver
[2021-05-15 10:03:45,512] {docker.py:276} INFO - 21/05/15 13:03:45 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 324) (4e8a4a26f4b5, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:45,513] {docker.py:276} INFO - 21/05/15 13:03:45 INFO Executor: Running task 145.0 in stage 4.0 (TID 324)
21/05/15 13:03:45 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 320) in 2540 ms on 4e8a4a26f4b5 (executor driver) (142/200)
[2021-05-15 10:03:45,523] {docker.py:276} INFO - 21/05/15 13:03:45 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:45,525] {docker.py:276} INFO - 21/05/15 13:03:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057314994992128973504_0004_m_000145_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057314994992128973504_0004_m_000145_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057314994992128973504_0004}; taskId=attempt_202105151302057314994992128973504_0004_m_000145_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f00d97b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:45,525] {docker.py:276} INFO - 21/05/15 13:03:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105151302057314994992128973504_0004_m_000145_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057314994992128973504_0004_m_000145_324
[2021-05-15 10:03:45,528] {docker.py:276} INFO - 21/05/15 13:03:45 INFO StagingCommitter: Task committer attempt_202105151302057314994992128973504_0004_m_000145_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057314994992128973504_0004_m_000145_324 : duration 0:00.003s
[2021-05-15 10:03:45,876] {docker.py:276} INFO - 21/05/15 13:03:45 INFO StagingCommitter: Starting: Task committer attempt_20210515130205827195627572166116_0004_m_000142_321: needsTaskCommit() Task attempt_20210515130205827195627572166116_0004_m_000142_321
[2021-05-15 10:03:45,877] {docker.py:276} INFO - 21/05/15 13:03:45 INFO StagingCommitter: Task committer attempt_20210515130205827195627572166116_0004_m_000142_321: needsTaskCommit() Task attempt_20210515130205827195627572166116_0004_m_000142_321: duration 0:00.001s
21/05/15 13:03:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205827195627572166116_0004_m_000142_321
[2021-05-15 10:03:45,878] {docker.py:276} INFO - 21/05/15 13:03:45 INFO Executor: Finished task 142.0 in stage 4.0 (TID 321). 4587 bytes result sent to driver
[2021-05-15 10:03:45,879] {docker.py:276} INFO - 21/05/15 13:03:45 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 325) (4e8a4a26f4b5, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:45,880] {docker.py:276} INFO - 21/05/15 13:03:45 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 321) in 2462 ms on 4e8a4a26f4b5 (executor driver) (143/200)
[2021-05-15 10:03:45,880] {docker.py:276} INFO - 21/05/15 13:03:45 INFO Executor: Running task 146.0 in stage 4.0 (TID 325)
[2021-05-15 10:03:45,889] {docker.py:276} INFO - 21/05/15 13:03:45 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:45,890] {docker.py:276} INFO - 21/05/15 13:03:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:45,891] {docker.py:276} INFO - 21/05/15 13:03:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:03:45,891] {docker.py:276} INFO - 21/05/15 13:03:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:45,892] {docker.py:276} INFO - 21/05/15 13:03:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:45,892] {docker.py:276} INFO - 21/05/15 13:03:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055527826688576275627_0004_m_000146_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055527826688576275627_0004_m_000146_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055527826688576275627_0004}; taskId=attempt_202105151302055527826688576275627_0004_m_000146_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1df5f13b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:45,893] {docker.py:276} INFO - 21/05/15 13:03:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105151302055527826688576275627_0004_m_000146_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055527826688576275627_0004_m_000146_325
[2021-05-15 10:03:45,895] {docker.py:276} INFO - 21/05/15 13:03:45 INFO StagingCommitter: Task committer attempt_202105151302055527826688576275627_0004_m_000146_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055527826688576275627_0004_m_000146_325 : duration 0:00.002s
[2021-05-15 10:03:46,074] {docker.py:276} INFO - 21/05/15 13:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105151302058405866039696197220_0004_m_000143_322: needsTaskCommit() Task attempt_202105151302058405866039696197220_0004_m_000143_322
[2021-05-15 10:03:46,075] {docker.py:276} INFO - 21/05/15 13:03:46 INFO StagingCommitter: Task committer attempt_202105151302058405866039696197220_0004_m_000143_322: needsTaskCommit() Task attempt_202105151302058405866039696197220_0004_m_000143_322: duration 0:00.001s
[2021-05-15 10:03:46,075] {docker.py:276} INFO - 21/05/15 13:03:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058405866039696197220_0004_m_000143_322
[2021-05-15 10:03:46,076] {docker.py:276} INFO - 21/05/15 13:03:46 INFO Executor: Finished task 143.0 in stage 4.0 (TID 322). 4587 bytes result sent to driver
[2021-05-15 10:03:46,077] {docker.py:276} INFO - 21/05/15 13:03:46 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 326) (4e8a4a26f4b5, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:46,077] {docker.py:276} INFO - 21/05/15 13:03:46 INFO Executor: Running task 147.0 in stage 4.0 (TID 326)
[2021-05-15 10:03:46,078] {docker.py:276} INFO - 21/05/15 13:03:46 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 322) in 2039 ms on 4e8a4a26f4b5 (executor driver) (144/200)
[2021-05-15 10:03:46,085] {docker.py:276} INFO - 21/05/15 13:03:46 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:46,085] {docker.py:276} INFO - 21/05/15 13:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:46,087] {docker.py:276} INFO - 21/05/15 13:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:46,087] {docker.py:276} INFO - 21/05/15 13:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:46,087] {docker.py:276} INFO - 21/05/15 13:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057256690555003572141_0004_m_000147_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057256690555003572141_0004_m_000147_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057256690555003572141_0004}; taskId=attempt_202105151302057256690555003572141_0004_m_000147_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6727ca11}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:46,088] {docker.py:276} INFO - 21/05/15 13:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:46,088] {docker.py:276} INFO - 21/05/15 13:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105151302057256690555003572141_0004_m_000147_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057256690555003572141_0004_m_000147_326
[2021-05-15 10:03:46,090] {docker.py:276} INFO - 21/05/15 13:03:46 INFO StagingCommitter: Task committer attempt_202105151302057256690555003572141_0004_m_000147_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057256690555003572141_0004_m_000147_326 : duration 0:00.003s
[2021-05-15 10:03:46,990] {docker.py:276} INFO - 21/05/15 13:03:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302053085660014183923668_0004_m_000144_323: needsTaskCommit() Task attempt_202105151302053085660014183923668_0004_m_000144_323
[2021-05-15 10:03:46,991] {docker.py:276} INFO - 21/05/15 13:03:47 INFO StagingCommitter: Task committer attempt_202105151302053085660014183923668_0004_m_000144_323: needsTaskCommit() Task attempt_202105151302053085660014183923668_0004_m_000144_323: duration 0:00.001s
21/05/15 13:03:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053085660014183923668_0004_m_000144_323
[2021-05-15 10:03:46,993] {docker.py:276} INFO - 21/05/15 13:03:47 INFO Executor: Finished task 144.0 in stage 4.0 (TID 323). 4587 bytes result sent to driver
[2021-05-15 10:03:46,995] {docker.py:276} INFO - 21/05/15 13:03:47 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 327) (4e8a4a26f4b5, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:46,995] {docker.py:276} INFO - 21/05/15 13:03:47 INFO Executor: Running task 148.0 in stage 4.0 (TID 327)
21/05/15 13:03:47 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 323) in 2479 ms on 4e8a4a26f4b5 (executor driver) (145/200)
[2021-05-15 10:03:47,005] {docker.py:276} INFO - 21/05/15 13:03:47 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:47,005] {docker.py:276} INFO - 21/05/15 13:03:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:47,007] {docker.py:276} INFO - 21/05/15 13:03:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:47,007] {docker.py:276} INFO - 21/05/15 13:03:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:47,007] {docker.py:276} INFO - 21/05/15 13:03:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051025417566096017770_0004_m_000148_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051025417566096017770_0004_m_000148_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051025417566096017770_0004}; taskId=attempt_202105151302051025417566096017770_0004_m_000148_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@759da24f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:47 INFO StagingCommitter: Starting: Task committer attempt_202105151302051025417566096017770_0004_m_000148_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051025417566096017770_0004_m_000148_327
[2021-05-15 10:03:47,010] {docker.py:276} INFO - 21/05/15 13:03:47 INFO StagingCommitter: Task committer attempt_202105151302051025417566096017770_0004_m_000148_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051025417566096017770_0004_m_000148_327 : duration 0:00.002s
[2021-05-15 10:03:47,997] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302057314994992128973504_0004_m_000145_324: needsTaskCommit() Task attempt_202105151302057314994992128973504_0004_m_000145_324
[2021-05-15 10:03:47,998] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Task committer attempt_202105151302057314994992128973504_0004_m_000145_324: needsTaskCommit() Task attempt_202105151302057314994992128973504_0004_m_000145_324: duration 0:00.001s
21/05/15 13:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057314994992128973504_0004_m_000145_324
[2021-05-15 10:03:47,999] {docker.py:276} INFO - 21/05/15 13:03:48 INFO Executor: Finished task 145.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-15 10:03:48,001] {docker.py:276} INFO - 21/05/15 13:03:48 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 328) (4e8a4a26f4b5, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:48,002] {docker.py:276} INFO - 21/05/15 13:03:48 INFO Executor: Running task 149.0 in stage 4.0 (TID 328)
[2021-05-15 10:03:48,003] {docker.py:276} INFO - 21/05/15 13:03:48 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 324) in 2494 ms on 4e8a4a26f4b5 (executor driver) (146/200)
[2021-05-15 10:03:48,012] {docker.py:276} INFO - 21/05/15 13:03:48 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:48,013] {docker.py:276} INFO - 21/05/15 13:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:03:48,015] {docker.py:276} INFO - 21/05/15 13:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:03:48,016] {docker.py:276} INFO - 21/05/15 13:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:48,016] {docker.py:276} INFO - 21/05/15 13:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:48,016] {docker.py:276} INFO - 21/05/15 13:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053166710906849786403_0004_m_000149_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053166710906849786403_0004_m_000149_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053166710906849786403_0004}; taskId=attempt_202105151302053166710906849786403_0004_m_000149_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c134e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:48,017] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302053166710906849786403_0004_m_000149_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053166710906849786403_0004_m_000149_328
[2021-05-15 10:03:48,019] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Task committer attempt_202105151302053166710906849786403_0004_m_000149_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053166710906849786403_0004_m_000149_328 : duration 0:00.003s
[2021-05-15 10:03:48,178] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302057256690555003572141_0004_m_000147_326: needsTaskCommit() Task attempt_202105151302057256690555003572141_0004_m_000147_326
[2021-05-15 10:03:48,180] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Task committer attempt_202105151302057256690555003572141_0004_m_000147_326: needsTaskCommit() Task attempt_202105151302057256690555003572141_0004_m_000147_326: duration 0:00.001s
21/05/15 13:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057256690555003572141_0004_m_000147_326
[2021-05-15 10:03:48,181] {docker.py:276} INFO - 21/05/15 13:03:48 INFO Executor: Finished task 147.0 in stage 4.0 (TID 326). 4544 bytes result sent to driver
[2021-05-15 10:03:48,182] {docker.py:276} INFO - 21/05/15 13:03:48 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 329) (4e8a4a26f4b5, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:48,183] {docker.py:276} INFO - 21/05/15 13:03:48 INFO Executor: Running task 150.0 in stage 4.0 (TID 329)
[2021-05-15 10:03:48,184] {docker.py:276} INFO - 21/05/15 13:03:48 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 326) in 2109 ms on 4e8a4a26f4b5 (executor driver) (147/200)
[2021-05-15 10:03:48,193] {docker.py:276} INFO - 21/05/15 13:03:48 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:48,195] {docker.py:276} INFO - 21/05/15 13:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055395398256409907846_0004_m_000150_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055395398256409907846_0004_m_000150_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055395398256409907846_0004}; taskId=attempt_202105151302055395398256409907846_0004_m_000150_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22345c11}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:48,196] {docker.py:276} INFO - 21/05/15 13:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:48,196] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302055395398256409907846_0004_m_000150_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055395398256409907846_0004_m_000150_329
[2021-05-15 10:03:48,198] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Task committer attempt_202105151302055395398256409907846_0004_m_000150_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055395398256409907846_0004_m_000150_329 : duration 0:00.003s
[2021-05-15 10:03:48,335] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302055527826688576275627_0004_m_000146_325: needsTaskCommit() Task attempt_202105151302055527826688576275627_0004_m_000146_325
[2021-05-15 10:03:48,336] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Task committer attempt_202105151302055527826688576275627_0004_m_000146_325: needsTaskCommit() Task attempt_202105151302055527826688576275627_0004_m_000146_325: duration 0:00.001s
21/05/15 13:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055527826688576275627_0004_m_000146_325
[2021-05-15 10:03:48,336] {docker.py:276} INFO - 21/05/15 13:03:48 INFO Executor: Finished task 146.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
[2021-05-15 10:03:48,337] {docker.py:276} INFO - 21/05/15 13:03:48 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 330) (4e8a4a26f4b5, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:48,338] {docker.py:276} INFO - 21/05/15 13:03:48 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 325) in 2462 ms on 4e8a4a26f4b5 (executor driver) (148/200)
[2021-05-15 10:03:48,338] {docker.py:276} INFO - 21/05/15 13:03:48 INFO Executor: Running task 151.0 in stage 4.0 (TID 330)
[2021-05-15 10:03:48,344] {docker.py:276} INFO - 21/05/15 13:03:48 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:48,346] {docker.py:276} INFO - 21/05/15 13:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056341009764990921342_0004_m_000151_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056341009764990921342_0004_m_000151_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056341009764990921342_0004}; taskId=attempt_202105151302056341009764990921342_0004_m_000151_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f678d91}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:48,346] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105151302056341009764990921342_0004_m_000151_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056341009764990921342_0004_m_000151_330
[2021-05-15 10:03:48,349] {docker.py:276} INFO - 21/05/15 13:03:48 INFO StagingCommitter: Task committer attempt_202105151302056341009764990921342_0004_m_000151_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056341009764990921342_0004_m_000151_330 : duration 0:00.003s
[2021-05-15 10:03:49,445] {docker.py:276} INFO - 21/05/15 13:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105151302051025417566096017770_0004_m_000148_327: needsTaskCommit() Task attempt_202105151302051025417566096017770_0004_m_000148_327
[2021-05-15 10:03:49,446] {docker.py:276} INFO - 21/05/15 13:03:49 INFO StagingCommitter: Task committer attempt_202105151302051025417566096017770_0004_m_000148_327: needsTaskCommit() Task attempt_202105151302051025417566096017770_0004_m_000148_327: duration 0:00.002s
[2021-05-15 10:03:49,447] {docker.py:276} INFO - 21/05/15 13:03:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051025417566096017770_0004_m_000148_327
[2021-05-15 10:03:49,449] {docker.py:276} INFO - 21/05/15 13:03:49 INFO Executor: Finished task 148.0 in stage 4.0 (TID 327). 4544 bytes result sent to driver
[2021-05-15 10:03:49,450] {docker.py:276} INFO - 21/05/15 13:03:49 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 331) (4e8a4a26f4b5, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:49,451] {docker.py:276} INFO - 21/05/15 13:03:49 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 327) in 2460 ms on 4e8a4a26f4b5 (executor driver) (149/200)
[2021-05-15 10:03:49,452] {docker.py:276} INFO - 21/05/15 13:03:49 INFO Executor: Running task 152.0 in stage 4.0 (TID 331)
[2021-05-15 10:03:49,461] {docker.py:276} INFO - 21/05/15 13:03:49 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:49,463] {docker.py:276} INFO - 21/05/15 13:03:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051204668702768750678_0004_m_000152_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051204668702768750678_0004_m_000152_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051204668702768750678_0004}; taskId=attempt_202105151302051204668702768750678_0004_m_000152_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5648689d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:49,463] {docker.py:276} INFO - 21/05/15 13:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105151302051204668702768750678_0004_m_000152_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051204668702768750678_0004_m_000152_331
[2021-05-15 10:03:49,467] {docker.py:276} INFO - 21/05/15 13:03:49 INFO StagingCommitter: Task committer attempt_202105151302051204668702768750678_0004_m_000152_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051204668702768750678_0004_m_000152_331 : duration 0:00.003s
[2021-05-15 10:03:50,415] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302053166710906849786403_0004_m_000149_328: needsTaskCommit() Task attempt_202105151302053166710906849786403_0004_m_000149_328
[2021-05-15 10:03:50,416] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Task committer attempt_202105151302053166710906849786403_0004_m_000149_328: needsTaskCommit() Task attempt_202105151302053166710906849786403_0004_m_000149_328: duration 0:00.000s
21/05/15 13:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053166710906849786403_0004_m_000149_328
[2021-05-15 10:03:50,418] {docker.py:276} INFO - 21/05/15 13:03:50 INFO Executor: Finished task 149.0 in stage 4.0 (TID 328). 4544 bytes result sent to driver
[2021-05-15 10:03:50,419] {docker.py:276} INFO - 21/05/15 13:03:50 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 332) (4e8a4a26f4b5, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:50,421] {docker.py:276} INFO - 21/05/15 13:03:50 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 328) in 2423 ms on 4e8a4a26f4b5 (executor driver) (150/200)
21/05/15 13:03:50 INFO Executor: Running task 153.0 in stage 4.0 (TID 332)
[2021-05-15 10:03:50,430] {docker.py:276} INFO - 21/05/15 13:03:50 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:50,432] {docker.py:276} INFO - 21/05/15 13:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054419628853109491506_0004_m_000153_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054419628853109491506_0004_m_000153_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054419628853109491506_0004}; taskId=attempt_202105151302054419628853109491506_0004_m_000153_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77020770}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302054419628853109491506_0004_m_000153_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054419628853109491506_0004_m_000153_332
[2021-05-15 10:03:50,435] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Task committer attempt_202105151302054419628853109491506_0004_m_000153_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054419628853109491506_0004_m_000153_332 : duration 0:00.003s
[2021-05-15 10:03:50,727] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302056341009764990921342_0004_m_000151_330: needsTaskCommit() Task attempt_202105151302056341009764990921342_0004_m_000151_330
[2021-05-15 10:03:50,728] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Task committer attempt_202105151302056341009764990921342_0004_m_000151_330: needsTaskCommit() Task attempt_202105151302056341009764990921342_0004_m_000151_330: duration 0:00.001s
21/05/15 13:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056341009764990921342_0004_m_000151_330
[2021-05-15 10:03:50,730] {docker.py:276} INFO - 21/05/15 13:03:50 INFO Executor: Finished task 151.0 in stage 4.0 (TID 330). 4544 bytes result sent to driver
[2021-05-15 10:03:50,731] {docker.py:276} INFO - 21/05/15 13:03:50 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 333) (4e8a4a26f4b5, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:50,732] {docker.py:276} INFO - 21/05/15 13:03:50 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 330) in 2397 ms on 4e8a4a26f4b5 (executor driver) (151/200)
[2021-05-15 10:03:50,732] {docker.py:276} INFO - 21/05/15 13:03:50 INFO Executor: Running task 154.0 in stage 4.0 (TID 333)
[2021-05-15 10:03:50,735] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302055395398256409907846_0004_m_000150_329: needsTaskCommit() Task attempt_202105151302055395398256409907846_0004_m_000150_329
[2021-05-15 10:03:50,736] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Task committer attempt_202105151302055395398256409907846_0004_m_000150_329: needsTaskCommit() Task attempt_202105151302055395398256409907846_0004_m_000150_329: duration 0:00.000s
21/05/15 13:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055395398256409907846_0004_m_000150_329
[2021-05-15 10:03:50,738] {docker.py:276} INFO - 21/05/15 13:03:50 INFO Executor: Finished task 150.0 in stage 4.0 (TID 329). 4544 bytes result sent to driver
[2021-05-15 10:03:50,739] {docker.py:276} INFO - 21/05/15 13:03:50 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 334) (4e8a4a26f4b5, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:50,740] {docker.py:276} INFO - 21/05/15 13:03:50 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 329) in 2562 ms on 4e8a4a26f4b5 (executor driver) (152/200)
[2021-05-15 10:03:50,741] {docker.py:276} INFO - 21/05/15 13:03:50 INFO Executor: Running task 155.0 in stage 4.0 (TID 334)
[2021-05-15 10:03:50,746] {docker.py:276} INFO - 21/05/15 13:03:50 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:50,747] {docker.py:276} INFO - 21/05/15 13:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:50,748] {docker.py:276} INFO - 21/05/15 13:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:50,749] {docker.py:276} INFO - 21/05/15 13:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:50,750] {docker.py:276} INFO - 21/05/15 13:03:50 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:50,750] {docker.py:276} INFO - 21/05/15 13:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059052444090793344336_0004_m_000154_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059052444090793344336_0004_m_000154_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059052444090793344336_0004}; taskId=attempt_202105151302059052444090793344336_0004_m_000154_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ec3a933}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:50,750] {docker.py:276} INFO - 21/05/15 13:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:50,751] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302059052444090793344336_0004_m_000154_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059052444090793344336_0004_m_000154_333
[2021-05-15 10:03:50,751] {docker.py:276} INFO - 21/05/15 13:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:50,751] {docker.py:276} INFO - 21/05/15 13:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055837952498610995317_0004_m_000155_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055837952498610995317_0004_m_000155_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055837952498610995317_0004}; taskId=attempt_202105151302055837952498610995317_0004_m_000155_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd4265b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105151302055837952498610995317_0004_m_000155_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055837952498610995317_0004_m_000155_334
[2021-05-15 10:03:50,754] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Task committer attempt_202105151302059052444090793344336_0004_m_000154_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059052444090793344336_0004_m_000154_333 : duration 0:00.004s
[2021-05-15 10:03:50,758] {docker.py:276} INFO - 21/05/15 13:03:50 INFO StagingCommitter: Task committer attempt_202105151302055837952498610995317_0004_m_000155_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055837952498610995317_0004_m_000155_334 : duration 0:00.007s
[2021-05-15 10:03:51,694] {docker.py:276} INFO - 21/05/15 13:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105151302051204668702768750678_0004_m_000152_331: needsTaskCommit() Task attempt_202105151302051204668702768750678_0004_m_000152_331
[2021-05-15 10:03:51,695] {docker.py:276} INFO - 21/05/15 13:03:51 INFO StagingCommitter: Task committer attempt_202105151302051204668702768750678_0004_m_000152_331: needsTaskCommit() Task attempt_202105151302051204668702768750678_0004_m_000152_331: duration 0:00.001s
21/05/15 13:03:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051204668702768750678_0004_m_000152_331
[2021-05-15 10:03:51,696] {docker.py:276} INFO - 21/05/15 13:03:51 INFO Executor: Finished task 152.0 in stage 4.0 (TID 331). 4544 bytes result sent to driver
[2021-05-15 10:03:51,697] {docker.py:276} INFO - 21/05/15 13:03:51 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 335) (4e8a4a26f4b5, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:51 INFO Executor: Running task 156.0 in stage 4.0 (TID 335)
21/05/15 13:03:51 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 331) in 2250 ms on 4e8a4a26f4b5 (executor driver) (153/200)
[2021-05-15 10:03:51,708] {docker.py:276} INFO - 21/05/15 13:03:51 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:51,710] {docker.py:276} INFO - 21/05/15 13:03:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051513020597397263397481910_0004_m_000156_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020597397263397481910_0004_m_000156_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051513020597397263397481910_0004}; taskId=attempt_2021051513020597397263397481910_0004_m_000156_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55d6178}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:51,710] {docker.py:276} INFO - 21/05/15 13:03:51 INFO StagingCommitter: Starting: Task committer attempt_2021051513020597397263397481910_0004_m_000156_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020597397263397481910_0004_m_000156_335
[2021-05-15 10:03:51,713] {docker.py:276} INFO - 21/05/15 13:03:51 INFO StagingCommitter: Task committer attempt_2021051513020597397263397481910_0004_m_000156_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_2021051513020597397263397481910_0004_m_000156_335 : duration 0:00.003s
[2021-05-15 10:03:52,961] {docker.py:276} INFO - 21/05/15 13:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105151302054419628853109491506_0004_m_000153_332: needsTaskCommit() Task attempt_202105151302054419628853109491506_0004_m_000153_332
21/05/15 13:03:52 INFO StagingCommitter: Task committer attempt_202105151302054419628853109491506_0004_m_000153_332: needsTaskCommit() Task attempt_202105151302054419628853109491506_0004_m_000153_332: duration 0:00.001s
21/05/15 13:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054419628853109491506_0004_m_000153_332
[2021-05-15 10:03:52,963] {docker.py:276} INFO - 21/05/15 13:03:52 INFO Executor: Finished task 153.0 in stage 4.0 (TID 332). 4544 bytes result sent to driver
[2021-05-15 10:03:52,965] {docker.py:276} INFO - 21/05/15 13:03:52 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 336) (4e8a4a26f4b5, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:52 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 332) in 2548 ms on 4e8a4a26f4b5 (executor driver) (154/200)
[2021-05-15 10:03:52,965] {docker.py:276} INFO - 21/05/15 13:03:52 INFO Executor: Running task 157.0 in stage 4.0 (TID 336)
[2021-05-15 10:03:52,987] {docker.py:276} INFO - 21/05/15 13:03:53 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:52,988] {docker.py:276} INFO - 21/05/15 13:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:52,989] {docker.py:276} INFO - 21/05/15 13:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056739412413509390967_0004_m_000157_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056739412413509390967_0004_m_000157_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056739412413509390967_0004}; taskId=attempt_202105151302056739412413509390967_0004_m_000157_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b50975c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:52,989] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302056739412413509390967_0004_m_000157_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056739412413509390967_0004_m_000157_336
[2021-05-15 10:03:52,992] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Task committer attempt_202105151302056739412413509390967_0004_m_000157_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056739412413509390967_0004_m_000157_336 : duration 0:00.003s
[2021-05-15 10:03:53,105] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302055837952498610995317_0004_m_000155_334: needsTaskCommit() Task attempt_202105151302055837952498610995317_0004_m_000155_334
[2021-05-15 10:03:53,106] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Task committer attempt_202105151302055837952498610995317_0004_m_000155_334: needsTaskCommit() Task attempt_202105151302055837952498610995317_0004_m_000155_334: duration 0:00.000s
[2021-05-15 10:03:53,106] {docker.py:276} INFO - 21/05/15 13:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055837952498610995317_0004_m_000155_334
[2021-05-15 10:03:53,108] {docker.py:276} INFO - 21/05/15 13:03:53 INFO Executor: Finished task 155.0 in stage 4.0 (TID 334). 4587 bytes result sent to driver
[2021-05-15 10:03:53,110] {docker.py:276} INFO - 21/05/15 13:03:53 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 337) (4e8a4a26f4b5, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:53,111] {docker.py:276} INFO - 21/05/15 13:03:53 INFO Executor: Running task 158.0 in stage 4.0 (TID 337)
[2021-05-15 10:03:53,112] {docker.py:276} INFO - 21/05/15 13:03:53 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 334) in 2375 ms on 4e8a4a26f4b5 (executor driver) (155/200)
[2021-05-15 10:03:53,120] {docker.py:276} INFO - 21/05/15 13:03:53 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:53,122] {docker.py:276} INFO - 21/05/15 13:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051109553956036339106_0004_m_000158_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051109553956036339106_0004_m_000158_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051109553956036339106_0004}; taskId=attempt_202105151302051109553956036339106_0004_m_000158_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4093b588}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:53,122] {docker.py:276} INFO - 21/05/15 13:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302051109553956036339106_0004_m_000158_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051109553956036339106_0004_m_000158_337
[2021-05-15 10:03:53,125] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Task committer attempt_202105151302051109553956036339106_0004_m_000158_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051109553956036339106_0004_m_000158_337 : duration 0:00.003s
[2021-05-15 10:03:53,186] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302059052444090793344336_0004_m_000154_333: needsTaskCommit() Task attempt_202105151302059052444090793344336_0004_m_000154_333
21/05/15 13:03:53 INFO StagingCommitter: Task committer attempt_202105151302059052444090793344336_0004_m_000154_333: needsTaskCommit() Task attempt_202105151302059052444090793344336_0004_m_000154_333: duration 0:00.001s
21/05/15 13:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059052444090793344336_0004_m_000154_333
[2021-05-15 10:03:53,187] {docker.py:276} INFO - 21/05/15 13:03:53 INFO Executor: Finished task 154.0 in stage 4.0 (TID 333). 4587 bytes result sent to driver
[2021-05-15 10:03:53,189] {docker.py:276} INFO - 21/05/15 13:03:53 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 338) (4e8a4a26f4b5, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:53,189] {docker.py:276} INFO - 21/05/15 13:03:53 INFO Executor: Running task 159.0 in stage 4.0 (TID 338)
[2021-05-15 10:03:53,190] {docker.py:276} INFO - 21/05/15 13:03:53 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 333) in 2462 ms on 4e8a4a26f4b5 (executor driver) (156/200)
[2021-05-15 10:03:53,199] {docker.py:276} INFO - 21/05/15 13:03:53 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:03:53,200] {docker.py:276} INFO - 21/05/15 13:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:53,202] {docker.py:276} INFO - 21/05/15 13:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:03:53,202] {docker.py:276} INFO - 21/05/15 13:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:53,203] {docker.py:276} INFO - 21/05/15 13:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:53,203] {docker.py:276} INFO - 21/05/15 13:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051538859946232531370_0004_m_000159_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051538859946232531370_0004_m_000159_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051538859946232531370_0004}; taskId=attempt_202105151302051538859946232531370_0004_m_000159_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7421bdeb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:53,203] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105151302051538859946232531370_0004_m_000159_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051538859946232531370_0004_m_000159_338
[2021-05-15 10:03:53,206] {docker.py:276} INFO - 21/05/15 13:03:53 INFO StagingCommitter: Task committer attempt_202105151302051538859946232531370_0004_m_000159_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051538859946232531370_0004_m_000159_338 : duration 0:00.003s
[2021-05-15 10:03:54,107] {docker.py:276} INFO - 21/05/15 13:03:54 INFO StagingCommitter: Starting: Task committer attempt_2021051513020597397263397481910_0004_m_000156_335: needsTaskCommit() Task attempt_2021051513020597397263397481910_0004_m_000156_335
[2021-05-15 10:03:54,108] {docker.py:276} INFO - 21/05/15 13:03:54 INFO StagingCommitter: Task committer attempt_2021051513020597397263397481910_0004_m_000156_335: needsTaskCommit() Task attempt_2021051513020597397263397481910_0004_m_000156_335: duration 0:00.001s
21/05/15 13:03:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051513020597397263397481910_0004_m_000156_335
[2021-05-15 10:03:54,109] {docker.py:276} INFO - 21/05/15 13:03:54 INFO Executor: Finished task 156.0 in stage 4.0 (TID 335). 4587 bytes result sent to driver
[2021-05-15 10:03:54,110] {docker.py:276} INFO - 21/05/15 13:03:54 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 339) (4e8a4a26f4b5, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:54 INFO Executor: Running task 160.0 in stage 4.0 (TID 339)
[2021-05-15 10:03:54,111] {docker.py:276} INFO - 21/05/15 13:03:54 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 335) in 2417 ms on 4e8a4a26f4b5 (executor driver) (157/200)
[2021-05-15 10:03:54,123] {docker.py:276} INFO - 21/05/15 13:03:54 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:54,124] {docker.py:276} INFO - 21/05/15 13:03:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:03:54,125] {docker.py:276} INFO - 21/05/15 13:03:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:54,125] {docker.py:276} INFO - 21/05/15 13:03:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:54,125] {docker.py:276} INFO - 21/05/15 13:03:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055598385249633114326_0004_m_000160_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055598385249633114326_0004_m_000160_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055598385249633114326_0004}; taskId=attempt_202105151302055598385249633114326_0004_m_000160_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c5a73c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:54,126] {docker.py:276} INFO - 21/05/15 13:03:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:54,126] {docker.py:276} INFO - 21/05/15 13:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105151302055598385249633114326_0004_m_000160_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055598385249633114326_0004_m_000160_339
[2021-05-15 10:03:54,130] {docker.py:276} INFO - 21/05/15 13:03:54 INFO StagingCommitter: Task committer attempt_202105151302055598385249633114326_0004_m_000160_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055598385249633114326_0004_m_000160_339 : duration 0:00.004s
[2021-05-15 10:03:55,524] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302051109553956036339106_0004_m_000158_337: needsTaskCommit() Task attempt_202105151302051109553956036339106_0004_m_000158_337
[2021-05-15 10:03:55,525] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Task committer attempt_202105151302051109553956036339106_0004_m_000158_337: needsTaskCommit() Task attempt_202105151302051109553956036339106_0004_m_000158_337: duration 0:00.001s
21/05/15 13:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051109553956036339106_0004_m_000158_337
[2021-05-15 10:03:55,525] {docker.py:276} INFO - 21/05/15 13:03:55 INFO Executor: Finished task 158.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
[2021-05-15 10:03:55,526] {docker.py:276} INFO - 21/05/15 13:03:55 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 340) (4e8a4a26f4b5, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:55,527] {docker.py:276} INFO - 21/05/15 13:03:55 INFO Executor: Running task 161.0 in stage 4.0 (TID 340)
[2021-05-15 10:03:55,528] {docker.py:276} INFO - 21/05/15 13:03:55 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 337) in 2385 ms on 4e8a4a26f4b5 (executor driver) (158/200)
[2021-05-15 10:03:55,531] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302056739412413509390967_0004_m_000157_336: needsTaskCommit() Task attempt_202105151302056739412413509390967_0004_m_000157_336
21/05/15 13:03:55 INFO StagingCommitter: Task committer attempt_202105151302056739412413509390967_0004_m_000157_336: needsTaskCommit() Task attempt_202105151302056739412413509390967_0004_m_000157_336: duration 0:00.000s
21/05/15 13:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056739412413509390967_0004_m_000157_336
[2021-05-15 10:03:55,532] {docker.py:276} INFO - 21/05/15 13:03:55 INFO Executor: Finished task 157.0 in stage 4.0 (TID 336). 4587 bytes result sent to driver
[2021-05-15 10:03:55,533] {docker.py:276} INFO - 21/05/15 13:03:55 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 341) (4e8a4a26f4b5, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:55,533] {docker.py:276} INFO - 21/05/15 13:03:55 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 336) in 2538 ms on 4e8a4a26f4b5 (executor driver) (159/200)
21/05/15 13:03:55 INFO Executor: Running task 162.0 in stage 4.0 (TID 341)
[2021-05-15 10:03:55,537] {docker.py:276} INFO - 21/05/15 13:03:55 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:55,539] {docker.py:276} INFO - 21/05/15 13:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:03:55,539] {docker.py:276} INFO - 21/05/15 13:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:55,539] {docker.py:276} INFO - 21/05/15 13:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056584628863039206111_0004_m_000161_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056584628863039206111_0004_m_000161_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056584628863039206111_0004}; taskId=attempt_202105151302056584628863039206111_0004_m_000161_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72095021}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:55,540] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302056584628863039206111_0004_m_000161_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056584628863039206111_0004_m_000161_340
[2021-05-15 10:03:55,541] {docker.py:276} INFO - 21/05/15 13:03:55 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:55,542] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Task committer attempt_202105151302056584628863039206111_0004_m_000161_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056584628863039206111_0004_m_000161_340 : duration 0:00.002s
[2021-05-15 10:03:55,543] {docker.py:276} INFO - 21/05/15 13:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054469968570057846190_0004_m_000162_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054469968570057846190_0004_m_000162_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054469968570057846190_0004}; taskId=attempt_202105151302054469968570057846190_0004_m_000162_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@203987d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:55,544] {docker.py:276} INFO - 21/05/15 13:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302054469968570057846190_0004_m_000162_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054469968570057846190_0004_m_000162_341
[2021-05-15 10:03:55,546] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Task committer attempt_202105151302054469968570057846190_0004_m_000162_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054469968570057846190_0004_m_000162_341 : duration 0:00.003s
[2021-05-15 10:03:55,601] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302051538859946232531370_0004_m_000159_338: needsTaskCommit() Task attempt_202105151302051538859946232531370_0004_m_000159_338
[2021-05-15 10:03:55,602] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Task committer attempt_202105151302051538859946232531370_0004_m_000159_338: needsTaskCommit() Task attempt_202105151302051538859946232531370_0004_m_000159_338: duration 0:00.001s
21/05/15 13:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051538859946232531370_0004_m_000159_338
[2021-05-15 10:03:55,604] {docker.py:276} INFO - 21/05/15 13:03:55 INFO Executor: Finished task 159.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-15 10:03:55,605] {docker.py:276} INFO - 21/05/15 13:03:55 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 342) (4e8a4a26f4b5, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:03:55 INFO Executor: Running task 163.0 in stage 4.0 (TID 342)
21/05/15 13:03:55 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 338) in 2385 ms on 4e8a4a26f4b5 (executor driver) (160/200)
[2021-05-15 10:03:55,615] {docker.py:276} INFO - 21/05/15 13:03:55 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:55,618] {docker.py:276} INFO - 21/05/15 13:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056501402338781865007_0004_m_000163_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056501402338781865007_0004_m_000163_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056501402338781865007_0004}; taskId=attempt_202105151302056501402338781865007_0004_m_000163_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58f240fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:55,618] {docker.py:276} INFO - 21/05/15 13:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105151302056501402338781865007_0004_m_000163_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056501402338781865007_0004_m_000163_342
[2021-05-15 10:03:55,621] {docker.py:276} INFO - 21/05/15 13:03:55 INFO StagingCommitter: Task committer attempt_202105151302056501402338781865007_0004_m_000163_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056501402338781865007_0004_m_000163_342 : duration 0:00.003s
[2021-05-15 10:03:56,606] {docker.py:276} INFO - 21/05/15 13:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105151302055598385249633114326_0004_m_000160_339: needsTaskCommit() Task attempt_202105151302055598385249633114326_0004_m_000160_339
[2021-05-15 10:03:56,608] {docker.py:276} INFO - 21/05/15 13:03:56 INFO StagingCommitter: Task committer attempt_202105151302055598385249633114326_0004_m_000160_339: needsTaskCommit() Task attempt_202105151302055598385249633114326_0004_m_000160_339: duration 0:00.001s
[2021-05-15 10:03:56,608] {docker.py:276} INFO - 21/05/15 13:03:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055598385249633114326_0004_m_000160_339
[2021-05-15 10:03:56,609] {docker.py:276} INFO - 21/05/15 13:03:56 INFO Executor: Finished task 160.0 in stage 4.0 (TID 339). 4544 bytes result sent to driver
[2021-05-15 10:03:56,611] {docker.py:276} INFO - 21/05/15 13:03:56 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 343) (4e8a4a26f4b5, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:56,612] {docker.py:276} INFO - 21/05/15 13:03:56 INFO Executor: Running task 164.0 in stage 4.0 (TID 343)
[2021-05-15 10:03:56,612] {docker.py:276} INFO - 21/05/15 13:03:56 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 339) in 2471 ms on 4e8a4a26f4b5 (executor driver) (161/200)
[2021-05-15 10:03:56,621] {docker.py:276} INFO - 21/05/15 13:03:56 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:56,623] {docker.py:276} INFO - 21/05/15 13:03:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052743670200080788403_0004_m_000164_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052743670200080788403_0004_m_000164_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052743670200080788403_0004}; taskId=attempt_202105151302052743670200080788403_0004_m_000164_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5817eb70}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:56,623] {docker.py:276} INFO - 21/05/15 13:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105151302052743670200080788403_0004_m_000164_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052743670200080788403_0004_m_000164_343
[2021-05-15 10:03:56,626] {docker.py:276} INFO - 21/05/15 13:03:56 INFO StagingCommitter: Task committer attempt_202105151302052743670200080788403_0004_m_000164_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052743670200080788403_0004_m_000164_343 : duration 0:00.003s
[2021-05-15 10:03:57,966] {docker.py:276} INFO - 21/05/15 13:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105151302056584628863039206111_0004_m_000161_340: needsTaskCommit() Task attempt_202105151302056584628863039206111_0004_m_000161_340
[2021-05-15 10:03:57,968] {docker.py:276} INFO - 21/05/15 13:03:57 INFO StagingCommitter: Task committer attempt_202105151302056584628863039206111_0004_m_000161_340: needsTaskCommit() Task attempt_202105151302056584628863039206111_0004_m_000161_340: duration 0:00.002s
21/05/15 13:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056584628863039206111_0004_m_000161_340
[2021-05-15 10:03:57,969] {docker.py:276} INFO - 21/05/15 13:03:57 INFO Executor: Finished task 161.0 in stage 4.0 (TID 340). 4544 bytes result sent to driver
[2021-05-15 10:03:57,970] {docker.py:276} INFO - 21/05/15 13:03:57 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 344) (4e8a4a26f4b5, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:57,971] {docker.py:276} INFO - 21/05/15 13:03:57 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 340) in 2448 ms on 4e8a4a26f4b5 (executor driver) (162/200)
[2021-05-15 10:03:57,972] {docker.py:276} INFO - 21/05/15 13:03:57 INFO Executor: Running task 165.0 in stage 4.0 (TID 344)
[2021-05-15 10:03:57,983] {docker.py:276} INFO - 21/05/15 13:03:57 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:57,985] {docker.py:276} INFO - 21/05/15 13:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051458179511627099135_0004_m_000165_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051458179511627099135_0004_m_000165_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051458179511627099135_0004}; taskId=attempt_202105151302051458179511627099135_0004_m_000165_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@589703d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105151302051458179511627099135_0004_m_000165_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051458179511627099135_0004_m_000165_344
[2021-05-15 10:03:57,988] {docker.py:276} INFO - 21/05/15 13:03:57 INFO StagingCommitter: Task committer attempt_202105151302051458179511627099135_0004_m_000165_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051458179511627099135_0004_m_000165_344 : duration 0:00.003s
[2021-05-15 10:03:58,048] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105151302054469968570057846190_0004_m_000162_341: needsTaskCommit() Task attempt_202105151302054469968570057846190_0004_m_000162_341
[2021-05-15 10:03:58,049] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Task committer attempt_202105151302054469968570057846190_0004_m_000162_341: needsTaskCommit() Task attempt_202105151302054469968570057846190_0004_m_000162_341: duration 0:00.000s
21/05/15 13:03:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054469968570057846190_0004_m_000162_341
[2021-05-15 10:03:58,050] {docker.py:276} INFO - 21/05/15 13:03:58 INFO Executor: Finished task 162.0 in stage 4.0 (TID 341). 4544 bytes result sent to driver
[2021-05-15 10:03:58,051] {docker.py:276} INFO - 21/05/15 13:03:58 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 345) (4e8a4a26f4b5, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:58,052] {docker.py:276} INFO - 21/05/15 13:03:58 INFO Executor: Running task 166.0 in stage 4.0 (TID 345)
[2021-05-15 10:03:58,052] {docker.py:276} INFO - 21/05/15 13:03:58 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 341) in 2521 ms on 4e8a4a26f4b5 (executor driver) (163/200)
[2021-05-15 10:03:58,061] {docker.py:276} INFO - 21/05/15 13:03:58 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:58,063] {docker.py:276} INFO - 21/05/15 13:03:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057508106203576779791_0004_m_000166_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057508106203576779791_0004_m_000166_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057508106203576779791_0004}; taskId=attempt_202105151302057508106203576779791_0004_m_000166_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43bcbefb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:03:58,064] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105151302057508106203576779791_0004_m_000166_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057508106203576779791_0004_m_000166_345
[2021-05-15 10:03:58,066] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Task committer attempt_202105151302057508106203576779791_0004_m_000166_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057508106203576779791_0004_m_000166_345 : duration 0:00.004s
[2021-05-15 10:03:58,118] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105151302056501402338781865007_0004_m_000163_342: needsTaskCommit() Task attempt_202105151302056501402338781865007_0004_m_000163_342
[2021-05-15 10:03:58,118] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Task committer attempt_202105151302056501402338781865007_0004_m_000163_342: needsTaskCommit() Task attempt_202105151302056501402338781865007_0004_m_000163_342: duration 0:00.001s
[2021-05-15 10:03:58,119] {docker.py:276} INFO - 21/05/15 13:03:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056501402338781865007_0004_m_000163_342
[2021-05-15 10:03:58,121] {docker.py:276} INFO - 21/05/15 13:03:58 INFO Executor: Finished task 163.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-15 10:03:58,122] {docker.py:276} INFO - 21/05/15 13:03:58 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 346) (4e8a4a26f4b5, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:58,123] {docker.py:276} INFO - 21/05/15 13:03:58 INFO Executor: Running task 167.0 in stage 4.0 (TID 346)
[2021-05-15 10:03:58,124] {docker.py:276} INFO - 21/05/15 13:03:58 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 342) in 2522 ms on 4e8a4a26f4b5 (executor driver) (164/200)
[2021-05-15 10:03:58,139] {docker.py:276} INFO - 21/05/15 13:03:58 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:58,141] {docker.py:276} INFO - 21/05/15 13:03:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057357909648738004014_0004_m_000167_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057357909648738004014_0004_m_000167_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057357909648738004014_0004}; taskId=attempt_202105151302057357909648738004014_0004_m_000167_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@579be898}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:03:58,141] {docker.py:276} INFO - 21/05/15 13:03:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105151302057357909648738004014_0004_m_000167_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057357909648738004014_0004_m_000167_346
[2021-05-15 10:03:58,143] {docker.py:276} INFO - 21/05/15 13:03:58 INFO StagingCommitter: Task committer attempt_202105151302057357909648738004014_0004_m_000167_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057357909648738004014_0004_m_000167_346 : duration 0:00.003s
[2021-05-15 10:03:59,017] {docker.py:276} INFO - 21/05/15 13:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105151302052743670200080788403_0004_m_000164_343: needsTaskCommit() Task attempt_202105151302052743670200080788403_0004_m_000164_343
[2021-05-15 10:03:59,019] {docker.py:276} INFO - 21/05/15 13:03:59 INFO StagingCommitter: Task committer attempt_202105151302052743670200080788403_0004_m_000164_343: needsTaskCommit() Task attempt_202105151302052743670200080788403_0004_m_000164_343: duration 0:00.002s
21/05/15 13:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052743670200080788403_0004_m_000164_343
[2021-05-15 10:03:59,022] {docker.py:276} INFO - 21/05/15 13:03:59 INFO Executor: Finished task 164.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-15 10:03:59,023] {docker.py:276} INFO - 21/05/15 13:03:59 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 347) (4e8a4a26f4b5, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:03:59,024] {docker.py:276} INFO - 21/05/15 13:03:59 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 343) in 2415 ms on 4e8a4a26f4b5 (executor driver) (165/200)
[2021-05-15 10:03:59,024] {docker.py:276} INFO - 21/05/15 13:03:59 INFO Executor: Running task 168.0 in stage 4.0 (TID 347)
[2021-05-15 10:03:59,034] {docker.py:276} INFO - 21/05/15 13:03:59 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:03:59,036] {docker.py:276} INFO - 21/05/15 13:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056148718088128341209_0004_m_000168_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056148718088128341209_0004_m_000168_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056148718088128341209_0004}; taskId=attempt_202105151302056148718088128341209_0004_m_000168_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74371b7b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105151302056148718088128341209_0004_m_000168_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056148718088128341209_0004_m_000168_347
[2021-05-15 10:03:59,038] {docker.py:276} INFO - 21/05/15 13:03:59 INFO StagingCommitter: Task committer attempt_202105151302056148718088128341209_0004_m_000168_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056148718088128341209_0004_m_000168_347 : duration 0:00.003s
[2021-05-15 10:04:00,419] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302057508106203576779791_0004_m_000166_345: needsTaskCommit() Task attempt_202105151302057508106203576779791_0004_m_000166_345
[2021-05-15 10:04:00,420] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Task committer attempt_202105151302057508106203576779791_0004_m_000166_345: needsTaskCommit() Task attempt_202105151302057508106203576779791_0004_m_000166_345: duration 0:00.001s
21/05/15 13:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057508106203576779791_0004_m_000166_345
[2021-05-15 10:04:00,422] {docker.py:276} INFO - 21/05/15 13:04:00 INFO Executor: Finished task 166.0 in stage 4.0 (TID 345). 4544 bytes result sent to driver
[2021-05-15 10:04:00,424] {docker.py:276} INFO - 21/05/15 13:04:00 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 348) (4e8a4a26f4b5, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:00,425] {docker.py:276} INFO - 21/05/15 13:04:00 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 345) in 2377 ms on 4e8a4a26f4b5 (executor driver) (166/200)
[2021-05-15 10:04:00,426] {docker.py:276} INFO - 21/05/15 13:04:00 INFO Executor: Running task 169.0 in stage 4.0 (TID 348)
[2021-05-15 10:04:00,445] {docker.py:276} INFO - 21/05/15 13:04:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:00,447] {docker.py:276} INFO - 21/05/15 13:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051932568635310440462_0004_m_000169_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051932568635310440462_0004_m_000169_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051932568635310440462_0004}; taskId=attempt_202105151302051932568635310440462_0004_m_000169_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@165dfd69}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:00,447] {docker.py:276} INFO - 21/05/15 13:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302051932568635310440462_0004_m_000169_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051932568635310440462_0004_m_000169_348
[2021-05-15 10:04:00,450] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Task committer attempt_202105151302051932568635310440462_0004_m_000169_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051932568635310440462_0004_m_000169_348 : duration 0:00.003s
[2021-05-15 10:04:00,485] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302057357909648738004014_0004_m_000167_346: needsTaskCommit() Task attempt_202105151302057357909648738004014_0004_m_000167_346
[2021-05-15 10:04:00,486] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Task committer attempt_202105151302057357909648738004014_0004_m_000167_346: needsTaskCommit() Task attempt_202105151302057357909648738004014_0004_m_000167_346: duration 0:00.001s
21/05/15 13:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057357909648738004014_0004_m_000167_346
[2021-05-15 10:04:00,489] {docker.py:276} INFO - 21/05/15 13:04:00 INFO Executor: Finished task 167.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-15 10:04:00,489] {docker.py:276} INFO - 21/05/15 13:04:00 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 349) (4e8a4a26f4b5, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:00,490] {docker.py:276} INFO - 21/05/15 13:04:00 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 346) in 2371 ms on 4e8a4a26f4b5 (executor driver) (167/200)
[2021-05-15 10:04:00,491] {docker.py:276} INFO - 21/05/15 13:04:00 INFO Executor: Running task 170.0 in stage 4.0 (TID 349)
[2021-05-15 10:04:00,500] {docker.py:276} INFO - 21/05/15 13:04:00 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:00,502] {docker.py:276} INFO - 21/05/15 13:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302057588277677592917941_0004_m_000170_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057588277677592917941_0004_m_000170_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302057588277677592917941_0004}; taskId=attempt_202105151302057588277677592917941_0004_m_000170_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f614184}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302057588277677592917941_0004_m_000170_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057588277677592917941_0004_m_000170_349
[2021-05-15 10:04:00,505] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Task committer attempt_202105151302057588277677592917941_0004_m_000170_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302057588277677592917941_0004_m_000170_349 : duration 0:00.003s
[2021-05-15 10:04:00,522] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302051458179511627099135_0004_m_000165_344: needsTaskCommit() Task attempt_202105151302051458179511627099135_0004_m_000165_344
21/05/15 13:04:00 INFO StagingCommitter: Task committer attempt_202105151302051458179511627099135_0004_m_000165_344: needsTaskCommit() Task attempt_202105151302051458179511627099135_0004_m_000165_344: duration 0:00.001s
[2021-05-15 10:04:00,523] {docker.py:276} INFO - 21/05/15 13:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051458179511627099135_0004_m_000165_344
[2021-05-15 10:04:00,524] {docker.py:276} INFO - 21/05/15 13:04:00 INFO Executor: Finished task 165.0 in stage 4.0 (TID 344). 4587 bytes result sent to driver
[2021-05-15 10:04:00,525] {docker.py:276} INFO - 21/05/15 13:04:00 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 350) (4e8a4a26f4b5, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:00,525] {docker.py:276} INFO - 21/05/15 13:04:00 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 344) in 2559 ms on 4e8a4a26f4b5 (executor driver) (168/200)
[2021-05-15 10:04:00,526] {docker.py:276} INFO - 21/05/15 13:04:00 INFO Executor: Running task 171.0 in stage 4.0 (TID 350)
[2021-05-15 10:04:00,533] {docker.py:276} INFO - 21/05/15 13:04:00 INFO ShuffleBlockFetcherIterator: Getting 4 (13.2 KiB) non-empty blocks including 4 (13.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:00,535] {docker.py:276} INFO - 21/05/15 13:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056404170318590893199_0004_m_000171_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056404170318590893199_0004_m_000171_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056404170318590893199_0004}; taskId=attempt_202105151302056404170318590893199_0004_m_000171_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@158311}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105151302056404170318590893199_0004_m_000171_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056404170318590893199_0004_m_000171_350
[2021-05-15 10:04:00,537] {docker.py:276} INFO - 21/05/15 13:04:00 INFO StagingCommitter: Task committer attempt_202105151302056404170318590893199_0004_m_000171_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056404170318590893199_0004_m_000171_350 : duration 0:00.002s
[2021-05-15 10:04:01,435] {docker.py:276} INFO - 21/05/15 13:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105151302056148718088128341209_0004_m_000168_347: needsTaskCommit() Task attempt_202105151302056148718088128341209_0004_m_000168_347
[2021-05-15 10:04:01,436] {docker.py:276} INFO - 21/05/15 13:04:01 INFO StagingCommitter: Task committer attempt_202105151302056148718088128341209_0004_m_000168_347: needsTaskCommit() Task attempt_202105151302056148718088128341209_0004_m_000168_347: duration 0:00.001s
[2021-05-15 10:04:01,436] {docker.py:276} INFO - 21/05/15 13:04:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056148718088128341209_0004_m_000168_347
[2021-05-15 10:04:01,438] {docker.py:276} INFO - 21/05/15 13:04:01 INFO Executor: Finished task 168.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-15 10:04:01,439] {docker.py:276} INFO - 21/05/15 13:04:01 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 351) (4e8a4a26f4b5, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:01,441] {docker.py:276} INFO - 21/05/15 13:04:01 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 347) in 2422 ms on 4e8a4a26f4b5 (executor driver) (169/200)
21/05/15 13:04:01 INFO Executor: Running task 172.0 in stage 4.0 (TID 351)
[2021-05-15 10:04:01,450] {docker.py:276} INFO - 21/05/15 13:04:01 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:01,452] {docker.py:276} INFO - 21/05/15 13:04:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055638329913154016309_0004_m_000172_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055638329913154016309_0004_m_000172_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055638329913154016309_0004}; taskId=attempt_202105151302055638329913154016309_0004_m_000172_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@fa8915}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105151302055638329913154016309_0004_m_000172_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055638329913154016309_0004_m_000172_351
[2021-05-15 10:04:01,455] {docker.py:276} INFO - 21/05/15 13:04:01 INFO StagingCommitter: Task committer attempt_202105151302055638329913154016309_0004_m_000172_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055638329913154016309_0004_m_000172_351 : duration 0:00.003s
[2021-05-15 10:04:02,959] {docker.py:276} INFO - 21/05/15 13:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105151302057588277677592917941_0004_m_000170_349: needsTaskCommit() Task attempt_202105151302057588277677592917941_0004_m_000170_349
[2021-05-15 10:04:02,959] {docker.py:276} INFO - 21/05/15 13:04:02 INFO StagingCommitter: Task committer attempt_202105151302057588277677592917941_0004_m_000170_349: needsTaskCommit() Task attempt_202105151302057588277677592917941_0004_m_000170_349: duration 0:00.001s
21/05/15 13:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302057588277677592917941_0004_m_000170_349
[2021-05-15 10:04:02,961] {docker.py:276} INFO - 21/05/15 13:04:02 INFO Executor: Finished task 170.0 in stage 4.0 (TID 349). 4544 bytes result sent to driver
[2021-05-15 10:04:02,962] {docker.py:276} INFO - 21/05/15 13:04:02 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 352) (4e8a4a26f4b5, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:02,963] {docker.py:276} INFO - 21/05/15 13:04:02 INFO Executor: Running task 173.0 in stage 4.0 (TID 352)
[2021-05-15 10:04:02,963] {docker.py:276} INFO - 21/05/15 13:04:02 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 349) in 2477 ms on 4e8a4a26f4b5 (executor driver) (170/200)
[2021-05-15 10:04:02,972] {docker.py:276} INFO - 21/05/15 13:04:02 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:02,974] {docker.py:276} INFO - 21/05/15 13:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051480000326213564703_0004_m_000173_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051480000326213564703_0004_m_000173_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051480000326213564703_0004}; taskId=attempt_202105151302051480000326213564703_0004_m_000173_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a91557d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105151302051480000326213564703_0004_m_000173_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051480000326213564703_0004_m_000173_352
[2021-05-15 10:04:02,976] {docker.py:276} INFO - 21/05/15 13:04:02 INFO StagingCommitter: Task committer attempt_202105151302051480000326213564703_0004_m_000173_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051480000326213564703_0004_m_000173_352 : duration 0:00.002s
[2021-05-15 10:04:03,007] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105151302056404170318590893199_0004_m_000171_350: needsTaskCommit() Task attempt_202105151302056404170318590893199_0004_m_000171_350
[2021-05-15 10:04:03,008] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Task committer attempt_202105151302056404170318590893199_0004_m_000171_350: needsTaskCommit() Task attempt_202105151302056404170318590893199_0004_m_000171_350: duration 0:00.001s
21/05/15 13:04:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056404170318590893199_0004_m_000171_350
[2021-05-15 10:04:03,010] {docker.py:276} INFO - 21/05/15 13:04:03 INFO Executor: Finished task 171.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-15 10:04:03,012] {docker.py:276} INFO - 21/05/15 13:04:03 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 353) (4e8a4a26f4b5, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:03,013] {docker.py:276} INFO - 21/05/15 13:04:03 INFO Executor: Running task 174.0 in stage 4.0 (TID 353)
[2021-05-15 10:04:03,015] {docker.py:276} INFO - 21/05/15 13:04:03 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 350) in 2493 ms on 4e8a4a26f4b5 (executor driver) (171/200)
[2021-05-15 10:04:03,016] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105151302051932568635310440462_0004_m_000169_348: needsTaskCommit() Task attempt_202105151302051932568635310440462_0004_m_000169_348
[2021-05-15 10:04:03,017] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Task committer attempt_202105151302051932568635310440462_0004_m_000169_348: needsTaskCommit() Task attempt_202105151302051932568635310440462_0004_m_000169_348: duration 0:00.001s
21/05/15 13:04:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051932568635310440462_0004_m_000169_348
[2021-05-15 10:04:03,019] {docker.py:276} INFO - 21/05/15 13:04:03 INFO Executor: Finished task 169.0 in stage 4.0 (TID 348). 4587 bytes result sent to driver
[2021-05-15 10:04:03,019] {docker.py:276} INFO - 21/05/15 13:04:03 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 354) (4e8a4a26f4b5, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:03,021] {docker.py:276} INFO - 21/05/15 13:04:03 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 348) in 2600 ms on 4e8a4a26f4b5 (executor driver) (172/200)
[2021-05-15 10:04:03,023] {docker.py:276} INFO - 21/05/15 13:04:03 INFO Executor: Running task 175.0 in stage 4.0 (TID 354)
[2021-05-15 10:04:03,030] {docker.py:276} INFO - 21/05/15 13:04:03 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:03,031] {docker.py:276} INFO - 21/05/15 13:04:03 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:04:03,031] {docker.py:276} INFO - 21/05/15 13:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:03,032] {docker.py:276} INFO - 21/05/15 13:04:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:03,032] {docker.py:276} INFO - 21/05/15 13:04:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054403124823043312152_0004_m_000174_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054403124823043312152_0004_m_000174_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054403124823043312152_0004}; taskId=attempt_202105151302054403124823043312152_0004_m_000174_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27d44b1e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105151302054403124823043312152_0004_m_000174_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054403124823043312152_0004_m_000174_353
[2021-05-15 10:04:03,033] {docker.py:276} INFO - 21/05/15 13:04:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:04:03,033] {docker.py:276} INFO - 21/05/15 13:04:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:03,034] {docker.py:276} INFO - 21/05/15 13:04:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205312164617272789210_0004_m_000175_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205312164617272789210_0004_m_000175_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205312164617272789210_0004}; taskId=attempt_20210515130205312164617272789210_0004_m_000175_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a4cdfb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:03 INFO StagingCommitter: Starting: Task committer attempt_20210515130205312164617272789210_0004_m_000175_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205312164617272789210_0004_m_000175_354
[2021-05-15 10:04:03,035] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Task committer attempt_202105151302054403124823043312152_0004_m_000174_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054403124823043312152_0004_m_000174_353 : duration 0:00.003s
[2021-05-15 10:04:03,039] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Task committer attempt_20210515130205312164617272789210_0004_m_000175_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205312164617272789210_0004_m_000175_354 : duration 0:00.004s
[2021-05-15 10:04:03,872] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105151302055638329913154016309_0004_m_000172_351: needsTaskCommit() Task attempt_202105151302055638329913154016309_0004_m_000172_351
[2021-05-15 10:04:03,873] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Task committer attempt_202105151302055638329913154016309_0004_m_000172_351: needsTaskCommit() Task attempt_202105151302055638329913154016309_0004_m_000172_351: duration 0:00.002s
21/05/15 13:04:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055638329913154016309_0004_m_000172_351
[2021-05-15 10:04:03,874] {docker.py:276} INFO - 21/05/15 13:04:03 INFO Executor: Finished task 172.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-15 10:04:03,876] {docker.py:276} INFO - 21/05/15 13:04:03 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 355) (4e8a4a26f4b5, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:03,877] {docker.py:276} INFO - 21/05/15 13:04:03 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 351) in 2440 ms on 4e8a4a26f4b5 (executor driver) (173/200)
[2021-05-15 10:04:03,877] {docker.py:276} INFO - 21/05/15 13:04:03 INFO Executor: Running task 176.0 in stage 4.0 (TID 355)
[2021-05-15 10:04:03,887] {docker.py:276} INFO - 21/05/15 13:04:03 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:03,889] {docker.py:276} INFO - 21/05/15 13:04:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205204915169232661352_0004_m_000176_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205204915169232661352_0004_m_000176_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205204915169232661352_0004}; taskId=attempt_20210515130205204915169232661352_0004_m_000176_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54b25f56}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:03,889] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Starting: Task committer attempt_20210515130205204915169232661352_0004_m_000176_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205204915169232661352_0004_m_000176_355
[2021-05-15 10:04:03,893] {docker.py:276} INFO - 21/05/15 13:04:03 INFO StagingCommitter: Task committer attempt_20210515130205204915169232661352_0004_m_000176_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205204915169232661352_0004_m_000176_355 : duration 0:00.003s
[2021-05-15 10:04:05,438] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105151302054403124823043312152_0004_m_000174_353: needsTaskCommit() Task attempt_202105151302054403124823043312152_0004_m_000174_353
[2021-05-15 10:04:05,439] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Task committer attempt_202105151302054403124823043312152_0004_m_000174_353: needsTaskCommit() Task attempt_202105151302054403124823043312152_0004_m_000174_353: duration 0:00.001s
21/05/15 13:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054403124823043312152_0004_m_000174_353
[2021-05-15 10:04:05,441] {docker.py:276} INFO - 21/05/15 13:04:05 INFO Executor: Finished task 174.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-15 10:04:05,442] {docker.py:276} INFO - 21/05/15 13:04:05 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 356) (4e8a4a26f4b5, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:05,443] {docker.py:276} INFO - 21/05/15 13:04:05 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 353) in 2436 ms on 4e8a4a26f4b5 (executor driver) (174/200)
21/05/15 13:04:05 INFO Executor: Running task 177.0 in stage 4.0 (TID 356)
[2021-05-15 10:04:05,453] {docker.py:276} INFO - 21/05/15 13:04:05 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:05,455] {docker.py:276} INFO - 21/05/15 13:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205911678394603072314_0004_m_000177_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205911678394603072314_0004_m_000177_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205911678394603072314_0004}; taskId=attempt_20210515130205911678394603072314_0004_m_000177_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4359cced}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:05 INFO StagingCommitter: Starting: Task committer attempt_20210515130205911678394603072314_0004_m_000177_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205911678394603072314_0004_m_000177_356
[2021-05-15 10:04:05,458] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Task committer attempt_20210515130205911678394603072314_0004_m_000177_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205911678394603072314_0004_m_000177_356 : duration 0:00.003s
[2021-05-15 10:04:05,519] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105151302051480000326213564703_0004_m_000173_352: needsTaskCommit() Task attempt_202105151302051480000326213564703_0004_m_000173_352
[2021-05-15 10:04:05,519] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Task committer attempt_202105151302051480000326213564703_0004_m_000173_352: needsTaskCommit() Task attempt_202105151302051480000326213564703_0004_m_000173_352: duration 0:00.001s
[2021-05-15 10:04:05,519] {docker.py:276} INFO - 21/05/15 13:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051480000326213564703_0004_m_000173_352
[2021-05-15 10:04:05,520] {docker.py:276} INFO - 21/05/15 13:04:05 INFO Executor: Finished task 173.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-15 10:04:05,521] {docker.py:276} INFO - 21/05/15 13:04:05 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 357) (4e8a4a26f4b5, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:05,522] {docker.py:276} INFO - 21/05/15 13:04:05 INFO Executor: Running task 178.0 in stage 4.0 (TID 357)
[2021-05-15 10:04:05,522] {docker.py:276} INFO - 21/05/15 13:04:05 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 352) in 2564 ms on 4e8a4a26f4b5 (executor driver) (175/200)
[2021-05-15 10:04:05,528] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Starting: Task committer attempt_20210515130205312164617272789210_0004_m_000175_354: needsTaskCommit() Task attempt_20210515130205312164617272789210_0004_m_000175_354
[2021-05-15 10:04:05,529] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Task committer attempt_20210515130205312164617272789210_0004_m_000175_354: needsTaskCommit() Task attempt_20210515130205312164617272789210_0004_m_000175_354: duration 0:00.000s
21/05/15 13:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205312164617272789210_0004_m_000175_354
[2021-05-15 10:04:05,529] {docker.py:276} INFO - 21/05/15 13:04:05 INFO Executor: Finished task 175.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-15 10:04:05,530] {docker.py:276} INFO - 21/05/15 13:04:05 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 358) (4e8a4a26f4b5, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:05,530] {docker.py:276} INFO - 21/05/15 13:04:05 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 354) in 2514 ms on 4e8a4a26f4b5 (executor driver) (176/200)
[2021-05-15 10:04:05,531] {docker.py:276} INFO - 21/05/15 13:04:05 INFO Executor: Running task 179.0 in stage 4.0 (TID 358)
[2021-05-15 10:04:05,533] {docker.py:276} INFO - 21/05/15 13:04:05 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:05,535] {docker.py:276} INFO - 21/05/15 13:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053104760886423865457_0004_m_000178_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053104760886423865457_0004_m_000178_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053104760886423865457_0004}; taskId=attempt_202105151302053104760886423865457_0004_m_000178_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29c24348}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:05,535] {docker.py:276} INFO - 21/05/15 13:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105151302053104760886423865457_0004_m_000178_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053104760886423865457_0004_m_000178_357
[2021-05-15 10:04:05,538] {docker.py:276} INFO - 21/05/15 13:04:05 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:04:05,538] {docker.py:276} INFO - 21/05/15 13:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/15 13:04:05 INFO StagingCommitter: Task committer attempt_202105151302053104760886423865457_0004_m_000178_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053104760886423865457_0004_m_000178_357 : duration 0:00.003s
[2021-05-15 10:04:05,539] {docker.py:276} INFO - 21/05/15 13:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:05,540] {docker.py:276} INFO - 21/05/15 13:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058830696898600247273_0004_m_000179_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058830696898600247273_0004_m_000179_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058830696898600247273_0004}; taskId=attempt_202105151302058830696898600247273_0004_m_000179_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10d2d381}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105151302058830696898600247273_0004_m_000179_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058830696898600247273_0004_m_000179_358
[2021-05-15 10:04:05,542] {docker.py:276} INFO - 21/05/15 13:04:05 INFO StagingCommitter: Task committer attempt_202105151302058830696898600247273_0004_m_000179_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058830696898600247273_0004_m_000179_358 : duration 0:00.002s
[2021-05-15 10:04:06,360] {docker.py:276} INFO - 21/05/15 13:04:06 INFO StagingCommitter: Starting: Task committer attempt_20210515130205204915169232661352_0004_m_000176_355: needsTaskCommit() Task attempt_20210515130205204915169232661352_0004_m_000176_355
[2021-05-15 10:04:06,361] {docker.py:276} INFO - 21/05/15 13:04:06 INFO StagingCommitter: Task committer attempt_20210515130205204915169232661352_0004_m_000176_355: needsTaskCommit() Task attempt_20210515130205204915169232661352_0004_m_000176_355: duration 0:00.001s
21/05/15 13:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205204915169232661352_0004_m_000176_355
[2021-05-15 10:04:06,362] {docker.py:276} INFO - 21/05/15 13:04:06 INFO Executor: Finished task 176.0 in stage 4.0 (TID 355). 4544 bytes result sent to driver
[2021-05-15 10:04:06,364] {docker.py:276} INFO - 21/05/15 13:04:06 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 359) (4e8a4a26f4b5, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:06,365] {docker.py:276} INFO - 21/05/15 13:04:06 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 355) in 2493 ms on 4e8a4a26f4b5 (executor driver) (177/200)
[2021-05-15 10:04:06,366] {docker.py:276} INFO - 21/05/15 13:04:06 INFO Executor: Running task 180.0 in stage 4.0 (TID 359)
[2021-05-15 10:04:06,375] {docker.py:276} INFO - 21/05/15 13:04:06 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:06,376] {docker.py:276} INFO - 21/05/15 13:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302059086115723581975499_0004_m_000180_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059086115723581975499_0004_m_000180_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302059086115723581975499_0004}; taskId=attempt_202105151302059086115723581975499_0004_m_000180_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55868fa1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:06,377] {docker.py:276} INFO - 21/05/15 13:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105151302059086115723581975499_0004_m_000180_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059086115723581975499_0004_m_000180_359
[2021-05-15 10:04:06,380] {docker.py:276} INFO - 21/05/15 13:04:06 INFO StagingCommitter: Task committer attempt_202105151302059086115723581975499_0004_m_000180_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302059086115723581975499_0004_m_000180_359 : duration 0:00.003s
[2021-05-15 10:04:07,677] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Starting: Task committer attempt_20210515130205911678394603072314_0004_m_000177_356: needsTaskCommit() Task attempt_20210515130205911678394603072314_0004_m_000177_356
[2021-05-15 10:04:07,677] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Task committer attempt_20210515130205911678394603072314_0004_m_000177_356: needsTaskCommit() Task attempt_20210515130205911678394603072314_0004_m_000177_356: duration 0:00.000s
21/05/15 13:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205911678394603072314_0004_m_000177_356
[2021-05-15 10:04:07,679] {docker.py:276} INFO - 21/05/15 13:04:07 INFO Executor: Finished task 177.0 in stage 4.0 (TID 356). 4544 bytes result sent to driver
[2021-05-15 10:04:07,681] {docker.py:276} INFO - 21/05/15 13:04:07 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 360) (4e8a4a26f4b5, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:04:07 INFO Executor: Running task 181.0 in stage 4.0 (TID 360)
21/05/15 13:04:07 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 356) in 2242 ms on 4e8a4a26f4b5 (executor driver) (178/200)
[2021-05-15 10:04:07,691] {docker.py:276} INFO - 21/05/15 13:04:07 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:07,692] {docker.py:276} INFO - 21/05/15 13:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054948142518542908793_0004_m_000181_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054948142518542908793_0004_m_000181_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054948142518542908793_0004}; taskId=attempt_202105151302054948142518542908793_0004_m_000181_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23fb4857}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:07,693] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105151302054948142518542908793_0004_m_000181_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054948142518542908793_0004_m_000181_360
[2021-05-15 10:04:07,695] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Task committer attempt_202105151302054948142518542908793_0004_m_000181_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054948142518542908793_0004_m_000181_360 : duration 0:00.003s
[2021-05-15 10:04:07,899] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105151302053104760886423865457_0004_m_000178_357: needsTaskCommit() Task attempt_202105151302053104760886423865457_0004_m_000178_357
[2021-05-15 10:04:07,900] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Task committer attempt_202105151302053104760886423865457_0004_m_000178_357: needsTaskCommit() Task attempt_202105151302053104760886423865457_0004_m_000178_357: duration 0:00.002s
[2021-05-15 10:04:07,901] {docker.py:276} INFO - 21/05/15 13:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053104760886423865457_0004_m_000178_357
[2021-05-15 10:04:07,902] {docker.py:276} INFO - 21/05/15 13:04:07 INFO Executor: Finished task 178.0 in stage 4.0 (TID 357). 4544 bytes result sent to driver
[2021-05-15 10:04:07,903] {docker.py:276} INFO - 21/05/15 13:04:07 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 361) (4e8a4a26f4b5, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:07,904] {docker.py:276} INFO - 21/05/15 13:04:07 INFO Executor: Running task 182.0 in stage 4.0 (TID 361)
[2021-05-15 10:04:07,905] {docker.py:276} INFO - 21/05/15 13:04:07 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 357) in 2386 ms on 4e8a4a26f4b5 (executor driver) (179/200)
[2021-05-15 10:04:07,925] {docker.py:276} INFO - 21/05/15 13:04:07 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:07,926] {docker.py:276} INFO - 21/05/15 13:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052780137678395539392_0004_m_000182_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052780137678395539392_0004_m_000182_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052780137678395539392_0004}; taskId=attempt_202105151302052780137678395539392_0004_m_000182_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@516deda9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105151302052780137678395539392_0004_m_000182_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052780137678395539392_0004_m_000182_361
[2021-05-15 10:04:07,929] {docker.py:276} INFO - 21/05/15 13:04:07 INFO StagingCommitter: Task committer attempt_202105151302052780137678395539392_0004_m_000182_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052780137678395539392_0004_m_000182_361 : duration 0:00.003s
[2021-05-15 10:04:08,199] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105151302058830696898600247273_0004_m_000179_358: needsTaskCommit() Task attempt_202105151302058830696898600247273_0004_m_000179_358
[2021-05-15 10:04:08,200] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Task committer attempt_202105151302058830696898600247273_0004_m_000179_358: needsTaskCommit() Task attempt_202105151302058830696898600247273_0004_m_000179_358: duration 0:00.001s
[2021-05-15 10:04:08,201] {docker.py:276} INFO - 21/05/15 13:04:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058830696898600247273_0004_m_000179_358
[2021-05-15 10:04:08,201] {docker.py:276} INFO - 21/05/15 13:04:08 INFO Executor: Finished task 179.0 in stage 4.0 (TID 358). 4587 bytes result sent to driver
[2021-05-15 10:04:08,202] {docker.py:276} INFO - 21/05/15 13:04:08 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 362) (4e8a4a26f4b5, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:08,203] {docker.py:276} INFO - 21/05/15 13:04:08 INFO Executor: Running task 183.0 in stage 4.0 (TID 362)
[2021-05-15 10:04:08,204] {docker.py:276} INFO - 21/05/15 13:04:08 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 358) in 2677 ms on 4e8a4a26f4b5 (executor driver) (180/200)
[2021-05-15 10:04:08,211] {docker.py:276} INFO - 21/05/15 13:04:08 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:08,213] {docker.py:276} INFO - 21/05/15 13:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:04:08,213] {docker.py:276} INFO - 21/05/15 13:04:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:08,214] {docker.py:276} INFO - 21/05/15 13:04:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051312215375218382788_0004_m_000183_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051312215375218382788_0004_m_000183_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051312215375218382788_0004}; taskId=attempt_202105151302051312215375218382788_0004_m_000183_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c016e30}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:08,214] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105151302051312215375218382788_0004_m_000183_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051312215375218382788_0004_m_000183_362
[2021-05-15 10:04:08,217] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Task committer attempt_202105151302051312215375218382788_0004_m_000183_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051312215375218382788_0004_m_000183_362 : duration 0:00.003s
[2021-05-15 10:04:08,373] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105151302059086115723581975499_0004_m_000180_359: needsTaskCommit() Task attempt_202105151302059086115723581975499_0004_m_000180_359
[2021-05-15 10:04:08,374] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Task committer attempt_202105151302059086115723581975499_0004_m_000180_359: needsTaskCommit() Task attempt_202105151302059086115723581975499_0004_m_000180_359: duration 0:00.002s
21/05/15 13:04:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302059086115723581975499_0004_m_000180_359
[2021-05-15 10:04:08,376] {docker.py:276} INFO - 21/05/15 13:04:08 INFO Executor: Finished task 180.0 in stage 4.0 (TID 359). 4587 bytes result sent to driver
[2021-05-15 10:04:08,377] {docker.py:276} INFO - 21/05/15 13:04:08 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 363) (4e8a4a26f4b5, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:08,379] {docker.py:276} INFO - 21/05/15 13:04:08 INFO Executor: Running task 184.0 in stage 4.0 (TID 363)
[2021-05-15 10:04:08,379] {docker.py:276} INFO - 21/05/15 13:04:08 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 359) in 2018 ms on 4e8a4a26f4b5 (executor driver) (181/200)
[2021-05-15 10:04:08,388] {docker.py:276} INFO - 21/05/15 13:04:08 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:08,390] {docker.py:276} INFO - 21/05/15 13:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:04:08,390] {docker.py:276} INFO - 21/05/15 13:04:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:08,391] {docker.py:276} INFO - 21/05/15 13:04:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056211072199804829882_0004_m_000184_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056211072199804829882_0004_m_000184_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056211072199804829882_0004}; taskId=attempt_202105151302056211072199804829882_0004_m_000184_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d698b20}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:08,391] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105151302056211072199804829882_0004_m_000184_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056211072199804829882_0004_m_000184_363
[2021-05-15 10:04:08,395] {docker.py:276} INFO - 21/05/15 13:04:08 INFO StagingCommitter: Task committer attempt_202105151302056211072199804829882_0004_m_000184_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056211072199804829882_0004_m_000184_363 : duration 0:00.005s
[2021-05-15 10:04:10,131] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105151302054948142518542908793_0004_m_000181_360: needsTaskCommit() Task attempt_202105151302054948142518542908793_0004_m_000181_360
[2021-05-15 10:04:10,133] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Task committer attempt_202105151302054948142518542908793_0004_m_000181_360: needsTaskCommit() Task attempt_202105151302054948142518542908793_0004_m_000181_360: duration 0:00.001s
21/05/15 13:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054948142518542908793_0004_m_000181_360
[2021-05-15 10:04:10,135] {docker.py:276} INFO - 21/05/15 13:04:10 INFO Executor: Finished task 181.0 in stage 4.0 (TID 360). 4587 bytes result sent to driver
[2021-05-15 10:04:10,136] {docker.py:276} INFO - 21/05/15 13:04:10 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 364) (4e8a4a26f4b5, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:10,137] {docker.py:276} INFO - 21/05/15 13:04:10 INFO Executor: Running task 185.0 in stage 4.0 (TID 364)
[2021-05-15 10:04:10,138] {docker.py:276} INFO - 21/05/15 13:04:10 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 360) in 2461 ms on 4e8a4a26f4b5 (executor driver) (182/200)
[2021-05-15 10:04:10,147] {docker.py:276} INFO - 21/05/15 13:04:10 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:10,148] {docker.py:276} INFO - 21/05/15 13:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130205797663835170122789_0004_m_000185_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205797663835170122789_0004_m_000185_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130205797663835170122789_0004}; taskId=attempt_20210515130205797663835170122789_0004_m_000185_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dea788f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:10,149] {docker.py:276} INFO - 21/05/15 13:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:10 INFO StagingCommitter: Starting: Task committer attempt_20210515130205797663835170122789_0004_m_000185_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205797663835170122789_0004_m_000185_364
[2021-05-15 10:04:10,152] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Task committer attempt_20210515130205797663835170122789_0004_m_000185_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_20210515130205797663835170122789_0004_m_000185_364 : duration 0:00.003s
[2021-05-15 10:04:10,375] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105151302052780137678395539392_0004_m_000182_361: needsTaskCommit() Task attempt_202105151302052780137678395539392_0004_m_000182_361
[2021-05-15 10:04:10,376] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Task committer attempt_202105151302052780137678395539392_0004_m_000182_361: needsTaskCommit() Task attempt_202105151302052780137678395539392_0004_m_000182_361: duration 0:00.002s
21/05/15 13:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052780137678395539392_0004_m_000182_361
[2021-05-15 10:04:10,379] {docker.py:276} INFO - 21/05/15 13:04:10 INFO Executor: Finished task 182.0 in stage 4.0 (TID 361). 4587 bytes result sent to driver
[2021-05-15 10:04:10,380] {docker.py:276} INFO - 21/05/15 13:04:10 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 365) (4e8a4a26f4b5, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:10,381] {docker.py:276} INFO - 21/05/15 13:04:10 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 361) in 2481 ms on 4e8a4a26f4b5 (executor driver) (183/200)
[2021-05-15 10:04:10,382] {docker.py:276} INFO - 21/05/15 13:04:10 INFO Executor: Running task 186.0 in stage 4.0 (TID 365)
[2021-05-15 10:04:10,390] {docker.py:276} INFO - 21/05/15 13:04:10 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:10,392] {docker.py:276} INFO - 21/05/15 13:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051797998775974233032_0004_m_000186_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051797998775974233032_0004_m_000186_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051797998775974233032_0004}; taskId=attempt_202105151302051797998775974233032_0004_m_000186_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f81b2b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105151302051797998775974233032_0004_m_000186_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051797998775974233032_0004_m_000186_365
[2021-05-15 10:04:10,394] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Task committer attempt_202105151302051797998775974233032_0004_m_000186_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051797998775974233032_0004_m_000186_365 : duration 0:00.002s
[2021-05-15 10:04:10,862] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105151302056211072199804829882_0004_m_000184_363: needsTaskCommit() Task attempt_202105151302056211072199804829882_0004_m_000184_363
[2021-05-15 10:04:10,864] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Task committer attempt_202105151302056211072199804829882_0004_m_000184_363: needsTaskCommit() Task attempt_202105151302056211072199804829882_0004_m_000184_363: duration 0:00.002s
21/05/15 13:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056211072199804829882_0004_m_000184_363
[2021-05-15 10:04:10,867] {docker.py:276} INFO - 21/05/15 13:04:10 INFO Executor: Finished task 184.0 in stage 4.0 (TID 363). 4544 bytes result sent to driver
[2021-05-15 10:04:10,869] {docker.py:276} INFO - 21/05/15 13:04:10 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 366) (4e8a4a26f4b5, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:10,870] {docker.py:276} INFO - 21/05/15 13:04:10 INFO Executor: Running task 187.0 in stage 4.0 (TID 366)
[2021-05-15 10:04:10,870] {docker.py:276} INFO - 21/05/15 13:04:10 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 363) in 2496 ms on 4e8a4a26f4b5 (executor driver) (184/200)
[2021-05-15 10:04:10,878] {docker.py:276} INFO - 21/05/15 13:04:10 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:10,880] {docker.py:276} INFO - 21/05/15 13:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053939915399500004088_0004_m_000187_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053939915399500004088_0004_m_000187_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053939915399500004088_0004}; taskId=attempt_202105151302053939915399500004088_0004_m_000187_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ba92af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:10,880] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105151302053939915399500004088_0004_m_000187_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053939915399500004088_0004_m_000187_366
[2021-05-15 10:04:10,883] {docker.py:276} INFO - 21/05/15 13:04:10 INFO StagingCommitter: Task committer attempt_202105151302053939915399500004088_0004_m_000187_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053939915399500004088_0004_m_000187_366 : duration 0:00.003s
[2021-05-15 10:04:11,147] {docker.py:276} INFO - 21/05/15 13:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105151302051312215375218382788_0004_m_000183_362: needsTaskCommit() Task attempt_202105151302051312215375218382788_0004_m_000183_362
[2021-05-15 10:04:11,148] {docker.py:276} INFO - 21/05/15 13:04:11 INFO StagingCommitter: Task committer attempt_202105151302051312215375218382788_0004_m_000183_362: needsTaskCommit() Task attempt_202105151302051312215375218382788_0004_m_000183_362: duration 0:00.001s
21/05/15 13:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051312215375218382788_0004_m_000183_362
[2021-05-15 10:04:11,150] {docker.py:276} INFO - 21/05/15 13:04:11 INFO Executor: Finished task 183.0 in stage 4.0 (TID 362). 4544 bytes result sent to driver
[2021-05-15 10:04:11,151] {docker.py:276} INFO - 21/05/15 13:04:11 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 367) (4e8a4a26f4b5, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:11,151] {docker.py:276} INFO - 21/05/15 13:04:11 INFO Executor: Running task 188.0 in stage 4.0 (TID 367)
21/05/15 13:04:11 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 362) in 2954 ms on 4e8a4a26f4b5 (executor driver) (185/200)
[2021-05-15 10:04:11,161] {docker.py:276} INFO - 21/05/15 13:04:11 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:11,163] {docker.py:276} INFO - 21/05/15 13:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:04:11,164] {docker.py:276} INFO - 21/05/15 13:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055242815726246528300_0004_m_000188_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055242815726246528300_0004_m_000188_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055242815726246528300_0004}; taskId=attempt_202105151302055242815726246528300_0004_m_000188_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56699cb9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:11,164] {docker.py:276} INFO - 21/05/15 13:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105151302055242815726246528300_0004_m_000188_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055242815726246528300_0004_m_000188_367
[2021-05-15 10:04:11,167] {docker.py:276} INFO - 21/05/15 13:04:11 INFO StagingCommitter: Task committer attempt_202105151302055242815726246528300_0004_m_000188_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055242815726246528300_0004_m_000188_367 : duration 0:00.003s
[2021-05-15 10:04:12,485] {docker.py:276} INFO - 21/05/15 13:04:12 INFO StagingCommitter: Starting: Task committer attempt_20210515130205797663835170122789_0004_m_000185_364: needsTaskCommit() Task attempt_20210515130205797663835170122789_0004_m_000185_364
[2021-05-15 10:04:12,487] {docker.py:276} INFO - 21/05/15 13:04:12 INFO StagingCommitter: Task committer attempt_20210515130205797663835170122789_0004_m_000185_364: needsTaskCommit() Task attempt_20210515130205797663835170122789_0004_m_000185_364: duration 0:00.002s
21/05/15 13:04:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130205797663835170122789_0004_m_000185_364
[2021-05-15 10:04:12,488] {docker.py:276} INFO - 21/05/15 13:04:12 INFO Executor: Finished task 185.0 in stage 4.0 (TID 364). 4544 bytes result sent to driver
[2021-05-15 10:04:12,489] {docker.py:276} INFO - 21/05/15 13:04:12 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 368) (4e8a4a26f4b5, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:12,489] {docker.py:276} INFO - 21/05/15 13:04:12 INFO Executor: Running task 189.0 in stage 4.0 (TID 368)
[2021-05-15 10:04:12,491] {docker.py:276} INFO - 21/05/15 13:04:12 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 364) in 2357 ms on 4e8a4a26f4b5 (executor driver) (186/200)
[2021-05-15 10:04:12,500] {docker.py:276} INFO - 21/05/15 13:04:12 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:12,501] {docker.py:276} INFO - 21/05/15 13:04:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051677482619731813481_0004_m_000189_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051677482619731813481_0004_m_000189_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051677482619731813481_0004}; taskId=attempt_202105151302051677482619731813481_0004_m_000189_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75c64f5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105151302051677482619731813481_0004_m_000189_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051677482619731813481_0004_m_000189_368
[2021-05-15 10:04:12,504] {docker.py:276} INFO - 21/05/15 13:04:12 INFO StagingCommitter: Task committer attempt_202105151302051677482619731813481_0004_m_000189_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051677482619731813481_0004_m_000189_368 : duration 0:00.002s
[2021-05-15 10:04:12,933] {docker.py:276} INFO - 21/05/15 13:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105151302051797998775974233032_0004_m_000186_365: needsTaskCommit() Task attempt_202105151302051797998775974233032_0004_m_000186_365
[2021-05-15 10:04:12,934] {docker.py:276} INFO - 21/05/15 13:04:12 INFO StagingCommitter: Task committer attempt_202105151302051797998775974233032_0004_m_000186_365: needsTaskCommit() Task attempt_202105151302051797998775974233032_0004_m_000186_365: duration 0:00.001s
21/05/15 13:04:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051797998775974233032_0004_m_000186_365
[2021-05-15 10:04:12,935] {docker.py:276} INFO - 21/05/15 13:04:12 INFO Executor: Finished task 186.0 in stage 4.0 (TID 365). 4544 bytes result sent to driver
[2021-05-15 10:04:12,937] {docker.py:276} INFO - 21/05/15 13:04:12 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 369) (4e8a4a26f4b5, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:12,938] {docker.py:276} INFO - 21/05/15 13:04:12 INFO Executor: Running task 190.0 in stage 4.0 (TID 369)
21/05/15 13:04:12 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 365) in 2561 ms on 4e8a4a26f4b5 (executor driver) (187/200)
[2021-05-15 10:04:12,947] {docker.py:276} INFO - 21/05/15 13:04:12 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:12,949] {docker.py:276} INFO - 21/05/15 13:04:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:12,950] {docker.py:276} INFO - 21/05/15 13:04:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302053466949066543808145_0004_m_000190_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053466949066543808145_0004_m_000190_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302053466949066543808145_0004}; taskId=attempt_202105151302053466949066543808145_0004_m_000190_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69e910d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105151302053466949066543808145_0004_m_000190_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053466949066543808145_0004_m_000190_369
[2021-05-15 10:04:12,952] {docker.py:276} INFO - 21/05/15 13:04:12 INFO StagingCommitter: Task committer attempt_202105151302053466949066543808145_0004_m_000190_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302053466949066543808145_0004_m_000190_369 : duration 0:00.003s
[2021-05-15 10:04:13,364] {docker.py:276} INFO - 21/05/15 13:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105151302053939915399500004088_0004_m_000187_366: needsTaskCommit() Task attempt_202105151302053939915399500004088_0004_m_000187_366
[2021-05-15 10:04:13,365] {docker.py:276} INFO - 21/05/15 13:04:13 INFO StagingCommitter: Task committer attempt_202105151302053939915399500004088_0004_m_000187_366: needsTaskCommit() Task attempt_202105151302053939915399500004088_0004_m_000187_366: duration 0:00.001s
21/05/15 13:04:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053939915399500004088_0004_m_000187_366
[2021-05-15 10:04:13,366] {docker.py:276} INFO - 21/05/15 13:04:13 INFO Executor: Finished task 187.0 in stage 4.0 (TID 366). 4544 bytes result sent to driver
[2021-05-15 10:04:13,367] {docker.py:276} INFO - 21/05/15 13:04:13 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 370) (4e8a4a26f4b5, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:13,368] {docker.py:276} INFO - 21/05/15 13:04:13 INFO Executor: Running task 191.0 in stage 4.0 (TID 370)
[2021-05-15 10:04:13,368] {docker.py:276} INFO - 21/05/15 13:04:13 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 366) in 2503 ms on 4e8a4a26f4b5 (executor driver) (188/200)
[2021-05-15 10:04:13,375] {docker.py:276} INFO - 21/05/15 13:04:13 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:13,376] {docker.py:276} INFO - 21/05/15 13:04:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:13,377] {docker.py:276} INFO - 21/05/15 13:04:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051231553096732775587_0004_m_000191_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051231553096732775587_0004_m_000191_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051231553096732775587_0004}; taskId=attempt_202105151302051231553096732775587_0004_m_000191_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ccefb00}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105151302051231553096732775587_0004_m_000191_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051231553096732775587_0004_m_000191_370
[2021-05-15 10:04:13,379] {docker.py:276} INFO - 21/05/15 13:04:13 INFO StagingCommitter: Task committer attempt_202105151302051231553096732775587_0004_m_000191_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051231553096732775587_0004_m_000191_370 : duration 0:00.003s
[2021-05-15 10:04:13,632] {docker.py:276} INFO - 21/05/15 13:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105151302055242815726246528300_0004_m_000188_367: needsTaskCommit() Task attempt_202105151302055242815726246528300_0004_m_000188_367
[2021-05-15 10:04:13,632] {docker.py:276} INFO - 21/05/15 13:04:13 INFO StagingCommitter: Task committer attempt_202105151302055242815726246528300_0004_m_000188_367: needsTaskCommit() Task attempt_202105151302055242815726246528300_0004_m_000188_367: duration 0:00.001s
21/05/15 13:04:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055242815726246528300_0004_m_000188_367
[2021-05-15 10:04:13,633] {docker.py:276} INFO - 21/05/15 13:04:13 INFO Executor: Finished task 188.0 in stage 4.0 (TID 367). 4544 bytes result sent to driver
[2021-05-15 10:04:13,637] {docker.py:276} INFO - 21/05/15 13:04:13 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 371) (4e8a4a26f4b5, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:13,638] {docker.py:276} INFO - 21/05/15 13:04:13 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 367) in 2490 ms on 4e8a4a26f4b5 (executor driver) (189/200)
[2021-05-15 10:04:13,639] {docker.py:276} INFO - 21/05/15 13:04:13 INFO Executor: Running task 192.0 in stage 4.0 (TID 371)
[2021-05-15 10:04:13,648] {docker.py:276} INFO - 21/05/15 13:04:13 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:13,649] {docker.py:276} INFO - 21/05/15 13:04:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302052361223524521084504_0004_m_000192_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052361223524521084504_0004_m_000192_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302052361223524521084504_0004}; taskId=attempt_202105151302052361223524521084504_0004_m_000192_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9f06205}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:13,650] {docker.py:276} INFO - 21/05/15 13:04:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105151302052361223524521084504_0004_m_000192_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052361223524521084504_0004_m_000192_371
[2021-05-15 10:04:13,653] {docker.py:276} INFO - 21/05/15 13:04:13 INFO StagingCommitter: Task committer attempt_202105151302052361223524521084504_0004_m_000192_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302052361223524521084504_0004_m_000192_371 : duration 0:00.004s
[2021-05-15 10:04:15,018] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302051677482619731813481_0004_m_000189_368: needsTaskCommit() Task attempt_202105151302051677482619731813481_0004_m_000189_368
[2021-05-15 10:04:15,019] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Task committer attempt_202105151302051677482619731813481_0004_m_000189_368: needsTaskCommit() Task attempt_202105151302051677482619731813481_0004_m_000189_368: duration 0:00.001s
21/05/15 13:04:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051677482619731813481_0004_m_000189_368
[2021-05-15 10:04:15,020] {docker.py:276} INFO - 21/05/15 13:04:15 INFO Executor: Finished task 189.0 in stage 4.0 (TID 368). 4544 bytes result sent to driver
[2021-05-15 10:04:15,022] {docker.py:276} INFO - 21/05/15 13:04:15 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 372) (4e8a4a26f4b5, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:15,023] {docker.py:276} INFO - 21/05/15 13:04:15 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 368) in 2538 ms on 4e8a4a26f4b5 (executor driver) (190/200)
[2021-05-15 10:04:15,024] {docker.py:276} INFO - 21/05/15 13:04:15 INFO Executor: Running task 193.0 in stage 4.0 (TID 372)
[2021-05-15 10:04:15,034] {docker.py:276} INFO - 21/05/15 13:04:15 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:15,036] {docker.py:276} INFO - 21/05/15 13:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:04:15,037] {docker.py:276} INFO - 21/05/15 13:04:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:15,037] {docker.py:276} INFO - 21/05/15 13:04:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302058379465895226506606_0004_m_000193_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058379465895226506606_0004_m_000193_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302058379465895226506606_0004}; taskId=attempt_202105151302058379465895226506606_0004_m_000193_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79c1cbca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:15,038] {docker.py:276} INFO - 21/05/15 13:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:15,038] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302058379465895226506606_0004_m_000193_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058379465895226506606_0004_m_000193_372
[2021-05-15 10:04:15,041] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Task committer attempt_202105151302058379465895226506606_0004_m_000193_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302058379465895226506606_0004_m_000193_372 : duration 0:00.004s
[2021-05-15 10:04:15,191] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302051231553096732775587_0004_m_000191_370: needsTaskCommit() Task attempt_202105151302051231553096732775587_0004_m_000191_370
[2021-05-15 10:04:15,193] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Task committer attempt_202105151302051231553096732775587_0004_m_000191_370: needsTaskCommit() Task attempt_202105151302051231553096732775587_0004_m_000191_370: duration 0:00.001s
21/05/15 13:04:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051231553096732775587_0004_m_000191_370
[2021-05-15 10:04:15,194] {docker.py:276} INFO - 21/05/15 13:04:15 INFO Executor: Finished task 191.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-15 10:04:15,195] {docker.py:276} INFO - 21/05/15 13:04:15 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 373) (4e8a4a26f4b5, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:15,197] {docker.py:276} INFO - 21/05/15 13:04:15 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 370) in 1832 ms on 4e8a4a26f4b5 (executor driver) (191/200)
[2021-05-15 10:04:15,198] {docker.py:276} INFO - 21/05/15 13:04:15 INFO Executor: Running task 194.0 in stage 4.0 (TID 373)
[2021-05-15 10:04:15,207] {docker.py:276} INFO - 21/05/15 13:04:15 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:04:15,208] {docker.py:276} INFO - 21/05/15 13:04:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:15,209] {docker.py:276} INFO - 21/05/15 13:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:04:15,210] {docker.py:276} INFO - 21/05/15 13:04:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:15,210] {docker.py:276} INFO - 21/05/15 13:04:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302056369816250295477786_0004_m_000194_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056369816250295477786_0004_m_000194_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302056369816250295477786_0004}; taskId=attempt_202105151302056369816250295477786_0004_m_000194_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8f293d5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:15,210] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302056369816250295477786_0004_m_000194_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056369816250295477786_0004_m_000194_373
[2021-05-15 10:04:15,213] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Task committer attempt_202105151302056369816250295477786_0004_m_000194_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302056369816250295477786_0004_m_000194_373 : duration 0:00.003s
[2021-05-15 10:04:15,493] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302053466949066543808145_0004_m_000190_369: needsTaskCommit() Task attempt_202105151302053466949066543808145_0004_m_000190_369
[2021-05-15 10:04:15,494] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Task committer attempt_202105151302053466949066543808145_0004_m_000190_369: needsTaskCommit() Task attempt_202105151302053466949066543808145_0004_m_000190_369: duration 0:00.001s
[2021-05-15 10:04:15,494] {docker.py:276} INFO - 21/05/15 13:04:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302053466949066543808145_0004_m_000190_369
[2021-05-15 10:04:15,495] {docker.py:276} INFO - 21/05/15 13:04:15 INFO Executor: Finished task 190.0 in stage 4.0 (TID 369). 4544 bytes result sent to driver
[2021-05-15 10:04:15,496] {docker.py:276} INFO - 21/05/15 13:04:15 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 374) (4e8a4a26f4b5, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:15,497] {docker.py:276} INFO - 21/05/15 13:04:15 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 369) in 2563 ms on 4e8a4a26f4b5 (executor driver) (192/200)
[2021-05-15 10:04:15,498] {docker.py:276} INFO - 21/05/15 13:04:15 INFO Executor: Running task 195.0 in stage 4.0 (TID 374)
[2021-05-15 10:04:15,518] {docker.py:276} INFO - 21/05/15 13:04:15 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:15,520] {docker.py:276} INFO - 21/05/15 13:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051736704003083012100_0004_m_000195_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051736704003083012100_0004_m_000195_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051736704003083012100_0004}; taskId=attempt_202105151302051736704003083012100_0004_m_000195_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38ad841f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:15,520] {docker.py:276} INFO - 21/05/15 13:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:15,521] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105151302051736704003083012100_0004_m_000195_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051736704003083012100_0004_m_000195_374
[2021-05-15 10:04:15,524] {docker.py:276} INFO - 21/05/15 13:04:15 INFO StagingCommitter: Task committer attempt_202105151302051736704003083012100_0004_m_000195_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051736704003083012100_0004_m_000195_374 : duration 0:00.003s
[2021-05-15 10:04:16,063] {docker.py:276} INFO - 21/05/15 13:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302052361223524521084504_0004_m_000192_371: needsTaskCommit() Task attempt_202105151302052361223524521084504_0004_m_000192_371
[2021-05-15 10:04:16,064] {docker.py:276} INFO - 21/05/15 13:04:16 INFO StagingCommitter: Task committer attempt_202105151302052361223524521084504_0004_m_000192_371: needsTaskCommit() Task attempt_202105151302052361223524521084504_0004_m_000192_371: duration 0:00.002s
21/05/15 13:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302052361223524521084504_0004_m_000192_371
[2021-05-15 10:04:16,066] {docker.py:276} INFO - 21/05/15 13:04:16 INFO Executor: Finished task 192.0 in stage 4.0 (TID 371). 4587 bytes result sent to driver
[2021-05-15 10:04:16,068] {docker.py:276} INFO - 21/05/15 13:04:16 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 375) (4e8a4a26f4b5, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:16,069] {docker.py:276} INFO - 21/05/15 13:04:16 INFO Executor: Running task 196.0 in stage 4.0 (TID 375)
[2021-05-15 10:04:16,069] {docker.py:276} INFO - 21/05/15 13:04:16 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 371) in 2438 ms on 4e8a4a26f4b5 (executor driver) (193/200)
[2021-05-15 10:04:16,078] {docker.py:276} INFO - 21/05/15 13:04:16 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:16,080] {docker.py:276} INFO - 21/05/15 13:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302051586636123308842585_0004_m_000196_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051586636123308842585_0004_m_000196_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302051586636123308842585_0004}; taskId=attempt_202105151302051586636123308842585_0004_m_000196_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2767320e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:16,081] {docker.py:276} INFO - 21/05/15 13:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:16,081] {docker.py:276} INFO - 21/05/15 13:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105151302051586636123308842585_0004_m_000196_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051586636123308842585_0004_m_000196_375
[2021-05-15 10:04:16,083] {docker.py:276} INFO - 21/05/15 13:04:16 INFO StagingCommitter: Task committer attempt_202105151302051586636123308842585_0004_m_000196_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302051586636123308842585_0004_m_000196_375 : duration 0:00.003s
[2021-05-15 10:04:17,498] {docker.py:276} INFO - 21/05/15 13:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105151302056369816250295477786_0004_m_000194_373: needsTaskCommit() Task attempt_202105151302056369816250295477786_0004_m_000194_373
[2021-05-15 10:04:17,500] {docker.py:276} INFO - 21/05/15 13:04:17 INFO StagingCommitter: Task committer attempt_202105151302056369816250295477786_0004_m_000194_373: needsTaskCommit() Task attempt_202105151302056369816250295477786_0004_m_000194_373: duration 0:00.001s
21/05/15 13:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302056369816250295477786_0004_m_000194_373
[2021-05-15 10:04:17,501] {docker.py:276} INFO - 21/05/15 13:04:17 INFO Executor: Finished task 194.0 in stage 4.0 (TID 373). 4587 bytes result sent to driver
[2021-05-15 10:04:17,503] {docker.py:276} INFO - 21/05/15 13:04:17 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 376) (4e8a4a26f4b5, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:17,504] {docker.py:276} INFO - 21/05/15 13:04:17 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 373) in 2312 ms on 4e8a4a26f4b5 (executor driver) (194/200)
[2021-05-15 10:04:17,505] {docker.py:276} INFO - 21/05/15 13:04:17 INFO Executor: Running task 197.0 in stage 4.0 (TID 376)
[2021-05-15 10:04:17,515] {docker.py:276} INFO - 21/05/15 13:04:17 INFO ShuffleBlockFetcherIterator: Getting 4 (10.5 KiB) non-empty blocks including 4 (10.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:17,517] {docker.py:276} INFO - 21/05/15 13:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055146614072144440686_0004_m_000197_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055146614072144440686_0004_m_000197_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055146614072144440686_0004}; taskId=attempt_202105151302055146614072144440686_0004_m_000197_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@770bc467}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:17,517] {docker.py:276} INFO - 21/05/15 13:04:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:17,518] {docker.py:276} INFO - 21/05/15 13:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105151302055146614072144440686_0004_m_000197_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055146614072144440686_0004_m_000197_376
[2021-05-15 10:04:17,521] {docker.py:276} INFO - 21/05/15 13:04:17 INFO StagingCommitter: Task committer attempt_202105151302055146614072144440686_0004_m_000197_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055146614072144440686_0004_m_000197_376 : duration 0:00.003s
[2021-05-15 10:04:17,961] {docker.py:276} INFO - 21/05/15 13:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105151302058379465895226506606_0004_m_000193_372: needsTaskCommit() Task attempt_202105151302058379465895226506606_0004_m_000193_372
[2021-05-15 10:04:17,961] {docker.py:276} INFO - 21/05/15 13:04:17 INFO StagingCommitter: Task committer attempt_202105151302058379465895226506606_0004_m_000193_372: needsTaskCommit() Task attempt_202105151302058379465895226506606_0004_m_000193_372: duration 0:00.000s
21/05/15 13:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302058379465895226506606_0004_m_000193_372
[2021-05-15 10:04:17,962] {docker.py:276} INFO - 21/05/15 13:04:17 INFO Executor: Finished task 193.0 in stage 4.0 (TID 372). 4587 bytes result sent to driver
[2021-05-15 10:04:17,964] {docker.py:276} INFO - 21/05/15 13:04:17 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 377) (4e8a4a26f4b5, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:17,964] {docker.py:276} INFO - 21/05/15 13:04:17 INFO Executor: Running task 198.0 in stage 4.0 (TID 377)
21/05/15 13:04:17 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 372) in 2947 ms on 4e8a4a26f4b5 (executor driver) (195/200)
[2021-05-15 10:04:17,972] {docker.py:276} INFO - 21/05/15 13:04:17 INFO ShuffleBlockFetcherIterator: Getting 4 (11.3 KiB) non-empty blocks including 4 (11.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:17,974] {docker.py:276} INFO - 21/05/15 13:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302054694795219039121749_0004_m_000198_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054694795219039121749_0004_m_000198_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302054694795219039121749_0004}; taskId=attempt_202105151302054694795219039121749_0004_m_000198_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bfc589f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302054694795219039121749_0004_m_000198_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054694795219039121749_0004_m_000198_377
[2021-05-15 10:04:17,977] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Task committer attempt_202105151302054694795219039121749_0004_m_000198_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302054694795219039121749_0004_m_000198_377 : duration 0:00.002s
[2021-05-15 10:04:18,027] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302051736704003083012100_0004_m_000195_374: needsTaskCommit() Task attempt_202105151302051736704003083012100_0004_m_000195_374
[2021-05-15 10:04:18,028] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Task committer attempt_202105151302051736704003083012100_0004_m_000195_374: needsTaskCommit() Task attempt_202105151302051736704003083012100_0004_m_000195_374: duration 0:00.002s
21/05/15 13:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051736704003083012100_0004_m_000195_374
[2021-05-15 10:04:18,029] {docker.py:276} INFO - 21/05/15 13:04:18 INFO Executor: Finished task 195.0 in stage 4.0 (TID 374). 4587 bytes result sent to driver
[2021-05-15 10:04:18,030] {docker.py:276} INFO - 21/05/15 13:04:18 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 378) (4e8a4a26f4b5, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:04:18,031] {docker.py:276} INFO - 21/05/15 13:04:18 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 374) in 2539 ms on 4e8a4a26f4b5 (executor driver) (196/200)
[2021-05-15 10:04:18,032] {docker.py:276} INFO - 21/05/15 13:04:18 INFO Executor: Running task 199.0 in stage 4.0 (TID 378)
[2021-05-15 10:04:18,042] {docker.py:276} INFO - 21/05/15 13:04:18 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:04:18,043] {docker.py:276} INFO - 21/05/15 13:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:04:18,044] {docker.py:276} INFO - 21/05/15 13:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:18,045] {docker.py:276} INFO - 21/05/15 13:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151302055088063528018807327_0004_m_000199_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055088063528018807327_0004_m_000199_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151302055088063528018807327_0004}; taskId=attempt_202105151302055088063528018807327_0004_m_000199_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6308aa01}; outputPath=file:/home/jovyan/tmp/staging/jovyan/973576e3-0214-48f0-bf25-cee4be362788/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:18,045] {docker.py:276} INFO - 21/05/15 13:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:04:18,045] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302055088063528018807327_0004_m_000199_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055088063528018807327_0004_m_000199_378
[2021-05-15 10:04:18,048] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Task committer attempt_202105151302055088063528018807327_0004_m_000199_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/973576e3-0214-48f0-bf25-cee4be362788/_temporary/0/_temporary/attempt_202105151302055088063528018807327_0004_m_000199_378 : duration 0:00.003s
[2021-05-15 10:04:18,152] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105151302051586636123308842585_0004_m_000196_375: needsTaskCommit() Task attempt_202105151302051586636123308842585_0004_m_000196_375
[2021-05-15 10:04:18,153] {docker.py:276} INFO - 21/05/15 13:04:18 INFO StagingCommitter: Task committer attempt_202105151302051586636123308842585_0004_m_000196_375: needsTaskCommit() Task attempt_202105151302051586636123308842585_0004_m_000196_375: duration 0:00.002s
21/05/15 13:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302051586636123308842585_0004_m_000196_375
[2021-05-15 10:04:18,154] {docker.py:276} INFO - 21/05/15 13:04:18 INFO Executor: Finished task 196.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
[2021-05-15 10:04:18,157] {docker.py:276} INFO - 21/05/15 13:04:18 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 375) in 2092 ms on 4e8a4a26f4b5 (executor driver) (197/200)
[2021-05-15 10:04:19,561] {docker.py:276} INFO - 21/05/15 13:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105151302055146614072144440686_0004_m_000197_376: needsTaskCommit() Task attempt_202105151302055146614072144440686_0004_m_000197_376
[2021-05-15 10:04:19,563] {docker.py:276} INFO - 21/05/15 13:04:19 INFO StagingCommitter: Task committer attempt_202105151302055146614072144440686_0004_m_000197_376: needsTaskCommit() Task attempt_202105151302055146614072144440686_0004_m_000197_376: duration 0:00.002s
21/05/15 13:04:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055146614072144440686_0004_m_000197_376
[2021-05-15 10:04:19,565] {docker.py:276} INFO - 21/05/15 13:04:19 INFO Executor: Finished task 197.0 in stage 4.0 (TID 376). 4544 bytes result sent to driver
[2021-05-15 10:04:19,566] {docker.py:276} INFO - 21/05/15 13:04:19 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 376) in 2066 ms on 4e8a4a26f4b5 (executor driver) (198/200)
[2021-05-15 10:04:20,029] {docker.py:276} INFO - 21/05/15 13:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302055088063528018807327_0004_m_000199_378: needsTaskCommit() Task attempt_202105151302055088063528018807327_0004_m_000199_378
[2021-05-15 10:04:20,030] {docker.py:276} INFO - 21/05/15 13:04:20 INFO StagingCommitter: Task committer attempt_202105151302055088063528018807327_0004_m_000199_378: needsTaskCommit() Task attempt_202105151302055088063528018807327_0004_m_000199_378: duration 0:00.001s
21/05/15 13:04:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302055088063528018807327_0004_m_000199_378
[2021-05-15 10:04:20,032] {docker.py:276} INFO - 21/05/15 13:04:20 INFO Executor: Finished task 199.0 in stage 4.0 (TID 378). 4544 bytes result sent to driver
[2021-05-15 10:04:20,034] {docker.py:276} INFO - 21/05/15 13:04:20 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 378) in 2007 ms on 4e8a4a26f4b5 (executor driver) (199/200)
[2021-05-15 10:04:20,405] {docker.py:276} INFO - 21/05/15 13:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105151302054694795219039121749_0004_m_000198_377: needsTaskCommit() Task attempt_202105151302054694795219039121749_0004_m_000198_377
[2021-05-15 10:04:20,406] {docker.py:276} INFO - 21/05/15 13:04:20 INFO StagingCommitter: Task committer attempt_202105151302054694795219039121749_0004_m_000198_377: needsTaskCommit() Task attempt_202105151302054694795219039121749_0004_m_000198_377: duration 0:00.001s
21/05/15 13:04:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151302054694795219039121749_0004_m_000198_377
[2021-05-15 10:04:20,411] {docker.py:276} INFO - 21/05/15 13:04:20 INFO Executor: Finished task 198.0 in stage 4.0 (TID 377). 4544 bytes result sent to driver
[2021-05-15 10:04:20,412] {docker.py:276} INFO - 21/05/15 13:04:20 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 377) in 2453 ms on 4e8a4a26f4b5 (executor driver) (200/200)
21/05/15 13:04:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-15 10:04:20,413] {docker.py:276} INFO - 21/05/15 13:04:20 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 122.657 s
[2021-05-15 10:04:20,415] {docker.py:276} INFO - 21/05/15 13:04:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 13:04:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-15 10:04:20,416] {docker.py:276} INFO - 21/05/15 13:04:20 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 135.005755 s
[2021-05-15 10:04:20,418] {docker.py:276} INFO - 21/05/15 13:04:20 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105151302058523257983960410204_0000_m_000000_0: commitJob((no job ID))
[2021-05-15 10:04:20,435] {docker.py:276} INFO - 21/05/15 13:04:20 WARN AbstractS3ACommitter: Task committer attempt_202105151302058523257983960410204_0000_m_000000_0: No pending uploads to commit
[2021-05-15 10:04:21,004] {docker.py:276} INFO - 21/05/15 13:04:21 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/15 13:04:21 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-15 10:04:21,185] {docker.py:276} INFO - 21/05/15 13:04:21 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.181s
21/05/15 13:04:21 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.181s
[2021-05-15 10:04:21,186] {docker.py:276} INFO - 21/05/15 13:04:21 INFO AbstractS3ACommitter: Task committer attempt_202105151302058523257983960410204_0000_m_000000_0: commitJob((no job ID)): duration 0:00.768s
[2021-05-15 10:04:21,701] {docker.py:276} INFO - 21/05/15 13:04:21 INFO FileFormatWriter: Write Job 973576e3-0214-48f0-bf25-cee4be362788 committed.
[2021-05-15 10:04:21,709] {docker.py:276} INFO - 21/05/15 13:04:21 INFO FileFormatWriter: Finished processing stats for write job 973576e3-0214-48f0-bf25-cee4be362788.
[2021-05-15 10:04:21,813] {docker.py:276} INFO - 21/05/15 13:04:21 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-15 10:04:21,827] {docker.py:276} INFO - 21/05/15 13:04:21 INFO SparkUI: Stopped Spark web UI at http://4e8a4a26f4b5:4040
[2021-05-15 10:04:21,848] {docker.py:276} INFO - 21/05/15 13:04:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-15 10:04:21,864] {docker.py:276} INFO - 21/05/15 13:04:21 INFO MemoryStore: MemoryStore cleared
[2021-05-15 10:04:21,864] {docker.py:276} INFO - 21/05/15 13:04:21 INFO BlockManager: BlockManager stopped
[2021-05-15 10:04:21,868] {docker.py:276} INFO - 21/05/15 13:04:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-15 10:04:21,873] {docker.py:276} INFO - 21/05/15 13:04:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-15 10:04:21,887] {docker.py:276} INFO - 21/05/15 13:04:21 INFO SparkContext: Successfully stopped SparkContext
[2021-05-15 10:04:21,888] {docker.py:276} INFO - 21/05/15 13:04:21 INFO ShutdownHookManager: Shutdown hook called
[2021-05-15 10:04:21,889] {docker.py:276} INFO - 21/05/15 13:04:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-36a2d13f-56f9-4a44-8146-8f02c3460671
[2021-05-15 10:04:21,892] {docker.py:276} INFO - 21/05/15 13:04:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e14284-540f-4e5d-82e2-dab2b0678068
[2021-05-15 10:04:21,894] {docker.py:276} INFO - 21/05/15 13:04:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e14284-540f-4e5d-82e2-dab2b0678068/pyspark-76ae8ed5-e5fe-4eea-8383-76dfff6c3925
[2021-05-15 10:04:21,899] {docker.py:276} INFO - 21/05/15 13:04:21 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-15 10:04:21,900] {docker.py:276} INFO - 21/05/15 13:04:21 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-15 10:04:21,901] {docker.py:276} INFO - 21/05/15 13:04:21 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-15 10:04:22,115] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210515T040000, start_date=20210515T130131, end_date=20210515T130422
[2021-05-15 10:04:22,163] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-15 10:04:22,204] {local_task_job.py:146} INFO - Task exited with return code 0
