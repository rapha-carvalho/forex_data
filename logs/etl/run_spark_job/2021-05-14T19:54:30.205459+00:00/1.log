[2021-05-14 16:55:00,050] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T19:54:30.205459+00:00 [queued]>
[2021-05-14 16:55:00,056] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T19:54:30.205459+00:00 [queued]>
[2021-05-14 16:55:00,056] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 16:55:00,056] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 16:55:00,056] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 16:55:00,061] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T19:54:30.205459+00:00
[2021-05-14 16:55:00,064] {standard_task_runner.py:52} INFO - Started process 23600 to run task
[2021-05-14 16:55:00,071] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T19:54:30.205459+00:00', '--job-id', '482', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp93wws0tc', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpr78xphcm']
[2021-05-14 16:55:00,073] {standard_task_runner.py:77} INFO - Job 482: Subtask run_spark_job
[2021-05-14 16:55:00,100] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T19:54:30.205459+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2021-05-14 16:55:00,120] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T19:54:30.205459+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T19:54:30.205459+00:00
[2021-05-14 16:55:00,123] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 16:55:03,086] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 16:55:03,088] {docker.py:312} INFO - Digest: sha256:3ff139a9dbefaff7945c18f6cbdbc77460bc981cf21cbe1d495f99b227e826c8
[2021-05-14 16:55:03,089] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 16:55:03,096] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 16:55:04,994] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 16:55:05,549] {docker.py:276} INFO - 21/05/14 19:55:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 16:55:09,036] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 16:55:09,051] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 16:55:09,115] {docker.py:276} INFO - 21/05/14 19:55:09 INFO ResourceUtils: ==============================================================
[2021-05-14 16:55:09,116] {docker.py:276} INFO - 21/05/14 19:55:09 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 16:55:09,122] {docker.py:276} INFO - 21/05/14 19:55:09 INFO ResourceUtils: ==============================================================
[2021-05-14 16:55:09,122] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SparkContext: Submitted application: spark.py
[2021-05-14 16:55:09,153] {docker.py:276} INFO - 21/05/14 19:55:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 16:55:09,174] {docker.py:276} INFO - 21/05/14 19:55:09 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 16:55:09,175] {docker.py:276} INFO - 21/05/14 19:55:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 16:55:09,248] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-14 16:55:09,249] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 16:55:09,249] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SecurityManager: Changing view acls groups to:
[2021-05-14 16:55:09,249] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SecurityManager: Changing modify acls groups to: 
21/05/14 19:55:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 16:55:09,623] {docker.py:276} INFO - 21/05/14 19:55:09 INFO Utils: Successfully started service 'sparkDriver' on port 33797.
[2021-05-14 16:55:09,666] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 16:55:09,713] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 16:55:09,746] {docker.py:276} INFO - 21/05/14 19:55:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 16:55:09,747] {docker.py:276} INFO - 21/05/14 19:55:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 16:55:09,754] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 16:55:09,772] {docker.py:276} INFO - 21/05/14 19:55:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-270f941b-fcfd-4f59-8eb0-3e8fdf74656c
[2021-05-14 16:55:09,799] {docker.py:276} INFO - 21/05/14 19:55:09 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 16:55:09,829] {docker.py:276} INFO - 21/05/14 19:55:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 16:55:10,107] {docker.py:276} INFO - 21/05/14 19:55:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 16:55:10,200] {docker.py:276} INFO - 21/05/14 19:55:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://379a553b2355:4040
[2021-05-14 16:55:10,444] {docker.py:276} INFO - 21/05/14 19:55:10 INFO Executor: Starting executor ID driver on host 379a553b2355
[2021-05-14 16:55:10,486] {docker.py:276} INFO - 21/05/14 19:55:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36973.
21/05/14 19:55:10 INFO NettyBlockTransferService: Server created on 379a553b2355:36973
[2021-05-14 16:55:10,489] {docker.py:276} INFO - 21/05/14 19:55:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 16:55:10,500] {docker.py:276} INFO - 21/05/14 19:55:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 379a553b2355, 36973, None)
[2021-05-14 16:55:10,509] {docker.py:276} INFO - 21/05/14 19:55:10 INFO BlockManagerMasterEndpoint: Registering block manager 379a553b2355:36973 with 934.4 MiB RAM, BlockManagerId(driver, 379a553b2355, 36973, None)
[2021-05-14 16:55:10,513] {docker.py:276} INFO - 21/05/14 19:55:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 379a553b2355, 36973, None)
[2021-05-14 16:55:10,515] {docker.py:276} INFO - 21/05/14 19:55:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 379a553b2355, 36973, None)
[2021-05-14 16:55:11,075] {docker.py:276} INFO - 21/05/14 19:55:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 16:55:11,075] {docker.py:276} INFO - 21/05/14 19:55:11 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 16:55:12,172] {docker.py:276} INFO - 21/05/14 19:55:12 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 16:55:12,219] {docker.py:276} INFO - 21/05/14 19:55:12 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/14 19:55:12 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 16:55:21,742] {docker.py:276} INFO - 21/05/14 19:55:21 INFO InMemoryFileIndex: It took 3170 ms to list leaf files for 18 paths.
[2021-05-14 16:55:24,804] {docker.py:276} INFO - 21/05/14 19:55:24 INFO InMemoryFileIndex: It took 2964 ms to list leaf files for 18 paths.
[2021-05-14 16:55:27,530] {docker.py:276} INFO - 21/05/14 19:55:27 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 16:55:27,536] {docker.py:276} INFO - 21/05/14 19:55:27 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 16:55:27,541] {docker.py:276} INFO - 21/05/14 19:55:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 16:55:28,363] {docker.py:276} INFO - 21/05/14 19:55:28 INFO CodeGenerator: Code generated in 301.905 ms
[2021-05-14 16:55:28,432] {docker.py:276} INFO - 21/05/14 19:55:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-14 16:55:28,531] {docker.py:276} INFO - 21/05/14 19:55:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 16:55:28,535] {docker.py:276} INFO - 21/05/14 19:55:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 379a553b2355:36973 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 16:55:28,548] {docker.py:276} INFO - 21/05/14 19:55:28 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 16:55:28,575] {docker.py:276} INFO - 21/05/14 19:55:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 19342797 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 16:55:28,738] {docker.py:276} INFO - 21/05/14 19:55:28 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 16:55:28,761] {docker.py:276} INFO - 21/05/14 19:55:28 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-14 16:55:28,762] {docker.py:276} INFO - 21/05/14 19:55:28 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 16:55:28,762] {docker.py:276} INFO - 21/05/14 19:55:28 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 16:55:28,765] {docker.py:276} INFO - 21/05/14 19:55:28 INFO DAGScheduler: Missing parents: List()
[2021-05-14 16:55:28,772] {docker.py:276} INFO - 21/05/14 19:55:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 16:55:28,862] {docker.py:276} INFO - 21/05/14 19:55:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 16:55:28,892] {docker.py:276} INFO - 21/05/14 19:55:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 16:55:28,892] {docker.py:276} INFO - 21/05/14 19:55:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 379a553b2355:36973 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 16:55:28,895] {docker.py:276} INFO - 21/05/14 19:55:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 16:55:28,912] {docker.py:276} INFO - 21/05/14 19:55:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-14 16:55:28,914] {docker.py:276} INFO - 21/05/14 19:55:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-05-14 16:55:28,999] {docker.py:276} INFO - 21/05/14 19:55:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (379a553b2355, executor driver, partition 0, PROCESS_LOCAL, 5345 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:29,021] {docker.py:276} INFO - 21/05/14 19:55:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 16:55:29,185] {docker.py:276} INFO - 21/05/14 19:55:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621014886_to_1621016686.csv, range: 0-104301, partition values: [empty row]
[2021-05-14 16:55:29,221] {docker.py:276} INFO - 21/05/14 19:55:29 INFO CodeGenerator: Code generated in 26.2153 ms
[2021-05-14 16:55:29,635] {docker.py:276} INFO - 21/05/14 19:55:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1564 bytes result sent to driver
[2021-05-14 16:55:29,647] {docker.py:276} INFO - 21/05/14 19:55:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 660 ms on 379a553b2355 (executor driver) (1/1)
[2021-05-14 16:55:29,652] {docker.py:276} INFO - 21/05/14 19:55:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 16:55:29,659] {docker.py:276} INFO - 21/05/14 19:55:29 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.861 s
[2021-05-14 16:55:29,665] {docker.py:276} INFO - 21/05/14 19:55:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 16:55:29,666] {docker.py:276} INFO - 21/05/14 19:55:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 16:55:29,670] {docker.py:276} INFO - 21/05/14 19:55:29 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.931861 s
[2021-05-14 16:55:29,702] {docker.py:276} INFO - 21/05/14 19:55:29 INFO CodeGenerator: Code generated in 14.7833 ms
[2021-05-14 16:55:29,771] {docker.py:276} INFO - 21/05/14 19:55:29 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 16:55:29,772] {docker.py:276} INFO - 21/05/14 19:55:29 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 16:55:29,772] {docker.py:276} INFO - 21/05/14 19:55:29 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 16:55:29,780] {docker.py:276} INFO - 21/05/14 19:55:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-14 16:55:29,813] {docker.py:276} INFO - 21/05/14 19:55:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 16:55:29,826] {docker.py:276} INFO - 21/05/14 19:55:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 379a553b2355:36973 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 16:55:29,827] {docker.py:276} INFO - 21/05/14 19:55:29 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 16:55:29,829] {docker.py:276} INFO - 21/05/14 19:55:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 19342797 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 16:55:29,841] {docker.py:276} INFO - 21/05/14 19:55:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 379a553b2355:36973 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-14 16:55:30,469] {docker.py:276} INFO - 21/05/14 19:55:30 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 16:55:30,469] {docker.py:276} INFO - 21/05/14 19:55:30 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 16:55:30,469] {docker.py:276} INFO - 21/05/14 19:55:30 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 16:55:31,542] {docker.py:276} INFO - 21/05/14 19:55:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:31,546] {docker.py:276} INFO - 21/05/14 19:55:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:31,546] {docker.py:276} INFO - 21/05/14 19:55:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313798401951849224908_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313798401951849224908_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313798401951849224908_0000}; taskId=attempt_202105141955313798401951849224908_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d4f3d10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:31,547] {docker.py:276} INFO - 21/05/14 19:55:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:31,575] {docker.py:276} INFO - 21/05/14 19:55:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 16:55:31,675] {docker.py:276} INFO - 21/05/14 19:55:31 INFO CodeGenerator: Code generated in 65.8872 ms
[2021-05-14 16:55:31,677] {docker.py:276} INFO - 21/05/14 19:55:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 16:55:31,724] {docker.py:276} INFO - 21/05/14 19:55:31 INFO CodeGenerator: Code generated in 37.002 ms
[2021-05-14 16:55:31,729] {docker.py:276} INFO - 21/05/14 19:55:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-14 16:55:31,747] {docker.py:276} INFO - 21/05/14 19:55:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-14 16:55:31,749] {docker.py:276} INFO - 21/05/14 19:55:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 379a553b2355:36973 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 16:55:31,751] {docker.py:276} INFO - 21/05/14 19:55:31 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 16:55:31,757] {docker.py:276} INFO - 21/05/14 19:55:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 19342797 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 16:55:31,812] {docker.py:276} INFO - 21/05/14 19:55:31 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 379a553b2355:36973 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 16:55:31,850] {docker.py:276} INFO - 21/05/14 19:55:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 379a553b2355:36973 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 16:55:31,882] {docker.py:276} INFO - 21/05/14 19:55:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 16:55:31,887] {docker.py:276} INFO - 21/05/14 19:55:31 INFO DAGScheduler: Registering RDD 13 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 16:55:31,890] {docker.py:276} INFO - 21/05/14 19:55:31 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/14 19:55:31 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 16:55:31,891] {docker.py:276} INFO - 21/05/14 19:55:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2021-05-14 16:55:31,893] {docker.py:276} INFO - 21/05/14 19:55:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
[2021-05-14 16:55:31,895] {docker.py:276} INFO - 21/05/14 19:55:31 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 16:55:31,910] {docker.py:276} INFO - 21/05/14 19:55:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 934.2 MiB)
[2021-05-14 16:55:31,921] {docker.py:276} INFO - 21/05/14 19:55:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.2 MiB)
[2021-05-14 16:55:31,922] {docker.py:276} INFO - 21/05/14 19:55:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 379a553b2355:36973 (size: 12.0 KiB, free: 934.4 MiB)
[2021-05-14 16:55:31,923] {docker.py:276} INFO - 21/05/14 19:55:31 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
[2021-05-14 16:55:31,925] {docker.py:276} INFO - 21/05/14 19:55:31 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21/05/14 19:55:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0
[2021-05-14 16:55:31,929] {docker.py:276} INFO - 21/05/14 19:55:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (379a553b2355, executor driver, partition 0, PROCESS_LOCAL, 5334 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:31,930] {docker.py:276} INFO - 21/05/14 19:55:31 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (379a553b2355, executor driver, partition 1, PROCESS_LOCAL, 5334 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:31,931] {docker.py:276} INFO - 21/05/14 19:55:31 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (379a553b2355, executor driver, partition 2, PROCESS_LOCAL, 5334 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:31,932] {docker.py:276} INFO - 21/05/14 19:55:31 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (379a553b2355, executor driver, partition 3, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:31,933] {docker.py:276} INFO - 21/05/14 19:55:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2021-05-14 16:55:31,947] {docker.py:276} INFO - 21/05/14 19:55:31 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2021-05-14 16:55:31,948] {docker.py:276} INFO - 21/05/14 19:55:31 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[2021-05-14 16:55:31,953] {docker.py:276} INFO - 21/05/14 19:55:31 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
[2021-05-14 16:55:32,071] {docker.py:276} INFO - 21/05/14 19:55:32 INFO CodeGenerator: Code generated in 29.4622 ms
[2021-05-14 16:55:32,110] {docker.py:276} INFO - 21/05/14 19:55:32 INFO CodeGenerator: Code generated in 11.9937 ms
[2021-05-14 16:55:32,138] {docker.py:276} INFO - 21/05/14 19:55:32 INFO CodeGenerator: Code generated in 20.5873 ms
[2021-05-14 16:55:32,155] {docker.py:276} INFO - 21/05/14 19:55:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_16_54_46/from_1621018486_to_1621020286.csv, range: 0-104240, partition values: [empty row]
[2021-05-14 16:55:32,160] {docker.py:276} INFO - 21/05/14 19:55:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_16_54_46/from_1621013086_to_1621014886.csv, range: 0-104155, partition values: [empty row]
[2021-05-14 16:55:32,160] {docker.py:276} INFO - 21/05/14 19:55:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621014886_to_1621016686.csv, range: 0-104301, partition values: [empty row]
[2021-05-14 16:55:32,163] {docker.py:276} INFO - 21/05/14 19:55:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_16_54_46/from_1621014886_to_1621016686.csv, range: 0-103746, partition values: [empty row]
[2021-05-14 16:55:32,853] {docker.py:276} INFO - 21/05/14 19:55:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_16_54_46/from_1621009486_to_1621011286.csv, range: 0-104235, partition values: [empty row]
[2021-05-14 16:55:33,219] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_16_54_46/from_1621011286_to_1621013086.csv, range: 0-104230, partition values: [empty row]
[2021-05-14 16:55:33,361] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_16_54_46/from_1621018486_to_1621020286.csv, range: 0-103716, partition values: [empty row]
[2021-05-14 16:55:33,467] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621009486_to_1621011286.csv, range: 0-104119, partition values: [empty row]
[2021-05-14 16:55:33,470] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_16_54_46/from_1621016686_to_1621018486.csv, range: 0-104294, partition values: [empty row]
[2021-05-14 16:55:33,606] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621018486_to_1621020286.csv, range: 0-104213, partition values: [empty row]
[2021-05-14 16:55:33,851] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_16_54_46/from_1621016686_to_1621018486.csv, range: 0-103704, partition values: [empty row]
[2021-05-14 16:55:33,946] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621013086_to_1621014886.csv, range: 0-104288, partition values: [empty row]
[2021-05-14 16:55:33,947] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621011286_to_1621013086.csv, range: 0-104178, partition values: [empty row]
[2021-05-14 16:55:33,971] {docker.py:276} INFO - 21/05/14 19:55:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_16_54_46/from_1621009486_to_1621011286.csv, range: 0-103986, partition values: [empty row]
[2021-05-14 16:55:34,311] {docker.py:276} INFO - 21/05/14 19:55:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_16_54_46/from_1621014886_to_1621016686.csv, range: 0-104259, partition values: [empty row]
[2021-05-14 16:55:34,321] {docker.py:276} INFO - 21/05/14 19:55:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_16_54_46/from_1621011286_to_1621013086.csv, range: 0-103901, partition values: [empty row]
[2021-05-14 16:55:34,615] {docker.py:276} INFO - 21/05/14 19:55:34 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2722 bytes result sent to driver
21/05/14 19:55:34 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2722 bytes result sent to driver
[2021-05-14 16:55:34,625] {docker.py:276} INFO - 21/05/14 19:55:34 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2698 ms on 379a553b2355 (executor driver) (1/4)
[2021-05-14 16:55:34,627] {docker.py:276} INFO - 21/05/14 19:55:34 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2698 ms on 379a553b2355 (executor driver) (2/4)
[2021-05-14 16:55:34,669] {docker.py:276} INFO - 21/05/14 19:55:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_16_54_46/from_1621013086_to_1621014886.csv, range: 0-103894, partition values: [empty row]
[2021-05-14 16:55:34,798] {docker.py:276} INFO - 21/05/14 19:55:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_16_54_46/from_1621016686_to_1621018486.csv, range: 0-104257, partition values: [empty row]
[2021-05-14 16:55:35,162] {docker.py:276} INFO - 21/05/14 19:55:35 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2679 bytes result sent to driver
[2021-05-14 16:55:35,176] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 3246 ms on 379a553b2355 (executor driver) (3/4)
[2021-05-14 16:55:35,317] {docker.py:276} INFO - 21/05/14 19:55:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2679 bytes result sent to driver
[2021-05-14 16:55:35,319] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3395 ms on 379a553b2355 (executor driver) (4/4)
[2021-05-14 16:55:35,319] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 16:55:35,321] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 3.424 s
[2021-05-14 16:55:35,322] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 16:55:35,323] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: running: Set()
[2021-05-14 16:55:35,324] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2021-05-14 16:55:35,325] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: failed: Set()
[2021-05-14 16:55:35,330] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 16:55:35,386] {docker.py:276} INFO - 21/05/14 19:55:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.1 KiB, free 934.0 MiB)
[2021-05-14 16:55:35,413] {docker.py:276} INFO - 21/05/14 19:55:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 933.9 MiB)
[2021-05-14 16:55:35,414] {docker.py:276} INFO - 21/05/14 19:55:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 379a553b2355:36973 (size: 74.1 KiB, free: 934.3 MiB)
[2021-05-14 16:55:35,417] {docker.py:276} INFO - 21/05/14 19:55:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
[2021-05-14 16:55:35,420] {docker.py:276} INFO - 21/05/14 19:55:35 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 16:55:35,422] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2021-05-14 16:55:35,430] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (379a553b2355, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:35,432] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (379a553b2355, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:35,435] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (379a553b2355, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:35,435] {docker.py:276} INFO - 21/05/14 19:55:35 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 8) (379a553b2355, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:35,436] {docker.py:276} INFO - 21/05/14 19:55:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
21/05/14 19:55:35 INFO Executor: Running task 3.0 in stage 2.0 (TID 8)
[2021-05-14 16:55:35,437] {docker.py:276} INFO - 21/05/14 19:55:35 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)
[2021-05-14 16:55:35,437] {docker.py:276} INFO - 21/05/14 19:55:35 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
[2021-05-14 16:55:35,584] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:35,588] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Getting 4 (4.9 KiB) non-empty blocks including 4 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:35,589] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2021-05-14 16:55:35,598] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
[2021-05-14 16:55:35,599] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:35,599] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms
[2021-05-14 16:55:35,599] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:35,600] {docker.py:276} INFO - 21/05/14 19:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
[2021-05-14 16:55:35,613] {docker.py:276} INFO - 21/05/14 19:55:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:35,614] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,614] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311324245208432389507_0002_m_000001_6, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311324245208432389507_0002_m_000001_6}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311324245208432389507_0002}; taskId=attempt_202105141955311324245208432389507_0002_m_000001_6, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21f6813c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,614] {docker.py:276} INFO - 21/05/14 19:55:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:35,617] {docker.py:276} INFO - 21/05/14 19:55:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955311324245208432389507_0002_m_000001_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311324245208432389507_0002_m_000001_6 
21/05/14 19:55:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:35,618] {docker.py:276} INFO - 21/05/14 19:55:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:35,618] {docker.py:276} INFO - 21/05/14 19:55:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:35,619] {docker.py:276} INFO - 21/05/14 19:55:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:35,620] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,620] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311326641153450603186_0002_m_000002_7, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311326641153450603186_0002_m_000002_7}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311326641153450603186_0002}; taskId=attempt_202105141955311326641153450603186_0002_m_000002_7, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51efe331}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,621] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,623] {docker.py:276} INFO - 21/05/14 19:55:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:35,623] {docker.py:276} INFO - 21/05/14 19:55:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:35,625] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051419553139155689686437623_0002_m_000000_5, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553139155689686437623_0002_m_000000_5}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051419553139155689686437623_0002}; taskId=attempt_2021051419553139155689686437623_0002_m_000000_5, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@374d8c4e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,625] {docker.py:276} INFO - 21/05/14 19:55:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:35,627] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:35,628] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955311326641153450603186_0002_m_000002_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311326641153450603186_0002_m_000002_7
[2021-05-14 16:55:35,629] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Starting: Task committer attempt_2021051419553139155689686437623_0002_m_000000_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553139155689686437623_0002_m_000000_5
[2021-05-14 16:55:35,630] {docker.py:276} INFO - 21/05/14 19:55:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316637711620759049197_0002_m_000003_8, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316637711620759049197_0002_m_000003_8}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316637711620759049197_0002}; taskId=attempt_202105141955316637711620759049197_0002_m_000003_8, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e870675}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:35,631] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955316637711620759049197_0002_m_000003_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316637711620759049197_0002_m_000003_8
[2021-05-14 16:55:35,647] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Task committer attempt_202105141955316637711620759049197_0002_m_000003_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316637711620759049197_0002_m_000003_8 : duration 0:00.016s
[2021-05-14 16:55:35,666] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Task committer attempt_202105141955311326641153450603186_0002_m_000002_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311326641153450603186_0002_m_000002_7 : duration 0:00.042s
[2021-05-14 16:55:35,670] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Task committer attempt_202105141955311324245208432389507_0002_m_000001_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311324245208432389507_0002_m_000001_6 : duration 0:00.051s
[2021-05-14 16:55:35,671] {docker.py:276} INFO - 21/05/14 19:55:35 INFO StagingCommitter: Task committer attempt_2021051419553139155689686437623_0002_m_000000_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553139155689686437623_0002_m_000000_5 : duration 0:00.044s
[2021-05-14 16:55:37,505] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955316637711620759049197_0002_m_000003_8: needsTaskCommit() Task attempt_202105141955316637711620759049197_0002_m_000003_8
[2021-05-14 16:55:37,505] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955311324245208432389507_0002_m_000001_6: needsTaskCommit() Task attempt_202105141955311324245208432389507_0002_m_000001_6
[2021-05-14 16:55:37,507] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_202105141955311324245208432389507_0002_m_000001_6: needsTaskCommit() Task attempt_202105141955311324245208432389507_0002_m_000001_6: duration 0:00.002s
21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_202105141955316637711620759049197_0002_m_000003_8: needsTaskCommit() Task attempt_202105141955316637711620759049197_0002_m_000003_8: duration 0:00.002s
[2021-05-14 16:55:37,507] {docker.py:276} INFO - 21/05/14 19:55:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311324245208432389507_0002_m_000001_6
[2021-05-14 16:55:37,508] {docker.py:276} INFO - 21/05/14 19:55:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316637711620759049197_0002_m_000003_8
[2021-05-14 16:55:37,510] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_2021051419553139155689686437623_0002_m_000000_5: needsTaskCommit() Task attempt_2021051419553139155689686437623_0002_m_000000_5
[2021-05-14 16:55:37,511] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_2021051419553139155689686437623_0002_m_000000_5: needsTaskCommit() Task attempt_2021051419553139155689686437623_0002_m_000000_5: duration 0:00.000s
[2021-05-14 16:55:37,512] {docker.py:276} INFO - 21/05/14 19:55:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051419553139155689686437623_0002_m_000000_5
[2021-05-14 16:55:37,523] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955311326641153450603186_0002_m_000002_7: needsTaskCommit() Task attempt_202105141955311326641153450603186_0002_m_000002_7
[2021-05-14 16:55:37,524] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_202105141955311326641153450603186_0002_m_000002_7: needsTaskCommit() Task attempt_202105141955311326641153450603186_0002_m_000002_7: duration 0:00.001s
[2021-05-14 16:55:37,524] {docker.py:276} INFO - 21/05/14 19:55:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311326641153450603186_0002_m_000002_7
[2021-05-14 16:55:37,525] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Finished task 3.0 in stage 2.0 (TID 8). 4587 bytes result sent to driver
[2021-05-14 16:55:37,525] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 4544 bytes result sent to driver
[2021-05-14 16:55:37,526] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 4587 bytes result sent to driver
[2021-05-14 16:55:37,527] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 4587 bytes result sent to driver
[2021-05-14 16:55:37,528] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 9) (379a553b2355, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:37,530] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Running task 4.0 in stage 2.0 (TID 9)
[2021-05-14 16:55:37,532] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 10) (379a553b2355, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:37,533] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 8) in 2101 ms on 379a553b2355 (executor driver) (1/200)
[2021-05-14 16:55:37,534] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 2103 ms on 379a553b2355 (executor driver) (2/200)
[2021-05-14 16:55:37,535] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 2109 ms on 379a553b2355 (executor driver) (3/200)
[2021-05-14 16:55:37,537] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 11) (379a553b2355, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:37,538] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Running task 6.0 in stage 2.0 (TID 11)
[2021-05-14 16:55:37,543] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 12) (379a553b2355, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:37,543] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Running task 7.0 in stage 2.0 (TID 12)
[2021-05-14 16:55:37,544] {docker.py:276} INFO - 21/05/14 19:55:37 INFO Executor: Running task 5.0 in stage 2.0 (TID 10)
[2021-05-14 16:55:37,548] {docker.py:276} INFO - 21/05/14 19:55:37 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 2120 ms on 379a553b2355 (executor driver) (4/200)
[2021-05-14 16:55:37,568] {docker.py:276} INFO - 21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:37,569] {docker.py:276} INFO - 21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:37,572] {docker.py:276} INFO - 21/05/14 19:55:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:37,572] {docker.py:276} INFO - 21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531619769354568552574_0002_m_000004_9, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531619769354568552574_0002_m_000004_9}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531619769354568552574_0002}; taskId=attempt_20210514195531619769354568552574_0002_m_000004_9, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e7e0bcd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:37,573] {docker.py:276} INFO - 21/05/14 19:55:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_20210514195531619769354568552574_0002_m_000004_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531619769354568552574_0002_m_000004_9
[2021-05-14 16:55:37,585] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_20210514195531619769354568552574_0002_m_000004_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531619769354568552574_0002_m_000004_9 : duration 0:00.014s
[2021-05-14 16:55:37,595] {docker.py:276} INFO - 21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:37,597] {docker.py:276} INFO - 21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-14 16:55:37,602] {docker.py:276} INFO - 21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-14 16:55:37,604] {docker.py:276} INFO - 21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:37,605] {docker.py:276} INFO - 21/05/14 19:55:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:37,606] {docker.py:276} INFO - 21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312295666689521665846_0002_m_000007_12, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312295666689521665846_0002_m_000007_12}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312295666689521665846_0002}; taskId=attempt_202105141955312295666689521665846_0002_m_000007_12, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a32f8d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955312295666689521665846_0002_m_000007_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312295666689521665846_0002_m_000007_12
[2021-05-14 16:55:37,608] {docker.py:276} INFO - 21/05/14 19:55:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:37,612] {docker.py:276} INFO - 21/05/14 19:55:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:37,614] {docker.py:276} INFO - 21/05/14 19:55:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:37,615] {docker.py:276} INFO - 21/05/14 19:55:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:37,615] {docker.py:276} INFO - 21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:37,616] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_202105141955312295666689521665846_0002_m_000007_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312295666689521665846_0002_m_000007_12 : duration 0:00.008s
[2021-05-14 16:55:37,616] {docker.py:276} INFO - 21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314797427925162795668_0002_m_000006_11, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314797427925162795668_0002_m_000006_11}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314797427925162795668_0002}; taskId=attempt_202105141955314797427925162795668_0002_m_000006_11, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d8834ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:37,617] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955314797427925162795668_0002_m_000006_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314797427925162795668_0002_m_000006_11
[2021-05-14 16:55:37,619] {docker.py:276} INFO - 21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:37,620] {docker.py:276} INFO - 21/05/14 19:55:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314537146416374952045_0002_m_000005_10, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314537146416374952045_0002_m_000005_10}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314537146416374952045_0002}; taskId=attempt_202105141955314537146416374952045_0002_m_000005_10, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1358cb0f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:37,620] {docker.py:276} INFO - 21/05/14 19:55:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:37,620] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955314537146416374952045_0002_m_000005_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314537146416374952045_0002_m_000005_10
[2021-05-14 16:55:37,629] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_202105141955314797427925162795668_0002_m_000006_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314797427925162795668_0002_m_000006_11 : duration 0:00.012s
[2021-05-14 16:55:37,632] {docker.py:276} INFO - 21/05/14 19:55:37 INFO StagingCommitter: Task committer attempt_202105141955314537146416374952045_0002_m_000005_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314537146416374952045_0002_m_000005_10 : duration 0:00.013s
[2021-05-14 16:55:39,255] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955312295666689521665846_0002_m_000007_12: needsTaskCommit() Task attempt_202105141955312295666689521665846_0002_m_000007_12
[2021-05-14 16:55:39,256] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955312295666689521665846_0002_m_000007_12: needsTaskCommit() Task attempt_202105141955312295666689521665846_0002_m_000007_12: duration 0:00.001s
21/05/14 19:55:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312295666689521665846_0002_m_000007_12
[2021-05-14 16:55:39,259] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Finished task 7.0 in stage 2.0 (TID 12). 4587 bytes result sent to driver
[2021-05-14 16:55:39,262] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 13) (379a553b2355, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:39,264] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Running task 8.0 in stage 2.0 (TID 13)
[2021-05-14 16:55:39,265] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 12) in 1727 ms on 379a553b2355 (executor driver) (5/200)
[2021-05-14 16:55:39,279] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:39,282] {docker.py:276} INFO - 21/05/14 19:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:39,282] {docker.py:276} INFO - 21/05/14 19:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:39,283] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:39,283] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316508502379434255216_0002_m_000008_13, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316508502379434255216_0002_m_000008_13}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316508502379434255216_0002}; taskId=attempt_202105141955316508502379434255216_0002_m_000008_13, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@446f432}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:39,283] {docker.py:276} INFO - 21/05/14 19:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:39,283] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955316508502379434255216_0002_m_000008_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316508502379434255216_0002_m_000008_13
[2021-05-14 16:55:39,288] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955314797427925162795668_0002_m_000006_11: needsTaskCommit() Task attempt_202105141955314797427925162795668_0002_m_000006_11
21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955316508502379434255216_0002_m_000008_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316508502379434255216_0002_m_000008_13 : duration 0:00.005s
[2021-05-14 16:55:39,289] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955314797427925162795668_0002_m_000006_11: needsTaskCommit() Task attempt_202105141955314797427925162795668_0002_m_000006_11: duration 0:00.001s
[2021-05-14 16:55:39,289] {docker.py:276} INFO - 21/05/14 19:55:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314797427925162795668_0002_m_000006_11
[2021-05-14 16:55:39,294] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Finished task 6.0 in stage 2.0 (TID 11). 4587 bytes result sent to driver
[2021-05-14 16:55:39,296] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 14) (379a553b2355, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:39,297] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 11) in 1763 ms on 379a553b2355 (executor driver) (6/200)
[2021-05-14 16:55:39,298] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Running task 9.0 in stage 2.0 (TID 14)
[2021-05-14 16:55:39,303] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955314537146416374952045_0002_m_000005_10: needsTaskCommit() Task attempt_202105141955314537146416374952045_0002_m_000005_10
[2021-05-14 16:55:39,309] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955314537146416374952045_0002_m_000005_10: needsTaskCommit() Task attempt_202105141955314537146416374952045_0002_m_000005_10: duration 0:00.006s
[2021-05-14 16:55:39,310] {docker.py:276} INFO - 21/05/14 19:55:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314537146416374952045_0002_m_000005_10
[2021-05-14 16:55:39,312] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Finished task 5.0 in stage 2.0 (TID 10). 4587 bytes result sent to driver
[2021-05-14 16:55:39,313] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 15) (379a553b2355, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:39,315] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 10) in 1786 ms on 379a553b2355 (executor driver) (7/200)
[2021-05-14 16:55:39,322] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:39,326] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Running task 10.0 in stage 2.0 (TID 15)
[2021-05-14 16:55:39,327] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:39,327] {docker.py:276} INFO - 21/05/14 19:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:39,327] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:39,327] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311315515865843330428_0002_m_000009_14, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311315515865843330428_0002_m_000009_14}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311315515865843330428_0002}; taskId=attempt_202105141955311315515865843330428_0002_m_000009_14, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6873404f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:39,328] {docker.py:276} INFO - 21/05/14 19:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:39,328] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955311315515865843330428_0002_m_000009_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311315515865843330428_0002_m_000009_14
[2021-05-14 16:55:39,333] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955311315515865843330428_0002_m_000009_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311315515865843330428_0002_m_000009_14 : duration 0:00.006s
[2021-05-14 16:55:39,338] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_20210514195531619769354568552574_0002_m_000004_9: needsTaskCommit() Task attempt_20210514195531619769354568552574_0002_m_000004_9
[2021-05-14 16:55:39,339] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_20210514195531619769354568552574_0002_m_000004_9: needsTaskCommit() Task attempt_20210514195531619769354568552574_0002_m_000004_9: duration 0:00.002s
[2021-05-14 16:55:39,339] {docker.py:276} INFO - 21/05/14 19:55:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531619769354568552574_0002_m_000004_9
[2021-05-14 16:55:39,341] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Finished task 4.0 in stage 2.0 (TID 9). 4587 bytes result sent to driver
[2021-05-14 16:55:39,342] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 16) (379a553b2355, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:39,343] {docker.py:276} INFO - 21/05/14 19:55:39 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 9) in 1818 ms on 379a553b2355 (executor driver) (8/200)
[2021-05-14 16:55:39,344] {docker.py:276} INFO - 21/05/14 19:55:39 INFO Executor: Running task 11.0 in stage 2.0 (TID 16)
[2021-05-14 16:55:39,359] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:39,360] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:39,363] {docker.py:276} INFO - 21/05/14 19:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:39,364] {docker.py:276} INFO - 21/05/14 19:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:39,364] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:39,365] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312967041851496475537_0002_m_000010_15, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312967041851496475537_0002_m_000010_15}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312967041851496475537_0002}; taskId=attempt_202105141955312967041851496475537_0002_m_000010_15, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f76103f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:39,365] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955312967041851496475537_0002_m_000010_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312967041851496475537_0002_m_000010_15
[2021-05-14 16:55:39,370] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955312967041851496475537_0002_m_000010_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312967041851496475537_0002_m_000010_15 : duration 0:00.005s
[2021-05-14 16:55:39,373] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:39,373] {docker.py:276} INFO - 21/05/14 19:55:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:39,375] {docker.py:276} INFO - 21/05/14 19:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:39,376] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:39,376] {docker.py:276} INFO - 21/05/14 19:55:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315503941864602601780_0002_m_000011_16, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315503941864602601780_0002_m_000011_16}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315503941864602601780_0002}; taskId=attempt_202105141955315503941864602601780_0002_m_000011_16, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24d6c1e7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:39,376] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955315503941864602601780_0002_m_000011_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315503941864602601780_0002_m_000011_16
[2021-05-14 16:55:39,380] {docker.py:276} INFO - 21/05/14 19:55:39 INFO StagingCommitter: Task committer attempt_202105141955315503941864602601780_0002_m_000011_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315503941864602601780_0002_m_000011_16 : duration 0:00.004s
[2021-05-14 16:55:40,915] {docker.py:276} INFO - 21/05/14 19:55:40 INFO StagingCommitter: Starting: Task committer attempt_202105141955311315515865843330428_0002_m_000009_14: needsTaskCommit() Task attempt_202105141955311315515865843330428_0002_m_000009_14
[2021-05-14 16:55:40,916] {docker.py:276} INFO - 21/05/14 19:55:40 INFO StagingCommitter: Task committer attempt_202105141955311315515865843330428_0002_m_000009_14: needsTaskCommit() Task attempt_202105141955311315515865843330428_0002_m_000009_14: duration 0:00.002s
[2021-05-14 16:55:40,917] {docker.py:276} INFO - 21/05/14 19:55:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311315515865843330428_0002_m_000009_14
[2021-05-14 16:55:40,921] {docker.py:276} INFO - 21/05/14 19:55:40 INFO Executor: Finished task 9.0 in stage 2.0 (TID 14). 4544 bytes result sent to driver
[2021-05-14 16:55:40,922] {docker.py:276} INFO - 21/05/14 19:55:40 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 17) (379a553b2355, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:40,923] {docker.py:276} INFO - 21/05/14 19:55:40 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 14) in 1630 ms on 379a553b2355 (executor driver) (9/200)
[2021-05-14 16:55:40,924] {docker.py:276} INFO - 21/05/14 19:55:40 INFO Executor: Running task 12.0 in stage 2.0 (TID 17)
[2021-05-14 16:55:40,937] {docker.py:276} INFO - 21/05/14 19:55:40 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:40,937] {docker.py:276} INFO - 21/05/14 19:55:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:40,940] {docker.py:276} INFO - 21/05/14 19:55:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:40,940] {docker.py:276} INFO - 21/05/14 19:55:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:40,941] {docker.py:276} INFO - 21/05/14 19:55:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:40,941] {docker.py:276} INFO - 21/05/14 19:55:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318028985980403141391_0002_m_000012_17, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318028985980403141391_0002_m_000012_17}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318028985980403141391_0002}; taskId=attempt_202105141955318028985980403141391_0002_m_000012_17, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2af4ca2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:40,942] {docker.py:276} INFO - 21/05/14 19:55:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:40 INFO StagingCommitter: Starting: Task committer attempt_202105141955318028985980403141391_0002_m_000012_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318028985980403141391_0002_m_000012_17
[2021-05-14 16:55:40,946] {docker.py:276} INFO - 21/05/14 19:55:40 INFO StagingCommitter: Task committer attempt_202105141955318028985980403141391_0002_m_000012_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318028985980403141391_0002_m_000012_17 : duration 0:00.004s
[2021-05-14 16:55:40,968] {docker.py:276} INFO - 21/05/14 19:55:40 INFO StagingCommitter: Starting: Task committer attempt_202105141955315503941864602601780_0002_m_000011_16: needsTaskCommit() Task attempt_202105141955315503941864602601780_0002_m_000011_16
[2021-05-14 16:55:40,968] {docker.py:276} INFO - 21/05/14 19:55:40 INFO StagingCommitter: Task committer attempt_202105141955315503941864602601780_0002_m_000011_16: needsTaskCommit() Task attempt_202105141955315503941864602601780_0002_m_000011_16: duration 0:00.001s
21/05/14 19:55:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315503941864602601780_0002_m_000011_16
[2021-05-14 16:55:40,970] {docker.py:276} INFO - 21/05/14 19:55:41 INFO Executor: Finished task 11.0 in stage 2.0 (TID 16). 4544 bytes result sent to driver
[2021-05-14 16:55:40,971] {docker.py:276} INFO - 21/05/14 19:55:41 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 18) (379a553b2355, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:40,972] {docker.py:276} INFO - 21/05/14 19:55:41 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 16) in 1632 ms on 379a553b2355 (executor driver) (10/200)
[2021-05-14 16:55:40,973] {docker.py:276} INFO - 21/05/14 19:55:41 INFO Executor: Running task 13.0 in stage 2.0 (TID 18)
[2021-05-14 16:55:40,984] {docker.py:276} INFO - 21/05/14 19:55:41 INFO ShuffleBlockFetcherIterator: Getting 4 (5.0 KiB) non-empty blocks including 4 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:40,986] {docker.py:276} INFO - 21/05/14 19:55:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315377077920931636234_0002_m_000013_18, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315377077920931636234_0002_m_000013_18}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315377077920931636234_0002}; taskId=attempt_202105141955315377077920931636234_0002_m_000013_18, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19a0de9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:40,987] {docker.py:276} INFO - 21/05/14 19:55:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:40,987] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955315377077920931636234_0002_m_000013_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315377077920931636234_0002_m_000013_18
[2021-05-14 16:55:40,992] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Task committer attempt_202105141955315377077920931636234_0002_m_000013_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315377077920931636234_0002_m_000013_18 : duration 0:00.004s
[2021-05-14 16:55:41,066] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955316508502379434255216_0002_m_000008_13: needsTaskCommit() Task attempt_202105141955316508502379434255216_0002_m_000008_13
[2021-05-14 16:55:41,067] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Task committer attempt_202105141955316508502379434255216_0002_m_000008_13: needsTaskCommit() Task attempt_202105141955316508502379434255216_0002_m_000008_13: duration 0:00.002s
21/05/14 19:55:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316508502379434255216_0002_m_000008_13
[2021-05-14 16:55:41,068] {docker.py:276} INFO - 21/05/14 19:55:41 INFO Executor: Finished task 8.0 in stage 2.0 (TID 13). 4544 bytes result sent to driver
[2021-05-14 16:55:41,070] {docker.py:276} INFO - 21/05/14 19:55:41 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 19) (379a553b2355, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:41,071] {docker.py:276} INFO - 21/05/14 19:55:41 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 13) in 1813 ms on 379a553b2355 (executor driver) (11/200)
[2021-05-14 16:55:41,072] {docker.py:276} INFO - 21/05/14 19:55:41 INFO Executor: Running task 14.0 in stage 2.0 (TID 19)
[2021-05-14 16:55:41,079] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955312967041851496475537_0002_m_000010_15: needsTaskCommit() Task attempt_202105141955312967041851496475537_0002_m_000010_15
[2021-05-14 16:55:41,080] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Task committer attempt_202105141955312967041851496475537_0002_m_000010_15: needsTaskCommit() Task attempt_202105141955312967041851496475537_0002_m_000010_15: duration 0:00.001s
21/05/14 19:55:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312967041851496475537_0002_m_000010_15
[2021-05-14 16:55:41,081] {docker.py:276} INFO - 21/05/14 19:55:41 INFO Executor: Finished task 10.0 in stage 2.0 (TID 15). 4544 bytes result sent to driver
[2021-05-14 16:55:41,083] {docker.py:276} INFO - 21/05/14 19:55:41 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 20) (379a553b2355, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:41,084] {docker.py:276} INFO - 21/05/14 19:55:41 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 15) in 1773 ms on 379a553b2355 (executor driver) (12/200)
[2021-05-14 16:55:41,085] {docker.py:276} INFO - 21/05/14 19:55:41 INFO Executor: Running task 15.0 in stage 2.0 (TID 20)
[2021-05-14 16:55:41,086] {docker.py:276} INFO - 21/05/14 19:55:41 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:41,090] {docker.py:276} INFO - 21/05/14 19:55:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:41,091] {docker.py:276} INFO - 21/05/14 19:55:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316621574630360241686_0002_m_000014_19, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316621574630360241686_0002_m_000014_19}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316621574630360241686_0002}; taskId=attempt_202105141955316621574630360241686_0002_m_000014_19, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17428057}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:41,091] {docker.py:276} INFO - 21/05/14 19:55:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:41,092] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955316621574630360241686_0002_m_000014_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316621574630360241686_0002_m_000014_19
[2021-05-14 16:55:41,095] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Task committer attempt_202105141955316621574630360241686_0002_m_000014_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316621574630360241686_0002_m_000014_19 : duration 0:00.005s
[2021-05-14 16:55:41,102] {docker.py:276} INFO - 21/05/14 19:55:41 INFO ShuffleBlockFetcherIterator: Getting 4 (5.0 KiB) non-empty blocks including 4 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:41,103] {docker.py:276} INFO - 21/05/14 19:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:41,107] {docker.py:276} INFO - 21/05/14 19:55:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:41,108] {docker.py:276} INFO - 21/05/14 19:55:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:41,108] {docker.py:276} INFO - 21/05/14 19:55:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317891520431060371069_0002_m_000015_20, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317891520431060371069_0002_m_000015_20}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317891520431060371069_0002}; taskId=attempt_202105141955317891520431060371069_0002_m_000015_20, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@492dd6ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:41,109] {docker.py:276} INFO - 21/05/14 19:55:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:41,109] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955317891520431060371069_0002_m_000015_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317891520431060371069_0002_m_000015_20
[2021-05-14 16:55:41,113] {docker.py:276} INFO - 21/05/14 19:55:41 INFO StagingCommitter: Task committer attempt_202105141955317891520431060371069_0002_m_000015_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317891520431060371069_0002_m_000015_20 : duration 0:00.004s
[2021-05-14 16:55:42,538] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955318028985980403141391_0002_m_000012_17: needsTaskCommit() Task attempt_202105141955318028985980403141391_0002_m_000012_17
[2021-05-14 16:55:42,539] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955318028985980403141391_0002_m_000012_17: needsTaskCommit() Task attempt_202105141955318028985980403141391_0002_m_000012_17: duration 0:00.002s
[2021-05-14 16:55:42,540] {docker.py:276} INFO - 21/05/14 19:55:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318028985980403141391_0002_m_000012_17
[2021-05-14 16:55:42,543] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Finished task 12.0 in stage 2.0 (TID 17). 4587 bytes result sent to driver
[2021-05-14 16:55:42,545] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 21) (379a553b2355, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:42,547] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Running task 16.0 in stage 2.0 (TID 21)
[2021-05-14 16:55:42,547] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 17) in 1627 ms on 379a553b2355 (executor driver) (13/200)
[2021-05-14 16:55:42,560] {docker.py:276} INFO - 21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:42,563] {docker.py:276} INFO - 21/05/14 19:55:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315735533927848254033_0002_m_000016_21, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315735533927848254033_0002_m_000016_21}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315735533927848254033_0002}; taskId=attempt_202105141955315735533927848254033_0002_m_000016_21, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72434797}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:42,563] {docker.py:276} INFO - 21/05/14 19:55:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955315735533927848254033_0002_m_000016_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315735533927848254033_0002_m_000016_21
[2021-05-14 16:55:42,568] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955315735533927848254033_0002_m_000016_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315735533927848254033_0002_m_000016_21 : duration 0:00.005s
[2021-05-14 16:55:42,705] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955315377077920931636234_0002_m_000013_18: needsTaskCommit() Task attempt_202105141955315377077920931636234_0002_m_000013_18
[2021-05-14 16:55:42,706] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955315377077920931636234_0002_m_000013_18: needsTaskCommit() Task attempt_202105141955315377077920931636234_0002_m_000013_18: duration 0:00.002s
21/05/14 19:55:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315377077920931636234_0002_m_000013_18
[2021-05-14 16:55:42,709] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Finished task 13.0 in stage 2.0 (TID 18). 4587 bytes result sent to driver
[2021-05-14 16:55:42,711] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 22) (379a553b2355, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:42,713] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Running task 17.0 in stage 2.0 (TID 22)
[2021-05-14 16:55:42,713] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 18) in 1744 ms on 379a553b2355 (executor driver) (14/200)
[2021-05-14 16:55:42,725] {docker.py:276} INFO - 21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Getting 4 (5.0 KiB) non-empty blocks including 4 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:42,728] {docker.py:276} INFO - 21/05/14 19:55:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:42,729] {docker.py:276} INFO - 21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317174421390107783893_0002_m_000017_22, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317174421390107783893_0002_m_000017_22}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317174421390107783893_0002}; taskId=attempt_202105141955317174421390107783893_0002_m_000017_22, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c7c7018}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955317174421390107783893_0002_m_000017_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317174421390107783893_0002_m_000017_22
[2021-05-14 16:55:42,733] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955317174421390107783893_0002_m_000017_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317174421390107783893_0002_m_000017_22 : duration 0:00.004s
[2021-05-14 16:55:42,745] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955317891520431060371069_0002_m_000015_20: needsTaskCommit() Task attempt_202105141955317891520431060371069_0002_m_000015_20
[2021-05-14 16:55:42,745] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955317891520431060371069_0002_m_000015_20: needsTaskCommit() Task attempt_202105141955317891520431060371069_0002_m_000015_20: duration 0:00.002s
21/05/14 19:55:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317891520431060371069_0002_m_000015_20
[2021-05-14 16:55:42,747] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Finished task 15.0 in stage 2.0 (TID 20). 4587 bytes result sent to driver
[2021-05-14 16:55:42,749] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 23) (379a553b2355, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:42,750] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Running task 18.0 in stage 2.0 (TID 23)
[2021-05-14 16:55:42,751] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 20) in 1671 ms on 379a553b2355 (executor driver) (15/200)
[2021-05-14 16:55:42,759] {docker.py:276} INFO - 21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:42,761] {docker.py:276} INFO - 21/05/14 19:55:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:42,762] {docker.py:276} INFO - 21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:42,762] {docker.py:276} INFO - 21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311424786384990201275_0002_m_000018_23, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311424786384990201275_0002_m_000018_23}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311424786384990201275_0002}; taskId=attempt_202105141955311424786384990201275_0002_m_000018_23, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f638517}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:42,763] {docker.py:276} INFO - 21/05/14 19:55:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:42,763] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955311424786384990201275_0002_m_000018_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311424786384990201275_0002_m_000018_23
[2021-05-14 16:55:42,766] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955311424786384990201275_0002_m_000018_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311424786384990201275_0002_m_000018_23 : duration 0:00.004s
[2021-05-14 16:55:42,772] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955316621574630360241686_0002_m_000014_19: needsTaskCommit() Task attempt_202105141955316621574630360241686_0002_m_000014_19
[2021-05-14 16:55:42,773] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955316621574630360241686_0002_m_000014_19: needsTaskCommit() Task attempt_202105141955316621574630360241686_0002_m_000014_19: duration 0:00.002s
[2021-05-14 16:55:42,773] {docker.py:276} INFO - 21/05/14 19:55:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316621574630360241686_0002_m_000014_19
[2021-05-14 16:55:42,777] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Finished task 14.0 in stage 2.0 (TID 19). 4587 bytes result sent to driver
[2021-05-14 16:55:42,778] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 24) (379a553b2355, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:42,780] {docker.py:276} INFO - 21/05/14 19:55:42 INFO Executor: Running task 19.0 in stage 2.0 (TID 24)
[2021-05-14 16:55:42,780] {docker.py:276} INFO - 21/05/14 19:55:42 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 19) in 1712 ms on 379a553b2355 (executor driver) (16/200)
[2021-05-14 16:55:42,790] {docker.py:276} INFO - 21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:42,792] {docker.py:276} INFO - 21/05/14 19:55:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:42,793] {docker.py:276} INFO - 21/05/14 19:55:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318278178769407905451_0002_m_000019_24, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318278178769407905451_0002_m_000019_24}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318278178769407905451_0002}; taskId=attempt_202105141955318278178769407905451_0002_m_000019_24, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a6a66aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:42,793] {docker.py:276} INFO - 21/05/14 19:55:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955318278178769407905451_0002_m_000019_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318278178769407905451_0002_m_000019_24
[2021-05-14 16:55:42,797] {docker.py:276} INFO - 21/05/14 19:55:42 INFO StagingCommitter: Task committer attempt_202105141955318278178769407905451_0002_m_000019_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318278178769407905451_0002_m_000019_24 : duration 0:00.004s
[2021-05-14 16:55:44,557] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955311424786384990201275_0002_m_000018_23: needsTaskCommit() Task attempt_202105141955311424786384990201275_0002_m_000018_23
21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955317174421390107783893_0002_m_000017_22: needsTaskCommit() Task attempt_202105141955317174421390107783893_0002_m_000017_22
[2021-05-14 16:55:44,558] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955318278178769407905451_0002_m_000019_24: needsTaskCommit() Task attempt_202105141955318278178769407905451_0002_m_000019_24
[2021-05-14 16:55:44,559] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955315735533927848254033_0002_m_000016_21: needsTaskCommit() Task attempt_202105141955315735533927848254033_0002_m_000016_21
[2021-05-14 16:55:44,559] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_202105141955311424786384990201275_0002_m_000018_23: needsTaskCommit() Task attempt_202105141955311424786384990201275_0002_m_000018_23: duration 0:00.004s
[2021-05-14 16:55:44,560] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_202105141955318278178769407905451_0002_m_000019_24: needsTaskCommit() Task attempt_202105141955318278178769407905451_0002_m_000019_24: duration 0:00.001s
21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_202105141955315735533927848254033_0002_m_000016_21: needsTaskCommit() Task attempt_202105141955315735533927848254033_0002_m_000016_21: duration 0:00.005s
21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_202105141955317174421390107783893_0002_m_000017_22: needsTaskCommit() Task attempt_202105141955317174421390107783893_0002_m_000017_22: duration 0:00.003s
[2021-05-14 16:55:44,561] {docker.py:276} INFO - 21/05/14 19:55:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311424786384990201275_0002_m_000018_23
[2021-05-14 16:55:44,562] {docker.py:276} INFO - 21/05/14 19:55:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315735533927848254033_0002_m_000016_21
[2021-05-14 16:55:44,563] {docker.py:276} INFO - 21/05/14 19:55:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317174421390107783893_0002_m_000017_22
[2021-05-14 16:55:44,564] {docker.py:276} INFO - 21/05/14 19:55:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318278178769407905451_0002_m_000019_24
[2021-05-14 16:55:44,566] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Finished task 19.0 in stage 2.0 (TID 24). 4544 bytes result sent to driver
[2021-05-14 16:55:44,567] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Finished task 18.0 in stage 2.0 (TID 23). 4544 bytes result sent to driver
[2021-05-14 16:55:44,567] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 25) (379a553b2355, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:44,569] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 24) in 1757 ms on 379a553b2355 (executor driver) (17/200)
[2021-05-14 16:55:44,570] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Running task 20.0 in stage 2.0 (TID 25)
[2021-05-14 16:55:44,570] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 26) (379a553b2355, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:44,572] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 23) in 1791 ms on 379a553b2355 (executor driver) (18/200)
[2021-05-14 16:55:44,572] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Running task 21.0 in stage 2.0 (TID 26)
[2021-05-14 16:55:44,573] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Finished task 16.0 in stage 2.0 (TID 21). 4587 bytes result sent to driver
[2021-05-14 16:55:44,574] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 27) (379a553b2355, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:44,575] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Running task 22.0 in stage 2.0 (TID 27)
[2021-05-14 16:55:44,576] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Finished task 17.0 in stage 2.0 (TID 22). 4587 bytes result sent to driver
[2021-05-14 16:55:44,580] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 21) in 1998 ms on 379a553b2355 (executor driver) (19/200)
[2021-05-14 16:55:44,581] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 28) (379a553b2355, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:44,581] {docker.py:276} INFO - 21/05/14 19:55:44 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 22) in 1835 ms on 379a553b2355 (executor driver) (20/200)
[2021-05-14 16:55:44,581] {docker.py:276} INFO - 21/05/14 19:55:44 INFO Executor: Running task 23.0 in stage 2.0 (TID 28)
[2021-05-14 16:55:44,584] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:44,584] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:44,585] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:44,586] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:44,587] {docker.py:276} INFO - 21/05/14 19:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:44,588] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:44,588] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312020403039548368730_0002_m_000021_26, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312020403039548368730_0002_m_000021_26}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312020403039548368730_0002}; taskId=attempt_202105141955312020403039548368730_0002_m_000021_26, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@114c1a63}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:44,589] {docker.py:276} INFO - 21/05/14 19:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955312020403039548368730_0002_m_000021_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312020403039548368730_0002_m_000021_26
[2021-05-14 16:55:44,589] {docker.py:276} INFO - 21/05/14 19:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:44,590] {docker.py:276} INFO - 21/05/14 19:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:44,590] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:44,591] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314895720158257113911_0002_m_000022_27, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314895720158257113911_0002_m_000022_27}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314895720158257113911_0002}; taskId=attempt_202105141955314895720158257113911_0002_m_000022_27, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7797788}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:44,592] {docker.py:276} INFO - 21/05/14 19:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:44,592] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955314895720158257113911_0002_m_000022_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314895720158257113911_0002_m_000022_27
[2021-05-14 16:55:44,599] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_202105141955314895720158257113911_0002_m_000022_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314895720158257113911_0002_m_000022_27 : duration 0:00.009s
[2021-05-14 16:55:44,610] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:44,610] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
[2021-05-14 16:55:44,612] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_202105141955312020403039548368730_0002_m_000021_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312020403039548368730_0002_m_000021_26 : duration 0:00.024s
[2021-05-14 16:55:44,613] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:44,617] {docker.py:276} INFO - 21/05/14 19:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:44,618] {docker.py:276} INFO - 21/05/14 19:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:44,619] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:44,620] {docker.py:276} INFO - 21/05/14 19:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:44,620] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531775314326725056702_0002_m_000020_25, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531775314326725056702_0002_m_000020_25}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531775314326725056702_0002}; taskId=attempt_20210514195531775314326725056702_0002_m_000020_25, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fd54cec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:44,622] {docker.py:276} INFO - 21/05/14 19:55:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531695467923329808933_0002_m_000023_28, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531695467923329808933_0002_m_000023_28}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531695467923329808933_0002}; taskId=attempt_20210514195531695467923329808933_0002_m_000023_28, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@660d54ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_20210514195531695467923329808933_0002_m_000023_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531695467923329808933_0002_m_000023_28
[2021-05-14 16:55:44,623] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Starting: Task committer attempt_20210514195531775314326725056702_0002_m_000020_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531775314326725056702_0002_m_000020_25
[2021-05-14 16:55:44,628] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_20210514195531695467923329808933_0002_m_000023_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531695467923329808933_0002_m_000023_28 : duration 0:00.006s
[2021-05-14 16:55:44,629] {docker.py:276} INFO - 21/05/14 19:55:44 INFO StagingCommitter: Task committer attempt_20210514195531775314326725056702_0002_m_000020_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531775314326725056702_0002_m_000020_25 : duration 0:00.008s
[2021-05-14 16:55:46,188] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955312020403039548368730_0002_m_000021_26: needsTaskCommit() Task attempt_202105141955312020403039548368730_0002_m_000021_26
[2021-05-14 16:55:46,189] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_202105141955312020403039548368730_0002_m_000021_26: needsTaskCommit() Task attempt_202105141955312020403039548368730_0002_m_000021_26: duration 0:00.001s
21/05/14 19:55:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312020403039548368730_0002_m_000021_26
[2021-05-14 16:55:46,191] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Finished task 21.0 in stage 2.0 (TID 26). 4544 bytes result sent to driver
[2021-05-14 16:55:46,192] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 29) (379a553b2355, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:46,193] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Running task 24.0 in stage 2.0 (TID 29)
[2021-05-14 16:55:46,193] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 26) in 1626 ms on 379a553b2355 (executor driver) (21/200)
[2021-05-14 16:55:46,196] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_20210514195531695467923329808933_0002_m_000023_28: needsTaskCommit() Task attempt_20210514195531695467923329808933_0002_m_000023_28
[2021-05-14 16:55:46,196] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_20210514195531775314326725056702_0002_m_000020_25: needsTaskCommit() Task attempt_20210514195531775314326725056702_0002_m_000020_25
[2021-05-14 16:55:46,197] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_20210514195531695467923329808933_0002_m_000023_28: needsTaskCommit() Task attempt_20210514195531695467923329808933_0002_m_000023_28: duration 0:00.000s
[2021-05-14 16:55:46,197] {docker.py:276} INFO - 21/05/14 19:55:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531695467923329808933_0002_m_000023_28
[2021-05-14 16:55:46,198] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_20210514195531775314326725056702_0002_m_000020_25: needsTaskCommit() Task attempt_20210514195531775314326725056702_0002_m_000020_25: duration 0:00.001s
21/05/14 19:55:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531775314326725056702_0002_m_000020_25
[2021-05-14 16:55:46,198] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Finished task 23.0 in stage 2.0 (TID 28). 4544 bytes result sent to driver
[2021-05-14 16:55:46,200] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 30) (379a553b2355, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:46,202] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 28) in 1626 ms on 379a553b2355 (executor driver) (22/200)
[2021-05-14 16:55:46,202] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Running task 25.0 in stage 2.0 (TID 30)
[2021-05-14 16:55:46,203] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Finished task 20.0 in stage 2.0 (TID 25). 4544 bytes result sent to driver
[2021-05-14 16:55:46,204] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 31) (379a553b2355, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:46,204] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 25) in 1639 ms on 379a553b2355 (executor driver) (23/200)
[2021-05-14 16:55:46,205] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Running task 26.0 in stage 2.0 (TID 31)
[2021-05-14 16:55:46,212] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Getting 4 (6.4 KiB) non-empty blocks including 4 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:46,213] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:46,213] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:46,215] {docker.py:276} INFO - 21/05/14 19:55:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:46,216] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:46,217] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317893276541894125499_0002_m_000025_30, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317893276541894125499_0002_m_000025_30}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317893276541894125499_0002}; taskId=attempt_202105141955317893276541894125499_0002_m_000025_30, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67d2cc19}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:46,217] {docker.py:276} INFO - 21/05/14 19:55:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:46,217] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955317893276541894125499_0002_m_000025_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317893276541894125499_0002_m_000025_30
[2021-05-14 16:55:46,229] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_202105141955317893276541894125499_0002_m_000025_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317893276541894125499_0002_m_000025_30 : duration 0:00.012s
[2021-05-14 16:55:46,237] {docker.py:276} INFO - 21/05/14 19:55:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:46,237] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:46,238] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-14 16:55:46,239] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:46,239] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313041543960344430566_0002_m_000024_29, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313041543960344430566_0002_m_000024_29}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313041543960344430566_0002}; taskId=attempt_202105141955313041543960344430566_0002_m_000024_29, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ee6c0a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:46,240] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955313041543960344430566_0002_m_000024_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313041543960344430566_0002_m_000024_29
[2021-05-14 16:55:46,243] {docker.py:276} INFO - 21/05/14 19:55:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:46,244] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314561710279334805432_0002_m_000026_31, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314561710279334805432_0002_m_000026_31}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314561710279334805432_0002}; taskId=attempt_202105141955314561710279334805432_0002_m_000026_31, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b865eef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:46,245] {docker.py:276} INFO - 21/05/14 19:55:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955314561710279334805432_0002_m_000026_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314561710279334805432_0002_m_000026_31
[2021-05-14 16:55:46,253] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_202105141955313041543960344430566_0002_m_000024_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313041543960344430566_0002_m_000024_29 : duration 0:00.014s
[2021-05-14 16:55:46,256] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_202105141955314561710279334805432_0002_m_000026_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314561710279334805432_0002_m_000026_31 : duration 0:00.011s
[2021-05-14 16:55:46,325] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955314895720158257113911_0002_m_000022_27: needsTaskCommit() Task attempt_202105141955314895720158257113911_0002_m_000022_27
[2021-05-14 16:55:46,325] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_202105141955314895720158257113911_0002_m_000022_27: needsTaskCommit() Task attempt_202105141955314895720158257113911_0002_m_000022_27: duration 0:00.001s
[2021-05-14 16:55:46,326] {docker.py:276} INFO - 21/05/14 19:55:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314895720158257113911_0002_m_000022_27
[2021-05-14 16:55:46,328] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Finished task 22.0 in stage 2.0 (TID 27). 4587 bytes result sent to driver
[2021-05-14 16:55:46,329] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 32) (379a553b2355, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:46,331] {docker.py:276} INFO - 21/05/14 19:55:46 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 27) in 1759 ms on 379a553b2355 (executor driver) (24/200)
[2021-05-14 16:55:46,331] {docker.py:276} INFO - 21/05/14 19:55:46 INFO Executor: Running task 27.0 in stage 2.0 (TID 32)
[2021-05-14 16:55:46,340] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:46,341] {docker.py:276} INFO - 21/05/14 19:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:46,343] {docker.py:276} INFO - 21/05/14 19:55:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:46,344] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:46,344] {docker.py:276} INFO - 21/05/14 19:55:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314415100924483352944_0002_m_000027_32, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314415100924483352944_0002_m_000027_32}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314415100924483352944_0002}; taskId=attempt_202105141955314415100924483352944_0002_m_000027_32, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1eb03263}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955314415100924483352944_0002_m_000027_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314415100924483352944_0002_m_000027_32
[2021-05-14 16:55:46,348] {docker.py:276} INFO - 21/05/14 19:55:46 INFO StagingCommitter: Task committer attempt_202105141955314415100924483352944_0002_m_000027_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314415100924483352944_0002_m_000027_32 : duration 0:00.005s
[2021-05-14 16:55:47,831] {docker.py:276} INFO - 21/05/14 19:55:47 INFO StagingCommitter: Starting: Task committer attempt_202105141955317893276541894125499_0002_m_000025_30: needsTaskCommit() Task attempt_202105141955317893276541894125499_0002_m_000025_30
21/05/14 19:55:47 INFO StagingCommitter: Starting: Task committer attempt_202105141955313041543960344430566_0002_m_000024_29: needsTaskCommit() Task attempt_202105141955313041543960344430566_0002_m_000024_29
[2021-05-14 16:55:47,831] {docker.py:276} INFO - 21/05/14 19:55:47 INFO StagingCommitter: Task committer attempt_202105141955317893276541894125499_0002_m_000025_30: needsTaskCommit() Task attempt_202105141955317893276541894125499_0002_m_000025_30: duration 0:00.002s
21/05/14 19:55:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317893276541894125499_0002_m_000025_30
[2021-05-14 16:55:47,832] {docker.py:276} INFO - 21/05/14 19:55:47 INFO StagingCommitter: Task committer attempt_202105141955313041543960344430566_0002_m_000024_29: needsTaskCommit() Task attempt_202105141955313041543960344430566_0002_m_000024_29: duration 0:00.002s
[2021-05-14 16:55:47,833] {docker.py:276} INFO - 21/05/14 19:55:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313041543960344430566_0002_m_000024_29
[2021-05-14 16:55:47,834] {docker.py:276} INFO - 21/05/14 19:55:47 INFO Executor: Finished task 25.0 in stage 2.0 (TID 30). 4587 bytes result sent to driver
[2021-05-14 16:55:47,835] {docker.py:276} INFO - 21/05/14 19:55:47 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 33) (379a553b2355, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:47,836] {docker.py:276} INFO - 21/05/14 19:55:47 INFO Executor: Finished task 24.0 in stage 2.0 (TID 29). 4587 bytes result sent to driver
[2021-05-14 16:55:47,837] {docker.py:276} INFO - 21/05/14 19:55:47 INFO Executor: Running task 28.0 in stage 2.0 (TID 33)
[2021-05-14 16:55:47,837] {docker.py:276} INFO - 21/05/14 19:55:47 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 30) in 1640 ms on 379a553b2355 (executor driver) (25/200)
[2021-05-14 16:55:47,839] {docker.py:276} INFO - 21/05/14 19:55:47 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 34) (379a553b2355, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:47,840] {docker.py:276} INFO - 21/05/14 19:55:47 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 29) in 1650 ms on 379a553b2355 (executor driver) (26/200)
[2021-05-14 16:55:47,840] {docker.py:276} INFO - 21/05/14 19:55:47 INFO Executor: Running task 29.0 in stage 2.0 (TID 34)
[2021-05-14 16:55:47,850] {docker.py:276} INFO - 21/05/14 19:55:47 INFO ShuffleBlockFetcherIterator: Getting 4 (5.0 KiB) non-empty blocks including 4 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:47,851] {docker.py:276} INFO - 21/05/14 19:55:47 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:47,851] {docker.py:276} INFO - 21/05/14 19:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:47,852] {docker.py:276} INFO - 21/05/14 19:55:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:47,853] {docker.py:276} INFO - 21/05/14 19:55:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531772039913774692022_0002_m_000028_33, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531772039913774692022_0002_m_000028_33}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531772039913774692022_0002}; taskId=attempt_20210514195531772039913774692022_0002_m_000028_33, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6cace243}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:47 INFO StagingCommitter: Starting: Task committer attempt_20210514195531772039913774692022_0002_m_000028_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531772039913774692022_0002_m_000028_33
[2021-05-14 16:55:47,855] {docker.py:276} INFO - 21/05/14 19:55:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:47,855] {docker.py:276} INFO - 21/05/14 19:55:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:47,856] {docker.py:276} INFO - 21/05/14 19:55:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317155526622060585950_0002_m_000029_34, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317155526622060585950_0002_m_000029_34}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317155526622060585950_0002}; taskId=attempt_202105141955317155526622060585950_0002_m_000029_34, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b245d48}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:47,856] {docker.py:276} INFO - 21/05/14 19:55:47 INFO StagingCommitter: Task committer attempt_20210514195531772039913774692022_0002_m_000028_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531772039913774692022_0002_m_000028_33 : duration 0:00.003s
[2021-05-14 16:55:47,857] {docker.py:276} INFO - 21/05/14 19:55:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:47 INFO StagingCommitter: Starting: Task committer attempt_202105141955317155526622060585950_0002_m_000029_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317155526622060585950_0002_m_000029_34
[2021-05-14 16:55:47,861] {docker.py:276} INFO - 21/05/14 19:55:47 INFO StagingCommitter: Task committer attempt_202105141955317155526622060585950_0002_m_000029_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317155526622060585950_0002_m_000029_34 : duration 0:00.004s
[2021-05-14 16:55:48,009] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Starting: Task committer attempt_202105141955314415100924483352944_0002_m_000027_32: needsTaskCommit() Task attempt_202105141955314415100924483352944_0002_m_000027_32
[2021-05-14 16:55:48,011] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Task committer attempt_202105141955314415100924483352944_0002_m_000027_32: needsTaskCommit() Task attempt_202105141955314415100924483352944_0002_m_000027_32: duration 0:00.001s
[2021-05-14 16:55:48,012] {docker.py:276} INFO - 21/05/14 19:55:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314415100924483352944_0002_m_000027_32
[2021-05-14 16:55:48,014] {docker.py:276} INFO - 21/05/14 19:55:48 INFO Executor: Finished task 27.0 in stage 2.0 (TID 32). 4544 bytes result sent to driver
[2021-05-14 16:55:48,016] {docker.py:276} INFO - 21/05/14 19:55:48 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 35) (379a553b2355, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:48,017] {docker.py:276} INFO - 21/05/14 19:55:48 INFO Executor: Running task 30.0 in stage 2.0 (TID 35)
[2021-05-14 16:55:48,017] {docker.py:276} INFO - 21/05/14 19:55:48 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 32) in 1690 ms on 379a553b2355 (executor driver) (27/200)
[2021-05-14 16:55:48,028] {docker.py:276} INFO - 21/05/14 19:55:48 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:48,029] {docker.py:276} INFO - 21/05/14 19:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:48,030] {docker.py:276} INFO - 21/05/14 19:55:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:48,031] {docker.py:276} INFO - 21/05/14 19:55:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:48,031] {docker.py:276} INFO - 21/05/14 19:55:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:48,032] {docker.py:276} INFO - 21/05/14 19:55:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312058726877817395629_0002_m_000030_35, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312058726877817395629_0002_m_000030_35}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312058726877817395629_0002}; taskId=attempt_202105141955312058726877817395629_0002_m_000030_35, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59bf146b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:48,032] {docker.py:276} INFO - 21/05/14 19:55:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:48,032] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Starting: Task committer attempt_202105141955312058726877817395629_0002_m_000030_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312058726877817395629_0002_m_000030_35
[2021-05-14 16:55:48,034] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Task committer attempt_202105141955312058726877817395629_0002_m_000030_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312058726877817395629_0002_m_000030_35 : duration 0:00.003s
[2021-05-14 16:55:48,310] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Starting: Task committer attempt_202105141955314561710279334805432_0002_m_000026_31: needsTaskCommit() Task attempt_202105141955314561710279334805432_0002_m_000026_31
[2021-05-14 16:55:48,312] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Task committer attempt_202105141955314561710279334805432_0002_m_000026_31: needsTaskCommit() Task attempt_202105141955314561710279334805432_0002_m_000026_31: duration 0:00.003s
21/05/14 19:55:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314561710279334805432_0002_m_000026_31
[2021-05-14 16:55:48,313] {docker.py:276} INFO - 21/05/14 19:55:48 INFO Executor: Finished task 26.0 in stage 2.0 (TID 31). 4587 bytes result sent to driver
[2021-05-14 16:55:48,315] {docker.py:276} INFO - 21/05/14 19:55:48 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 36) (379a553b2355, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:48,316] {docker.py:276} INFO - 21/05/14 19:55:48 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 31) in 2116 ms on 379a553b2355 (executor driver) (28/200)
[2021-05-14 16:55:48,316] {docker.py:276} INFO - 21/05/14 19:55:48 INFO Executor: Running task 31.0 in stage 2.0 (TID 36)
[2021-05-14 16:55:48,325] {docker.py:276} INFO - 21/05/14 19:55:48 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:48,340] {docker.py:276} INFO - 21/05/14 19:55:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:48,341] {docker.py:276} INFO - 21/05/14 19:55:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531455491525379853606_0002_m_000031_36, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531455491525379853606_0002_m_000031_36}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531455491525379853606_0002}; taskId=attempt_20210514195531455491525379853606_0002_m_000031_36, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14eef05e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:48 INFO StagingCommitter: Starting: Task committer attempt_20210514195531455491525379853606_0002_m_000031_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531455491525379853606_0002_m_000031_36
[2021-05-14 16:55:48,341] {docker.py:276} INFO - 21/05/14 19:55:48 INFO StagingCommitter: Task committer attempt_20210514195531455491525379853606_0002_m_000031_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531455491525379853606_0002_m_000031_36 : duration 0:00.003s
[2021-05-14 16:55:50,083] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_202105141955312058726877817395629_0002_m_000030_35: needsTaskCommit() Task attempt_202105141955312058726877817395629_0002_m_000030_35
21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_202105141955317155526622060585950_0002_m_000029_34: needsTaskCommit() Task attempt_202105141955317155526622060585950_0002_m_000029_34
21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_202105141955312058726877817395629_0002_m_000030_35: needsTaskCommit() Task attempt_202105141955312058726877817395629_0002_m_000030_35: duration 0:00.003s
21/05/14 19:55:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312058726877817395629_0002_m_000030_35
[2021-05-14 16:55:50,086] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_202105141955317155526622060585950_0002_m_000029_34: needsTaskCommit() Task attempt_202105141955317155526622060585950_0002_m_000029_34: duration 0:00.004s
21/05/14 19:55:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317155526622060585950_0002_m_000029_34
21/05/14 19:55:50 INFO Executor: Finished task 30.0 in stage 2.0 (TID 35). 4544 bytes result sent to driver
21/05/14 19:55:50 INFO Executor: Finished task 29.0 in stage 2.0 (TID 34). 4544 bytes result sent to driver
[2021-05-14 16:55:50,087] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 37) (379a553b2355, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:55:50 INFO Executor: Running task 32.0 in stage 2.0 (TID 37)
[2021-05-14 16:55:50,087] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 38) (379a553b2355, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:50,088] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 34) in 2252 ms on 379a553b2355 (executor driver) (29/200)
[2021-05-14 16:55:50,089] {docker.py:276} INFO - 21/05/14 19:55:50 INFO Executor: Running task 33.0 in stage 2.0 (TID 38)
21/05/14 19:55:50 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 35) in 2075 ms on 379a553b2355 (executor driver) (30/200)
[2021-05-14 16:55:50,098] {docker.py:276} INFO - 21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:50,100] {docker.py:276} INFO - 21/05/14 19:55:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531102549258306529259_0002_m_000032_37, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531102549258306529259_0002_m_000032_37}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531102549258306529259_0002}; taskId=attempt_20210514195531102549258306529259_0002_m_000032_37, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6dc6f369}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_20210514195531102549258306529259_0002_m_000032_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531102549258306529259_0002_m_000032_37
[2021-05-14 16:55:50,101] {docker.py:276} INFO - 21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:50,103] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_20210514195531102549258306529259_0002_m_000032_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531102549258306529259_0002_m_000032_37 : duration 0:00.003s
[2021-05-14 16:55:50,105] {docker.py:276} INFO - 21/05/14 19:55:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314008129125626914619_0002_m_000033_38, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314008129125626914619_0002_m_000033_38}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314008129125626914619_0002}; taskId=attempt_202105141955314008129125626914619_0002_m_000033_38, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26e1c9c3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_202105141955314008129125626914619_0002_m_000033_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314008129125626914619_0002_m_000033_38
[2021-05-14 16:55:50,108] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_202105141955314008129125626914619_0002_m_000033_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314008129125626914619_0002_m_000033_38 : duration 0:00.004s
[2021-05-14 16:55:50,267] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_20210514195531772039913774692022_0002_m_000028_33: needsTaskCommit() Task attempt_20210514195531772039913774692022_0002_m_000028_33
[2021-05-14 16:55:50,268] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_20210514195531772039913774692022_0002_m_000028_33: needsTaskCommit() Task attempt_20210514195531772039913774692022_0002_m_000028_33: duration 0:00.003s
21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_20210514195531455491525379853606_0002_m_000031_36: needsTaskCommit() Task attempt_20210514195531455491525379853606_0002_m_000031_36
21/05/14 19:55:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531772039913774692022_0002_m_000028_33
[2021-05-14 16:55:50,268] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_20210514195531455491525379853606_0002_m_000031_36: needsTaskCommit() Task attempt_20210514195531455491525379853606_0002_m_000031_36: duration 0:00.004s
21/05/14 19:55:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531455491525379853606_0002_m_000031_36
[2021-05-14 16:55:50,270] {docker.py:276} INFO - 21/05/14 19:55:50 INFO Executor: Finished task 28.0 in stage 2.0 (TID 33). 4544 bytes result sent to driver
[2021-05-14 16:55:50,272] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 39) (379a553b2355, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:50,273] {docker.py:276} INFO - 21/05/14 19:55:50 INFO Executor: Finished task 31.0 in stage 2.0 (TID 36). 4544 bytes result sent to driver
[2021-05-14 16:55:50,275] {docker.py:276} INFO - 21/05/14 19:55:50 INFO Executor: Running task 34.0 in stage 2.0 (TID 39)
[2021-05-14 16:55:50,276] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 40) (379a553b2355, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:50,277] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 33) in 2444 ms on 379a553b2355 (executor driver) (31/200)
21/05/14 19:55:50 INFO Executor: Running task 35.0 in stage 2.0 (TID 40)
[2021-05-14 16:55:50,277] {docker.py:276} INFO - 21/05/14 19:55:50 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 36) in 1964 ms on 379a553b2355 (executor driver) (32/200)
[2021-05-14 16:55:50,287] {docker.py:276} INFO - 21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Getting 4 (4.8 KiB) non-empty blocks including 4 (4.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 19:55:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:50,290] {docker.py:276} INFO - 21/05/14 19:55:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315693977159876109756_0002_m_000034_39, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315693977159876109756_0002_m_000034_39}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315693977159876109756_0002}; taskId=attempt_202105141955315693977159876109756_0002_m_000034_39, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b82e9d3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:50,290] {docker.py:276} INFO - 21/05/14 19:55:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:50,291] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_202105141955315693977159876109756_0002_m_000034_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315693977159876109756_0002_m_000034_39 
21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:50,292] {docker.py:276} INFO - 21/05/14 19:55:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051419553166311146624690840_0002_m_000035_40, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553166311146624690840_0002_m_000035_40}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051419553166311146624690840_0002}; taskId=attempt_2021051419553166311146624690840_0002_m_000035_40, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44bebfe9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:50 INFO StagingCommitter: Starting: Task committer attempt_2021051419553166311146624690840_0002_m_000035_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553166311146624690840_0002_m_000035_40
[2021-05-14 16:55:50,295] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_202105141955315693977159876109756_0002_m_000034_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315693977159876109756_0002_m_000034_39 : duration 0:00.004s
[2021-05-14 16:55:50,296] {docker.py:276} INFO - 21/05/14 19:55:50 INFO StagingCommitter: Task committer attempt_2021051419553166311146624690840_0002_m_000035_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553166311146624690840_0002_m_000035_40 : duration 0:00.005s
[2021-05-14 16:55:51,829] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Starting: Task committer attempt_20210514195531102549258306529259_0002_m_000032_37: needsTaskCommit() Task attempt_20210514195531102549258306529259_0002_m_000032_37
[2021-05-14 16:55:51,830] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Task committer attempt_20210514195531102549258306529259_0002_m_000032_37: needsTaskCommit() Task attempt_20210514195531102549258306529259_0002_m_000032_37: duration 0:00.000s
21/05/14 19:55:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531102549258306529259_0002_m_000032_37
[2021-05-14 16:55:51,831] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955314008129125626914619_0002_m_000033_38: needsTaskCommit() Task attempt_202105141955314008129125626914619_0002_m_000033_38
[2021-05-14 16:55:51,832] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Task committer attempt_202105141955314008129125626914619_0002_m_000033_38: needsTaskCommit() Task attempt_202105141955314008129125626914619_0002_m_000033_38: duration 0:00.001s
[2021-05-14 16:55:51,832] {docker.py:276} INFO - 21/05/14 19:55:51 INFO Executor: Finished task 32.0 in stage 2.0 (TID 37). 4544 bytes result sent to driver
[2021-05-14 16:55:51,833] {docker.py:276} INFO - 21/05/14 19:55:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314008129125626914619_0002_m_000033_38
[2021-05-14 16:55:51,834] {docker.py:276} INFO - 21/05/14 19:55:51 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 41) (379a553b2355, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:51,834] {docker.py:276} INFO - 21/05/14 19:55:51 INFO Executor: Finished task 33.0 in stage 2.0 (TID 38). 4544 bytes result sent to driver
[2021-05-14 16:55:51,835] {docker.py:276} INFO - 21/05/14 19:55:51 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 37) in 1752 ms on 379a553b2355 (executor driver) (33/200)
[2021-05-14 16:55:51,835] {docker.py:276} INFO - 21/05/14 19:55:51 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 42) (379a553b2355, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:51,839] {docker.py:276} INFO - 21/05/14 19:55:51 INFO Executor: Running task 37.0 in stage 2.0 (TID 42)
[2021-05-14 16:55:51,840] {docker.py:276} INFO - 21/05/14 19:55:51 INFO Executor: Running task 36.0 in stage 2.0 (TID 41)
21/05/14 19:55:51 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 38) in 1753 ms on 379a553b2355 (executor driver) (34/200)
[2021-05-14 16:55:51,857] {docker.py:276} INFO - 21/05/14 19:55:51 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:51,857] {docker.py:276} INFO - 21/05/14 19:55:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:51,858] {docker.py:276} INFO - 21/05/14 19:55:51 INFO ShuffleBlockFetcherIterator: Getting 4 (6.5 KiB) non-empty blocks including 4 (6.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:51,858] {docker.py:276} INFO - 21/05/14 19:55:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 16:55:51,861] {docker.py:276} INFO - 21/05/14 19:55:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:51,861] {docker.py:276} INFO - 21/05/14 19:55:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318556702571293487560_0002_m_000037_42, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318556702571293487560_0002_m_000037_42}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318556702571293487560_0002}; taskId=attempt_202105141955318556702571293487560_0002_m_000037_42, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@244e33c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:51,862] {docker.py:276} INFO - 21/05/14 19:55:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955318556702571293487560_0002_m_000037_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318556702571293487560_0002_m_000037_42
[2021-05-14 16:55:51,863] {docker.py:276} INFO - 21/05/14 19:55:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:51,863] {docker.py:276} INFO - 21/05/14 19:55:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:51,864] {docker.py:276} INFO - 21/05/14 19:55:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:51,864] {docker.py:276} INFO - 21/05/14 19:55:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312663731114493950330_0002_m_000036_41, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312663731114493950330_0002_m_000036_41}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312663731114493950330_0002}; taskId=attempt_202105141955312663731114493950330_0002_m_000036_41, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4be82c64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:51,865] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955312663731114493950330_0002_m_000036_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312663731114493950330_0002_m_000036_41
[2021-05-14 16:55:51,866] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Task committer attempt_202105141955318556702571293487560_0002_m_000037_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318556702571293487560_0002_m_000037_42 : duration 0:00.005s
[2021-05-14 16:55:51,871] {docker.py:276} INFO - 21/05/14 19:55:51 INFO StagingCommitter: Task committer attempt_202105141955312663731114493950330_0002_m_000036_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312663731114493950330_0002_m_000036_41 : duration 0:00.006s
[2021-05-14 16:55:52,135] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955315693977159876109756_0002_m_000034_39: needsTaskCommit() Task attempt_202105141955315693977159876109756_0002_m_000034_39
[2021-05-14 16:55:52,155] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Task committer attempt_202105141955315693977159876109756_0002_m_000034_39: needsTaskCommit() Task attempt_202105141955315693977159876109756_0002_m_000034_39: duration 0:00.001s
21/05/14 19:55:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315693977159876109756_0002_m_000034_39
[2021-05-14 16:55:52,155] {docker.py:276} INFO - 21/05/14 19:55:52 INFO Executor: Finished task 34.0 in stage 2.0 (TID 39). 4587 bytes result sent to driver
[2021-05-14 16:55:52,156] {docker.py:276} INFO - 21/05/14 19:55:52 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 43) (379a553b2355, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:52,156] {docker.py:276} INFO - 21/05/14 19:55:52 INFO Executor: Running task 38.0 in stage 2.0 (TID 43)
[2021-05-14 16:55:52,156] {docker.py:276} INFO - 21/05/14 19:55:52 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 39) in 1872 ms on 379a553b2355 (executor driver) (35/200)
[2021-05-14 16:55:52,156] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Starting: Task committer attempt_2021051419553166311146624690840_0002_m_000035_40: needsTaskCommit() Task attempt_2021051419553166311146624690840_0002_m_000035_40
21/05/14 19:55:52 INFO StagingCommitter: Task committer attempt_2021051419553166311146624690840_0002_m_000035_40: needsTaskCommit() Task attempt_2021051419553166311146624690840_0002_m_000035_40: duration 0:00.001s
21/05/14 19:55:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051419553166311146624690840_0002_m_000035_40
[2021-05-14 16:55:52,157] {docker.py:276} INFO - 21/05/14 19:55:52 INFO Executor: Finished task 35.0 in stage 2.0 (TID 40). 4587 bytes result sent to driver
[2021-05-14 16:55:52,157] {docker.py:276} INFO - 21/05/14 19:55:52 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 44) (379a553b2355, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:52,157] {docker.py:276} INFO - 21/05/14 19:55:52 INFO Executor: Running task 39.0 in stage 2.0 (TID 44)
21/05/14 19:55:52 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 40) in 1878 ms on 379a553b2355 (executor driver) (36/200)
[2021-05-14 16:55:52,158] {docker.py:276} INFO - 21/05/14 19:55:52 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:55:52,158] {docker.py:276} INFO - 21/05/14 19:55:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:52,158] {docker.py:276} INFO - 21/05/14 19:55:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312741496951260355648_0002_m_000038_43, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312741496951260355648_0002_m_000038_43}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312741496951260355648_0002}; taskId=attempt_202105141955312741496951260355648_0002_m_000038_43, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44fc9a08}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:52,158] {docker.py:276} INFO - 21/05/14 19:55:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:52,158] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955312741496951260355648_0002_m_000038_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312741496951260355648_0002_m_000038_43
[2021-05-14 16:55:52,159] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Task committer attempt_202105141955312741496951260355648_0002_m_000038_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312741496951260355648_0002_m_000038_43 : duration 0:00.003s
[2021-05-14 16:55:52,162] {docker.py:276} INFO - 21/05/14 19:55:52 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:52,165] {docker.py:276} INFO - 21/05/14 19:55:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:52,165] {docker.py:276} INFO - 21/05/14 19:55:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312514043293012582588_0002_m_000039_44, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312514043293012582588_0002_m_000039_44}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312514043293012582588_0002}; taskId=attempt_202105141955312514043293012582588_0002_m_000039_44, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@179816bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:52,166] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955312514043293012582588_0002_m_000039_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312514043293012582588_0002_m_000039_44
[2021-05-14 16:55:52,169] {docker.py:276} INFO - 21/05/14 19:55:52 INFO StagingCommitter: Task committer attempt_202105141955312514043293012582588_0002_m_000039_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312514043293012582588_0002_m_000039_44 : duration 0:00.003s
[2021-05-14 16:55:53,565] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955312663731114493950330_0002_m_000036_41: needsTaskCommit() Task attempt_202105141955312663731114493950330_0002_m_000036_41
21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955312663731114493950330_0002_m_000036_41: needsTaskCommit() Task attempt_202105141955312663731114493950330_0002_m_000036_41: duration 0:00.000s
[2021-05-14 16:55:53,566] {docker.py:276} INFO - 21/05/14 19:55:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312663731114493950330_0002_m_000036_41
[2021-05-14 16:55:53,569] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Finished task 36.0 in stage 2.0 (TID 41). 4587 bytes result sent to driver
[2021-05-14 16:55:53,572] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 45) (379a553b2355, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:53,573] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 41) in 1743 ms on 379a553b2355 (executor driver) (37/200)
21/05/14 19:55:53 INFO Executor: Running task 40.0 in stage 2.0 (TID 45)
[2021-05-14 16:55:53,584] {docker.py:276} INFO - 21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:53,586] {docker.py:276} INFO - 21/05/14 19:55:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:53,586] {docker.py:276} INFO - 21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:53,587] {docker.py:276} INFO - 21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317089135116339566794_0002_m_000040_45, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317089135116339566794_0002_m_000040_45}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317089135116339566794_0002}; taskId=attempt_202105141955317089135116339566794_0002_m_000040_45, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11c06b82}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:53,587] {docker.py:276} INFO - 21/05/14 19:55:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955317089135116339566794_0002_m_000040_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317089135116339566794_0002_m_000040_45
[2021-05-14 16:55:53,591] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955318556702571293487560_0002_m_000037_42: needsTaskCommit() Task attempt_202105141955318556702571293487560_0002_m_000037_42
[2021-05-14 16:55:53,592] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955318556702571293487560_0002_m_000037_42: needsTaskCommit() Task attempt_202105141955318556702571293487560_0002_m_000037_42: duration 0:00.000s
21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955317089135116339566794_0002_m_000040_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317089135116339566794_0002_m_000040_45 : duration 0:00.005s
21/05/14 19:55:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318556702571293487560_0002_m_000037_42
[2021-05-14 16:55:53,594] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Finished task 37.0 in stage 2.0 (TID 42). 4587 bytes result sent to driver
[2021-05-14 16:55:53,595] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 46) (379a553b2355, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:53,596] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Running task 41.0 in stage 2.0 (TID 46)
[2021-05-14 16:55:53,597] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 42) in 1763 ms on 379a553b2355 (executor driver) (38/200)
[2021-05-14 16:55:53,605] {docker.py:276} INFO - 21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:53,607] {docker.py:276} INFO - 21/05/14 19:55:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:53,608] {docker.py:276} INFO - 21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:53,608] {docker.py:276} INFO - 21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311274986189158790777_0002_m_000041_46, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311274986189158790777_0002_m_000041_46}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311274986189158790777_0002}; taskId=attempt_202105141955311274986189158790777_0002_m_000041_46, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b06196d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:53,608] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955311274986189158790777_0002_m_000041_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311274986189158790777_0002_m_000041_46
[2021-05-14 16:55:53,611] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955311274986189158790777_0002_m_000041_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311274986189158790777_0002_m_000041_46 : duration 0:00.003s
[2021-05-14 16:55:53,701] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955312514043293012582588_0002_m_000039_44: needsTaskCommit() Task attempt_202105141955312514043293012582588_0002_m_000039_44
[2021-05-14 16:55:53,702] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955312514043293012582588_0002_m_000039_44: needsTaskCommit() Task attempt_202105141955312514043293012582588_0002_m_000039_44: duration 0:00.001s
[2021-05-14 16:55:53,703] {docker.py:276} INFO - 21/05/14 19:55:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312514043293012582588_0002_m_000039_44
[2021-05-14 16:55:53,704] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Finished task 39.0 in stage 2.0 (TID 44). 4544 bytes result sent to driver
[2021-05-14 16:55:53,705] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 47) (379a553b2355, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:53,707] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 44) in 1559 ms on 379a553b2355 (executor driver) (39/200)
[2021-05-14 16:55:53,707] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Running task 42.0 in stage 2.0 (TID 47)
[2021-05-14 16:55:53,717] {docker.py:276} INFO - 21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:53,719] {docker.py:276} INFO - 21/05/14 19:55:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:53,720] {docker.py:276} INFO - 21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311376352761697824897_0002_m_000042_47, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311376352761697824897_0002_m_000042_47}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311376352761697824897_0002}; taskId=attempt_202105141955311376352761697824897_0002_m_000042_47, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32929b15}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955311376352761697824897_0002_m_000042_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311376352761697824897_0002_m_000042_47
[2021-05-14 16:55:53,722] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955311376352761697824897_0002_m_000042_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311376352761697824897_0002_m_000042_47 : duration 0:00.003s
[2021-05-14 16:55:53,745] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955312741496951260355648_0002_m_000038_43: needsTaskCommit() Task attempt_202105141955312741496951260355648_0002_m_000038_43
[2021-05-14 16:55:53,745] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955312741496951260355648_0002_m_000038_43: needsTaskCommit() Task attempt_202105141955312741496951260355648_0002_m_000038_43: duration 0:00.000s
[2021-05-14 16:55:53,746] {docker.py:276} INFO - 21/05/14 19:55:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312741496951260355648_0002_m_000038_43
[2021-05-14 16:55:53,747] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Finished task 38.0 in stage 2.0 (TID 43). 4544 bytes result sent to driver
[2021-05-14 16:55:53,748] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 48) (379a553b2355, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:53,749] {docker.py:276} INFO - 21/05/14 19:55:53 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 43) in 1612 ms on 379a553b2355 (executor driver) (40/200)
[2021-05-14 16:55:53,749] {docker.py:276} INFO - 21/05/14 19:55:53 INFO Executor: Running task 43.0 in stage 2.0 (TID 48)
[2021-05-14 16:55:53,758] {docker.py:276} INFO - 21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:53,761] {docker.py:276} INFO - 21/05/14 19:55:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:53,762] {docker.py:276} INFO - 21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314729211928560307357_0002_m_000043_48, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314729211928560307357_0002_m_000043_48}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314729211928560307357_0002}; taskId=attempt_202105141955314729211928560307357_0002_m_000043_48, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6dbc5a37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:53,762] {docker.py:276} INFO - 21/05/14 19:55:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:53,762] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955314729211928560307357_0002_m_000043_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314729211928560307357_0002_m_000043_48
[2021-05-14 16:55:53,765] {docker.py:276} INFO - 21/05/14 19:55:53 INFO StagingCommitter: Task committer attempt_202105141955314729211928560307357_0002_m_000043_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314729211928560307357_0002_m_000043_48 : duration 0:00.003s
[2021-05-14 16:55:54,777] {docker.py:276} INFO - 21/05/14 19:55:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955317089135116339566794_0002_m_000040_45: needsTaskCommit() Task attempt_202105141955317089135116339566794_0002_m_000040_45
[2021-05-14 16:55:54,777] {docker.py:276} INFO - 21/05/14 19:55:54 INFO StagingCommitter: Task committer attempt_202105141955317089135116339566794_0002_m_000040_45: needsTaskCommit() Task attempt_202105141955317089135116339566794_0002_m_000040_45: duration 0:00.000s
[2021-05-14 16:55:54,778] {docker.py:276} INFO - 21/05/14 19:55:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317089135116339566794_0002_m_000040_45
[2021-05-14 16:55:54,780] {docker.py:276} INFO - 21/05/14 19:55:54 INFO Executor: Finished task 40.0 in stage 2.0 (TID 45). 4544 bytes result sent to driver
[2021-05-14 16:55:54,780] {docker.py:276} INFO - 21/05/14 19:55:54 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 49) (379a553b2355, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:54,781] {docker.py:276} INFO - 21/05/14 19:55:54 INFO Executor: Running task 44.0 in stage 2.0 (TID 49)
[2021-05-14 16:55:54,781] {docker.py:276} INFO - 21/05/14 19:55:54 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 45) in 1212 ms on 379a553b2355 (executor driver) (41/200)
[2021-05-14 16:55:54,790] {docker.py:276} INFO - 21/05/14 19:55:54 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:54,792] {docker.py:276} INFO - 21/05/14 19:55:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:54,792] {docker.py:276} INFO - 21/05/14 19:55:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315143055898468202170_0002_m_000044_49, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315143055898468202170_0002_m_000044_49}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315143055898468202170_0002}; taskId=attempt_202105141955315143055898468202170_0002_m_000044_49, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4cc85cae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:54,793] {docker.py:276} INFO - 21/05/14 19:55:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955315143055898468202170_0002_m_000044_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315143055898468202170_0002_m_000044_49
[2021-05-14 16:55:54,795] {docker.py:276} INFO - 21/05/14 19:55:54 INFO StagingCommitter: Task committer attempt_202105141955315143055898468202170_0002_m_000044_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315143055898468202170_0002_m_000044_49 : duration 0:00.003s
[2021-05-14 16:55:55,175] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955311274986189158790777_0002_m_000041_46: needsTaskCommit() Task attempt_202105141955311274986189158790777_0002_m_000041_46
[2021-05-14 16:55:55,176] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Task committer attempt_202105141955311274986189158790777_0002_m_000041_46: needsTaskCommit() Task attempt_202105141955311274986189158790777_0002_m_000041_46: duration 0:00.001s
21/05/14 19:55:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311274986189158790777_0002_m_000041_46
[2021-05-14 16:55:55,179] {docker.py:276} INFO - 21/05/14 19:55:55 INFO Executor: Finished task 41.0 in stage 2.0 (TID 46). 4544 bytes result sent to driver
[2021-05-14 16:55:55,181] {docker.py:276} INFO - 21/05/14 19:55:55 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 50) (379a553b2355, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:55,182] {docker.py:276} INFO - 21/05/14 19:55:55 INFO Executor: Running task 45.0 in stage 2.0 (TID 50)
[2021-05-14 16:55:55,183] {docker.py:276} INFO - 21/05/14 19:55:55 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 46) in 1589 ms on 379a553b2355 (executor driver) (42/200)
[2021-05-14 16:55:55,193] {docker.py:276} INFO - 21/05/14 19:55:55 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:55,194] {docker.py:276} INFO - 21/05/14 19:55:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:55,196] {docker.py:276} INFO - 21/05/14 19:55:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:55,196] {docker.py:276} INFO - 21/05/14 19:55:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313567265815761692125_0002_m_000045_50, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313567265815761692125_0002_m_000045_50}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313567265815761692125_0002}; taskId=attempt_202105141955313567265815761692125_0002_m_000045_50, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a46f3cd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:55,196] {docker.py:276} INFO - 21/05/14 19:55:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955313567265815761692125_0002_m_000045_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313567265815761692125_0002_m_000045_50
[2021-05-14 16:55:55,200] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Task committer attempt_202105141955313567265815761692125_0002_m_000045_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313567265815761692125_0002_m_000045_50 : duration 0:00.003s
[2021-05-14 16:55:55,263] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955311376352761697824897_0002_m_000042_47: needsTaskCommit() Task attempt_202105141955311376352761697824897_0002_m_000042_47
[2021-05-14 16:55:55,263] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Task committer attempt_202105141955311376352761697824897_0002_m_000042_47: needsTaskCommit() Task attempt_202105141955311376352761697824897_0002_m_000042_47: duration 0:00.001s
[2021-05-14 16:55:55,264] {docker.py:276} INFO - 21/05/14 19:55:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311376352761697824897_0002_m_000042_47
[2021-05-14 16:55:55,265] {docker.py:276} INFO - 21/05/14 19:55:55 INFO Executor: Finished task 42.0 in stage 2.0 (TID 47). 4544 bytes result sent to driver
[2021-05-14 16:55:55,266] {docker.py:276} INFO - 21/05/14 19:55:55 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 51) (379a553b2355, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:55,266] {docker.py:276} INFO - 21/05/14 19:55:55 INFO Executor: Running task 46.0 in stage 2.0 (TID 51)
[2021-05-14 16:55:55,267] {docker.py:276} INFO - 21/05/14 19:55:55 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 47) in 1563 ms on 379a553b2355 (executor driver) (43/200)
[2021-05-14 16:55:55,274] {docker.py:276} INFO - 21/05/14 19:55:55 INFO ShuffleBlockFetcherIterator: Getting 4 (6.2 KiB) non-empty blocks including 4 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:55,276] {docker.py:276} INFO - 21/05/14 19:55:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311835467078197575613_0002_m_000046_51, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311835467078197575613_0002_m_000046_51}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311835467078197575613_0002}; taskId=attempt_202105141955311835467078197575613_0002_m_000046_51, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40b5b496}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:55,277] {docker.py:276} INFO - 21/05/14 19:55:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955311835467078197575613_0002_m_000046_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311835467078197575613_0002_m_000046_51
[2021-05-14 16:55:55,279] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Task committer attempt_202105141955311835467078197575613_0002_m_000046_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311835467078197575613_0002_m_000046_51 : duration 0:00.003s
[2021-05-14 16:55:55,897] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955314729211928560307357_0002_m_000043_48: needsTaskCommit() Task attempt_202105141955314729211928560307357_0002_m_000043_48
[2021-05-14 16:55:55,899] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Task committer attempt_202105141955314729211928560307357_0002_m_000043_48: needsTaskCommit() Task attempt_202105141955314729211928560307357_0002_m_000043_48: duration 0:00.001s
21/05/14 19:55:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314729211928560307357_0002_m_000043_48
[2021-05-14 16:55:55,901] {docker.py:276} INFO - 21/05/14 19:55:55 INFO Executor: Finished task 43.0 in stage 2.0 (TID 48). 4544 bytes result sent to driver
[2021-05-14 16:55:55,903] {docker.py:276} INFO - 21/05/14 19:55:55 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 52) (379a553b2355, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:55,904] {docker.py:276} INFO - 21/05/14 19:55:55 INFO Executor: Running task 47.0 in stage 2.0 (TID 52)
[2021-05-14 16:55:55,905] {docker.py:276} INFO - 21/05/14 19:55:55 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 48) in 2158 ms on 379a553b2355 (executor driver) (44/200)
[2021-05-14 16:55:55,917] {docker.py:276} INFO - 21/05/14 19:55:55 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:55,917] {docker.py:276} INFO - 21/05/14 19:55:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:55,919] {docker.py:276} INFO - 21/05/14 19:55:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:55,920] {docker.py:276} INFO - 21/05/14 19:55:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:55,920] {docker.py:276} INFO - 21/05/14 19:55:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312800223703272446653_0002_m_000047_52, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312800223703272446653_0002_m_000047_52}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312800223703272446653_0002}; taskId=attempt_202105141955312800223703272446653_0002_m_000047_52, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@360f25d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:55,921] {docker.py:276} INFO - 21/05/14 19:55:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:55,921] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955312800223703272446653_0002_m_000047_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312800223703272446653_0002_m_000047_52
[2021-05-14 16:55:55,923] {docker.py:276} INFO - 21/05/14 19:55:55 INFO StagingCommitter: Task committer attempt_202105141955312800223703272446653_0002_m_000047_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312800223703272446653_0002_m_000047_52 : duration 0:00.002s
[2021-05-14 16:55:56,481] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Starting: Task committer attempt_202105141955315143055898468202170_0002_m_000044_49: needsTaskCommit() Task attempt_202105141955315143055898468202170_0002_m_000044_49
[2021-05-14 16:55:56,481] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Task committer attempt_202105141955315143055898468202170_0002_m_000044_49: needsTaskCommit() Task attempt_202105141955315143055898468202170_0002_m_000044_49: duration 0:00.000s
21/05/14 19:55:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315143055898468202170_0002_m_000044_49
[2021-05-14 16:55:56,483] {docker.py:276} INFO - 21/05/14 19:55:56 INFO Executor: Finished task 44.0 in stage 2.0 (TID 49). 4544 bytes result sent to driver
[2021-05-14 16:55:56,484] {docker.py:276} INFO - 21/05/14 19:55:56 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 53) (379a553b2355, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:56,485] {docker.py:276} INFO - 21/05/14 19:55:56 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 49) in 1707 ms on 379a553b2355 (executor driver) (45/200)
[2021-05-14 16:55:56,486] {docker.py:276} INFO - 21/05/14 19:55:56 INFO Executor: Running task 48.0 in stage 2.0 (TID 53)
[2021-05-14 16:55:56,494] {docker.py:276} INFO - 21/05/14 19:55:56 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:56,496] {docker.py:276} INFO - 21/05/14 19:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:56,497] {docker.py:276} INFO - 21/05/14 19:55:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314059773090393484717_0002_m_000048_53, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314059773090393484717_0002_m_000048_53}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314059773090393484717_0002}; taskId=attempt_202105141955314059773090393484717_0002_m_000048_53, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d57aeaa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:56,497] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Starting: Task committer attempt_202105141955314059773090393484717_0002_m_000048_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314059773090393484717_0002_m_000048_53
[2021-05-14 16:55:56,500] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Task committer attempt_202105141955314059773090393484717_0002_m_000048_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314059773090393484717_0002_m_000048_53 : duration 0:00.003s
[2021-05-14 16:55:56,809] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Starting: Task committer attempt_202105141955313567265815761692125_0002_m_000045_50: needsTaskCommit() Task attempt_202105141955313567265815761692125_0002_m_000045_50
[2021-05-14 16:55:56,810] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Task committer attempt_202105141955313567265815761692125_0002_m_000045_50: needsTaskCommit() Task attempt_202105141955313567265815761692125_0002_m_000045_50: duration 0:00.001s
[2021-05-14 16:55:56,811] {docker.py:276} INFO - 21/05/14 19:55:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313567265815761692125_0002_m_000045_50
[2021-05-14 16:55:56,813] {docker.py:276} INFO - 21/05/14 19:55:56 INFO Executor: Finished task 45.0 in stage 2.0 (TID 50). 4544 bytes result sent to driver
[2021-05-14 16:55:56,814] {docker.py:276} INFO - 21/05/14 19:55:56 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 54) (379a553b2355, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:56,815] {docker.py:276} INFO - 21/05/14 19:55:56 INFO Executor: Running task 49.0 in stage 2.0 (TID 54)
[2021-05-14 16:55:56,816] {docker.py:276} INFO - 21/05/14 19:55:56 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 50) in 1637 ms on 379a553b2355 (executor driver) (46/200)
[2021-05-14 16:55:56,837] {docker.py:276} INFO - 21/05/14 19:55:56 INFO ShuffleBlockFetcherIterator: Getting 4 (6.4 KiB) non-empty blocks including 4 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:56,839] {docker.py:276} INFO - 21/05/14 19:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:56,840] {docker.py:276} INFO - 21/05/14 19:55:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:56,840] {docker.py:276} INFO - 21/05/14 19:55:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312208724222140953264_0002_m_000049_54, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312208724222140953264_0002_m_000049_54}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312208724222140953264_0002}; taskId=attempt_202105141955312208724222140953264_0002_m_000049_54, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@184b4c8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:56,841] {docker.py:276} INFO - 21/05/14 19:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:56,841] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Starting: Task committer attempt_202105141955312208724222140953264_0002_m_000049_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312208724222140953264_0002_m_000049_54
[2021-05-14 16:55:56,844] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Task committer attempt_202105141955312208724222140953264_0002_m_000049_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312208724222140953264_0002_m_000049_54 : duration 0:00.003s
[2021-05-14 16:55:56,851] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Starting: Task committer attempt_202105141955311835467078197575613_0002_m_000046_51: needsTaskCommit() Task attempt_202105141955311835467078197575613_0002_m_000046_51
[2021-05-14 16:55:56,852] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Task committer attempt_202105141955311835467078197575613_0002_m_000046_51: needsTaskCommit() Task attempt_202105141955311835467078197575613_0002_m_000046_51: duration 0:00.000s
21/05/14 19:55:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311835467078197575613_0002_m_000046_51
[2021-05-14 16:55:56,853] {docker.py:276} INFO - 21/05/14 19:55:56 INFO Executor: Finished task 46.0 in stage 2.0 (TID 51). 4587 bytes result sent to driver
[2021-05-14 16:55:56,854] {docker.py:276} INFO - 21/05/14 19:55:56 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 55) (379a553b2355, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:56,855] {docker.py:276} INFO - 21/05/14 19:55:56 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 51) in 1591 ms on 379a553b2355 (executor driver) (47/200)
21/05/14 19:55:56 INFO Executor: Running task 50.0 in stage 2.0 (TID 55)
[2021-05-14 16:55:56,863] {docker.py:276} INFO - 21/05/14 19:55:56 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:56,865] {docker.py:276} INFO - 21/05/14 19:55:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:56,866] {docker.py:276} INFO - 21/05/14 19:55:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318115852378696671560_0002_m_000050_55, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318115852378696671560_0002_m_000050_55}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318115852378696671560_0002}; taskId=attempt_202105141955318115852378696671560_0002_m_000050_55, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e7304ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:56 INFO StagingCommitter: Starting: Task committer attempt_202105141955318115852378696671560_0002_m_000050_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318115852378696671560_0002_m_000050_55
[2021-05-14 16:55:56,869] {docker.py:276} INFO - 21/05/14 19:55:56 INFO StagingCommitter: Task committer attempt_202105141955318115852378696671560_0002_m_000050_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318115852378696671560_0002_m_000050_55 : duration 0:00.003s
[2021-05-14 16:55:57,567] {docker.py:276} INFO - 21/05/14 19:55:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955312800223703272446653_0002_m_000047_52: needsTaskCommit() Task attempt_202105141955312800223703272446653_0002_m_000047_52
21/05/14 19:55:57 INFO StagingCommitter: Task committer attempt_202105141955312800223703272446653_0002_m_000047_52: needsTaskCommit() Task attempt_202105141955312800223703272446653_0002_m_000047_52: duration 0:00.001s
21/05/14 19:55:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312800223703272446653_0002_m_000047_52
[2021-05-14 16:55:57,569] {docker.py:276} INFO - 21/05/14 19:55:57 INFO Executor: Finished task 47.0 in stage 2.0 (TID 52). 4587 bytes result sent to driver
[2021-05-14 16:55:57,572] {docker.py:276} INFO - 21/05/14 19:55:57 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 56) (379a553b2355, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:57,574] {docker.py:276} INFO - 21/05/14 19:55:57 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 52) in 1672 ms on 379a553b2355 (executor driver) (48/200)
21/05/14 19:55:57 INFO Executor: Running task 51.0 in stage 2.0 (TID 56)
[2021-05-14 16:55:57,585] {docker.py:276} INFO - 21/05/14 19:55:57 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:57,588] {docker.py:276} INFO - 21/05/14 19:55:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318717035955673658065_0002_m_000051_56, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318717035955673658065_0002_m_000051_56}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318717035955673658065_0002}; taskId=attempt_202105141955318717035955673658065_0002_m_000051_56, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7052a843}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955318717035955673658065_0002_m_000051_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318717035955673658065_0002_m_000051_56
[2021-05-14 16:55:57,591] {docker.py:276} INFO - 21/05/14 19:55:57 INFO StagingCommitter: Task committer attempt_202105141955318717035955673658065_0002_m_000051_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318717035955673658065_0002_m_000051_56 : duration 0:00.003s
[2021-05-14 16:55:58,163] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955314059773090393484717_0002_m_000048_53: needsTaskCommit() Task attempt_202105141955314059773090393484717_0002_m_000048_53
[2021-05-14 16:55:58,163] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Task committer attempt_202105141955314059773090393484717_0002_m_000048_53: needsTaskCommit() Task attempt_202105141955314059773090393484717_0002_m_000048_53: duration 0:00.001s
21/05/14 19:55:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314059773090393484717_0002_m_000048_53
[2021-05-14 16:55:58,164] {docker.py:276} INFO - 21/05/14 19:55:58 INFO Executor: Finished task 48.0 in stage 2.0 (TID 53). 4587 bytes result sent to driver
[2021-05-14 16:55:58,166] {docker.py:276} INFO - 21/05/14 19:55:58 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 57) (379a553b2355, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:58,167] {docker.py:276} INFO - 21/05/14 19:55:58 INFO Executor: Running task 52.0 in stage 2.0 (TID 57)
21/05/14 19:55:58 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 53) in 1685 ms on 379a553b2355 (executor driver) (49/200)
[2021-05-14 16:55:58,176] {docker.py:276} INFO - 21/05/14 19:55:58 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:58,178] {docker.py:276} INFO - 21/05/14 19:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:58,179] {docker.py:276} INFO - 21/05/14 19:55:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531615260326157311184_0002_m_000052_57, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531615260326157311184_0002_m_000052_57}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531615260326157311184_0002}; taskId=attempt_20210514195531615260326157311184_0002_m_000052_57, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38f28865}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:55:58 INFO StagingCommitter: Starting: Task committer attempt_20210514195531615260326157311184_0002_m_000052_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531615260326157311184_0002_m_000052_57
[2021-05-14 16:55:58,182] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Task committer attempt_20210514195531615260326157311184_0002_m_000052_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531615260326157311184_0002_m_000052_57 : duration 0:00.004s
[2021-05-14 16:55:58,304] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955312208724222140953264_0002_m_000049_54: needsTaskCommit() Task attempt_202105141955312208724222140953264_0002_m_000049_54
[2021-05-14 16:55:58,304] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Task committer attempt_202105141955312208724222140953264_0002_m_000049_54: needsTaskCommit() Task attempt_202105141955312208724222140953264_0002_m_000049_54: duration 0:00.000s
21/05/14 19:55:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312208724222140953264_0002_m_000049_54
[2021-05-14 16:55:58,306] {docker.py:276} INFO - 21/05/14 19:55:58 INFO Executor: Finished task 49.0 in stage 2.0 (TID 54). 4587 bytes result sent to driver
[2021-05-14 16:55:58,307] {docker.py:276} INFO - 21/05/14 19:55:58 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 58) (379a553b2355, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:58,308] {docker.py:276} INFO - 21/05/14 19:55:58 INFO Executor: Running task 53.0 in stage 2.0 (TID 58)
21/05/14 19:55:58 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 54) in 1497 ms on 379a553b2355 (executor driver) (50/200)
[2021-05-14 16:55:58,319] {docker.py:276} INFO - 21/05/14 19:55:58 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:58,319] {docker.py:276} INFO - 21/05/14 19:55:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:58,321] {docker.py:276} INFO - 21/05/14 19:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:55:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315166856564383057639_0002_m_000053_58, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315166856564383057639_0002_m_000053_58}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315166856564383057639_0002}; taskId=attempt_202105141955315166856564383057639_0002_m_000053_58, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e5cbdfd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:58,322] {docker.py:276} INFO - 21/05/14 19:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:58,322] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955315166856564383057639_0002_m_000053_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315166856564383057639_0002_m_000053_58
[2021-05-14 16:55:58,325] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Task committer attempt_202105141955315166856564383057639_0002_m_000053_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315166856564383057639_0002_m_000053_58 : duration 0:00.003s
[2021-05-14 16:55:58,470] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955318115852378696671560_0002_m_000050_55: needsTaskCommit() Task attempt_202105141955318115852378696671560_0002_m_000050_55
[2021-05-14 16:55:58,471] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Task committer attempt_202105141955318115852378696671560_0002_m_000050_55: needsTaskCommit() Task attempt_202105141955318115852378696671560_0002_m_000050_55: duration 0:00.001s
21/05/14 19:55:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318115852378696671560_0002_m_000050_55
[2021-05-14 16:55:58,478] {docker.py:276} INFO - 21/05/14 19:55:58 INFO Executor: Finished task 50.0 in stage 2.0 (TID 55). 4544 bytes result sent to driver
[2021-05-14 16:55:58,479] {docker.py:276} INFO - 21/05/14 19:55:58 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 59) (379a553b2355, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:58,479] {docker.py:276} INFO - 21/05/14 19:55:58 INFO Executor: Running task 54.0 in stage 2.0 (TID 59)
[2021-05-14 16:55:58,484] {docker.py:276} INFO - 21/05/14 19:55:58 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 55) in 1624 ms on 379a553b2355 (executor driver) (51/200)
[2021-05-14 16:55:58,495] {docker.py:276} INFO - 21/05/14 19:55:58 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:58,496] {docker.py:276} INFO - 21/05/14 19:55:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:58,505] {docker.py:276} INFO - 21/05/14 19:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:58,506] {docker.py:276} INFO - 21/05/14 19:55:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:58,506] {docker.py:276} INFO - 21/05/14 19:55:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312337970911551798485_0002_m_000054_59, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312337970911551798485_0002_m_000054_59}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312337970911551798485_0002}; taskId=attempt_202105141955312337970911551798485_0002_m_000054_59, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8af8fcc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:58,507] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955312337970911551798485_0002_m_000054_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312337970911551798485_0002_m_000054_59
[2021-05-14 16:55:58,507] {docker.py:276} INFO - 21/05/14 19:55:58 INFO StagingCommitter: Task committer attempt_202105141955312337970911551798485_0002_m_000054_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312337970911551798485_0002_m_000054_59 : duration 0:00.004s
[2021-05-14 16:55:59,208] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Starting: Task committer attempt_202105141955318717035955673658065_0002_m_000051_56: needsTaskCommit() Task attempt_202105141955318717035955673658065_0002_m_000051_56
[2021-05-14 16:55:59,209] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Task committer attempt_202105141955318717035955673658065_0002_m_000051_56: needsTaskCommit() Task attempt_202105141955318717035955673658065_0002_m_000051_56: duration 0:00.000s
[2021-05-14 16:55:59,209] {docker.py:276} INFO - 21/05/14 19:55:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318717035955673658065_0002_m_000051_56
[2021-05-14 16:55:59,210] {docker.py:276} INFO - 21/05/14 19:55:59 INFO Executor: Finished task 51.0 in stage 2.0 (TID 56). 4544 bytes result sent to driver
[2021-05-14 16:55:59,211] {docker.py:276} INFO - 21/05/14 19:55:59 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 60) (379a553b2355, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:59,212] {docker.py:276} INFO - 21/05/14 19:55:59 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 56) in 1644 ms on 379a553b2355 (executor driver) (52/200)
[2021-05-14 16:55:59,212] {docker.py:276} INFO - 21/05/14 19:55:59 INFO Executor: Running task 55.0 in stage 2.0 (TID 60)
[2021-05-14 16:55:59,221] {docker.py:276} INFO - 21/05/14 19:55:59 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:55:59,221] {docker.py:276} INFO - 21/05/14 19:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:59,224] {docker.py:276} INFO - 21/05/14 19:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:55:59,224] {docker.py:276} INFO - 21/05/14 19:55:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:59,224] {docker.py:276} INFO - 21/05/14 19:55:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:59,225] {docker.py:276} INFO - 21/05/14 19:55:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315389540128603399417_0002_m_000055_60, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315389540128603399417_0002_m_000055_60}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315389540128603399417_0002}; taskId=attempt_202105141955315389540128603399417_0002_m_000055_60, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70f38911}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:59,225] {docker.py:276} INFO - 21/05/14 19:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:59,225] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Starting: Task committer attempt_202105141955315389540128603399417_0002_m_000055_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315389540128603399417_0002_m_000055_60
[2021-05-14 16:55:59,227] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Task committer attempt_202105141955315389540128603399417_0002_m_000055_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315389540128603399417_0002_m_000055_60 : duration 0:00.002s
[2021-05-14 16:55:59,837] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Starting: Task committer attempt_20210514195531615260326157311184_0002_m_000052_57: needsTaskCommit() Task attempt_20210514195531615260326157311184_0002_m_000052_57
21/05/14 19:55:59 INFO StagingCommitter: Task committer attempt_20210514195531615260326157311184_0002_m_000052_57: needsTaskCommit() Task attempt_20210514195531615260326157311184_0002_m_000052_57: duration 0:00.000s
21/05/14 19:55:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531615260326157311184_0002_m_000052_57
[2021-05-14 16:55:59,839] {docker.py:276} INFO - 21/05/14 19:55:59 INFO Executor: Finished task 52.0 in stage 2.0 (TID 57). 4544 bytes result sent to driver
[2021-05-14 16:55:59,841] {docker.py:276} INFO - 21/05/14 19:55:59 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 61) (379a553b2355, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:59,842] {docker.py:276} INFO - 21/05/14 19:55:59 INFO Executor: Running task 56.0 in stage 2.0 (TID 61)
21/05/14 19:55:59 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 57) in 1678 ms on 379a553b2355 (executor driver) (53/200)
[2021-05-14 16:55:59,852] {docker.py:276} INFO - 21/05/14 19:55:59 INFO ShuffleBlockFetcherIterator: Getting 4 (4.6 KiB) non-empty blocks including 4 (4.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:59,854] {docker.py:276} INFO - 21/05/14 19:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:55:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:55:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:59,854] {docker.py:276} INFO - 21/05/14 19:55:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955319029543432736427591_0002_m_000056_61, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319029543432736427591_0002_m_000056_61}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955319029543432736427591_0002}; taskId=attempt_202105141955319029543432736427591_0002_m_000056_61, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b13fd66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:55:59,855] {docker.py:276} INFO - 21/05/14 19:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:59,855] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Starting: Task committer attempt_202105141955319029543432736427591_0002_m_000056_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319029543432736427591_0002_m_000056_61
[2021-05-14 16:55:59,858] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Task committer attempt_202105141955319029543432736427591_0002_m_000056_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319029543432736427591_0002_m_000056_61 : duration 0:00.004s
[2021-05-14 16:55:59,968] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Starting: Task committer attempt_202105141955315166856564383057639_0002_m_000053_58: needsTaskCommit() Task attempt_202105141955315166856564383057639_0002_m_000053_58
[2021-05-14 16:55:59,969] {docker.py:276} INFO - 21/05/14 19:55:59 INFO StagingCommitter: Task committer attempt_202105141955315166856564383057639_0002_m_000053_58: needsTaskCommit() Task attempt_202105141955315166856564383057639_0002_m_000053_58: duration 0:00.001s
[2021-05-14 16:55:59,969] {docker.py:276} INFO - 21/05/14 19:55:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315166856564383057639_0002_m_000053_58
[2021-05-14 16:55:59,971] {docker.py:276} INFO - 21/05/14 19:55:59 INFO Executor: Finished task 53.0 in stage 2.0 (TID 58). 4544 bytes result sent to driver
[2021-05-14 16:55:59,972] {docker.py:276} INFO - 21/05/14 19:55:59 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 62) (379a553b2355, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:55:59,973] {docker.py:276} INFO - 21/05/14 19:55:59 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 58) in 1669 ms on 379a553b2355 (executor driver) (54/200)
21/05/14 19:55:59 INFO Executor: Running task 57.0 in stage 2.0 (TID 62)
[2021-05-14 16:55:59,983] {docker.py:276} INFO - 21/05/14 19:56:00 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:55:59,985] {docker.py:276} INFO - 21/05/14 19:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:55:59,986] {docker.py:276} INFO - 21/05/14 19:56:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318141358316072204974_0002_m_000057_62, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318141358316072204974_0002_m_000057_62}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318141358316072204974_0002}; taskId=attempt_202105141955318141358316072204974_0002_m_000057_62, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@213064ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:55:59,986] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955318141358316072204974_0002_m_000057_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318141358316072204974_0002_m_000057_62
[2021-05-14 16:55:59,989] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Task committer attempt_202105141955318141358316072204974_0002_m_000057_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318141358316072204974_0002_m_000057_62 : duration 0:00.003s
[2021-05-14 16:56:00,117] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955312337970911551798485_0002_m_000054_59: needsTaskCommit() Task attempt_202105141955312337970911551798485_0002_m_000054_59
[2021-05-14 16:56:00,117] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Task committer attempt_202105141955312337970911551798485_0002_m_000054_59: needsTaskCommit() Task attempt_202105141955312337970911551798485_0002_m_000054_59: duration 0:00.001s
21/05/14 19:56:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312337970911551798485_0002_m_000054_59
[2021-05-14 16:56:00,119] {docker.py:276} INFO - 21/05/14 19:56:00 INFO Executor: Finished task 54.0 in stage 2.0 (TID 59). 4544 bytes result sent to driver
[2021-05-14 16:56:00,121] {docker.py:276} INFO - 21/05/14 19:56:00 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 63) (379a553b2355, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:00,122] {docker.py:276} INFO - 21/05/14 19:56:00 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 59) in 1651 ms on 379a553b2355 (executor driver) (55/200)
21/05/14 19:56:00 INFO Executor: Running task 58.0 in stage 2.0 (TID 63)
[2021-05-14 16:56:00,131] {docker.py:276} INFO - 21/05/14 19:56:00 INFO ShuffleBlockFetcherIterator: Getting 4 (4.9 KiB) non-empty blocks including 4 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:00,133] {docker.py:276} INFO - 21/05/14 19:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316758389446821999539_0002_m_000058_63, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316758389446821999539_0002_m_000058_63}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316758389446821999539_0002}; taskId=attempt_202105141955316758389446821999539_0002_m_000058_63, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1563a30a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:00,133] {docker.py:276} INFO - 21/05/14 19:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955316758389446821999539_0002_m_000058_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316758389446821999539_0002_m_000058_63
[2021-05-14 16:56:00,136] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Task committer attempt_202105141955316758389446821999539_0002_m_000058_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316758389446821999539_0002_m_000058_63 : duration 0:00.002s
[2021-05-14 16:56:00,852] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955315389540128603399417_0002_m_000055_60: needsTaskCommit() Task attempt_202105141955315389540128603399417_0002_m_000055_60
[2021-05-14 16:56:00,852] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Task committer attempt_202105141955315389540128603399417_0002_m_000055_60: needsTaskCommit() Task attempt_202105141955315389540128603399417_0002_m_000055_60: duration 0:00.001s
[2021-05-14 16:56:00,853] {docker.py:276} INFO - 21/05/14 19:56:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315389540128603399417_0002_m_000055_60
[2021-05-14 16:56:00,854] {docker.py:276} INFO - 21/05/14 19:56:00 INFO Executor: Finished task 55.0 in stage 2.0 (TID 60). 4544 bytes result sent to driver
[2021-05-14 16:56:00,856] {docker.py:276} INFO - 21/05/14 19:56:00 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 64) (379a553b2355, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:00,857] {docker.py:276} INFO - 21/05/14 19:56:00 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 60) in 1648 ms on 379a553b2355 (executor driver) (56/200)
[2021-05-14 16:56:00,857] {docker.py:276} INFO - 21/05/14 19:56:00 INFO Executor: Running task 59.0 in stage 2.0 (TID 64)
[2021-05-14 16:56:00,867] {docker.py:276} INFO - 21/05/14 19:56:00 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:00,868] {docker.py:276} INFO - 21/05/14 19:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:00,870] {docker.py:276} INFO - 21/05/14 19:56:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:00,870] {docker.py:276} INFO - 21/05/14 19:56:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313252835209288268384_0002_m_000059_64, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313252835209288268384_0002_m_000059_64}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313252835209288268384_0002}; taskId=attempt_202105141955313252835209288268384_0002_m_000059_64, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@618f4e93}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:00,870] {docker.py:276} INFO - 21/05/14 19:56:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955313252835209288268384_0002_m_000059_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313252835209288268384_0002_m_000059_64
[2021-05-14 16:56:00,873] {docker.py:276} INFO - 21/05/14 19:56:00 INFO StagingCommitter: Task committer attempt_202105141955313252835209288268384_0002_m_000059_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313252835209288268384_0002_m_000059_64 : duration 0:00.003s
[2021-05-14 16:56:01,545] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955318141358316072204974_0002_m_000057_62: needsTaskCommit() Task attempt_202105141955318141358316072204974_0002_m_000057_62
21/05/14 19:56:01 INFO StagingCommitter: Task committer attempt_202105141955318141358316072204974_0002_m_000057_62: needsTaskCommit() Task attempt_202105141955318141358316072204974_0002_m_000057_62: duration 0:00.001s
21/05/14 19:56:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318141358316072204974_0002_m_000057_62
[2021-05-14 16:56:01,548] {docker.py:276} INFO - 21/05/14 19:56:01 INFO Executor: Finished task 57.0 in stage 2.0 (TID 62). 4544 bytes result sent to driver
[2021-05-14 16:56:01,549] {docker.py:276} INFO - 21/05/14 19:56:01 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 65) (379a553b2355, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:01,550] {docker.py:276} INFO - 21/05/14 19:56:01 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 62) in 1580 ms on 379a553b2355 (executor driver) (57/200)
[2021-05-14 16:56:01,551] {docker.py:276} INFO - 21/05/14 19:56:01 INFO Executor: Running task 60.0 in stage 2.0 (TID 65)
[2021-05-14 16:56:01,561] {docker.py:276} INFO - 21/05/14 19:56:01 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:01,563] {docker.py:276} INFO - 21/05/14 19:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:01,563] {docker.py:276} INFO - 21/05/14 19:56:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:01,564] {docker.py:276} INFO - 21/05/14 19:56:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313038787828178747604_0002_m_000060_65, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313038787828178747604_0002_m_000060_65}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313038787828178747604_0002}; taskId=attempt_202105141955313038787828178747604_0002_m_000060_65, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5245c657}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:01,564] {docker.py:276} INFO - 21/05/14 19:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:01,565] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955313038787828178747604_0002_m_000060_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313038787828178747604_0002_m_000060_65
[2021-05-14 16:56:01,567] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Task committer attempt_202105141955313038787828178747604_0002_m_000060_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313038787828178747604_0002_m_000060_65 : duration 0:00.003s
[2021-05-14 16:56:01,583] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955319029543432736427591_0002_m_000056_61: needsTaskCommit() Task attempt_202105141955319029543432736427591_0002_m_000056_61
[2021-05-14 16:56:01,583] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Task committer attempt_202105141955319029543432736427591_0002_m_000056_61: needsTaskCommit() Task attempt_202105141955319029543432736427591_0002_m_000056_61: duration 0:00.000s
21/05/14 19:56:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955319029543432736427591_0002_m_000056_61
[2021-05-14 16:56:01,584] {docker.py:276} INFO - 21/05/14 19:56:01 INFO Executor: Finished task 56.0 in stage 2.0 (TID 61). 4544 bytes result sent to driver
[2021-05-14 16:56:01,585] {docker.py:276} INFO - 21/05/14 19:56:01 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 66) (379a553b2355, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:01,587] {docker.py:276} INFO - 21/05/14 19:56:01 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 61) in 1749 ms on 379a553b2355 (executor driver) (58/200)
[2021-05-14 16:56:01,587] {docker.py:276} INFO - 21/05/14 19:56:01 INFO Executor: Running task 61.0 in stage 2.0 (TID 66)
[2021-05-14 16:56:01,595] {docker.py:276} INFO - 21/05/14 19:56:01 INFO ShuffleBlockFetcherIterator: Getting 4 (4.8 KiB) non-empty blocks including 4 (4.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:01,596] {docker.py:276} INFO - 21/05/14 19:56:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:01,598] {docker.py:276} INFO - 21/05/14 19:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:01,598] {docker.py:276} INFO - 21/05/14 19:56:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:01,598] {docker.py:276} INFO - 21/05/14 19:56:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:01,599] {docker.py:276} INFO - 21/05/14 19:56:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531500999293121161597_0002_m_000061_66, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531500999293121161597_0002_m_000061_66}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531500999293121161597_0002}; taskId=attempt_20210514195531500999293121161597_0002_m_000061_66, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30e1bbe3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:01,599] {docker.py:276} INFO - 21/05/14 19:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:01,599] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Starting: Task committer attempt_20210514195531500999293121161597_0002_m_000061_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531500999293121161597_0002_m_000061_66
[2021-05-14 16:56:01,601] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Task committer attempt_20210514195531500999293121161597_0002_m_000061_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531500999293121161597_0002_m_000061_66 : duration 0:00.003s
[2021-05-14 16:56:01,717] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955316758389446821999539_0002_m_000058_63: needsTaskCommit() Task attempt_202105141955316758389446821999539_0002_m_000058_63
[2021-05-14 16:56:01,718] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Task committer attempt_202105141955316758389446821999539_0002_m_000058_63: needsTaskCommit() Task attempt_202105141955316758389446821999539_0002_m_000058_63: duration 0:00.000s
21/05/14 19:56:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316758389446821999539_0002_m_000058_63
[2021-05-14 16:56:01,720] {docker.py:276} INFO - 21/05/14 19:56:01 INFO Executor: Finished task 58.0 in stage 2.0 (TID 63). 4544 bytes result sent to driver
[2021-05-14 16:56:01,721] {docker.py:276} INFO - 21/05/14 19:56:01 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 67) (379a553b2355, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:01,722] {docker.py:276} INFO - 21/05/14 19:56:01 INFO Executor: Running task 62.0 in stage 2.0 (TID 67)
[2021-05-14 16:56:01,723] {docker.py:276} INFO - 21/05/14 19:56:01 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 63) in 1605 ms on 379a553b2355 (executor driver) (59/200)
[2021-05-14 16:56:01,733] {docker.py:276} INFO - 21/05/14 19:56:01 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:01,733] {docker.py:276} INFO - 21/05/14 19:56:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:01,736] {docker.py:276} INFO - 21/05/14 19:56:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:01,736] {docker.py:276} INFO - 21/05/14 19:56:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:01,737] {docker.py:276} INFO - 21/05/14 19:56:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312938243346443635375_0002_m_000062_67, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312938243346443635375_0002_m_000062_67}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312938243346443635375_0002}; taskId=attempt_202105141955312938243346443635375_0002_m_000062_67, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fd965e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:01,737] {docker.py:276} INFO - 21/05/14 19:56:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:01,737] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955312938243346443635375_0002_m_000062_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312938243346443635375_0002_m_000062_67
[2021-05-14 16:56:01,749] {docker.py:276} INFO - 21/05/14 19:56:01 INFO StagingCommitter: Task committer attempt_202105141955312938243346443635375_0002_m_000062_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312938243346443635375_0002_m_000062_67 : duration 0:00.012s
[2021-05-14 16:56:02,496] {docker.py:276} INFO - 21/05/14 19:56:02 INFO StagingCommitter: Starting: Task committer attempt_202105141955313252835209288268384_0002_m_000059_64: needsTaskCommit() Task attempt_202105141955313252835209288268384_0002_m_000059_64
21/05/14 19:56:02 INFO StagingCommitter: Task committer attempt_202105141955313252835209288268384_0002_m_000059_64: needsTaskCommit() Task attempt_202105141955313252835209288268384_0002_m_000059_64: duration 0:00.001s
[2021-05-14 16:56:02,497] {docker.py:276} INFO - 21/05/14 19:56:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313252835209288268384_0002_m_000059_64
[2021-05-14 16:56:02,499] {docker.py:276} INFO - 21/05/14 19:56:02 INFO Executor: Finished task 59.0 in stage 2.0 (TID 64). 4587 bytes result sent to driver
[2021-05-14 16:56:02,501] {docker.py:276} INFO - 21/05/14 19:56:02 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 68) (379a553b2355, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:02,502] {docker.py:276} INFO - 21/05/14 19:56:02 INFO Executor: Running task 63.0 in stage 2.0 (TID 68)
[2021-05-14 16:56:02,503] {docker.py:276} INFO - 21/05/14 19:56:02 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 64) in 1649 ms on 379a553b2355 (executor driver) (60/200)
[2021-05-14 16:56:02,513] {docker.py:276} INFO - 21/05/14 19:56:02 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:02,515] {docker.py:276} INFO - 21/05/14 19:56:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314834586340468656326_0002_m_000063_68, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314834586340468656326_0002_m_000063_68}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314834586340468656326_0002}; taskId=attempt_202105141955314834586340468656326_0002_m_000063_68, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3603ba79}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:02,515] {docker.py:276} INFO - 21/05/14 19:56:02 INFO StagingCommitter: Starting: Task committer attempt_202105141955314834586340468656326_0002_m_000063_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314834586340468656326_0002_m_000063_68
[2021-05-14 16:56:02,518] {docker.py:276} INFO - 21/05/14 19:56:02 INFO StagingCommitter: Task committer attempt_202105141955314834586340468656326_0002_m_000063_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314834586340468656326_0002_m_000063_68 : duration 0:00.003s
[2021-05-14 16:56:03,186] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955313038787828178747604_0002_m_000060_65: needsTaskCommit() Task attempt_202105141955313038787828178747604_0002_m_000060_65
[2021-05-14 16:56:03,187] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Task committer attempt_202105141955313038787828178747604_0002_m_000060_65: needsTaskCommit() Task attempt_202105141955313038787828178747604_0002_m_000060_65: duration 0:00.001s
21/05/14 19:56:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313038787828178747604_0002_m_000060_65
[2021-05-14 16:56:03,189] {docker.py:276} INFO - 21/05/14 19:56:03 INFO Executor: Finished task 60.0 in stage 2.0 (TID 65). 4587 bytes result sent to driver
[2021-05-14 16:56:03,191] {docker.py:276} INFO - 21/05/14 19:56:03 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 69) (379a553b2355, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:03,192] {docker.py:276} INFO - 21/05/14 19:56:03 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 65) in 1646 ms on 379a553b2355 (executor driver) (61/200)
21/05/14 19:56:03 INFO Executor: Running task 64.0 in stage 2.0 (TID 69)
[2021-05-14 16:56:03,202] {docker.py:276} INFO - 21/05/14 19:56:03 INFO ShuffleBlockFetcherIterator: Getting 4 (4.8 KiB) non-empty blocks including 4 (4.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:03,204] {docker.py:276} INFO - 21/05/14 19:56:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:03,204] {docker.py:276} INFO - 21/05/14 19:56:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:03,205] {docker.py:276} INFO - 21/05/14 19:56:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311222269356996634209_0002_m_000064_69, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311222269356996634209_0002_m_000064_69}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311222269356996634209_0002}; taskId=attempt_202105141955311222269356996634209_0002_m_000064_69, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2069eabd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955311222269356996634209_0002_m_000064_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311222269356996634209_0002_m_000064_69
[2021-05-14 16:56:03,208] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Task committer attempt_202105141955311222269356996634209_0002_m_000064_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311222269356996634209_0002_m_000064_69 : duration 0:00.004s
[2021-05-14 16:56:03,237] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Starting: Task committer attempt_20210514195531500999293121161597_0002_m_000061_66: needsTaskCommit() Task attempt_20210514195531500999293121161597_0002_m_000061_66
[2021-05-14 16:56:03,237] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Task committer attempt_20210514195531500999293121161597_0002_m_000061_66: needsTaskCommit() Task attempt_20210514195531500999293121161597_0002_m_000061_66: duration 0:00.001s
21/05/14 19:56:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531500999293121161597_0002_m_000061_66
[2021-05-14 16:56:03,240] {docker.py:276} INFO - 21/05/14 19:56:03 INFO Executor: Finished task 61.0 in stage 2.0 (TID 66). 4587 bytes result sent to driver
[2021-05-14 16:56:03,241] {docker.py:276} INFO - 21/05/14 19:56:03 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 70) (379a553b2355, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:03,242] {docker.py:276} INFO - 21/05/14 19:56:03 INFO Executor: Running task 65.0 in stage 2.0 (TID 70)
[2021-05-14 16:56:03,243] {docker.py:276} INFO - 21/05/14 19:56:03 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 66) in 1658 ms on 379a553b2355 (executor driver) (62/200)
[2021-05-14 16:56:03,253] {docker.py:276} INFO - 21/05/14 19:56:03 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:03,255] {docker.py:276} INFO - 21/05/14 19:56:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:03,255] {docker.py:276} INFO - 21/05/14 19:56:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:03,255] {docker.py:276} INFO - 21/05/14 19:56:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311133983042687910142_0002_m_000065_70, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311133983042687910142_0002_m_000065_70}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311133983042687910142_0002}; taskId=attempt_202105141955311133983042687910142_0002_m_000065_70, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24d26c98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:03,256] {docker.py:276} INFO - 21/05/14 19:56:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955311133983042687910142_0002_m_000065_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311133983042687910142_0002_m_000065_70
[2021-05-14 16:56:03,259] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Task committer attempt_202105141955311133983042687910142_0002_m_000065_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311133983042687910142_0002_m_000065_70 : duration 0:00.003s
[2021-05-14 16:56:03,441] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955312938243346443635375_0002_m_000062_67: needsTaskCommit() Task attempt_202105141955312938243346443635375_0002_m_000062_67
[2021-05-14 16:56:03,442] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Task committer attempt_202105141955312938243346443635375_0002_m_000062_67: needsTaskCommit() Task attempt_202105141955312938243346443635375_0002_m_000062_67: duration 0:00.001s
21/05/14 19:56:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312938243346443635375_0002_m_000062_67
[2021-05-14 16:56:03,443] {docker.py:276} INFO - 21/05/14 19:56:03 INFO Executor: Finished task 62.0 in stage 2.0 (TID 67). 4587 bytes result sent to driver
[2021-05-14 16:56:03,447] {docker.py:276} INFO - 21/05/14 19:56:03 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 71) (379a553b2355, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:03,448] {docker.py:276} INFO - 21/05/14 19:56:03 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 67) in 1729 ms on 379a553b2355 (executor driver) (63/200)
[2021-05-14 16:56:03,449] {docker.py:276} INFO - 21/05/14 19:56:03 INFO Executor: Running task 66.0 in stage 2.0 (TID 71)
[2021-05-14 16:56:03,459] {docker.py:276} INFO - 21/05/14 19:56:03 INFO ShuffleBlockFetcherIterator: Getting 4 (7.1 KiB) non-empty blocks including 4 (7.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:03,461] {docker.py:276} INFO - 21/05/14 19:56:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:03,462] {docker.py:276} INFO - 21/05/14 19:56:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:03,462] {docker.py:276} INFO - 21/05/14 19:56:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312338324094105966822_0002_m_000066_71, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312338324094105966822_0002_m_000066_71}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312338324094105966822_0002}; taskId=attempt_202105141955312338324094105966822_0002_m_000066_71, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@161b776}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:03,462] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955312338324094105966822_0002_m_000066_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312338324094105966822_0002_m_000066_71
[2021-05-14 16:56:03,464] {docker.py:276} INFO - 21/05/14 19:56:03 INFO StagingCommitter: Task committer attempt_202105141955312338324094105966822_0002_m_000066_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312338324094105966822_0002_m_000066_71 : duration 0:00.002s
[2021-05-14 16:56:04,136] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955314834586340468656326_0002_m_000063_68: needsTaskCommit() Task attempt_202105141955314834586340468656326_0002_m_000063_68
[2021-05-14 16:56:04,138] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Task committer attempt_202105141955314834586340468656326_0002_m_000063_68: needsTaskCommit() Task attempt_202105141955314834586340468656326_0002_m_000063_68: duration 0:00.001s
21/05/14 19:56:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314834586340468656326_0002_m_000063_68
[2021-05-14 16:56:04,142] {docker.py:276} INFO - 21/05/14 19:56:04 INFO Executor: Finished task 63.0 in stage 2.0 (TID 68). 4544 bytes result sent to driver
[2021-05-14 16:56:04,143] {docker.py:276} INFO - 21/05/14 19:56:04 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 72) (379a553b2355, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:04,144] {docker.py:276} INFO - 21/05/14 19:56:04 INFO Executor: Running task 67.0 in stage 2.0 (TID 72)
[2021-05-14 16:56:04,145] {docker.py:276} INFO - 21/05/14 19:56:04 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 68) in 1646 ms on 379a553b2355 (executor driver) (64/200)
[2021-05-14 16:56:04,154] {docker.py:276} INFO - 21/05/14 19:56:04 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:04,156] {docker.py:276} INFO - 21/05/14 19:56:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315722866436380964431_0002_m_000067_72, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315722866436380964431_0002_m_000067_72}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315722866436380964431_0002}; taskId=attempt_202105141955315722866436380964431_0002_m_000067_72, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44aa44e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:04,157] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955315722866436380964431_0002_m_000067_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315722866436380964431_0002_m_000067_72
[2021-05-14 16:56:04,159] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Task committer attempt_202105141955315722866436380964431_0002_m_000067_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315722866436380964431_0002_m_000067_72 : duration 0:00.003s
[2021-05-14 16:56:04,827] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955311133983042687910142_0002_m_000065_70: needsTaskCommit() Task attempt_202105141955311133983042687910142_0002_m_000065_70
[2021-05-14 16:56:04,829] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Task committer attempt_202105141955311133983042687910142_0002_m_000065_70: needsTaskCommit() Task attempt_202105141955311133983042687910142_0002_m_000065_70: duration 0:00.001s
[2021-05-14 16:56:04,829] {docker.py:276} INFO - 21/05/14 19:56:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311133983042687910142_0002_m_000065_70
[2021-05-14 16:56:04,832] {docker.py:276} INFO - 21/05/14 19:56:04 INFO Executor: Finished task 65.0 in stage 2.0 (TID 70). 4544 bytes result sent to driver
[2021-05-14 16:56:04,834] {docker.py:276} INFO - 21/05/14 19:56:04 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 73) (379a553b2355, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:04,835] {docker.py:276} INFO - 21/05/14 19:56:04 INFO Executor: Running task 68.0 in stage 2.0 (TID 73)
[2021-05-14 16:56:04,835] {docker.py:276} INFO - 21/05/14 19:56:04 INFO TaskSetManager: Finished task 65.0 in stage 2.0 (TID 70) in 1596 ms on 379a553b2355 (executor driver) (65/200)
[2021-05-14 16:56:04,845] {docker.py:276} INFO - 21/05/14 19:56:04 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:04,847] {docker.py:276} INFO - 21/05/14 19:56:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:04,847] {docker.py:276} INFO - 21/05/14 19:56:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:04,848] {docker.py:276} INFO - 21/05/14 19:56:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:04,848] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955311222269356996634209_0002_m_000064_69: needsTaskCommit() Task attempt_202105141955311222269356996634209_0002_m_000064_69
21/05/14 19:56:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311057241526998784294_0002_m_000068_73, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311057241526998784294_0002_m_000068_73}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311057241526998784294_0002}; taskId=attempt_202105141955311057241526998784294_0002_m_000068_73, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@290f47f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:04,848] {docker.py:276} INFO - 21/05/14 19:56:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:04,849] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955311057241526998784294_0002_m_000068_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311057241526998784294_0002_m_000068_73
[2021-05-14 16:56:04,849] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Task committer attempt_202105141955311222269356996634209_0002_m_000064_69: needsTaskCommit() Task attempt_202105141955311222269356996634209_0002_m_000064_69: duration 0:00.002s
21/05/14 19:56:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311222269356996634209_0002_m_000064_69
[2021-05-14 16:56:04,850] {docker.py:276} INFO - 21/05/14 19:56:04 INFO Executor: Finished task 64.0 in stage 2.0 (TID 69). 4544 bytes result sent to driver
[2021-05-14 16:56:04,851] {docker.py:276} INFO - 21/05/14 19:56:04 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 74) (379a553b2355, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:04,852] {docker.py:276} INFO - 21/05/14 19:56:04 INFO Executor: Running task 69.0 in stage 2.0 (TID 74)
[2021-05-14 16:56:04,853] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Task committer attempt_202105141955311057241526998784294_0002_m_000068_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311057241526998784294_0002_m_000068_73 : duration 0:00.006s
[2021-05-14 16:56:04,854] {docker.py:276} INFO - 21/05/14 19:56:04 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 69) in 1666 ms on 379a553b2355 (executor driver) (66/200)
[2021-05-14 16:56:04,860] {docker.py:276} INFO - 21/05/14 19:56:04 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:04,862] {docker.py:276} INFO - 21/05/14 19:56:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:04,862] {docker.py:276} INFO - 21/05/14 19:56:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312124115927736542108_0002_m_000069_74, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312124115927736542108_0002_m_000069_74}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312124115927736542108_0002}; taskId=attempt_202105141955312124115927736542108_0002_m_000069_74, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dc04f0c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:04,863] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955312124115927736542108_0002_m_000069_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312124115927736542108_0002_m_000069_74
[2021-05-14 16:56:04,865] {docker.py:276} INFO - 21/05/14 19:56:04 INFO StagingCommitter: Task committer attempt_202105141955312124115927736542108_0002_m_000069_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312124115927736542108_0002_m_000069_74 : duration 0:00.003s
[2021-05-14 16:56:05,141] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955312338324094105966822_0002_m_000066_71: needsTaskCommit() Task attempt_202105141955312338324094105966822_0002_m_000066_71
[2021-05-14 16:56:05,142] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Task committer attempt_202105141955312338324094105966822_0002_m_000066_71: needsTaskCommit() Task attempt_202105141955312338324094105966822_0002_m_000066_71: duration 0:00.001s
21/05/14 19:56:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312338324094105966822_0002_m_000066_71
[2021-05-14 16:56:05,144] {docker.py:276} INFO - 21/05/14 19:56:05 INFO Executor: Finished task 66.0 in stage 2.0 (TID 71). 4544 bytes result sent to driver
[2021-05-14 16:56:05,145] {docker.py:276} INFO - 21/05/14 19:56:05 INFO TaskSetManager: Starting task 70.0 in stage 2.0 (TID 75) (379a553b2355, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:05,146] {docker.py:276} INFO - 21/05/14 19:56:05 INFO TaskSetManager: Finished task 66.0 in stage 2.0 (TID 71) in 1702 ms on 379a553b2355 (executor driver) (67/200)
[2021-05-14 16:56:05,150] {docker.py:276} INFO - 21/05/14 19:56:05 INFO Executor: Running task 70.0 in stage 2.0 (TID 75)
[2021-05-14 16:56:05,159] {docker.py:276} INFO - 21/05/14 19:56:05 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:05,162] {docker.py:276} INFO - 21/05/14 19:56:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:05,163] {docker.py:276} INFO - 21/05/14 19:56:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316368342713856105289_0002_m_000070_75, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316368342713856105289_0002_m_000070_75}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316368342713856105289_0002}; taskId=attempt_202105141955316368342713856105289_0002_m_000070_75, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45aade6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955316368342713856105289_0002_m_000070_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316368342713856105289_0002_m_000070_75
[2021-05-14 16:56:05,168] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Task committer attempt_202105141955316368342713856105289_0002_m_000070_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316368342713856105289_0002_m_000070_75 : duration 0:00.006s
[2021-05-14 16:56:05,751] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955315722866436380964431_0002_m_000067_72: needsTaskCommit() Task attempt_202105141955315722866436380964431_0002_m_000067_72
[2021-05-14 16:56:05,752] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Task committer attempt_202105141955315722866436380964431_0002_m_000067_72: needsTaskCommit() Task attempt_202105141955315722866436380964431_0002_m_000067_72: duration 0:00.000s
21/05/14 19:56:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315722866436380964431_0002_m_000067_72
[2021-05-14 16:56:05,754] {docker.py:276} INFO - 21/05/14 19:56:05 INFO Executor: Finished task 67.0 in stage 2.0 (TID 72). 4544 bytes result sent to driver
[2021-05-14 16:56:05,755] {docker.py:276} INFO - 21/05/14 19:56:05 INFO TaskSetManager: Starting task 71.0 in stage 2.0 (TID 76) (379a553b2355, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:05,756] {docker.py:276} INFO - 21/05/14 19:56:05 INFO TaskSetManager: Finished task 67.0 in stage 2.0 (TID 72) in 1616 ms on 379a553b2355 (executor driver) (68/200)
[2021-05-14 16:56:05,757] {docker.py:276} INFO - 21/05/14 19:56:05 INFO Executor: Running task 71.0 in stage 2.0 (TID 76)
[2021-05-14 16:56:05,767] {docker.py:276} INFO - 21/05/14 19:56:05 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:05,769] {docker.py:276} INFO - 21/05/14 19:56:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:05,770] {docker.py:276} INFO - 21/05/14 19:56:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317556447217935608500_0002_m_000071_76, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317556447217935608500_0002_m_000071_76}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317556447217935608500_0002}; taskId=attempt_202105141955317556447217935608500_0002_m_000071_76, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@275c8530}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:05,770] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955317556447217935608500_0002_m_000071_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317556447217935608500_0002_m_000071_76
[2021-05-14 16:56:05,772] {docker.py:276} INFO - 21/05/14 19:56:05 INFO StagingCommitter: Task committer attempt_202105141955317556447217935608500_0002_m_000071_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317556447217935608500_0002_m_000071_76 : duration 0:00.003s
[2021-05-14 16:56:06,500] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955311057241526998784294_0002_m_000068_73: needsTaskCommit() Task attempt_202105141955311057241526998784294_0002_m_000068_73
[2021-05-14 16:56:06,501] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Task committer attempt_202105141955311057241526998784294_0002_m_000068_73: needsTaskCommit() Task attempt_202105141955311057241526998784294_0002_m_000068_73: duration 0:00.002s
21/05/14 19:56:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311057241526998784294_0002_m_000068_73
[2021-05-14 16:56:06,504] {docker.py:276} INFO - 21/05/14 19:56:06 INFO Executor: Finished task 68.0 in stage 2.0 (TID 73). 4544 bytes result sent to driver
[2021-05-14 16:56:06,505] {docker.py:276} INFO - 21/05/14 19:56:06 INFO TaskSetManager: Starting task 72.0 in stage 2.0 (TID 77) (379a553b2355, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:06,506] {docker.py:276} INFO - 21/05/14 19:56:06 INFO Executor: Running task 72.0 in stage 2.0 (TID 77)
[2021-05-14 16:56:06,507] {docker.py:276} INFO - 21/05/14 19:56:06 INFO TaskSetManager: Finished task 68.0 in stage 2.0 (TID 73) in 1675 ms on 379a553b2355 (executor driver) (69/200)
[2021-05-14 16:56:06,516] {docker.py:276} INFO - 21/05/14 19:56:06 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:06,519] {docker.py:276} INFO - 21/05/14 19:56:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:06,519] {docker.py:276} INFO - 21/05/14 19:56:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315569630520404616707_0002_m_000072_77, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315569630520404616707_0002_m_000072_77}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315569630520404616707_0002}; taskId=attempt_202105141955315569630520404616707_0002_m_000072_77, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b5b4904}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:06,519] {docker.py:276} INFO - 21/05/14 19:56:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955315569630520404616707_0002_m_000072_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315569630520404616707_0002_m_000072_77
[2021-05-14 16:56:06,522] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Task committer attempt_202105141955315569630520404616707_0002_m_000072_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315569630520404616707_0002_m_000072_77 : duration 0:00.003s
[2021-05-14 16:56:06,597] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955312124115927736542108_0002_m_000069_74: needsTaskCommit() Task attempt_202105141955312124115927736542108_0002_m_000069_74
[2021-05-14 16:56:06,598] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Task committer attempt_202105141955312124115927736542108_0002_m_000069_74: needsTaskCommit() Task attempt_202105141955312124115927736542108_0002_m_000069_74: duration 0:00.000s
21/05/14 19:56:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312124115927736542108_0002_m_000069_74
[2021-05-14 16:56:06,600] {docker.py:276} INFO - 21/05/14 19:56:06 INFO Executor: Finished task 69.0 in stage 2.0 (TID 74). 4544 bytes result sent to driver
[2021-05-14 16:56:06,601] {docker.py:276} INFO - 21/05/14 19:56:06 INFO TaskSetManager: Starting task 73.0 in stage 2.0 (TID 78) (379a553b2355, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:06,603] {docker.py:276} INFO - 21/05/14 19:56:06 INFO Executor: Running task 73.0 in stage 2.0 (TID 78)
[2021-05-14 16:56:06,603] {docker.py:276} INFO - 21/05/14 19:56:06 INFO TaskSetManager: Finished task 69.0 in stage 2.0 (TID 74) in 1753 ms on 379a553b2355 (executor driver) (70/200)
[2021-05-14 16:56:06,613] {docker.py:276} INFO - 21/05/14 19:56:06 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:06,614] {docker.py:276} INFO - 21/05/14 19:56:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:06,616] {docker.py:276} INFO - 21/05/14 19:56:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:06,616] {docker.py:276} INFO - 21/05/14 19:56:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312506261628116562428_0002_m_000073_78, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312506261628116562428_0002_m_000073_78}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312506261628116562428_0002}; taskId=attempt_202105141955312506261628116562428_0002_m_000073_78, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69360f79}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:06,617] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955312506261628116562428_0002_m_000073_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312506261628116562428_0002_m_000073_78
[2021-05-14 16:56:06,619] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Task committer attempt_202105141955312506261628116562428_0002_m_000073_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312506261628116562428_0002_m_000073_78 : duration 0:00.003s
[2021-05-14 16:56:06,776] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955316368342713856105289_0002_m_000070_75: needsTaskCommit() Task attempt_202105141955316368342713856105289_0002_m_000070_75
[2021-05-14 16:56:06,778] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Task committer attempt_202105141955316368342713856105289_0002_m_000070_75: needsTaskCommit() Task attempt_202105141955316368342713856105289_0002_m_000070_75: duration 0:00.001s
[2021-05-14 16:56:06,779] {docker.py:276} INFO - 21/05/14 19:56:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316368342713856105289_0002_m_000070_75
[2021-05-14 16:56:06,781] {docker.py:276} INFO - 21/05/14 19:56:06 INFO Executor: Finished task 70.0 in stage 2.0 (TID 75). 4544 bytes result sent to driver
[2021-05-14 16:56:06,783] {docker.py:276} INFO - 21/05/14 19:56:06 INFO TaskSetManager: Starting task 74.0 in stage 2.0 (TID 79) (379a553b2355, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:06,784] {docker.py:276} INFO - 21/05/14 19:56:06 INFO Executor: Running task 74.0 in stage 2.0 (TID 79)
21/05/14 19:56:06 INFO TaskSetManager: Finished task 70.0 in stage 2.0 (TID 75) in 1640 ms on 379a553b2355 (executor driver) (71/200)
[2021-05-14 16:56:06,793] {docker.py:276} INFO - 21/05/14 19:56:06 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:06,795] {docker.py:276} INFO - 21/05/14 19:56:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:06,795] {docker.py:276} INFO - 21/05/14 19:56:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317451936530313282326_0002_m_000074_79, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317451936530313282326_0002_m_000074_79}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317451936530313282326_0002}; taskId=attempt_202105141955317451936530313282326_0002_m_000074_79, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37cf64d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:06,796] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955317451936530313282326_0002_m_000074_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317451936530313282326_0002_m_000074_79
[2021-05-14 16:56:06,798] {docker.py:276} INFO - 21/05/14 19:56:06 INFO StagingCommitter: Task committer attempt_202105141955317451936530313282326_0002_m_000074_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317451936530313282326_0002_m_000074_79 : duration 0:00.003s
[2021-05-14 16:56:07,437] {docker.py:276} INFO - 21/05/14 19:56:07 INFO StagingCommitter: Starting: Task committer attempt_202105141955317556447217935608500_0002_m_000071_76: needsTaskCommit() Task attempt_202105141955317556447217935608500_0002_m_000071_76
[2021-05-14 16:56:07,438] {docker.py:276} INFO - 21/05/14 19:56:07 INFO StagingCommitter: Task committer attempt_202105141955317556447217935608500_0002_m_000071_76: needsTaskCommit() Task attempt_202105141955317556447217935608500_0002_m_000071_76: duration 0:00.001s
[2021-05-14 16:56:07,439] {docker.py:276} INFO - 21/05/14 19:56:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317556447217935608500_0002_m_000071_76
[2021-05-14 16:56:07,439] {docker.py:276} INFO - 21/05/14 19:56:07 INFO Executor: Finished task 71.0 in stage 2.0 (TID 76). 4544 bytes result sent to driver
[2021-05-14 16:56:07,441] {docker.py:276} INFO - 21/05/14 19:56:07 INFO TaskSetManager: Starting task 75.0 in stage 2.0 (TID 80) (379a553b2355, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:07,442] {docker.py:276} INFO - 21/05/14 19:56:07 INFO TaskSetManager: Finished task 71.0 in stage 2.0 (TID 76) in 1689 ms on 379a553b2355 (executor driver) (72/200)
21/05/14 19:56:07 INFO Executor: Running task 75.0 in stage 2.0 (TID 80)
[2021-05-14 16:56:07,462] {docker.py:276} INFO - 21/05/14 19:56:07 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:07,462] {docker.py:276} INFO - 21/05/14 19:56:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:07,464] {docker.py:276} INFO - 21/05/14 19:56:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:07,465] {docker.py:276} INFO - 21/05/14 19:56:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531621009503788258019_0002_m_000075_80, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531621009503788258019_0002_m_000075_80}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531621009503788258019_0002}; taskId=attempt_20210514195531621009503788258019_0002_m_000075_80, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bab3ea3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:07,465] {docker.py:276} INFO - 21/05/14 19:56:07 INFO StagingCommitter: Starting: Task committer attempt_20210514195531621009503788258019_0002_m_000075_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531621009503788258019_0002_m_000075_80
[2021-05-14 16:56:07,468] {docker.py:276} INFO - 21/05/14 19:56:07 INFO StagingCommitter: Task committer attempt_20210514195531621009503788258019_0002_m_000075_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531621009503788258019_0002_m_000075_80 : duration 0:00.002s
[2021-05-14 16:56:08,169] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Starting: Task committer attempt_202105141955312506261628116562428_0002_m_000073_78: needsTaskCommit() Task attempt_202105141955312506261628116562428_0002_m_000073_78
21/05/14 19:56:08 INFO StagingCommitter: Task committer attempt_202105141955312506261628116562428_0002_m_000073_78: needsTaskCommit() Task attempt_202105141955312506261628116562428_0002_m_000073_78: duration 0:00.001s
21/05/14 19:56:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312506261628116562428_0002_m_000073_78
[2021-05-14 16:56:08,171] {docker.py:276} INFO - 21/05/14 19:56:08 INFO Executor: Finished task 73.0 in stage 2.0 (TID 78). 4587 bytes result sent to driver
[2021-05-14 16:56:08,172] {docker.py:276} INFO - 21/05/14 19:56:08 INFO TaskSetManager: Starting task 76.0 in stage 2.0 (TID 81) (379a553b2355, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:08,173] {docker.py:276} INFO - 21/05/14 19:56:08 INFO Executor: Running task 76.0 in stage 2.0 (TID 81)
[2021-05-14 16:56:08,174] {docker.py:276} INFO - 21/05/14 19:56:08 INFO TaskSetManager: Finished task 73.0 in stage 2.0 (TID 78) in 1576 ms on 379a553b2355 (executor driver) (73/200)
[2021-05-14 16:56:08,183] {docker.py:276} INFO - 21/05/14 19:56:08 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:08,185] {docker.py:276} INFO - 21/05/14 19:56:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315963968592737109691_0002_m_000076_81, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315963968592737109691_0002_m_000076_81}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315963968592737109691_0002}; taskId=attempt_202105141955315963968592737109691_0002_m_000076_81, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22c8dc5e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:08,185] {docker.py:276} INFO - 21/05/14 19:56:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:08 INFO StagingCommitter: Starting: Task committer attempt_202105141955315963968592737109691_0002_m_000076_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315963968592737109691_0002_m_000076_81
[2021-05-14 16:56:08,188] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Task committer attempt_202105141955315963968592737109691_0002_m_000076_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315963968592737109691_0002_m_000076_81 : duration 0:00.003s
[2021-05-14 16:56:08,192] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Starting: Task committer attempt_202105141955315569630520404616707_0002_m_000072_77: needsTaskCommit() Task attempt_202105141955315569630520404616707_0002_m_000072_77
[2021-05-14 16:56:08,193] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Task committer attempt_202105141955315569630520404616707_0002_m_000072_77: needsTaskCommit() Task attempt_202105141955315569630520404616707_0002_m_000072_77: duration 0:00.001s
[2021-05-14 16:56:08,193] {docker.py:276} INFO - 21/05/14 19:56:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315569630520404616707_0002_m_000072_77
[2021-05-14 16:56:08,194] {docker.py:276} INFO - 21/05/14 19:56:08 INFO Executor: Finished task 72.0 in stage 2.0 (TID 77). 4587 bytes result sent to driver
[2021-05-14 16:56:08,194] {docker.py:276} INFO - 21/05/14 19:56:08 INFO TaskSetManager: Starting task 77.0 in stage 2.0 (TID 82) (379a553b2355, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:08,197] {docker.py:276} INFO - 21/05/14 19:56:08 INFO TaskSetManager: Finished task 72.0 in stage 2.0 (TID 77) in 1693 ms on 379a553b2355 (executor driver) (74/200)
[2021-05-14 16:56:08,197] {docker.py:276} INFO - 21/05/14 19:56:08 INFO Executor: Running task 77.0 in stage 2.0 (TID 82)
[2021-05-14 16:56:08,208] {docker.py:276} INFO - 21/05/14 19:56:08 INFO ShuffleBlockFetcherIterator: Getting 4 (6.3 KiB) non-empty blocks including 4 (6.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:08,209] {docker.py:276} INFO - 21/05/14 19:56:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:08,210] {docker.py:276} INFO - 21/05/14 19:56:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313163327130468574650_0002_m_000077_82, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313163327130468574650_0002_m_000077_82}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313163327130468574650_0002}; taskId=attempt_202105141955313163327130468574650_0002_m_000077_82, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72da38ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:08 INFO StagingCommitter: Starting: Task committer attempt_202105141955313163327130468574650_0002_m_000077_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313163327130468574650_0002_m_000077_82
[2021-05-14 16:56:08,212] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Task committer attempt_202105141955313163327130468574650_0002_m_000077_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313163327130468574650_0002_m_000077_82 : duration 0:00.002s
[2021-05-14 16:56:08,473] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Starting: Task committer attempt_202105141955317451936530313282326_0002_m_000074_79: needsTaskCommit() Task attempt_202105141955317451936530313282326_0002_m_000074_79
[2021-05-14 16:56:08,473] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Task committer attempt_202105141955317451936530313282326_0002_m_000074_79: needsTaskCommit() Task attempt_202105141955317451936530313282326_0002_m_000074_79: duration 0:00.001s
21/05/14 19:56:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317451936530313282326_0002_m_000074_79
[2021-05-14 16:56:08,475] {docker.py:276} INFO - 21/05/14 19:56:08 INFO Executor: Finished task 74.0 in stage 2.0 (TID 79). 4587 bytes result sent to driver
[2021-05-14 16:56:08,477] {docker.py:276} INFO - 21/05/14 19:56:08 INFO TaskSetManager: Finished task 74.0 in stage 2.0 (TID 79) in 1696 ms on 379a553b2355 (executor driver) (75/200)
[2021-05-14 16:56:08,479] {docker.py:276} INFO - 21/05/14 19:56:08 INFO TaskSetManager: Starting task 78.0 in stage 2.0 (TID 83) (379a553b2355, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:08,481] {docker.py:276} INFO - 21/05/14 19:56:08 INFO Executor: Running task 78.0 in stage 2.0 (TID 83)
[2021-05-14 16:56:08,490] {docker.py:276} INFO - 21/05/14 19:56:08 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:08,492] {docker.py:276} INFO - 21/05/14 19:56:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:08,492] {docker.py:276} INFO - 21/05/14 19:56:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315134093956995780515_0002_m_000078_83, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315134093956995780515_0002_m_000078_83}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315134093956995780515_0002}; taskId=attempt_202105141955315134093956995780515_0002_m_000078_83, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e757809}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:08,493] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Starting: Task committer attempt_202105141955315134093956995780515_0002_m_000078_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315134093956995780515_0002_m_000078_83
[2021-05-14 16:56:08,495] {docker.py:276} INFO - 21/05/14 19:56:08 INFO StagingCommitter: Task committer attempt_202105141955315134093956995780515_0002_m_000078_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315134093956995780515_0002_m_000078_83 : duration 0:00.003s
[2021-05-14 16:56:09,107] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Starting: Task committer attempt_20210514195531621009503788258019_0002_m_000075_80: needsTaskCommit() Task attempt_20210514195531621009503788258019_0002_m_000075_80
[2021-05-14 16:56:09,108] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Task committer attempt_20210514195531621009503788258019_0002_m_000075_80: needsTaskCommit() Task attempt_20210514195531621009503788258019_0002_m_000075_80: duration 0:00.000s
21/05/14 19:56:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531621009503788258019_0002_m_000075_80
[2021-05-14 16:56:09,110] {docker.py:276} INFO - 21/05/14 19:56:09 INFO Executor: Finished task 75.0 in stage 2.0 (TID 80). 4587 bytes result sent to driver
[2021-05-14 16:56:09,112] {docker.py:276} INFO - 21/05/14 19:56:09 INFO TaskSetManager: Starting task 79.0 in stage 2.0 (TID 84) (379a553b2355, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:09,112] {docker.py:276} INFO - 21/05/14 19:56:09 INFO TaskSetManager: Finished task 75.0 in stage 2.0 (TID 80) in 1674 ms on 379a553b2355 (executor driver) (76/200)
[2021-05-14 16:56:09,113] {docker.py:276} INFO - 21/05/14 19:56:09 INFO Executor: Running task 79.0 in stage 2.0 (TID 84)
[2021-05-14 16:56:09,122] {docker.py:276} INFO - 21/05/14 19:56:09 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:09,123] {docker.py:276} INFO - 21/05/14 19:56:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:09,125] {docker.py:276} INFO - 21/05/14 19:56:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:09,125] {docker.py:276} INFO - 21/05/14 19:56:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:09,125] {docker.py:276} INFO - 21/05/14 19:56:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317576758672254237035_0002_m_000079_84, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317576758672254237035_0002_m_000079_84}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317576758672254237035_0002}; taskId=attempt_202105141955317576758672254237035_0002_m_000079_84, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b1b7da9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:09,126] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Starting: Task committer attempt_202105141955317576758672254237035_0002_m_000079_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317576758672254237035_0002_m_000079_84
[2021-05-14 16:56:09,128] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Task committer attempt_202105141955317576758672254237035_0002_m_000079_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317576758672254237035_0002_m_000079_84 : duration 0:00.003s
[2021-05-14 16:56:09,869] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Starting: Task committer attempt_202105141955313163327130468574650_0002_m_000077_82: needsTaskCommit() Task attempt_202105141955313163327130468574650_0002_m_000077_82
[2021-05-14 16:56:09,869] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Task committer attempt_202105141955313163327130468574650_0002_m_000077_82: needsTaskCommit() Task attempt_202105141955313163327130468574650_0002_m_000077_82: duration 0:00.001s
21/05/14 19:56:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313163327130468574650_0002_m_000077_82
[2021-05-14 16:56:09,872] {docker.py:276} INFO - 21/05/14 19:56:09 INFO Executor: Finished task 77.0 in stage 2.0 (TID 82). 4544 bytes result sent to driver
[2021-05-14 16:56:09,873] {docker.py:276} INFO - 21/05/14 19:56:09 INFO TaskSetManager: Starting task 80.0 in stage 2.0 (TID 85) (379a553b2355, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:09,875] {docker.py:276} INFO - 21/05/14 19:56:09 INFO TaskSetManager: Finished task 77.0 in stage 2.0 (TID 82) in 1682 ms on 379a553b2355 (executor driver) (77/200)
[2021-05-14 16:56:09,876] {docker.py:276} INFO - 21/05/14 19:56:09 INFO Executor: Running task 80.0 in stage 2.0 (TID 85)
[2021-05-14 16:56:09,885] {docker.py:276} INFO - 21/05/14 19:56:09 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:09,886] {docker.py:276} INFO - 21/05/14 19:56:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:09,887] {docker.py:276} INFO - 21/05/14 19:56:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:09,888] {docker.py:276} INFO - 21/05/14 19:56:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:09,888] {docker.py:276} INFO - 21/05/14 19:56:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:09,889] {docker.py:276} INFO - 21/05/14 19:56:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314452362879669169800_0002_m_000080_85, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314452362879669169800_0002_m_000080_85}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314452362879669169800_0002}; taskId=attempt_202105141955314452362879669169800_0002_m_000080_85, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33888516}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:09 INFO StagingCommitter: Starting: Task committer attempt_202105141955314452362879669169800_0002_m_000080_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314452362879669169800_0002_m_000080_85
[2021-05-14 16:56:09,892] {docker.py:276} INFO - 21/05/14 19:56:09 INFO StagingCommitter: Task committer attempt_202105141955314452362879669169800_0002_m_000080_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314452362879669169800_0002_m_000080_85 : duration 0:00.003s
[2021-05-14 16:56:09,976] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Starting: Task committer attempt_202105141955315963968592737109691_0002_m_000076_81: needsTaskCommit() Task attempt_202105141955315963968592737109691_0002_m_000076_81
[2021-05-14 16:56:09,977] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Task committer attempt_202105141955315963968592737109691_0002_m_000076_81: needsTaskCommit() Task attempt_202105141955315963968592737109691_0002_m_000076_81: duration 0:00.001s
[2021-05-14 16:56:09,978] {docker.py:276} INFO - 21/05/14 19:56:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315963968592737109691_0002_m_000076_81
[2021-05-14 16:56:09,980] {docker.py:276} INFO - 21/05/14 19:56:10 INFO Executor: Finished task 76.0 in stage 2.0 (TID 81). 4544 bytes result sent to driver
[2021-05-14 16:56:09,982] {docker.py:276} INFO - 21/05/14 19:56:10 INFO TaskSetManager: Starting task 81.0 in stage 2.0 (TID 86) (379a553b2355, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:09,985] {docker.py:276} INFO - 21/05/14 19:56:10 INFO TaskSetManager: Finished task 76.0 in stage 2.0 (TID 81) in 1813 ms on 379a553b2355 (executor driver) (78/200)
[2021-05-14 16:56:09,985] {docker.py:276} INFO - 21/05/14 19:56:10 INFO Executor: Running task 81.0 in stage 2.0 (TID 86)
[2021-05-14 16:56:09,999] {docker.py:276} INFO - 21/05/14 19:56:10 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:10,000] {docker.py:276} INFO - 21/05/14 19:56:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:10,001] {docker.py:276} INFO - 21/05/14 19:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:10,002] {docker.py:276} INFO - 21/05/14 19:56:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:10,002] {docker.py:276} INFO - 21/05/14 19:56:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317132293228134152106_0002_m_000081_86, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317132293228134152106_0002_m_000081_86}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317132293228134152106_0002}; taskId=attempt_202105141955317132293228134152106_0002_m_000081_86, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bef02fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:10,002] {docker.py:276} INFO - 21/05/14 19:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:10,003] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Starting: Task committer attempt_202105141955317132293228134152106_0002_m_000081_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317132293228134152106_0002_m_000081_86
[2021-05-14 16:56:10,005] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Task committer attempt_202105141955317132293228134152106_0002_m_000081_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317132293228134152106_0002_m_000081_86 : duration 0:00.003s
[2021-05-14 16:56:10,097] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Starting: Task committer attempt_202105141955315134093956995780515_0002_m_000078_83: needsTaskCommit() Task attempt_202105141955315134093956995780515_0002_m_000078_83
21/05/14 19:56:10 INFO StagingCommitter: Task committer attempt_202105141955315134093956995780515_0002_m_000078_83: needsTaskCommit() Task attempt_202105141955315134093956995780515_0002_m_000078_83: duration 0:00.000s
[2021-05-14 16:56:10,098] {docker.py:276} INFO - 21/05/14 19:56:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315134093956995780515_0002_m_000078_83
[2021-05-14 16:56:10,099] {docker.py:276} INFO - 21/05/14 19:56:10 INFO Executor: Finished task 78.0 in stage 2.0 (TID 83). 4544 bytes result sent to driver
[2021-05-14 16:56:10,102] {docker.py:276} INFO - 21/05/14 19:56:10 INFO TaskSetManager: Starting task 82.0 in stage 2.0 (TID 87) (379a553b2355, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:10,103] {docker.py:276} INFO - 21/05/14 19:56:10 INFO TaskSetManager: Finished task 78.0 in stage 2.0 (TID 83) in 1627 ms on 379a553b2355 (executor driver) (79/200)
[2021-05-14 16:56:10,104] {docker.py:276} INFO - 21/05/14 19:56:10 INFO Executor: Running task 82.0 in stage 2.0 (TID 87)
[2021-05-14 16:56:10,114] {docker.py:276} INFO - 21/05/14 19:56:10 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:10,114] {docker.py:276} INFO - 21/05/14 19:56:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:10,116] {docker.py:276} INFO - 21/05/14 19:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:10,116] {docker.py:276} INFO - 21/05/14 19:56:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:10,117] {docker.py:276} INFO - 21/05/14 19:56:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316874949877616105600_0002_m_000082_87, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316874949877616105600_0002_m_000082_87}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316874949877616105600_0002}; taskId=attempt_202105141955316874949877616105600_0002_m_000082_87, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7809ba10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:10,117] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Starting: Task committer attempt_202105141955316874949877616105600_0002_m_000082_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316874949877616105600_0002_m_000082_87
[2021-05-14 16:56:10,120] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Task committer attempt_202105141955316874949877616105600_0002_m_000082_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316874949877616105600_0002_m_000082_87 : duration 0:00.003s
[2021-05-14 16:56:10,669] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Starting: Task committer attempt_202105141955317576758672254237035_0002_m_000079_84: needsTaskCommit() Task attempt_202105141955317576758672254237035_0002_m_000079_84
[2021-05-14 16:56:10,670] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Task committer attempt_202105141955317576758672254237035_0002_m_000079_84: needsTaskCommit() Task attempt_202105141955317576758672254237035_0002_m_000079_84: duration 0:00.001s
21/05/14 19:56:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317576758672254237035_0002_m_000079_84
[2021-05-14 16:56:10,672] {docker.py:276} INFO - 21/05/14 19:56:10 INFO Executor: Finished task 79.0 in stage 2.0 (TID 84). 4544 bytes result sent to driver
[2021-05-14 16:56:10,674] {docker.py:276} INFO - 21/05/14 19:56:10 INFO TaskSetManager: Starting task 83.0 in stage 2.0 (TID 88) (379a553b2355, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:10,675] {docker.py:276} INFO - 21/05/14 19:56:10 INFO TaskSetManager: Finished task 79.0 in stage 2.0 (TID 84) in 1566 ms on 379a553b2355 (executor driver) (80/200)
[2021-05-14 16:56:10,676] {docker.py:276} INFO - 21/05/14 19:56:10 INFO Executor: Running task 83.0 in stage 2.0 (TID 88)
[2021-05-14 16:56:10,685] {docker.py:276} INFO - 21/05/14 19:56:10 INFO ShuffleBlockFetcherIterator: Getting 4 (4.9 KiB) non-empty blocks including 4 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:10,688] {docker.py:276} INFO - 21/05/14 19:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312782897409853317111_0002_m_000083_88, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312782897409853317111_0002_m_000083_88}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312782897409853317111_0002}; taskId=attempt_202105141955312782897409853317111_0002_m_000083_88, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3539ea80}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:10,688] {docker.py:276} INFO - 21/05/14 19:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:10 INFO StagingCommitter: Starting: Task committer attempt_202105141955312782897409853317111_0002_m_000083_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312782897409853317111_0002_m_000083_88
[2021-05-14 16:56:10,691] {docker.py:276} INFO - 21/05/14 19:56:10 INFO StagingCommitter: Task committer attempt_202105141955312782897409853317111_0002_m_000083_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312782897409853317111_0002_m_000083_88 : duration 0:00.003s
[2021-05-14 16:56:11,506] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Starting: Task committer attempt_202105141955314452362879669169800_0002_m_000080_85: needsTaskCommit() Task attempt_202105141955314452362879669169800_0002_m_000080_85
[2021-05-14 16:56:11,507] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Task committer attempt_202105141955314452362879669169800_0002_m_000080_85: needsTaskCommit() Task attempt_202105141955314452362879669169800_0002_m_000080_85: duration 0:00.000s
21/05/14 19:56:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314452362879669169800_0002_m_000080_85
[2021-05-14 16:56:11,508] {docker.py:276} INFO - 21/05/14 19:56:11 INFO Executor: Finished task 80.0 in stage 2.0 (TID 85). 4544 bytes result sent to driver
[2021-05-14 16:56:11,509] {docker.py:276} INFO - 21/05/14 19:56:11 INFO TaskSetManager: Starting task 84.0 in stage 2.0 (TID 89) (379a553b2355, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:11,510] {docker.py:276} INFO - 21/05/14 19:56:11 INFO TaskSetManager: Finished task 80.0 in stage 2.0 (TID 85) in 1639 ms on 379a553b2355 (executor driver) (81/200)
[2021-05-14 16:56:11,511] {docker.py:276} INFO - 21/05/14 19:56:11 INFO Executor: Running task 84.0 in stage 2.0 (TID 89)
[2021-05-14 16:56:11,520] {docker.py:276} INFO - 21/05/14 19:56:11 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:11,521] {docker.py:276} INFO - 21/05/14 19:56:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:11,522] {docker.py:276} INFO - 21/05/14 19:56:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:11,522] {docker.py:276} INFO - 21/05/14 19:56:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316308323483670053874_0002_m_000084_89, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316308323483670053874_0002_m_000084_89}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316308323483670053874_0002}; taskId=attempt_202105141955316308323483670053874_0002_m_000084_89, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d41a919}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:11 INFO StagingCommitter: Starting: Task committer attempt_202105141955316308323483670053874_0002_m_000084_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316308323483670053874_0002_m_000084_89
[2021-05-14 16:56:11,524] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Task committer attempt_202105141955316308323483670053874_0002_m_000084_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316308323483670053874_0002_m_000084_89 : duration 0:00.002s
[2021-05-14 16:56:11,662] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Starting: Task committer attempt_202105141955317132293228134152106_0002_m_000081_86: needsTaskCommit() Task attempt_202105141955317132293228134152106_0002_m_000081_86
[2021-05-14 16:56:11,662] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Task committer attempt_202105141955317132293228134152106_0002_m_000081_86: needsTaskCommit() Task attempt_202105141955317132293228134152106_0002_m_000081_86: duration 0:00.001s
21/05/14 19:56:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317132293228134152106_0002_m_000081_86
[2021-05-14 16:56:11,664] {docker.py:276} INFO - 21/05/14 19:56:11 INFO Executor: Finished task 81.0 in stage 2.0 (TID 86). 4544 bytes result sent to driver
[2021-05-14 16:56:11,665] {docker.py:276} INFO - 21/05/14 19:56:11 INFO TaskSetManager: Starting task 85.0 in stage 2.0 (TID 90) (379a553b2355, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:11,666] {docker.py:276} INFO - 21/05/14 19:56:11 INFO Executor: Running task 85.0 in stage 2.0 (TID 90)
[2021-05-14 16:56:11,667] {docker.py:276} INFO - 21/05/14 19:56:11 INFO TaskSetManager: Finished task 81.0 in stage 2.0 (TID 86) in 1688 ms on 379a553b2355 (executor driver) (82/200)
[2021-05-14 16:56:11,676] {docker.py:276} INFO - 21/05/14 19:56:11 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:11,677] {docker.py:276} INFO - 21/05/14 19:56:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:11,678] {docker.py:276} INFO - 21/05/14 19:56:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317481304230612342412_0002_m_000085_90, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317481304230612342412_0002_m_000085_90}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317481304230612342412_0002}; taskId=attempt_202105141955317481304230612342412_0002_m_000085_90, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5da84c5a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:11,678] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Starting: Task committer attempt_202105141955317481304230612342412_0002_m_000085_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317481304230612342412_0002_m_000085_90
[2021-05-14 16:56:11,680] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Task committer attempt_202105141955317481304230612342412_0002_m_000085_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317481304230612342412_0002_m_000085_90 : duration 0:00.003s
[2021-05-14 16:56:11,900] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Starting: Task committer attempt_202105141955316874949877616105600_0002_m_000082_87: needsTaskCommit() Task attempt_202105141955316874949877616105600_0002_m_000082_87
[2021-05-14 16:56:11,900] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Task committer attempt_202105141955316874949877616105600_0002_m_000082_87: needsTaskCommit() Task attempt_202105141955316874949877616105600_0002_m_000082_87: duration 0:00.001s
[2021-05-14 16:56:11,901] {docker.py:276} INFO - 21/05/14 19:56:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316874949877616105600_0002_m_000082_87
[2021-05-14 16:56:11,902] {docker.py:276} INFO - 21/05/14 19:56:11 INFO Executor: Finished task 82.0 in stage 2.0 (TID 87). 4544 bytes result sent to driver
[2021-05-14 16:56:11,903] {docker.py:276} INFO - 21/05/14 19:56:11 INFO TaskSetManager: Starting task 86.0 in stage 2.0 (TID 91) (379a553b2355, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:11,904] {docker.py:276} INFO - 21/05/14 19:56:11 INFO TaskSetManager: Finished task 82.0 in stage 2.0 (TID 87) in 1804 ms on 379a553b2355 (executor driver) (83/200)
[2021-05-14 16:56:11,904] {docker.py:276} INFO - 21/05/14 19:56:11 INFO Executor: Running task 86.0 in stage 2.0 (TID 91)
[2021-05-14 16:56:11,913] {docker.py:276} INFO - 21/05/14 19:56:11 INFO ShuffleBlockFetcherIterator: Getting 4 (6.4 KiB) non-empty blocks including 4 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:11,913] {docker.py:276} INFO - 21/05/14 19:56:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:11,915] {docker.py:276} INFO - 21/05/14 19:56:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:11,915] {docker.py:276} INFO - 21/05/14 19:56:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312644676603076635620_0002_m_000086_91, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312644676603076635620_0002_m_000086_91}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312644676603076635620_0002}; taskId=attempt_202105141955312644676603076635620_0002_m_000086_91, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a837099}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:11,915] {docker.py:276} INFO - 21/05/14 19:56:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:11,916] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Starting: Task committer attempt_202105141955312644676603076635620_0002_m_000086_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312644676603076635620_0002_m_000086_91
[2021-05-14 16:56:11,920] {docker.py:276} INFO - 21/05/14 19:56:11 INFO StagingCommitter: Task committer attempt_202105141955312644676603076635620_0002_m_000086_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312644676603076635620_0002_m_000086_91 : duration 0:00.005s
[2021-05-14 16:56:12,247] {docker.py:276} INFO - 21/05/14 19:56:12 INFO StagingCommitter: Starting: Task committer attempt_202105141955312782897409853317111_0002_m_000083_88: needsTaskCommit() Task attempt_202105141955312782897409853317111_0002_m_000083_88
[2021-05-14 16:56:12,247] {docker.py:276} INFO - 21/05/14 19:56:12 INFO StagingCommitter: Task committer attempt_202105141955312782897409853317111_0002_m_000083_88: needsTaskCommit() Task attempt_202105141955312782897409853317111_0002_m_000083_88: duration 0:00.000s
21/05/14 19:56:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312782897409853317111_0002_m_000083_88
[2021-05-14 16:56:12,248] {docker.py:276} INFO - 21/05/14 19:56:12 INFO Executor: Finished task 83.0 in stage 2.0 (TID 88). 4544 bytes result sent to driver
[2021-05-14 16:56:12,250] {docker.py:276} INFO - 21/05/14 19:56:12 INFO TaskSetManager: Finished task 83.0 in stage 2.0 (TID 88) in 1578 ms on 379a553b2355 (executor driver) (84/200)
[2021-05-14 16:56:12,251] {docker.py:276} INFO - 21/05/14 19:56:12 INFO TaskSetManager: Starting task 87.0 in stage 2.0 (TID 92) (379a553b2355, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:12,252] {docker.py:276} INFO - 21/05/14 19:56:12 INFO Executor: Running task 87.0 in stage 2.0 (TID 92)
[2021-05-14 16:56:12,261] {docker.py:276} INFO - 21/05/14 19:56:12 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:12,263] {docker.py:276} INFO - 21/05/14 19:56:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315729611607145029188_0002_m_000087_92, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315729611607145029188_0002_m_000087_92}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315729611607145029188_0002}; taskId=attempt_202105141955315729611607145029188_0002_m_000087_92, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3511cc4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:12,264] {docker.py:276} INFO - 21/05/14 19:56:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:12,264] {docker.py:276} INFO - 21/05/14 19:56:12 INFO StagingCommitter: Starting: Task committer attempt_202105141955315729611607145029188_0002_m_000087_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315729611607145029188_0002_m_000087_92
[2021-05-14 16:56:12,267] {docker.py:276} INFO - 21/05/14 19:56:12 INFO StagingCommitter: Task committer attempt_202105141955315729611607145029188_0002_m_000087_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315729611607145029188_0002_m_000087_92 : duration 0:00.003s
[2021-05-14 16:56:13,102] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_202105141955316308323483670053874_0002_m_000084_89: needsTaskCommit() Task attempt_202105141955316308323483670053874_0002_m_000084_89
[2021-05-14 16:56:13,103] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_202105141955316308323483670053874_0002_m_000084_89: needsTaskCommit() Task attempt_202105141955316308323483670053874_0002_m_000084_89: duration 0:00.001s
21/05/14 19:56:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316308323483670053874_0002_m_000084_89
[2021-05-14 16:56:13,105] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Finished task 84.0 in stage 2.0 (TID 89). 4587 bytes result sent to driver
[2021-05-14 16:56:13,107] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Starting task 88.0 in stage 2.0 (TID 93) (379a553b2355, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:13,108] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Running task 88.0 in stage 2.0 (TID 93)
[2021-05-14 16:56:13,109] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Finished task 84.0 in stage 2.0 (TID 89) in 1602 ms on 379a553b2355 (executor driver) (85/200)
[2021-05-14 16:56:13,119] {docker.py:276} INFO - 21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:13,119] {docker.py:276} INFO - 21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:13,121] {docker.py:276} INFO - 21/05/14 19:56:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:13,122] {docker.py:276} INFO - 21/05/14 19:56:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:13,122] {docker.py:276} INFO - 21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:13,123] {docker.py:276} INFO - 21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314769583428930523464_0002_m_000088_93, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314769583428930523464_0002_m_000088_93}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314769583428930523464_0002}; taskId=attempt_202105141955314769583428930523464_0002_m_000088_93, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a970f74}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:13,123] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_202105141955314769583428930523464_0002_m_000088_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314769583428930523464_0002_m_000088_93
[2021-05-14 16:56:13,126] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_202105141955314769583428930523464_0002_m_000088_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314769583428930523464_0002_m_000088_93 : duration 0:00.003s
[2021-05-14 16:56:13,346] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_202105141955317481304230612342412_0002_m_000085_90: needsTaskCommit() Task attempt_202105141955317481304230612342412_0002_m_000085_90
[2021-05-14 16:56:13,347] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_202105141955317481304230612342412_0002_m_000085_90: needsTaskCommit() Task attempt_202105141955317481304230612342412_0002_m_000085_90: duration 0:00.001s
21/05/14 19:56:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317481304230612342412_0002_m_000085_90
[2021-05-14 16:56:13,349] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Finished task 85.0 in stage 2.0 (TID 90). 4587 bytes result sent to driver
[2021-05-14 16:56:13,351] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Starting task 89.0 in stage 2.0 (TID 94) (379a553b2355, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:13,352] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Finished task 85.0 in stage 2.0 (TID 90) in 1689 ms on 379a553b2355 (executor driver) (86/200)
[2021-05-14 16:56:13,353] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Running task 89.0 in stage 2.0 (TID 94)
[2021-05-14 16:56:13,363] {docker.py:276} INFO - 21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:13,364] {docker.py:276} INFO - 21/05/14 19:56:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:13,365] {docker.py:276} INFO - 21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531841211921051964891_0002_m_000089_94, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531841211921051964891_0002_m_000089_94}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531841211921051964891_0002}; taskId=attempt_20210514195531841211921051964891_0002_m_000089_94, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5233f965}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_20210514195531841211921051964891_0002_m_000089_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531841211921051964891_0002_m_000089_94
[2021-05-14 16:56:13,368] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_20210514195531841211921051964891_0002_m_000089_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531841211921051964891_0002_m_000089_94 : duration 0:00.002s
[2021-05-14 16:56:13,531] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_202105141955312644676603076635620_0002_m_000086_91: needsTaskCommit() Task attempt_202105141955312644676603076635620_0002_m_000086_91
[2021-05-14 16:56:13,532] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_202105141955312644676603076635620_0002_m_000086_91: needsTaskCommit() Task attempt_202105141955312644676603076635620_0002_m_000086_91: duration 0:00.001s
21/05/14 19:56:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312644676603076635620_0002_m_000086_91
[2021-05-14 16:56:13,533] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Finished task 86.0 in stage 2.0 (TID 91). 4587 bytes result sent to driver
[2021-05-14 16:56:13,534] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Starting task 90.0 in stage 2.0 (TID 95) (379a553b2355, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:13,535] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Running task 90.0 in stage 2.0 (TID 95)
[2021-05-14 16:56:13,536] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Finished task 86.0 in stage 2.0 (TID 91) in 1634 ms on 379a553b2355 (executor driver) (87/200)
[2021-05-14 16:56:13,548] {docker.py:276} INFO - 21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:13,549] {docker.py:276} INFO - 21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:13,550] {docker.py:276} INFO - 21/05/14 19:56:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:13,551] {docker.py:276} INFO - 21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531822871909320023154_0002_m_000090_95, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531822871909320023154_0002_m_000090_95}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531822871909320023154_0002}; taskId=attempt_20210514195531822871909320023154_0002_m_000090_95, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d5f6f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:13,551] {docker.py:276} INFO - 21/05/14 19:56:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:13,552] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_20210514195531822871909320023154_0002_m_000090_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531822871909320023154_0002_m_000090_95
[2021-05-14 16:56:13,554] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_20210514195531822871909320023154_0002_m_000090_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531822871909320023154_0002_m_000090_95 : duration 0:00.003s
[2021-05-14 16:56:13,818] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_202105141955315729611607145029188_0002_m_000087_92: needsTaskCommit() Task attempt_202105141955315729611607145029188_0002_m_000087_92
[2021-05-14 16:56:13,819] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_202105141955315729611607145029188_0002_m_000087_92: needsTaskCommit() Task attempt_202105141955315729611607145029188_0002_m_000087_92: duration 0:00.000s
21/05/14 19:56:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315729611607145029188_0002_m_000087_92
[2021-05-14 16:56:13,822] {docker.py:276} INFO - 21/05/14 19:56:13 INFO Executor: Finished task 87.0 in stage 2.0 (TID 92). 4587 bytes result sent to driver
[2021-05-14 16:56:13,823] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Starting task 91.0 in stage 2.0 (TID 96) (379a553b2355, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:13,824] {docker.py:276} INFO - 21/05/14 19:56:13 INFO TaskSetManager: Finished task 87.0 in stage 2.0 (TID 92) in 1540 ms on 379a553b2355 (executor driver) (88/200)
21/05/14 19:56:13 INFO Executor: Running task 91.0 in stage 2.0 (TID 96)
[2021-05-14 16:56:13,834] {docker.py:276} INFO - 21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Getting 4 (6.2 KiB) non-empty blocks including 4 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:13,836] {docker.py:276} INFO - 21/05/14 19:56:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316765155115661753646_0002_m_000091_96, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316765155115661753646_0002_m_000091_96}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316765155115661753646_0002}; taskId=attempt_202105141955316765155115661753646_0002_m_000091_96, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4503e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:13,837] {docker.py:276} INFO - 21/05/14 19:56:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:13 INFO StagingCommitter: Starting: Task committer attempt_202105141955316765155115661753646_0002_m_000091_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316765155115661753646_0002_m_000091_96
[2021-05-14 16:56:13,840] {docker.py:276} INFO - 21/05/14 19:56:13 INFO StagingCommitter: Task committer attempt_202105141955316765155115661753646_0002_m_000091_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316765155115661753646_0002_m_000091_96 : duration 0:00.003s
[2021-05-14 16:56:15,104] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_202105141955314769583428930523464_0002_m_000088_93: needsTaskCommit() Task attempt_202105141955314769583428930523464_0002_m_000088_93
[2021-05-14 16:56:15,105] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_202105141955314769583428930523464_0002_m_000088_93: needsTaskCommit() Task attempt_202105141955314769583428930523464_0002_m_000088_93: duration 0:00.001s
[2021-05-14 16:56:15,106] {docker.py:276} INFO - 21/05/14 19:56:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314769583428930523464_0002_m_000088_93
[2021-05-14 16:56:15,106] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Finished task 88.0 in stage 2.0 (TID 93). 4544 bytes result sent to driver
[2021-05-14 16:56:15,107] {docker.py:276} INFO - 21/05/14 19:56:15 INFO TaskSetManager: Finished task 88.0 in stage 2.0 (TID 93) in 1968 ms on 379a553b2355 (executor driver) (89/200)
[2021-05-14 16:56:15,108] {docker.py:276} INFO - 21/05/14 19:56:15 INFO TaskSetManager: Starting task 92.0 in stage 2.0 (TID 97) (379a553b2355, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:15,110] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Running task 92.0 in stage 2.0 (TID 97)
[2021-05-14 16:56:15,119] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:15,120] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:15,121] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_20210514195531822871909320023154_0002_m_000090_95: needsTaskCommit() Task attempt_20210514195531822871909320023154_0002_m_000090_95
[2021-05-14 16:56:15,122] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_20210514195531822871909320023154_0002_m_000090_95: needsTaskCommit() Task attempt_20210514195531822871909320023154_0002_m_000090_95: duration 0:00.000s
[2021-05-14 16:56:15,122] {docker.py:276} INFO - 21/05/14 19:56:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531822871909320023154_0002_m_000090_95
[2021-05-14 16:56:15,125] {docker.py:276} INFO - 21/05/14 19:56:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:15,126] {docker.py:276} INFO - 21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318683928658464115159_0002_m_000092_97, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318683928658464115159_0002_m_000092_97}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318683928658464115159_0002}; taskId=attempt_202105141955318683928658464115159_0002_m_000092_97, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3008b836}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_202105141955318683928658464115159_0002_m_000092_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318683928658464115159_0002_m_000092_97
[2021-05-14 16:56:15,126] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Finished task 90.0 in stage 2.0 (TID 95). 4544 bytes result sent to driver
[2021-05-14 16:56:15,127] {docker.py:276} INFO - 21/05/14 19:56:15 INFO TaskSetManager: Starting task 93.0 in stage 2.0 (TID 98) (379a553b2355, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:15,127] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Running task 93.0 in stage 2.0 (TID 98)
21/05/14 19:56:15 INFO TaskSetManager: Finished task 90.0 in stage 2.0 (TID 95) in 1559 ms on 379a553b2355 (executor driver) (90/200)
[2021-05-14 16:56:15,130] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_202105141955318683928658464115159_0002_m_000092_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318683928658464115159_0002_m_000092_97 : duration 0:00.006s
[2021-05-14 16:56:15,139] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:15,140] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:15,142] {docker.py:276} INFO - 21/05/14 19:56:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955319030058521426966593_0002_m_000093_98, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319030058521426966593_0002_m_000093_98}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955319030058521426966593_0002}; taskId=attempt_202105141955319030058521426966593_0002_m_000093_98, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d384e50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_202105141955319030058521426966593_0002_m_000093_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319030058521426966593_0002_m_000093_98
[2021-05-14 16:56:15,145] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_202105141955319030058521426966593_0002_m_000093_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319030058521426966593_0002_m_000093_98 : duration 0:00.002s
[2021-05-14 16:56:15,146] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_20210514195531841211921051964891_0002_m_000089_94: needsTaskCommit() Task attempt_20210514195531841211921051964891_0002_m_000089_94
[2021-05-14 16:56:15,147] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_20210514195531841211921051964891_0002_m_000089_94: needsTaskCommit() Task attempt_20210514195531841211921051964891_0002_m_000089_94: duration 0:00.000s
21/05/14 19:56:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531841211921051964891_0002_m_000089_94
[2021-05-14 16:56:15,148] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Finished task 89.0 in stage 2.0 (TID 94). 4544 bytes result sent to driver
[2021-05-14 16:56:15,149] {docker.py:276} INFO - 21/05/14 19:56:15 INFO TaskSetManager: Starting task 94.0 in stage 2.0 (TID 99) (379a553b2355, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:15,151] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Running task 94.0 in stage 2.0 (TID 99)
21/05/14 19:56:15 INFO TaskSetManager: Finished task 89.0 in stage 2.0 (TID 94) in 1767 ms on 379a553b2355 (executor driver) (91/200)
[2021-05-14 16:56:15,158] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:15,160] {docker.py:276} INFO - 21/05/14 19:56:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:15,160] {docker.py:276} INFO - 21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313304741939717627927_0002_m_000094_99, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313304741939717627927_0002_m_000094_99}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313304741939717627927_0002}; taskId=attempt_202105141955313304741939717627927_0002_m_000094_99, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72217c67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_202105141955313304741939717627927_0002_m_000094_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313304741939717627927_0002_m_000094_99
[2021-05-14 16:56:15,163] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_202105141955313304741939717627927_0002_m_000094_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313304741939717627927_0002_m_000094_99 : duration 0:00.002s
[2021-05-14 16:56:15,408] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_202105141955316765155115661753646_0002_m_000091_96: needsTaskCommit() Task attempt_202105141955316765155115661753646_0002_m_000091_96
[2021-05-14 16:56:15,408] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_202105141955316765155115661753646_0002_m_000091_96: needsTaskCommit() Task attempt_202105141955316765155115661753646_0002_m_000091_96: duration 0:00.001s
[2021-05-14 16:56:15,409] {docker.py:276} INFO - 21/05/14 19:56:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316765155115661753646_0002_m_000091_96
[2021-05-14 16:56:15,410] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Finished task 91.0 in stage 2.0 (TID 96). 4544 bytes result sent to driver
[2021-05-14 16:56:15,411] {docker.py:276} INFO - 21/05/14 19:56:15 INFO TaskSetManager: Starting task 95.0 in stage 2.0 (TID 100) (379a553b2355, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:15,412] {docker.py:276} INFO - 21/05/14 19:56:15 INFO TaskSetManager: Finished task 91.0 in stage 2.0 (TID 96) in 1591 ms on 379a553b2355 (executor driver) (92/200)
[2021-05-14 16:56:15,414] {docker.py:276} INFO - 21/05/14 19:56:15 INFO Executor: Running task 95.0 in stage 2.0 (TID 100)
[2021-05-14 16:56:15,425] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:15,426] {docker.py:276} INFO - 21/05/14 19:56:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:15,428] {docker.py:276} INFO - 21/05/14 19:56:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:15,428] {docker.py:276} INFO - 21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:15,429] {docker.py:276} INFO - 21/05/14 19:56:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955319129970900022884153_0002_m_000095_100, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319129970900022884153_0002_m_000095_100}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955319129970900022884153_0002}; taskId=attempt_202105141955319129970900022884153_0002_m_000095_100, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c291dda}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:15,430] {docker.py:276} INFO - 21/05/14 19:56:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:15,430] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Starting: Task committer attempt_202105141955319129970900022884153_0002_m_000095_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319129970900022884153_0002_m_000095_100
[2021-05-14 16:56:15,434] {docker.py:276} INFO - 21/05/14 19:56:15 INFO StagingCommitter: Task committer attempt_202105141955319129970900022884153_0002_m_000095_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319129970900022884153_0002_m_000095_100 : duration 0:00.004s
[2021-05-14 16:56:16,739] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Starting: Task committer attempt_202105141955319030058521426966593_0002_m_000093_98: needsTaskCommit() Task attempt_202105141955319030058521426966593_0002_m_000093_98
[2021-05-14 16:56:16,740] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Task committer attempt_202105141955319030058521426966593_0002_m_000093_98: needsTaskCommit() Task attempt_202105141955319030058521426966593_0002_m_000093_98: duration 0:00.001s
[2021-05-14 16:56:16,741] {docker.py:276} INFO - 21/05/14 19:56:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955319030058521426966593_0002_m_000093_98
[2021-05-14 16:56:16,741] {docker.py:276} INFO - 21/05/14 19:56:16 INFO Executor: Finished task 93.0 in stage 2.0 (TID 98). 4544 bytes result sent to driver
21/05/14 19:56:16 INFO StagingCommitter: Starting: Task committer attempt_202105141955318683928658464115159_0002_m_000092_97: needsTaskCommit() Task attempt_202105141955318683928658464115159_0002_m_000092_97
[2021-05-14 16:56:16,743] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Task committer attempt_202105141955318683928658464115159_0002_m_000092_97: needsTaskCommit() Task attempt_202105141955318683928658464115159_0002_m_000092_97: duration 0:00.002s
21/05/14 19:56:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318683928658464115159_0002_m_000092_97
[2021-05-14 16:56:16,744] {docker.py:276} INFO - 21/05/14 19:56:16 INFO TaskSetManager: Starting task 96.0 in stage 2.0 (TID 101) (379a553b2355, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:16,745] {docker.py:276} INFO - 21/05/14 19:56:16 INFO TaskSetManager: Finished task 93.0 in stage 2.0 (TID 98) in 1621 ms on 379a553b2355 (executor driver) (93/200)
[2021-05-14 16:56:16,746] {docker.py:276} INFO - 21/05/14 19:56:16 INFO Executor: Running task 96.0 in stage 2.0 (TID 101)
[2021-05-14 16:56:16,746] {docker.py:276} INFO - 21/05/14 19:56:16 INFO Executor: Finished task 92.0 in stage 2.0 (TID 97). 4544 bytes result sent to driver
[2021-05-14 16:56:16,747] {docker.py:276} INFO - 21/05/14 19:56:16 INFO TaskSetManager: Starting task 97.0 in stage 2.0 (TID 102) (379a553b2355, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:16,750] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Starting: Task committer attempt_202105141955313304741939717627927_0002_m_000094_99: needsTaskCommit() Task attempt_202105141955313304741939717627927_0002_m_000094_99
[2021-05-14 16:56:16,751] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Task committer attempt_202105141955313304741939717627927_0002_m_000094_99: needsTaskCommit() Task attempt_202105141955313304741939717627927_0002_m_000094_99: duration 0:00.000s
21/05/14 19:56:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313304741939717627927_0002_m_000094_99
[2021-05-14 16:56:16,751] {docker.py:276} INFO - 21/05/14 19:56:16 INFO TaskSetManager: Finished task 92.0 in stage 2.0 (TID 97) in 1642 ms on 379a553b2355 (executor driver) (94/200)
[2021-05-14 16:56:16,752] {docker.py:276} INFO - 21/05/14 19:56:16 INFO Executor: Finished task 94.0 in stage 2.0 (TID 99). 4544 bytes result sent to driver
[2021-05-14 16:56:16,752] {docker.py:276} INFO - 21/05/14 19:56:16 INFO Executor: Running task 97.0 in stage 2.0 (TID 102)
[2021-05-14 16:56:16,752] {docker.py:276} INFO - 21/05/14 19:56:16 INFO TaskSetManager: Starting task 98.0 in stage 2.0 (TID 103) (379a553b2355, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:16,754] {docker.py:276} INFO - 21/05/14 19:56:16 INFO TaskSetManager: Finished task 94.0 in stage 2.0 (TID 99) in 1606 ms on 379a553b2355 (executor driver) (95/200)
[2021-05-14 16:56:16,755] {docker.py:276} INFO - 21/05/14 19:56:16 INFO Executor: Running task 98.0 in stage 2.0 (TID 103)
[2021-05-14 16:56:16,760] {docker.py:276} INFO - 21/05/14 19:56:16 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:16,761] {docker.py:276} INFO - 21/05/14 19:56:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:16,761] {docker.py:276} INFO - 21/05/14 19:56:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:16,762] {docker.py:276} INFO - 21/05/14 19:56:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:16,762] {docker.py:276} INFO - 21/05/14 19:56:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:16,763] {docker.py:276} INFO - 21/05/14 19:56:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315978843320652299716_0002_m_000096_101, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315978843320652299716_0002_m_000096_101}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315978843320652299716_0002}; taskId=attempt_202105141955315978843320652299716_0002_m_000096_101, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c3c9b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:16,763] {docker.py:276} INFO - 21/05/14 19:56:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:16,764] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Starting: Task committer attempt_202105141955315978843320652299716_0002_m_000096_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315978843320652299716_0002_m_000096_101
[2021-05-14 16:56:16,766] {docker.py:276} INFO - 21/05/14 19:56:16 INFO ShuffleBlockFetcherIterator: Getting 4 (6.4 KiB) non-empty blocks including 4 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-14 16:56:16,766] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Task committer attempt_202105141955315978843320652299716_0002_m_000096_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315978843320652299716_0002_m_000096_101 : duration 0:00.003s
[2021-05-14 16:56:16,767] {docker.py:276} INFO - 21/05/14 19:56:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311868302540531194849_0002_m_000097_102, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311868302540531194849_0002_m_000097_102}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311868302540531194849_0002}; taskId=attempt_202105141955311868302540531194849_0002_m_000097_102, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6acf1ad1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:16,767] {docker.py:276} INFO - 21/05/14 19:56:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:16,768] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Starting: Task committer attempt_202105141955311868302540531194849_0002_m_000097_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311868302540531194849_0002_m_000097_102
[2021-05-14 16:56:16,768] {docker.py:276} INFO - 21/05/14 19:56:16 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:16,769] {docker.py:276} INFO - 21/05/14 19:56:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:16,770] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Task committer attempt_202105141955311868302540531194849_0002_m_000097_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311868302540531194849_0002_m_000097_102 : duration 0:00.003s
[2021-05-14 16:56:16,772] {docker.py:276} INFO - 21/05/14 19:56:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:16,772] {docker.py:276} INFO - 21/05/14 19:56:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:16,773] {docker.py:276} INFO - 21/05/14 19:56:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316615559927034666428_0002_m_000098_103, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316615559927034666428_0002_m_000098_103}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316615559927034666428_0002}; taskId=attempt_202105141955316615559927034666428_0002_m_000098_103, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7294e00d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:16 INFO StagingCommitter: Starting: Task committer attempt_202105141955316615559927034666428_0002_m_000098_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316615559927034666428_0002_m_000098_103
[2021-05-14 16:56:16,776] {docker.py:276} INFO - 21/05/14 19:56:16 INFO StagingCommitter: Task committer attempt_202105141955316615559927034666428_0002_m_000098_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316615559927034666428_0002_m_000098_103 : duration 0:00.004s
[2021-05-14 16:56:17,041] {docker.py:276} INFO - 21/05/14 19:56:17 INFO StagingCommitter: Starting: Task committer attempt_202105141955319129970900022884153_0002_m_000095_100: needsTaskCommit() Task attempt_202105141955319129970900022884153_0002_m_000095_100
[2021-05-14 16:56:17,042] {docker.py:276} INFO - 21/05/14 19:56:17 INFO StagingCommitter: Task committer attempt_202105141955319129970900022884153_0002_m_000095_100: needsTaskCommit() Task attempt_202105141955319129970900022884153_0002_m_000095_100: duration 0:00.001s
[2021-05-14 16:56:17,042] {docker.py:276} INFO - 21/05/14 19:56:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955319129970900022884153_0002_m_000095_100
[2021-05-14 16:56:17,043] {docker.py:276} INFO - 21/05/14 19:56:17 INFO Executor: Finished task 95.0 in stage 2.0 (TID 100). 4544 bytes result sent to driver
[2021-05-14 16:56:17,044] {docker.py:276} INFO - 21/05/14 19:56:17 INFO TaskSetManager: Starting task 99.0 in stage 2.0 (TID 104) (379a553b2355, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:17,045] {docker.py:276} INFO - 21/05/14 19:56:17 INFO Executor: Running task 99.0 in stage 2.0 (TID 104)
21/05/14 19:56:17 INFO TaskSetManager: Finished task 95.0 in stage 2.0 (TID 100) in 1636 ms on 379a553b2355 (executor driver) (96/200)
[2021-05-14 16:56:17,053] {docker.py:276} INFO - 21/05/14 19:56:17 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:17,055] {docker.py:276} INFO - 21/05/14 19:56:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:17,055] {docker.py:276} INFO - 21/05/14 19:56:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:17,056] {docker.py:276} INFO - 21/05/14 19:56:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315118948346427946743_0002_m_000099_104, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315118948346427946743_0002_m_000099_104}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315118948346427946743_0002}; taskId=attempt_202105141955315118948346427946743_0002_m_000099_104, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b028628}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:17,056] {docker.py:276} INFO - 21/05/14 19:56:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:17,056] {docker.py:276} INFO - 21/05/14 19:56:17 INFO StagingCommitter: Starting: Task committer attempt_202105141955315118948346427946743_0002_m_000099_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315118948346427946743_0002_m_000099_104
[2021-05-14 16:56:17,058] {docker.py:276} INFO - 21/05/14 19:56:17 INFO StagingCommitter: Task committer attempt_202105141955315118948346427946743_0002_m_000099_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315118948346427946743_0002_m_000099_104 : duration 0:00.003s
[2021-05-14 16:56:18,371] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955315978843320652299716_0002_m_000096_101: needsTaskCommit() Task attempt_202105141955315978843320652299716_0002_m_000096_101
[2021-05-14 16:56:18,372] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955315978843320652299716_0002_m_000096_101: needsTaskCommit() Task attempt_202105141955315978843320652299716_0002_m_000096_101: duration 0:00.000s
21/05/14 19:56:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315978843320652299716_0002_m_000096_101
[2021-05-14 16:56:18,374] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Finished task 96.0 in stage 2.0 (TID 101). 4544 bytes result sent to driver
[2021-05-14 16:56:18,376] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Starting task 100.0 in stage 2.0 (TID 105) (379a553b2355, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:18,377] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Running task 100.0 in stage 2.0 (TID 105)
21/05/14 19:56:18 INFO TaskSetManager: Finished task 96.0 in stage 2.0 (TID 101) in 1637 ms on 379a553b2355 (executor driver) (97/200)
[2021-05-14 16:56:18,396] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955311868302540531194849_0002_m_000097_102: needsTaskCommit() Task attempt_202105141955311868302540531194849_0002_m_000097_102
21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955311868302540531194849_0002_m_000097_102: needsTaskCommit() Task attempt_202105141955311868302540531194849_0002_m_000097_102: duration 0:00.000s
21/05/14 19:56:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311868302540531194849_0002_m_000097_102
[2021-05-14 16:56:18,397] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Finished task 97.0 in stage 2.0 (TID 102). 4587 bytes result sent to driver
21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:18,397] {docker.py:276} INFO - 21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:18,398] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Starting task 101.0 in stage 2.0 (TID 106) (379a553b2355, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:18,399] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Finished task 97.0 in stage 2.0 (TID 102) in 1654 ms on 379a553b2355 (executor driver) (98/200)
21/05/14 19:56:18 INFO Executor: Running task 101.0 in stage 2.0 (TID 106)
[2021-05-14 16:56:18,400] {docker.py:276} INFO - 21/05/14 19:56:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:18,401] {docker.py:276} INFO - 21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316237949535582722012_0002_m_000100_105, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316237949535582722012_0002_m_000100_105}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316237949535582722012_0002}; taskId=attempt_202105141955316237949535582722012_0002_m_000100_105, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2943e093}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:18,401] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955316237949535582722012_0002_m_000100_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316237949535582722012_0002_m_000100_105
[2021-05-14 16:56:18,403] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955316237949535582722012_0002_m_000100_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316237949535582722012_0002_m_000100_105 : duration 0:00.003s
[2021-05-14 16:56:18,409] {docker.py:276} INFO - 21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:18,410] {docker.py:276} INFO - 21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:18,413] {docker.py:276} INFO - 21/05/14 19:56:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:18,413] {docker.py:276} INFO - 21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:18,414] {docker.py:276} INFO - 21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311490974170523767665_0002_m_000101_106, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311490974170523767665_0002_m_000101_106}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311490974170523767665_0002}; taskId=attempt_202105141955311490974170523767665_0002_m_000101_106, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6813b607}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:18,414] {docker.py:276} INFO - 21/05/14 19:56:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955311490974170523767665_0002_m_000101_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311490974170523767665_0002_m_000101_106
[2021-05-14 16:56:18,420] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955311490974170523767665_0002_m_000101_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311490974170523767665_0002_m_000101_106 : duration 0:00.006s
[2021-05-14 16:56:18,651] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955315118948346427946743_0002_m_000099_104: needsTaskCommit() Task attempt_202105141955315118948346427946743_0002_m_000099_104
[2021-05-14 16:56:18,653] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955315118948346427946743_0002_m_000099_104: needsTaskCommit() Task attempt_202105141955315118948346427946743_0002_m_000099_104: duration 0:00.001s
21/05/14 19:56:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315118948346427946743_0002_m_000099_104
[2021-05-14 16:56:18,655] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Finished task 99.0 in stage 2.0 (TID 104). 4587 bytes result sent to driver
[2021-05-14 16:56:18,657] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Starting task 102.0 in stage 2.0 (TID 107) (379a553b2355, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:18,658] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Running task 102.0 in stage 2.0 (TID 107)
[2021-05-14 16:56:18,659] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Finished task 99.0 in stage 2.0 (TID 104) in 1617 ms on 379a553b2355 (executor driver) (99/200)
[2021-05-14 16:56:18,668] {docker.py:276} INFO - 21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:18,670] {docker.py:276} INFO - 21/05/14 19:56:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:18,671] {docker.py:276} INFO - 21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316628842061872717970_0002_m_000102_107, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316628842061872717970_0002_m_000102_107}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316628842061872717970_0002}; taskId=attempt_202105141955316628842061872717970_0002_m_000102_107, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61e54d26}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955316628842061872717970_0002_m_000102_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316628842061872717970_0002_m_000102_107
[2021-05-14 16:56:18,672] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955316615559927034666428_0002_m_000098_103: needsTaskCommit() Task attempt_202105141955316615559927034666428_0002_m_000098_103
[2021-05-14 16:56:18,673] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955316615559927034666428_0002_m_000098_103: needsTaskCommit() Task attempt_202105141955316615559927034666428_0002_m_000098_103: duration 0:00.001s
[2021-05-14 16:56:18,673] {docker.py:276} INFO - 21/05/14 19:56:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316615559927034666428_0002_m_000098_103
[2021-05-14 16:56:18,674] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Finished task 98.0 in stage 2.0 (TID 103). 4587 bytes result sent to driver
21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955316628842061872717970_0002_m_000102_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316628842061872717970_0002_m_000102_107 : duration 0:00.003s
[2021-05-14 16:56:18,676] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Starting task 103.0 in stage 2.0 (TID 108) (379a553b2355, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:18,676] {docker.py:276} INFO - 21/05/14 19:56:18 INFO TaskSetManager: Finished task 98.0 in stage 2.0 (TID 103) in 1927 ms on 379a553b2355 (executor driver) (100/200)
[2021-05-14 16:56:18,678] {docker.py:276} INFO - 21/05/14 19:56:18 INFO Executor: Running task 103.0 in stage 2.0 (TID 108)
[2021-05-14 16:56:18,686] {docker.py:276} INFO - 21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:18,688] {docker.py:276} INFO - 21/05/14 19:56:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317263352185646345750_0002_m_000103_108, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317263352185646345750_0002_m_000103_108}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317263352185646345750_0002}; taskId=attempt_202105141955317263352185646345750_0002_m_000103_108, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@95019b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:18,688] {docker.py:276} INFO - 21/05/14 19:56:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:18 INFO StagingCommitter: Starting: Task committer attempt_202105141955317263352185646345750_0002_m_000103_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317263352185646345750_0002_m_000103_108
[2021-05-14 16:56:18,692] {docker.py:276} INFO - 21/05/14 19:56:18 INFO StagingCommitter: Task committer attempt_202105141955317263352185646345750_0002_m_000103_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317263352185646345750_0002_m_000103_108 : duration 0:00.003s
[2021-05-14 16:56:20,005] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955316237949535582722012_0002_m_000100_105: needsTaskCommit() Task attempt_202105141955316237949535582722012_0002_m_000100_105
[2021-05-14 16:56:20,006] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955316237949535582722012_0002_m_000100_105: needsTaskCommit() Task attempt_202105141955316237949535582722012_0002_m_000100_105: duration 0:00.002s
21/05/14 19:56:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316237949535582722012_0002_m_000100_105
[2021-05-14 16:56:20,009] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Finished task 100.0 in stage 2.0 (TID 105). 4587 bytes result sent to driver
[2021-05-14 16:56:20,011] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Starting task 104.0 in stage 2.0 (TID 109) (379a553b2355, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:20,012] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Finished task 100.0 in stage 2.0 (TID 105) in 1639 ms on 379a553b2355 (executor driver) (101/200)
[2021-05-14 16:56:20,012] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Running task 104.0 in stage 2.0 (TID 109)
[2021-05-14 16:56:20,022] {docker.py:276} INFO - 21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:20,024] {docker.py:276} INFO - 21/05/14 19:56:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312712499087202911581_0002_m_000104_109, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312712499087202911581_0002_m_000104_109}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312712499087202911581_0002}; taskId=attempt_202105141955312712499087202911581_0002_m_000104_109, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c40184a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:20,024] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955312712499087202911581_0002_m_000104_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312712499087202911581_0002_m_000104_109
[2021-05-14 16:56:20,029] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955312712499087202911581_0002_m_000104_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312712499087202911581_0002_m_000104_109 : duration 0:00.005s
[2021-05-14 16:56:20,055] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955311490974170523767665_0002_m_000101_106: needsTaskCommit() Task attempt_202105141955311490974170523767665_0002_m_000101_106
[2021-05-14 16:56:20,055] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955311490974170523767665_0002_m_000101_106: needsTaskCommit() Task attempt_202105141955311490974170523767665_0002_m_000101_106: duration 0:00.000s
21/05/14 19:56:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311490974170523767665_0002_m_000101_106
[2021-05-14 16:56:20,056] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Finished task 101.0 in stage 2.0 (TID 106). 4544 bytes result sent to driver
[2021-05-14 16:56:20,058] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Starting task 105.0 in stage 2.0 (TID 110) (379a553b2355, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:20,058] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Finished task 101.0 in stage 2.0 (TID 106) in 1662 ms on 379a553b2355 (executor driver) (102/200)
21/05/14 19:56:20 INFO Executor: Running task 105.0 in stage 2.0 (TID 110)
[2021-05-14 16:56:20,066] {docker.py:276} INFO - 21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:20,068] {docker.py:276} INFO - 21/05/14 19:56:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311974488764499493948_0002_m_000105_110, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311974488764499493948_0002_m_000105_110}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311974488764499493948_0002}; taskId=attempt_202105141955311974488764499493948_0002_m_000105_110, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ab3a3c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:20,068] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955311974488764499493948_0002_m_000105_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311974488764499493948_0002_m_000105_110
[2021-05-14 16:56:20,071] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955311974488764499493948_0002_m_000105_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311974488764499493948_0002_m_000105_110 : duration 0:00.003s
[2021-05-14 16:56:20,259] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955317263352185646345750_0002_m_000103_108: needsTaskCommit() Task attempt_202105141955317263352185646345750_0002_m_000103_108
[2021-05-14 16:56:20,260] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955317263352185646345750_0002_m_000103_108: needsTaskCommit() Task attempt_202105141955317263352185646345750_0002_m_000103_108: duration 0:00.002s
21/05/14 19:56:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317263352185646345750_0002_m_000103_108
[2021-05-14 16:56:20,263] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Finished task 103.0 in stage 2.0 (TID 108). 4544 bytes result sent to driver
[2021-05-14 16:56:20,265] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955316628842061872717970_0002_m_000102_107: needsTaskCommit() Task attempt_202105141955316628842061872717970_0002_m_000102_107
[2021-05-14 16:56:20,265] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Starting task 106.0 in stage 2.0 (TID 111) (379a553b2355, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955316628842061872717970_0002_m_000102_107: needsTaskCommit() Task attempt_202105141955316628842061872717970_0002_m_000102_107: duration 0:00.001s
21/05/14 19:56:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316628842061872717970_0002_m_000102_107
[2021-05-14 16:56:20,266] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Finished task 103.0 in stage 2.0 (TID 108) in 1592 ms on 379a553b2355 (executor driver) (103/200)
[2021-05-14 16:56:20,267] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Running task 106.0 in stage 2.0 (TID 111)
[2021-05-14 16:56:20,268] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Finished task 102.0 in stage 2.0 (TID 107). 4544 bytes result sent to driver
[2021-05-14 16:56:20,269] {docker.py:276} INFO - 21/05/14 19:56:20 INFO TaskSetManager: Starting task 107.0 in stage 2.0 (TID 112) (379a553b2355, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:20,270] {docker.py:276} INFO - 21/05/14 19:56:20 INFO Executor: Running task 107.0 in stage 2.0 (TID 112)
21/05/14 19:56:20 INFO TaskSetManager: Finished task 102.0 in stage 2.0 (TID 107) in 1616 ms on 379a553b2355 (executor driver) (104/200)
[2021-05-14 16:56:20,278] {docker.py:276} INFO - 21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:20,280] {docker.py:276} INFO - 21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:20,281] {docker.py:276} INFO - 21/05/14 19:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:20,281] {docker.py:276} INFO - 21/05/14 19:56:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:20,281] {docker.py:276} INFO - 21/05/14 19:56:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:20,282] {docker.py:276} INFO - 21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:20,282] {docker.py:276} INFO - 21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311315308209102537103_0002_m_000106_111, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311315308209102537103_0002_m_000106_111}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311315308209102537103_0002}; taskId=attempt_202105141955311315308209102537103_0002_m_000106_111, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17ffe9e4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:20,282] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955311315308209102537103_0002_m_000106_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311315308209102537103_0002_m_000106_111
[2021-05-14 16:56:20,283] {docker.py:276} INFO - 21/05/14 19:56:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:20,283] {docker.py:276} INFO - 21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:20,283] {docker.py:276} INFO - 21/05/14 19:56:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955319122249739183881143_0002_m_000107_112, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319122249739183881143_0002_m_000107_112}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955319122249739183881143_0002}; taskId=attempt_202105141955319122249739183881143_0002_m_000107_112, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7785ed5a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:20,283] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Starting: Task committer attempt_202105141955319122249739183881143_0002_m_000107_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319122249739183881143_0002_m_000107_112
[2021-05-14 16:56:20,286] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955311315308209102537103_0002_m_000106_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311315308209102537103_0002_m_000106_111 : duration 0:00.004s
[2021-05-14 16:56:20,287] {docker.py:276} INFO - 21/05/14 19:56:20 INFO StagingCommitter: Task committer attempt_202105141955319122249739183881143_0002_m_000107_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319122249739183881143_0002_m_000107_112 : duration 0:00.004s
[2021-05-14 16:56:21,643] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955312712499087202911581_0002_m_000104_109: needsTaskCommit() Task attempt_202105141955312712499087202911581_0002_m_000104_109
[2021-05-14 16:56:21,644] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955312712499087202911581_0002_m_000104_109: needsTaskCommit() Task attempt_202105141955312712499087202911581_0002_m_000104_109: duration 0:00.001s
[2021-05-14 16:56:21,645] {docker.py:276} INFO - 21/05/14 19:56:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312712499087202911581_0002_m_000104_109
[2021-05-14 16:56:21,647] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Finished task 104.0 in stage 2.0 (TID 109). 4544 bytes result sent to driver
[2021-05-14 16:56:21,648] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Starting task 108.0 in stage 2.0 (TID 113) (379a553b2355, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:21,649] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Running task 108.0 in stage 2.0 (TID 113)
[2021-05-14 16:56:21,650] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Finished task 104.0 in stage 2.0 (TID 109) in 1641 ms on 379a553b2355 (executor driver) (105/200)
[2021-05-14 16:56:21,658] {docker.py:276} INFO - 21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:21,660] {docker.py:276} INFO - 21/05/14 19:56:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:21,660] {docker.py:276} INFO - 21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317198735499632887595_0002_m_000108_113, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317198735499632887595_0002_m_000108_113}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317198735499632887595_0002}; taskId=attempt_202105141955317198735499632887595_0002_m_000108_113, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3284c7d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:21,661] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955317198735499632887595_0002_m_000108_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317198735499632887595_0002_m_000108_113
[2021-05-14 16:56:21,664] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955317198735499632887595_0002_m_000108_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317198735499632887595_0002_m_000108_113 : duration 0:00.003s
[2021-05-14 16:56:21,669] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955311974488764499493948_0002_m_000105_110: needsTaskCommit() Task attempt_202105141955311974488764499493948_0002_m_000105_110
[2021-05-14 16:56:21,670] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955311974488764499493948_0002_m_000105_110: needsTaskCommit() Task attempt_202105141955311974488764499493948_0002_m_000105_110: duration 0:00.001s
21/05/14 19:56:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311974488764499493948_0002_m_000105_110
[2021-05-14 16:56:21,670] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Finished task 105.0 in stage 2.0 (TID 110). 4544 bytes result sent to driver
[2021-05-14 16:56:21,671] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Starting task 109.0 in stage 2.0 (TID 114) (379a553b2355, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:21,672] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Finished task 105.0 in stage 2.0 (TID 110) in 1616 ms on 379a553b2355 (executor driver) (106/200)
[2021-05-14 16:56:21,673] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Running task 109.0 in stage 2.0 (TID 114)
[2021-05-14 16:56:21,680] {docker.py:276} INFO - 21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:21,682] {docker.py:276} INFO - 21/05/14 19:56:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317889988237502519951_0002_m_000109_114, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317889988237502519951_0002_m_000109_114}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317889988237502519951_0002}; taskId=attempt_202105141955317889988237502519951_0002_m_000109_114, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e0640a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:21,682] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955317889988237502519951_0002_m_000109_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317889988237502519951_0002_m_000109_114
[2021-05-14 16:56:21,685] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955317889988237502519951_0002_m_000109_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317889988237502519951_0002_m_000109_114 : duration 0:00.003s
[2021-05-14 16:56:21,862] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955319122249739183881143_0002_m_000107_112: needsTaskCommit() Task attempt_202105141955319122249739183881143_0002_m_000107_112
21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955319122249739183881143_0002_m_000107_112: needsTaskCommit() Task attempt_202105141955319122249739183881143_0002_m_000107_112: duration 0:00.001s
[2021-05-14 16:56:21,863] {docker.py:276} INFO - 21/05/14 19:56:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955319122249739183881143_0002_m_000107_112
[2021-05-14 16:56:21,864] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Finished task 107.0 in stage 2.0 (TID 112). 4544 bytes result sent to driver
[2021-05-14 16:56:21,866] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Starting task 110.0 in stage 2.0 (TID 115) (379a553b2355, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:21,867] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955311315308209102537103_0002_m_000106_111: needsTaskCommit() Task attempt_202105141955311315308209102537103_0002_m_000106_111
[2021-05-14 16:56:21,868] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955311315308209102537103_0002_m_000106_111: needsTaskCommit() Task attempt_202105141955311315308209102537103_0002_m_000106_111: duration 0:00.002s
21/05/14 19:56:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311315308209102537103_0002_m_000106_111
[2021-05-14 16:56:21,869] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Finished task 107.0 in stage 2.0 (TID 112) in 1602 ms on 379a553b2355 (executor driver) (107/200)
[2021-05-14 16:56:21,870] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Finished task 106.0 in stage 2.0 (TID 111). 4544 bytes result sent to driver
[2021-05-14 16:56:21,870] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Running task 110.0 in stage 2.0 (TID 115)
[2021-05-14 16:56:21,872] {docker.py:276} INFO - 21/05/14 19:56:21 INFO TaskSetManager: Starting task 111.0 in stage 2.0 (TID 116) (379a553b2355, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:21,874] {docker.py:276} INFO - 21/05/14 19:56:21 INFO Executor: Running task 111.0 in stage 2.0 (TID 116)
21/05/14 19:56:21 INFO TaskSetManager: Finished task 106.0 in stage 2.0 (TID 111) in 1612 ms on 379a553b2355 (executor driver) (108/200)
[2021-05-14 16:56:21,882] {docker.py:276} INFO - 21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Getting 4 (4.9 KiB) non-empty blocks including 4 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:21,883] {docker.py:276} INFO - 21/05/14 19:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:21,884] {docker.py:276} INFO - 21/05/14 19:56:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:21,884] {docker.py:276} INFO - 21/05/14 19:56:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:21,885] {docker.py:276} INFO - 21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312650531895550800733_0002_m_000111_116, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312650531895550800733_0002_m_000111_116}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312650531895550800733_0002}; taskId=attempt_202105141955312650531895550800733_0002_m_000111_116, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ec3a298}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:21,885] {docker.py:276} INFO - 21/05/14 19:56:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955312650531895550800733_0002_m_000111_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312650531895550800733_0002_m_000111_116 
21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:21,885] {docker.py:276} INFO - 21/05/14 19:56:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311755577789174752824_0002_m_000110_115, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311755577789174752824_0002_m_000110_115}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311755577789174752824_0002}; taskId=attempt_202105141955311755577789174752824_0002_m_000110_115, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68d371a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:21,886] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Starting: Task committer attempt_202105141955311755577789174752824_0002_m_000110_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311755577789174752824_0002_m_000110_115
[2021-05-14 16:56:21,889] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955312650531895550800733_0002_m_000111_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312650531895550800733_0002_m_000111_116 : duration 0:00.005s
[2021-05-14 16:56:21,890] {docker.py:276} INFO - 21/05/14 19:56:21 INFO StagingCommitter: Task committer attempt_202105141955311755577789174752824_0002_m_000110_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311755577789174752824_0002_m_000110_115 : duration 0:00.005s
[2021-05-14 16:56:23,252] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955317198735499632887595_0002_m_000108_113: needsTaskCommit() Task attempt_202105141955317198735499632887595_0002_m_000108_113
[2021-05-14 16:56:23,253] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955317198735499632887595_0002_m_000108_113: needsTaskCommit() Task attempt_202105141955317198735499632887595_0002_m_000108_113: duration 0:00.001s
21/05/14 19:56:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317198735499632887595_0002_m_000108_113
[2021-05-14 16:56:23,255] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Finished task 108.0 in stage 2.0 (TID 113). 4544 bytes result sent to driver
[2021-05-14 16:56:23,256] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Starting task 112.0 in stage 2.0 (TID 117) (379a553b2355, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:23,258] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Finished task 108.0 in stage 2.0 (TID 113) in 1612 ms on 379a553b2355 (executor driver) (109/200)
[2021-05-14 16:56:23,259] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Running task 112.0 in stage 2.0 (TID 117)
[2021-05-14 16:56:23,268] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:23,269] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:23,270] {docker.py:276} INFO - 21/05/14 19:56:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:23,271] {docker.py:276} INFO - 21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316917681561137868812_0002_m_000112_117, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316917681561137868812_0002_m_000112_117}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316917681561137868812_0002}; taskId=attempt_202105141955316917681561137868812_0002_m_000112_117, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3def57dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:23,271] {docker.py:276} INFO - 21/05/14 19:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:23,272] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955316917681561137868812_0002_m_000112_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316917681561137868812_0002_m_000112_117
[2021-05-14 16:56:23,274] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955316917681561137868812_0002_m_000112_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316917681561137868812_0002_m_000112_117 : duration 0:00.004s
[2021-05-14 16:56:23,305] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955317889988237502519951_0002_m_000109_114: needsTaskCommit() Task attempt_202105141955317889988237502519951_0002_m_000109_114
[2021-05-14 16:56:23,306] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955317889988237502519951_0002_m_000109_114: needsTaskCommit() Task attempt_202105141955317889988237502519951_0002_m_000109_114: duration 0:00.000s
21/05/14 19:56:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317889988237502519951_0002_m_000109_114
[2021-05-14 16:56:23,308] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Finished task 109.0 in stage 2.0 (TID 114). 4544 bytes result sent to driver
[2021-05-14 16:56:23,309] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Starting task 113.0 in stage 2.0 (TID 118) (379a553b2355, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:23,310] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Running task 113.0 in stage 2.0 (TID 118)
[2021-05-14 16:56:23,311] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Finished task 109.0 in stage 2.0 (TID 114) in 1641 ms on 379a553b2355 (executor driver) (110/200)
[2021-05-14 16:56:23,321] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:23,322] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:23,325] {docker.py:276} INFO - 21/05/14 19:56:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:23,325] {docker.py:276} INFO - 21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531756023887588743206_0002_m_000113_118, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531756023887588743206_0002_m_000113_118}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531756023887588743206_0002}; taskId=attempt_20210514195531756023887588743206_0002_m_000113_118, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17a82526}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_20210514195531756023887588743206_0002_m_000113_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531756023887588743206_0002_m_000113_118
[2021-05-14 16:56:23,328] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_20210514195531756023887588743206_0002_m_000113_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531756023887588743206_0002_m_000113_118 : duration 0:00.003s
[2021-05-14 16:56:23,477] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955312650531895550800733_0002_m_000111_116: needsTaskCommit() Task attempt_202105141955312650531895550800733_0002_m_000111_116
[2021-05-14 16:56:23,478] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955312650531895550800733_0002_m_000111_116: needsTaskCommit() Task attempt_202105141955312650531895550800733_0002_m_000111_116: duration 0:00.001s
21/05/14 19:56:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312650531895550800733_0002_m_000111_116
[2021-05-14 16:56:23,480] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Finished task 111.0 in stage 2.0 (TID 116). 4544 bytes result sent to driver
[2021-05-14 16:56:23,482] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Starting task 114.0 in stage 2.0 (TID 119) (379a553b2355, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:23,483] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Running task 114.0 in stage 2.0 (TID 119)
21/05/14 19:56:23 INFO TaskSetManager: Finished task 111.0 in stage 2.0 (TID 116) in 1613 ms on 379a553b2355 (executor driver) (111/200)
[2021-05-14 16:56:23,504] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:23,506] {docker.py:276} INFO - 21/05/14 19:56:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312051334537661340460_0002_m_000114_119, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312051334537661340460_0002_m_000114_119}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312051334537661340460_0002}; taskId=attempt_202105141955312051334537661340460_0002_m_000114_119, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20e6f213}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:23,506] {docker.py:276} INFO - 21/05/14 19:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:23,506] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955312051334537661340460_0002_m_000114_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312051334537661340460_0002_m_000114_119
[2021-05-14 16:56:23,509] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955312051334537661340460_0002_m_000114_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312051334537661340460_0002_m_000114_119 : duration 0:00.002s
[2021-05-14 16:56:23,538] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955311755577789174752824_0002_m_000110_115: needsTaskCommit() Task attempt_202105141955311755577789174752824_0002_m_000110_115
21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955311755577789174752824_0002_m_000110_115: needsTaskCommit() Task attempt_202105141955311755577789174752824_0002_m_000110_115: duration 0:00.001s
21/05/14 19:56:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311755577789174752824_0002_m_000110_115
[2021-05-14 16:56:23,541] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Finished task 110.0 in stage 2.0 (TID 115). 4587 bytes result sent to driver
[2021-05-14 16:56:23,542] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Starting task 115.0 in stage 2.0 (TID 120) (379a553b2355, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:23,543] {docker.py:276} INFO - 21/05/14 19:56:23 INFO Executor: Running task 115.0 in stage 2.0 (TID 120)
[2021-05-14 16:56:23,543] {docker.py:276} INFO - 21/05/14 19:56:23 INFO TaskSetManager: Finished task 110.0 in stage 2.0 (TID 115) in 1680 ms on 379a553b2355 (executor driver) (112/200)
[2021-05-14 16:56:23,551] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:23,552] {docker.py:276} INFO - 21/05/14 19:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:23,554] {docker.py:276} INFO - 21/05/14 19:56:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:23,554] {docker.py:276} INFO - 21/05/14 19:56:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:23,554] {docker.py:276} INFO - 21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:23,555] {docker.py:276} INFO - 21/05/14 19:56:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955319151273660370100718_0002_m_000115_120, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319151273660370100718_0002_m_000115_120}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955319151273660370100718_0002}; taskId=attempt_202105141955319151273660370100718_0002_m_000115_120, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6095e732}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:23 INFO StagingCommitter: Starting: Task committer attempt_202105141955319151273660370100718_0002_m_000115_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319151273660370100718_0002_m_000115_120
[2021-05-14 16:56:23,558] {docker.py:276} INFO - 21/05/14 19:56:23 INFO StagingCommitter: Task committer attempt_202105141955319151273660370100718_0002_m_000115_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955319151273660370100718_0002_m_000115_120 : duration 0:00.003s
[2021-05-14 16:56:24,865] {docker.py:276} INFO - 21/05/14 19:56:24 INFO StagingCommitter: Starting: Task committer attempt_202105141955316917681561137868812_0002_m_000112_117: needsTaskCommit() Task attempt_202105141955316917681561137868812_0002_m_000112_117
[2021-05-14 16:56:24,866] {docker.py:276} INFO - 21/05/14 19:56:24 INFO StagingCommitter: Task committer attempt_202105141955316917681561137868812_0002_m_000112_117: needsTaskCommit() Task attempt_202105141955316917681561137868812_0002_m_000112_117: duration 0:00.001s
[2021-05-14 16:56:24,867] {docker.py:276} INFO - 21/05/14 19:56:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316917681561137868812_0002_m_000112_117
[2021-05-14 16:56:24,869] {docker.py:276} INFO - 21/05/14 19:56:24 INFO Executor: Finished task 112.0 in stage 2.0 (TID 117). 4587 bytes result sent to driver
[2021-05-14 16:56:24,871] {docker.py:276} INFO - 21/05/14 19:56:24 INFO TaskSetManager: Starting task 116.0 in stage 2.0 (TID 121) (379a553b2355, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:24,872] {docker.py:276} INFO - 21/05/14 19:56:24 INFO Executor: Running task 116.0 in stage 2.0 (TID 121)
21/05/14 19:56:24 INFO TaskSetManager: Finished task 112.0 in stage 2.0 (TID 117) in 1618 ms on 379a553b2355 (executor driver) (113/200)
[2021-05-14 16:56:24,882] {docker.py:276} INFO - 21/05/14 19:56:24 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:24,882] {docker.py:276} INFO - 21/05/14 19:56:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:24,884] {docker.py:276} INFO - 21/05/14 19:56:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316144899756110761260_0002_m_000116_121, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316144899756110761260_0002_m_000116_121}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316144899756110761260_0002}; taskId=attempt_202105141955316144899756110761260_0002_m_000116_121, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34c68467}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:24 INFO StagingCommitter: Starting: Task committer attempt_202105141955316144899756110761260_0002_m_000116_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316144899756110761260_0002_m_000116_121
[2021-05-14 16:56:24,887] {docker.py:276} INFO - 21/05/14 19:56:24 INFO StagingCommitter: Task committer attempt_202105141955316144899756110761260_0002_m_000116_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316144899756110761260_0002_m_000116_121 : duration 0:00.003s
[2021-05-14 16:56:24,918] {docker.py:276} INFO - 21/05/14 19:56:24 INFO StagingCommitter: Starting: Task committer attempt_20210514195531756023887588743206_0002_m_000113_118: needsTaskCommit() Task attempt_20210514195531756023887588743206_0002_m_000113_118
[2021-05-14 16:56:24,919] {docker.py:276} INFO - 21/05/14 19:56:24 INFO StagingCommitter: Task committer attempt_20210514195531756023887588743206_0002_m_000113_118: needsTaskCommit() Task attempt_20210514195531756023887588743206_0002_m_000113_118: duration 0:00.001s
21/05/14 19:56:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531756023887588743206_0002_m_000113_118
[2021-05-14 16:56:24,922] {docker.py:276} INFO - 21/05/14 19:56:24 INFO Executor: Finished task 113.0 in stage 2.0 (TID 118). 4587 bytes result sent to driver
[2021-05-14 16:56:24,923] {docker.py:276} INFO - 21/05/14 19:56:24 INFO TaskSetManager: Starting task 117.0 in stage 2.0 (TID 122) (379a553b2355, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:24,924] {docker.py:276} INFO - 21/05/14 19:56:24 INFO Executor: Running task 117.0 in stage 2.0 (TID 122)
[2021-05-14 16:56:24,925] {docker.py:276} INFO - 21/05/14 19:56:24 INFO TaskSetManager: Finished task 113.0 in stage 2.0 (TID 118) in 1619 ms on 379a553b2355 (executor driver) (114/200)
[2021-05-14 16:56:24,934] {docker.py:276} INFO - 21/05/14 19:56:24 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:24,936] {docker.py:276} INFO - 21/05/14 19:56:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:24,937] {docker.py:276} INFO - 21/05/14 19:56:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317982276838224052178_0002_m_000117_122, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317982276838224052178_0002_m_000117_122}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317982276838224052178_0002}; taskId=attempt_202105141955317982276838224052178_0002_m_000117_122, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@405978f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:24 INFO StagingCommitter: Starting: Task committer attempt_202105141955317982276838224052178_0002_m_000117_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317982276838224052178_0002_m_000117_122
[2021-05-14 16:56:24,939] {docker.py:276} INFO - 21/05/14 19:56:24 INFO StagingCommitter: Task committer attempt_202105141955317982276838224052178_0002_m_000117_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317982276838224052178_0002_m_000117_122 : duration 0:00.002s
[2021-05-14 16:56:25,136] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Starting: Task committer attempt_202105141955319151273660370100718_0002_m_000115_120: needsTaskCommit() Task attempt_202105141955319151273660370100718_0002_m_000115_120
[2021-05-14 16:56:25,137] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Task committer attempt_202105141955319151273660370100718_0002_m_000115_120: needsTaskCommit() Task attempt_202105141955319151273660370100718_0002_m_000115_120: duration 0:00.001s
21/05/14 19:56:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955319151273660370100718_0002_m_000115_120
[2021-05-14 16:56:25,139] {docker.py:276} INFO - 21/05/14 19:56:25 INFO Executor: Finished task 115.0 in stage 2.0 (TID 120). 4544 bytes result sent to driver
[2021-05-14 16:56:25,140] {docker.py:276} INFO - 21/05/14 19:56:25 INFO TaskSetManager: Starting task 118.0 in stage 2.0 (TID 123) (379a553b2355, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:25,141] {docker.py:276} INFO - 21/05/14 19:56:25 INFO TaskSetManager: Finished task 115.0 in stage 2.0 (TID 120) in 1602 ms on 379a553b2355 (executor driver) (115/200)
21/05/14 19:56:25 INFO Executor: Running task 118.0 in stage 2.0 (TID 123)
[2021-05-14 16:56:25,151] {docker.py:276} INFO - 21/05/14 19:56:25 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:25,151] {docker.py:276} INFO - 21/05/14 19:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:25,154] {docker.py:276} INFO - 21/05/14 19:56:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:25,154] {docker.py:276} INFO - 21/05/14 19:56:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:25,154] {docker.py:276} INFO - 21/05/14 19:56:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:25,155] {docker.py:276} INFO - 21/05/14 19:56:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531999474300655020719_0002_m_000118_123, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531999474300655020719_0002_m_000118_123}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531999474300655020719_0002}; taskId=attempt_20210514195531999474300655020719_0002_m_000118_123, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13971fb2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:25,155] {docker.py:276} INFO - 21/05/14 19:56:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:25,156] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Starting: Task committer attempt_20210514195531999474300655020719_0002_m_000118_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531999474300655020719_0002_m_000118_123
[2021-05-14 16:56:25,159] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Task committer attempt_20210514195531999474300655020719_0002_m_000118_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531999474300655020719_0002_m_000118_123 : duration 0:00.003s
[2021-05-14 16:56:25,351] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Starting: Task committer attempt_202105141955312051334537661340460_0002_m_000114_119: needsTaskCommit() Task attempt_202105141955312051334537661340460_0002_m_000114_119
[2021-05-14 16:56:25,352] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Task committer attempt_202105141955312051334537661340460_0002_m_000114_119: needsTaskCommit() Task attempt_202105141955312051334537661340460_0002_m_000114_119: duration 0:00.001s
21/05/14 19:56:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312051334537661340460_0002_m_000114_119
[2021-05-14 16:56:25,354] {docker.py:276} INFO - 21/05/14 19:56:25 INFO Executor: Finished task 114.0 in stage 2.0 (TID 119). 4587 bytes result sent to driver
[2021-05-14 16:56:25,356] {docker.py:276} INFO - 21/05/14 19:56:25 INFO TaskSetManager: Starting task 119.0 in stage 2.0 (TID 124) (379a553b2355, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:25,357] {docker.py:276} INFO - 21/05/14 19:56:25 INFO TaskSetManager: Finished task 114.0 in stage 2.0 (TID 119) in 1878 ms on 379a553b2355 (executor driver) (116/200)
[2021-05-14 16:56:25,358] {docker.py:276} INFO - 21/05/14 19:56:25 INFO Executor: Running task 119.0 in stage 2.0 (TID 124)
[2021-05-14 16:56:25,367] {docker.py:276} INFO - 21/05/14 19:56:25 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:25,367] {docker.py:276} INFO - 21/05/14 19:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:25,369] {docker.py:276} INFO - 21/05/14 19:56:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315277705580642313797_0002_m_000119_124, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315277705580642313797_0002_m_000119_124}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315277705580642313797_0002}; taskId=attempt_202105141955315277705580642313797_0002_m_000119_124, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c642c3a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:25,370] {docker.py:276} INFO - 21/05/14 19:56:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:25,370] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Starting: Task committer attempt_202105141955315277705580642313797_0002_m_000119_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315277705580642313797_0002_m_000119_124
[2021-05-14 16:56:25,373] {docker.py:276} INFO - 21/05/14 19:56:25 INFO StagingCommitter: Task committer attempt_202105141955315277705580642313797_0002_m_000119_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315277705580642313797_0002_m_000119_124 : duration 0:00.003s
[2021-05-14 16:56:26,458] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Starting: Task committer attempt_202105141955316144899756110761260_0002_m_000116_121: needsTaskCommit() Task attempt_202105141955316144899756110761260_0002_m_000116_121
[2021-05-14 16:56:26,459] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Task committer attempt_202105141955316144899756110761260_0002_m_000116_121: needsTaskCommit() Task attempt_202105141955316144899756110761260_0002_m_000116_121: duration 0:00.001s
21/05/14 19:56:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316144899756110761260_0002_m_000116_121
[2021-05-14 16:56:26,460] {docker.py:276} INFO - 21/05/14 19:56:26 INFO Executor: Finished task 116.0 in stage 2.0 (TID 121). 4544 bytes result sent to driver
[2021-05-14 16:56:26,462] {docker.py:276} INFO - 21/05/14 19:56:26 INFO TaskSetManager: Starting task 120.0 in stage 2.0 (TID 125) (379a553b2355, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:26,462] {docker.py:276} INFO - 21/05/14 19:56:26 INFO TaskSetManager: Finished task 116.0 in stage 2.0 (TID 121) in 1594 ms on 379a553b2355 (executor driver) (117/200)
[2021-05-14 16:56:26,463] {docker.py:276} INFO - 21/05/14 19:56:26 INFO Executor: Running task 120.0 in stage 2.0 (TID 125)
[2021-05-14 16:56:26,471] {docker.py:276} INFO - 21/05/14 19:56:26 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:26,471] {docker.py:276} INFO - 21/05/14 19:56:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:26,473] {docker.py:276} INFO - 21/05/14 19:56:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:26,474] {docker.py:276} INFO - 21/05/14 19:56:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:26,474] {docker.py:276} INFO - 21/05/14 19:56:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531860806653127967862_0002_m_000120_125, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531860806653127967862_0002_m_000120_125}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531860806653127967862_0002}; taskId=attempt_20210514195531860806653127967862_0002_m_000120_125, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75c7797b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:26 INFO StagingCommitter: Starting: Task committer attempt_20210514195531860806653127967862_0002_m_000120_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531860806653127967862_0002_m_000120_125
[2021-05-14 16:56:26,477] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Task committer attempt_20210514195531860806653127967862_0002_m_000120_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531860806653127967862_0002_m_000120_125 : duration 0:00.003s
[2021-05-14 16:56:26,529] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Starting: Task committer attempt_202105141955317982276838224052178_0002_m_000117_122: needsTaskCommit() Task attempt_202105141955317982276838224052178_0002_m_000117_122
[2021-05-14 16:56:26,529] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Task committer attempt_202105141955317982276838224052178_0002_m_000117_122: needsTaskCommit() Task attempt_202105141955317982276838224052178_0002_m_000117_122: duration 0:00.001s
21/05/14 19:56:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317982276838224052178_0002_m_000117_122
[2021-05-14 16:56:26,530] {docker.py:276} INFO - 21/05/14 19:56:26 INFO Executor: Finished task 117.0 in stage 2.0 (TID 122). 4544 bytes result sent to driver
[2021-05-14 16:56:26,532] {docker.py:276} INFO - 21/05/14 19:56:26 INFO TaskSetManager: Starting task 121.0 in stage 2.0 (TID 126) (379a553b2355, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:26,533] {docker.py:276} INFO - 21/05/14 19:56:26 INFO Executor: Running task 121.0 in stage 2.0 (TID 126)
21/05/14 19:56:26 INFO TaskSetManager: Finished task 117.0 in stage 2.0 (TID 122) in 1612 ms on 379a553b2355 (executor driver) (118/200)
[2021-05-14 16:56:26,541] {docker.py:276} INFO - 21/05/14 19:56:26 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:26,543] {docker.py:276} INFO - 21/05/14 19:56:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:26,543] {docker.py:276} INFO - 21/05/14 19:56:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318668463314493232431_0002_m_000121_126, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318668463314493232431_0002_m_000121_126}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318668463314493232431_0002}; taskId=attempt_202105141955318668463314493232431_0002_m_000121_126, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@573a2348}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:26 INFO StagingCommitter: Starting: Task committer attempt_202105141955318668463314493232431_0002_m_000121_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318668463314493232431_0002_m_000121_126
[2021-05-14 16:56:26,546] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Task committer attempt_202105141955318668463314493232431_0002_m_000121_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318668463314493232431_0002_m_000121_126 : duration 0:00.003s
[2021-05-14 16:56:26,724] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Starting: Task committer attempt_20210514195531999474300655020719_0002_m_000118_123: needsTaskCommit() Task attempt_20210514195531999474300655020719_0002_m_000118_123
[2021-05-14 16:56:26,725] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Task committer attempt_20210514195531999474300655020719_0002_m_000118_123: needsTaskCommit() Task attempt_20210514195531999474300655020719_0002_m_000118_123: duration 0:00.001s
21/05/14 19:56:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531999474300655020719_0002_m_000118_123
[2021-05-14 16:56:26,727] {docker.py:276} INFO - 21/05/14 19:56:26 INFO Executor: Finished task 118.0 in stage 2.0 (TID 123). 4544 bytes result sent to driver
[2021-05-14 16:56:26,728] {docker.py:276} INFO - 21/05/14 19:56:26 INFO TaskSetManager: Starting task 122.0 in stage 2.0 (TID 127) (379a553b2355, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:26,730] {docker.py:276} INFO - 21/05/14 19:56:26 INFO TaskSetManager: Finished task 118.0 in stage 2.0 (TID 123) in 1592 ms on 379a553b2355 (executor driver) (119/200)
[2021-05-14 16:56:26,731] {docker.py:276} INFO - 21/05/14 19:56:26 INFO Executor: Running task 122.0 in stage 2.0 (TID 127)
[2021-05-14 16:56:26,741] {docker.py:276} INFO - 21/05/14 19:56:26 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:26,743] {docker.py:276} INFO - 21/05/14 19:56:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:26,744] {docker.py:276} INFO - 21/05/14 19:56:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:26,744] {docker.py:276} INFO - 21/05/14 19:56:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316916101104342366261_0002_m_000122_127, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316916101104342366261_0002_m_000122_127}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316916101104342366261_0002}; taskId=attempt_202105141955316916101104342366261_0002_m_000122_127, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@718245cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:26,744] {docker.py:276} INFO - 21/05/14 19:56:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:26 INFO StagingCommitter: Starting: Task committer attempt_202105141955316916101104342366261_0002_m_000122_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316916101104342366261_0002_m_000122_127
[2021-05-14 16:56:26,746] {docker.py:276} INFO - 21/05/14 19:56:26 INFO StagingCommitter: Task committer attempt_202105141955316916101104342366261_0002_m_000122_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316916101104342366261_0002_m_000122_127 : duration 0:00.002s
[2021-05-14 16:56:27,019] {docker.py:276} INFO - 21/05/14 19:56:27 INFO StagingCommitter: Starting: Task committer attempt_202105141955315277705580642313797_0002_m_000119_124: needsTaskCommit() Task attempt_202105141955315277705580642313797_0002_m_000119_124
21/05/14 19:56:27 INFO StagingCommitter: Task committer attempt_202105141955315277705580642313797_0002_m_000119_124: needsTaskCommit() Task attempt_202105141955315277705580642313797_0002_m_000119_124: duration 0:00.000s
21/05/14 19:56:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315277705580642313797_0002_m_000119_124
[2021-05-14 16:56:27,022] {docker.py:276} INFO - 21/05/14 19:56:27 INFO Executor: Finished task 119.0 in stage 2.0 (TID 124). 4544 bytes result sent to driver
[2021-05-14 16:56:27,024] {docker.py:276} INFO - 21/05/14 19:56:27 INFO TaskSetManager: Starting task 123.0 in stage 2.0 (TID 128) (379a553b2355, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:27,025] {docker.py:276} INFO - 21/05/14 19:56:27 INFO Executor: Running task 123.0 in stage 2.0 (TID 128)
[2021-05-14 16:56:27,025] {docker.py:276} INFO - 21/05/14 19:56:27 INFO TaskSetManager: Finished task 119.0 in stage 2.0 (TID 124) in 1671 ms on 379a553b2355 (executor driver) (120/200)
[2021-05-14 16:56:27,034] {docker.py:276} INFO - 21/05/14 19:56:27 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:27,036] {docker.py:276} INFO - 21/05/14 19:56:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315517481964397144262_0002_m_000123_128, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315517481964397144262_0002_m_000123_128}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315517481964397144262_0002}; taskId=attempt_202105141955315517481964397144262_0002_m_000123_128, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bb19cfe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:27,037] {docker.py:276} INFO - 21/05/14 19:56:27 INFO StagingCommitter: Starting: Task committer attempt_202105141955315517481964397144262_0002_m_000123_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315517481964397144262_0002_m_000123_128
[2021-05-14 16:56:27,039] {docker.py:276} INFO - 21/05/14 19:56:27 INFO StagingCommitter: Task committer attempt_202105141955315517481964397144262_0002_m_000123_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315517481964397144262_0002_m_000123_128 : duration 0:00.003s
[2021-05-14 16:56:28,754] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Starting: Task committer attempt_202105141955318668463314493232431_0002_m_000121_126: needsTaskCommit() Task attempt_202105141955318668463314493232431_0002_m_000121_126
[2021-05-14 16:56:28,755] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Starting: Task committer attempt_20210514195531860806653127967862_0002_m_000120_125: needsTaskCommit() Task attempt_20210514195531860806653127967862_0002_m_000120_125
21/05/14 19:56:28 INFO StagingCommitter: Task committer attempt_202105141955318668463314493232431_0002_m_000121_126: needsTaskCommit() Task attempt_202105141955318668463314493232431_0002_m_000121_126: duration 0:00.000s
[2021-05-14 16:56:28,755] {docker.py:276} INFO - 21/05/14 19:56:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318668463314493232431_0002_m_000121_126
21/05/14 19:56:28 INFO StagingCommitter: Task committer attempt_20210514195531860806653127967862_0002_m_000120_125: needsTaskCommit() Task attempt_20210514195531860806653127967862_0002_m_000120_125: duration 0:00.001s
21/05/14 19:56:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531860806653127967862_0002_m_000120_125
[2021-05-14 16:56:28,756] {docker.py:276} INFO - 21/05/14 19:56:28 INFO Executor: Finished task 120.0 in stage 2.0 (TID 125). 4544 bytes result sent to driver
[2021-05-14 16:56:28,756] {docker.py:276} INFO - 21/05/14 19:56:28 INFO TaskSetManager: Starting task 124.0 in stage 2.0 (TID 129) (379a553b2355, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:28,757] {docker.py:276} INFO - 21/05/14 19:56:28 INFO Executor: Finished task 121.0 in stage 2.0 (TID 126). 4544 bytes result sent to driver
[2021-05-14 16:56:28,758] {docker.py:276} INFO - 21/05/14 19:56:28 INFO Executor: Running task 124.0 in stage 2.0 (TID 129)
[2021-05-14 16:56:28,759] {docker.py:276} INFO - 21/05/14 19:56:28 INFO TaskSetManager: Starting task 125.0 in stage 2.0 (TID 130) (379a553b2355, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:28,759] {docker.py:276} INFO - 21/05/14 19:56:28 INFO TaskSetManager: Finished task 121.0 in stage 2.0 (TID 126) in 2230 ms on 379a553b2355 (executor driver) (121/200)
[2021-05-14 16:56:28,760] {docker.py:276} INFO - 21/05/14 19:56:28 INFO TaskSetManager: Finished task 120.0 in stage 2.0 (TID 125) in 2301 ms on 379a553b2355 (executor driver) (122/200)
[2021-05-14 16:56:28,761] {docker.py:276} INFO - 21/05/14 19:56:28 INFO Executor: Running task 125.0 in stage 2.0 (TID 130)
[2021-05-14 16:56:28,769] {docker.py:276} INFO - 21/05/14 19:56:28 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:28,771] {docker.py:276} INFO - 21/05/14 19:56:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:28 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:28,771] {docker.py:276} INFO - 21/05/14 19:56:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313716794995508606071_0002_m_000124_129, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313716794995508606071_0002_m_000124_129}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313716794995508606071_0002}; taskId=attempt_202105141955313716794995508606071_0002_m_000124_129, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@410b5926}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 19:56:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:28 INFO StagingCommitter: Starting: Task committer attempt_202105141955313716794995508606071_0002_m_000124_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313716794995508606071_0002_m_000124_129
[2021-05-14 16:56:28,773] {docker.py:276} INFO - 21/05/14 19:56:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:28,774] {docker.py:276} INFO - 21/05/14 19:56:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:28,774] {docker.py:276} INFO - 21/05/14 19:56:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312073411700774843676_0002_m_000125_130, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312073411700774843676_0002_m_000125_130}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312073411700774843676_0002}; taskId=attempt_202105141955312073411700774843676_0002_m_000125_130, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61c6083a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:28,774] {docker.py:276} INFO - 21/05/14 19:56:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:28 INFO StagingCommitter: Starting: Task committer attempt_202105141955312073411700774843676_0002_m_000125_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312073411700774843676_0002_m_000125_130
[2021-05-14 16:56:28,775] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Task committer attempt_202105141955313716794995508606071_0002_m_000124_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313716794995508606071_0002_m_000124_129 : duration 0:00.004s
[2021-05-14 16:56:28,777] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Task committer attempt_202105141955312073411700774843676_0002_m_000125_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312073411700774843676_0002_m_000125_130 : duration 0:00.004s
[2021-05-14 16:56:28,920] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Starting: Task committer attempt_202105141955316916101104342366261_0002_m_000122_127: needsTaskCommit() Task attempt_202105141955316916101104342366261_0002_m_000122_127
[2021-05-14 16:56:28,921] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Task committer attempt_202105141955316916101104342366261_0002_m_000122_127: needsTaskCommit() Task attempt_202105141955316916101104342366261_0002_m_000122_127: duration 0:00.002s
21/05/14 19:56:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316916101104342366261_0002_m_000122_127
[2021-05-14 16:56:28,926] {docker.py:276} INFO - 21/05/14 19:56:28 INFO Executor: Finished task 122.0 in stage 2.0 (TID 127). 4544 bytes result sent to driver
[2021-05-14 16:56:28,928] {docker.py:276} INFO - 21/05/14 19:56:28 INFO TaskSetManager: Starting task 126.0 in stage 2.0 (TID 131) (379a553b2355, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:28,930] {docker.py:276} INFO - 21/05/14 19:56:28 INFO Executor: Running task 126.0 in stage 2.0 (TID 131)
[2021-05-14 16:56:28,931] {docker.py:276} INFO - 21/05/14 19:56:28 INFO TaskSetManager: Finished task 122.0 in stage 2.0 (TID 127) in 2205 ms on 379a553b2355 (executor driver) (123/200)
[2021-05-14 16:56:28,941] {docker.py:276} INFO - 21/05/14 19:56:28 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:28,943] {docker.py:276} INFO - 21/05/14 19:56:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:28,944] {docker.py:276} INFO - 21/05/14 19:56:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:28,944] {docker.py:276} INFO - 21/05/14 19:56:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:28,944] {docker.py:276} INFO - 21/05/14 19:56:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318607741890371988683_0002_m_000126_131, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318607741890371988683_0002_m_000126_131}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318607741890371988683_0002}; taskId=attempt_202105141955318607741890371988683_0002_m_000126_131, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3055dec0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:28,945] {docker.py:276} INFO - 21/05/14 19:56:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:28,945] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Starting: Task committer attempt_202105141955318607741890371988683_0002_m_000126_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318607741890371988683_0002_m_000126_131
[2021-05-14 16:56:28,948] {docker.py:276} INFO - 21/05/14 19:56:28 INFO StagingCommitter: Task committer attempt_202105141955318607741890371988683_0002_m_000126_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318607741890371988683_0002_m_000126_131 : duration 0:00.003s
[2021-05-14 16:56:29,276] {docker.py:276} INFO - 21/05/14 19:56:29 INFO StagingCommitter: Starting: Task committer attempt_202105141955315517481964397144262_0002_m_000123_128: needsTaskCommit() Task attempt_202105141955315517481964397144262_0002_m_000123_128
[2021-05-14 16:56:29,278] {docker.py:276} INFO - 21/05/14 19:56:29 INFO StagingCommitter: Task committer attempt_202105141955315517481964397144262_0002_m_000123_128: needsTaskCommit() Task attempt_202105141955315517481964397144262_0002_m_000123_128: duration 0:00.000s
21/05/14 19:56:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315517481964397144262_0002_m_000123_128
[2021-05-14 16:56:29,280] {docker.py:276} INFO - 21/05/14 19:56:29 INFO Executor: Finished task 123.0 in stage 2.0 (TID 128). 4544 bytes result sent to driver
[2021-05-14 16:56:29,282] {docker.py:276} INFO - 21/05/14 19:56:29 INFO TaskSetManager: Starting task 127.0 in stage 2.0 (TID 132) (379a553b2355, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:29,284] {docker.py:276} INFO - 21/05/14 19:56:29 INFO Executor: Running task 127.0 in stage 2.0 (TID 132)
21/05/14 19:56:29 INFO TaskSetManager: Finished task 123.0 in stage 2.0 (TID 128) in 2262 ms on 379a553b2355 (executor driver) (124/200)
[2021-05-14 16:56:29,293] {docker.py:276} INFO - 21/05/14 19:56:29 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:29,296] {docker.py:276} INFO - 21/05/14 19:56:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318621509675081052559_0002_m_000127_132, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318621509675081052559_0002_m_000127_132}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318621509675081052559_0002}; taskId=attempt_202105141955318621509675081052559_0002_m_000127_132, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13078cc4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:29,296] {docker.py:276} INFO - 21/05/14 19:56:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:29 INFO StagingCommitter: Starting: Task committer attempt_202105141955318621509675081052559_0002_m_000127_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318621509675081052559_0002_m_000127_132
[2021-05-14 16:56:29,299] {docker.py:276} INFO - 21/05/14 19:56:29 INFO StagingCommitter: Task committer attempt_202105141955318621509675081052559_0002_m_000127_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318621509675081052559_0002_m_000127_132 : duration 0:00.002s
[2021-05-14 16:56:31,309] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Starting: Task committer attempt_202105141955313716794995508606071_0002_m_000124_129: needsTaskCommit() Task attempt_202105141955313716794995508606071_0002_m_000124_129
[2021-05-14 16:56:31,310] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Task committer attempt_202105141955313716794995508606071_0002_m_000124_129: needsTaskCommit() Task attempt_202105141955313716794995508606071_0002_m_000124_129: duration 0:00.001s
21/05/14 19:56:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313716794995508606071_0002_m_000124_129
[2021-05-14 16:56:31,310] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Starting: Task committer attempt_202105141955312073411700774843676_0002_m_000125_130: needsTaskCommit() Task attempt_202105141955312073411700774843676_0002_m_000125_130
[2021-05-14 16:56:31,311] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Task committer attempt_202105141955312073411700774843676_0002_m_000125_130: needsTaskCommit() Task attempt_202105141955312073411700774843676_0002_m_000125_130: duration 0:00.002s
21/05/14 19:56:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312073411700774843676_0002_m_000125_130
[2021-05-14 16:56:31,312] {docker.py:276} INFO - 21/05/14 19:56:31 INFO Executor: Finished task 124.0 in stage 2.0 (TID 129). 4587 bytes result sent to driver
[2021-05-14 16:56:31,312] {docker.py:276} INFO - 21/05/14 19:56:31 INFO Executor: Finished task 125.0 in stage 2.0 (TID 130). 4587 bytes result sent to driver
[2021-05-14 16:56:31,313] {docker.py:276} INFO - 21/05/14 19:56:31 INFO TaskSetManager: Starting task 128.0 in stage 2.0 (TID 133) (379a553b2355, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:31,314] {docker.py:276} INFO - 21/05/14 19:56:31 INFO Executor: Running task 128.0 in stage 2.0 (TID 133)
21/05/14 19:56:31 INFO TaskSetManager: Starting task 129.0 in stage 2.0 (TID 134) (379a553b2355, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:31,314] {docker.py:276} INFO - 21/05/14 19:56:31 INFO TaskSetManager: Finished task 124.0 in stage 2.0 (TID 129) in 2564 ms on 379a553b2355 (executor driver) (125/200)
[2021-05-14 16:56:31,316] {docker.py:276} INFO - 21/05/14 19:56:31 INFO TaskSetManager: Finished task 125.0 in stage 2.0 (TID 130) in 2562 ms on 379a553b2355 (executor driver) (126/200)
[2021-05-14 16:56:31,318] {docker.py:276} INFO - 21/05/14 19:56:31 INFO Executor: Running task 129.0 in stage 2.0 (TID 134)
[2021-05-14 16:56:31,326] {docker.py:276} INFO - 21/05/14 19:56:31 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:31,326] {docker.py:276} INFO - 21/05/14 19:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 19:56:31 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:31,327] {docker.py:276} INFO - 21/05/14 19:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:31,328] {docker.py:276} INFO - 21/05/14 19:56:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:31,329] {docker.py:276} INFO - 21/05/14 19:56:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:31,330] {docker.py:276} INFO - 21/05/14 19:56:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312101459458881847890_0002_m_000129_134, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312101459458881847890_0002_m_000129_134}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312101459458881847890_0002}; taskId=attempt_202105141955312101459458881847890_0002_m_000129_134, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd5f6b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:31 INFO StagingCommitter: Starting: Task committer attempt_202105141955312101459458881847890_0002_m_000129_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312101459458881847890_0002_m_000129_134
[2021-05-14 16:56:31,330] {docker.py:276} INFO - 21/05/14 19:56:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315745913524854701087_0002_m_000128_133, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315745913524854701087_0002_m_000128_133}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315745913524854701087_0002}; taskId=attempt_202105141955315745913524854701087_0002_m_000128_133, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70354ed3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:31,331] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Starting: Task committer attempt_202105141955315745913524854701087_0002_m_000128_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315745913524854701087_0002_m_000128_133
[2021-05-14 16:56:31,333] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Task committer attempt_202105141955312101459458881847890_0002_m_000129_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312101459458881847890_0002_m_000129_134 : duration 0:00.004s
[2021-05-14 16:56:31,334] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Task committer attempt_202105141955315745913524854701087_0002_m_000128_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315745913524854701087_0002_m_000128_133 : duration 0:00.003s
[2021-05-14 16:56:31,475] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Starting: Task committer attempt_202105141955318607741890371988683_0002_m_000126_131: needsTaskCommit() Task attempt_202105141955318607741890371988683_0002_m_000126_131
[2021-05-14 16:56:31,476] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Task committer attempt_202105141955318607741890371988683_0002_m_000126_131: needsTaskCommit() Task attempt_202105141955318607741890371988683_0002_m_000126_131: duration 0:00.000s
21/05/14 19:56:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318607741890371988683_0002_m_000126_131
[2021-05-14 16:56:31,477] {docker.py:276} INFO - 21/05/14 19:56:31 INFO Executor: Finished task 126.0 in stage 2.0 (TID 131). 4587 bytes result sent to driver
[2021-05-14 16:56:31,478] {docker.py:276} INFO - 21/05/14 19:56:31 INFO TaskSetManager: Starting task 130.0 in stage 2.0 (TID 135) (379a553b2355, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:31,479] {docker.py:276} INFO - 21/05/14 19:56:31 INFO Executor: Running task 130.0 in stage 2.0 (TID 135)
[2021-05-14 16:56:31,479] {docker.py:276} INFO - 21/05/14 19:56:31 INFO TaskSetManager: Finished task 126.0 in stage 2.0 (TID 131) in 2555 ms on 379a553b2355 (executor driver) (127/200)
[2021-05-14 16:56:31,487] {docker.py:276} INFO - 21/05/14 19:56:31 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:31,490] {docker.py:276} INFO - 21/05/14 19:56:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:31,490] {docker.py:276} INFO - 21/05/14 19:56:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318767405431841547774_0002_m_000130_135, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318767405431841547774_0002_m_000130_135}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318767405431841547774_0002}; taskId=attempt_202105141955318767405431841547774_0002_m_000130_135, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b0a942c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:31,490] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Starting: Task committer attempt_202105141955318767405431841547774_0002_m_000130_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318767405431841547774_0002_m_000130_135
[2021-05-14 16:56:31,493] {docker.py:276} INFO - 21/05/14 19:56:31 INFO StagingCommitter: Task committer attempt_202105141955318767405431841547774_0002_m_000130_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318767405431841547774_0002_m_000130_135 : duration 0:00.002s
[2021-05-14 16:56:32,027] {docker.py:276} INFO - 21/05/14 19:56:32 INFO StagingCommitter: Starting: Task committer attempt_202105141955318621509675081052559_0002_m_000127_132: needsTaskCommit() Task attempt_202105141955318621509675081052559_0002_m_000127_132
[2021-05-14 16:56:32,028] {docker.py:276} INFO - 21/05/14 19:56:32 INFO StagingCommitter: Task committer attempt_202105141955318621509675081052559_0002_m_000127_132: needsTaskCommit() Task attempt_202105141955318621509675081052559_0002_m_000127_132: duration 0:00.000s
[2021-05-14 16:56:32,028] {docker.py:276} INFO - 21/05/14 19:56:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318621509675081052559_0002_m_000127_132
[2021-05-14 16:56:32,029] {docker.py:276} INFO - 21/05/14 19:56:32 INFO Executor: Finished task 127.0 in stage 2.0 (TID 132). 4587 bytes result sent to driver
[2021-05-14 16:56:32,030] {docker.py:276} INFO - 21/05/14 19:56:32 INFO TaskSetManager: Starting task 131.0 in stage 2.0 (TID 136) (379a553b2355, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:32,031] {docker.py:276} INFO - 21/05/14 19:56:32 INFO TaskSetManager: Finished task 127.0 in stage 2.0 (TID 132) in 2753 ms on 379a553b2355 (executor driver) (128/200)
[2021-05-14 16:56:32,032] {docker.py:276} INFO - 21/05/14 19:56:32 INFO Executor: Running task 131.0 in stage 2.0 (TID 136)
[2021-05-14 16:56:32,039] {docker.py:276} INFO - 21/05/14 19:56:32 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:32,041] {docker.py:276} INFO - 21/05/14 19:56:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317537723376625472593_0002_m_000131_136, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317537723376625472593_0002_m_000131_136}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317537723376625472593_0002}; taskId=attempt_202105141955317537723376625472593_0002_m_000131_136, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@708cfcb3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:32,041] {docker.py:276} INFO - 21/05/14 19:56:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:32,042] {docker.py:276} INFO - 21/05/14 19:56:32 INFO StagingCommitter: Starting: Task committer attempt_202105141955317537723376625472593_0002_m_000131_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317537723376625472593_0002_m_000131_136
[2021-05-14 16:56:32,044] {docker.py:276} INFO - 21/05/14 19:56:32 INFO StagingCommitter: Task committer attempt_202105141955317537723376625472593_0002_m_000131_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317537723376625472593_0002_m_000131_136 : duration 0:00.002s
[2021-05-14 16:56:33,052] {docker.py:276} INFO - 21/05/14 19:56:33 INFO StagingCommitter: Starting: Task committer attempt_202105141955312101459458881847890_0002_m_000129_134: needsTaskCommit() Task attempt_202105141955312101459458881847890_0002_m_000129_134
[2021-05-14 16:56:33,053] {docker.py:276} INFO - 21/05/14 19:56:33 INFO StagingCommitter: Task committer attempt_202105141955312101459458881847890_0002_m_000129_134: needsTaskCommit() Task attempt_202105141955312101459458881847890_0002_m_000129_134: duration 0:00.001s
21/05/14 19:56:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312101459458881847890_0002_m_000129_134
[2021-05-14 16:56:33,054] {docker.py:276} INFO - 21/05/14 19:56:33 INFO Executor: Finished task 129.0 in stage 2.0 (TID 134). 4544 bytes result sent to driver
[2021-05-14 16:56:33,056] {docker.py:276} INFO - 21/05/14 19:56:33 INFO TaskSetManager: Starting task 132.0 in stage 2.0 (TID 137) (379a553b2355, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:33,057] {docker.py:276} INFO - 21/05/14 19:56:33 INFO Executor: Running task 132.0 in stage 2.0 (TID 137)
[2021-05-14 16:56:33,058] {docker.py:276} INFO - 21/05/14 19:56:33 INFO TaskSetManager: Finished task 129.0 in stage 2.0 (TID 134) in 1746 ms on 379a553b2355 (executor driver) (129/200)
[2021-05-14 16:56:33,066] {docker.py:276} INFO - 21/05/14 19:56:33 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:33,068] {docker.py:276} INFO - 21/05/14 19:56:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316702677471585525717_0002_m_000132_137, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316702677471585525717_0002_m_000132_137}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316702677471585525717_0002}; taskId=attempt_202105141955316702677471585525717_0002_m_000132_137, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@416150a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:33 INFO StagingCommitter: Starting: Task committer attempt_202105141955316702677471585525717_0002_m_000132_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316702677471585525717_0002_m_000132_137
[2021-05-14 16:56:33,071] {docker.py:276} INFO - 21/05/14 19:56:33 INFO StagingCommitter: Task committer attempt_202105141955316702677471585525717_0002_m_000132_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316702677471585525717_0002_m_000132_137 : duration 0:00.002s
[2021-05-14 16:56:34,142] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105141955318767405431841547774_0002_m_000130_135: needsTaskCommit() Task attempt_202105141955318767405431841547774_0002_m_000130_135
[2021-05-14 16:56:34,143] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Task committer attempt_202105141955318767405431841547774_0002_m_000130_135: needsTaskCommit() Task attempt_202105141955318767405431841547774_0002_m_000130_135: duration 0:00.000s
[2021-05-14 16:56:34,144] {docker.py:276} INFO - 21/05/14 19:56:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318767405431841547774_0002_m_000130_135
[2021-05-14 16:56:34,146] {docker.py:276} INFO - 21/05/14 19:56:34 INFO Executor: Finished task 130.0 in stage 2.0 (TID 135). 4544 bytes result sent to driver
[2021-05-14 16:56:34,147] {docker.py:276} INFO - 21/05/14 19:56:34 INFO TaskSetManager: Starting task 133.0 in stage 2.0 (TID 138) (379a553b2355, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:34,148] {docker.py:276} INFO - 21/05/14 19:56:34 INFO Executor: Running task 133.0 in stage 2.0 (TID 138)
21/05/14 19:56:34 INFO TaskSetManager: Finished task 130.0 in stage 2.0 (TID 135) in 2674 ms on 379a553b2355 (executor driver) (130/200)
[2021-05-14 16:56:34,159] {docker.py:276} INFO - 21/05/14 19:56:34 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:34,159] {docker.py:276} INFO - 21/05/14 19:56:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:34,161] {docker.py:276} INFO - 21/05/14 19:56:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:34,161] {docker.py:276} INFO - 21/05/14 19:56:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316802336886338348344_0002_m_000133_138, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316802336886338348344_0002_m_000133_138}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316802336886338348344_0002}; taskId=attempt_202105141955316802336886338348344_0002_m_000133_138, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@791bc803}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:34,162] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105141955316802336886338348344_0002_m_000133_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316802336886338348344_0002_m_000133_138
[2021-05-14 16:56:34,164] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Task committer attempt_202105141955316802336886338348344_0002_m_000133_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316802336886338348344_0002_m_000133_138 : duration 0:00.002s
[2021-05-14 16:56:34,664] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105141955315745913524854701087_0002_m_000128_133: needsTaskCommit() Task attempt_202105141955315745913524854701087_0002_m_000128_133
[2021-05-14 16:56:34,665] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Task committer attempt_202105141955315745913524854701087_0002_m_000128_133: needsTaskCommit() Task attempt_202105141955315745913524854701087_0002_m_000128_133: duration 0:00.001s
21/05/14 19:56:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315745913524854701087_0002_m_000128_133
[2021-05-14 16:56:34,667] {docker.py:276} INFO - 21/05/14 19:56:34 INFO Executor: Finished task 128.0 in stage 2.0 (TID 133). 4544 bytes result sent to driver
[2021-05-14 16:56:34,669] {docker.py:276} INFO - 21/05/14 19:56:34 INFO TaskSetManager: Starting task 134.0 in stage 2.0 (TID 139) (379a553b2355, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:34,670] {docker.py:276} INFO - 21/05/14 19:56:34 INFO TaskSetManager: Finished task 128.0 in stage 2.0 (TID 133) in 3362 ms on 379a553b2355 (executor driver) (131/200)
[2021-05-14 16:56:34,670] {docker.py:276} INFO - 21/05/14 19:56:34 INFO Executor: Running task 134.0 in stage 2.0 (TID 139)
[2021-05-14 16:56:34,679] {docker.py:276} INFO - 21/05/14 19:56:34 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:34,681] {docker.py:276} INFO - 21/05/14 19:56:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:34,681] {docker.py:276} INFO - 21/05/14 19:56:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531375894676065453679_0002_m_000134_139, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531375894676065453679_0002_m_000134_139}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531375894676065453679_0002}; taskId=attempt_20210514195531375894676065453679_0002_m_000134_139, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bedd669}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:34 INFO StagingCommitter: Starting: Task committer attempt_20210514195531375894676065453679_0002_m_000134_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531375894676065453679_0002_m_000134_139
[2021-05-14 16:56:34,685] {docker.py:276} INFO - 21/05/14 19:56:34 INFO StagingCommitter: Task committer attempt_20210514195531375894676065453679_0002_m_000134_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531375894676065453679_0002_m_000134_139 : duration 0:00.003s
[2021-05-14 16:56:35,501] {docker.py:276} INFO - 21/05/14 19:56:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955317537723376625472593_0002_m_000131_136: needsTaskCommit() Task attempt_202105141955317537723376625472593_0002_m_000131_136
21/05/14 19:56:35 INFO StagingCommitter: Task committer attempt_202105141955317537723376625472593_0002_m_000131_136: needsTaskCommit() Task attempt_202105141955317537723376625472593_0002_m_000131_136: duration 0:00.000s
21/05/14 19:56:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317537723376625472593_0002_m_000131_136
[2021-05-14 16:56:35,504] {docker.py:276} INFO - 21/05/14 19:56:35 INFO Executor: Finished task 131.0 in stage 2.0 (TID 136). 4544 bytes result sent to driver
[2021-05-14 16:56:35,506] {docker.py:276} INFO - 21/05/14 19:56:35 INFO TaskSetManager: Starting task 135.0 in stage 2.0 (TID 140) (379a553b2355, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:56:35 INFO TaskSetManager: Finished task 131.0 in stage 2.0 (TID 136) in 3478 ms on 379a553b2355 (executor driver) (132/200)
21/05/14 19:56:35 INFO Executor: Running task 135.0 in stage 2.0 (TID 140)
[2021-05-14 16:56:35,513] {docker.py:276} INFO - 21/05/14 19:56:35 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:35,514] {docker.py:276} INFO - 21/05/14 19:56:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311973884316753114462_0002_m_000135_140, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311973884316753114462_0002_m_000135_140}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311973884316753114462_0002}; taskId=attempt_202105141955311973884316753114462_0002_m_000135_140, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@783e3fd9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:35,515] {docker.py:276} INFO - 21/05/14 19:56:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955311973884316753114462_0002_m_000135_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311973884316753114462_0002_m_000135_140
[2021-05-14 16:56:35,518] {docker.py:276} INFO - 21/05/14 19:56:35 INFO StagingCommitter: Task committer attempt_202105141955311973884316753114462_0002_m_000135_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311973884316753114462_0002_m_000135_140 : duration 0:00.003s
[2021-05-14 16:56:35,677] {docker.py:276} INFO - 21/05/14 19:56:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955316802336886338348344_0002_m_000133_138: needsTaskCommit() Task attempt_202105141955316802336886338348344_0002_m_000133_138
21/05/14 19:56:35 INFO StagingCommitter: Task committer attempt_202105141955316802336886338348344_0002_m_000133_138: needsTaskCommit() Task attempt_202105141955316802336886338348344_0002_m_000133_138: duration 0:00.002s
21/05/14 19:56:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316802336886338348344_0002_m_000133_138
[2021-05-14 16:56:35,680] {docker.py:276} INFO - 21/05/14 19:56:35 INFO Executor: Finished task 133.0 in stage 2.0 (TID 138). 4544 bytes result sent to driver
[2021-05-14 16:56:35,683] {docker.py:276} INFO - 21/05/14 19:56:35 INFO TaskSetManager: Starting task 136.0 in stage 2.0 (TID 141) (379a553b2355, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:35,684] {docker.py:276} INFO - 21/05/14 19:56:35 INFO Executor: Running task 136.0 in stage 2.0 (TID 141)
[2021-05-14 16:56:35,685] {docker.py:276} INFO - 21/05/14 19:56:35 INFO TaskSetManager: Finished task 133.0 in stage 2.0 (TID 138) in 1539 ms on 379a553b2355 (executor driver) (133/200)
[2021-05-14 16:56:35,695] {docker.py:276} INFO - 21/05/14 19:56:35 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:35,695] {docker.py:276} INFO - 21/05/14 19:56:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:35,698] {docker.py:276} INFO - 21/05/14 19:56:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:35,698] {docker.py:276} INFO - 21/05/14 19:56:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:35,699] {docker.py:276} INFO - 21/05/14 19:56:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:35,699] {docker.py:276} INFO - 21/05/14 19:56:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315003446008438050537_0002_m_000136_141, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315003446008438050537_0002_m_000136_141}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315003446008438050537_0002}; taskId=attempt_202105141955315003446008438050537_0002_m_000136_141, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48447cdd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:35,700] {docker.py:276} INFO - 21/05/14 19:56:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:35,700] {docker.py:276} INFO - 21/05/14 19:56:35 INFO StagingCommitter: Starting: Task committer attempt_202105141955315003446008438050537_0002_m_000136_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315003446008438050537_0002_m_000136_141
[2021-05-14 16:56:35,703] {docker.py:276} INFO - 21/05/14 19:56:35 INFO StagingCommitter: Task committer attempt_202105141955315003446008438050537_0002_m_000136_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315003446008438050537_0002_m_000136_141 : duration 0:00.004s
[2021-05-14 16:56:36,064] {docker.py:276} INFO - 21/05/14 19:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105141955316702677471585525717_0002_m_000132_137: needsTaskCommit() Task attempt_202105141955316702677471585525717_0002_m_000132_137
[2021-05-14 16:56:36,064] {docker.py:276} INFO - 21/05/14 19:56:36 INFO StagingCommitter: Task committer attempt_202105141955316702677471585525717_0002_m_000132_137: needsTaskCommit() Task attempt_202105141955316702677471585525717_0002_m_000132_137: duration 0:00.001s
21/05/14 19:56:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316702677471585525717_0002_m_000132_137
[2021-05-14 16:56:36,067] {docker.py:276} INFO - 21/05/14 19:56:36 INFO Executor: Finished task 132.0 in stage 2.0 (TID 137). 4544 bytes result sent to driver
[2021-05-14 16:56:36,068] {docker.py:276} INFO - 21/05/14 19:56:36 INFO TaskSetManager: Starting task 137.0 in stage 2.0 (TID 142) (379a553b2355, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:36,069] {docker.py:276} INFO - 21/05/14 19:56:36 INFO Executor: Running task 137.0 in stage 2.0 (TID 142)
[2021-05-14 16:56:36,070] {docker.py:276} INFO - 21/05/14 19:56:36 INFO TaskSetManager: Finished task 132.0 in stage 2.0 (TID 137) in 3017 ms on 379a553b2355 (executor driver) (134/200)
[2021-05-14 16:56:36,078] {docker.py:276} INFO - 21/05/14 19:56:36 INFO ShuffleBlockFetcherIterator: Getting 4 (5.0 KiB) non-empty blocks including 4 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:36,080] {docker.py:276} INFO - 21/05/14 19:56:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:36,081] {docker.py:276} INFO - 21/05/14 19:56:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313185624153276387437_0002_m_000137_142, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313185624153276387437_0002_m_000137_142}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313185624153276387437_0002}; taskId=attempt_202105141955313185624153276387437_0002_m_000137_142, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e298fdf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:36,081] {docker.py:276} INFO - 21/05/14 19:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105141955313185624153276387437_0002_m_000137_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313185624153276387437_0002_m_000137_142
[2021-05-14 16:56:36,083] {docker.py:276} INFO - 21/05/14 19:56:36 INFO StagingCommitter: Task committer attempt_202105141955313185624153276387437_0002_m_000137_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313185624153276387437_0002_m_000137_142 : duration 0:00.003s
[2021-05-14 16:56:36,738] {docker.py:276} INFO - 21/05/14 19:56:36 INFO StagingCommitter: Starting: Task committer attempt_20210514195531375894676065453679_0002_m_000134_139: needsTaskCommit() Task attempt_20210514195531375894676065453679_0002_m_000134_139
21/05/14 19:56:36 INFO StagingCommitter: Task committer attempt_20210514195531375894676065453679_0002_m_000134_139: needsTaskCommit() Task attempt_20210514195531375894676065453679_0002_m_000134_139: duration 0:00.001s
21/05/14 19:56:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531375894676065453679_0002_m_000134_139
[2021-05-14 16:56:36,739] {docker.py:276} INFO - 21/05/14 19:56:36 INFO Executor: Finished task 134.0 in stage 2.0 (TID 139). 4544 bytes result sent to driver
[2021-05-14 16:56:36,741] {docker.py:276} INFO - 21/05/14 19:56:36 INFO TaskSetManager: Starting task 138.0 in stage 2.0 (TID 143) (379a553b2355, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:36,742] {docker.py:276} INFO - 21/05/14 19:56:36 INFO TaskSetManager: Finished task 134.0 in stage 2.0 (TID 139) in 2075 ms on 379a553b2355 (executor driver) (135/200)
[2021-05-14 16:56:36,744] {docker.py:276} INFO - 21/05/14 19:56:36 INFO Executor: Running task 138.0 in stage 2.0 (TID 143)
[2021-05-14 16:56:36,754] {docker.py:276} INFO - 21/05/14 19:56:36 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:36,756] {docker.py:276} INFO - 21/05/14 19:56:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318637741303391439404_0002_m_000138_143, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318637741303391439404_0002_m_000138_143}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318637741303391439404_0002}; taskId=attempt_202105141955318637741303391439404_0002_m_000138_143, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26a626b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:36,757] {docker.py:276} INFO - 21/05/14 19:56:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105141955318637741303391439404_0002_m_000138_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318637741303391439404_0002_m_000138_143
[2021-05-14 16:56:36,760] {docker.py:276} INFO - 21/05/14 19:56:36 INFO StagingCommitter: Task committer attempt_202105141955318637741303391439404_0002_m_000138_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318637741303391439404_0002_m_000138_143 : duration 0:00.004s
[2021-05-14 16:56:37,598] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955311973884316753114462_0002_m_000135_140: needsTaskCommit() Task attempt_202105141955311973884316753114462_0002_m_000135_140
[2021-05-14 16:56:37,599] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Task committer attempt_202105141955311973884316753114462_0002_m_000135_140: needsTaskCommit() Task attempt_202105141955311973884316753114462_0002_m_000135_140: duration 0:00.000s
21/05/14 19:56:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311973884316753114462_0002_m_000135_140
[2021-05-14 16:56:37,601] {docker.py:276} INFO - 21/05/14 19:56:37 INFO Executor: Finished task 135.0 in stage 2.0 (TID 140). 4544 bytes result sent to driver
[2021-05-14 16:56:37,602] {docker.py:276} INFO - 21/05/14 19:56:37 INFO TaskSetManager: Starting task 139.0 in stage 2.0 (TID 144) (379a553b2355, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:37,604] {docker.py:276} INFO - 21/05/14 19:56:37 INFO Executor: Running task 139.0 in stage 2.0 (TID 144)
[2021-05-14 16:56:37,605] {docker.py:276} INFO - 21/05/14 19:56:37 INFO TaskSetManager: Finished task 135.0 in stage 2.0 (TID 140) in 2104 ms on 379a553b2355 (executor driver) (136/200)
[2021-05-14 16:56:37,613] {docker.py:276} INFO - 21/05/14 19:56:37 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:37,615] {docker.py:276} INFO - 21/05/14 19:56:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:37,615] {docker.py:276} INFO - 21/05/14 19:56:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:37,616] {docker.py:276} INFO - 21/05/14 19:56:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312174924678773311756_0002_m_000139_144, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312174924678773311756_0002_m_000139_144}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312174924678773311756_0002}; taskId=attempt_202105141955312174924678773311756_0002_m_000139_144, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d78adc0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:37,616] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955312174924678773311756_0002_m_000139_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312174924678773311756_0002_m_000139_144
[2021-05-14 16:56:37,619] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Task committer attempt_202105141955312174924678773311756_0002_m_000139_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312174924678773311756_0002_m_000139_144 : duration 0:00.003s
[2021-05-14 16:56:37,764] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955315003446008438050537_0002_m_000136_141: needsTaskCommit() Task attempt_202105141955315003446008438050537_0002_m_000136_141
[2021-05-14 16:56:37,764] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Task committer attempt_202105141955315003446008438050537_0002_m_000136_141: needsTaskCommit() Task attempt_202105141955315003446008438050537_0002_m_000136_141: duration 0:00.000s
21/05/14 19:56:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315003446008438050537_0002_m_000136_141
[2021-05-14 16:56:37,766] {docker.py:276} INFO - 21/05/14 19:56:37 INFO Executor: Finished task 136.0 in stage 2.0 (TID 141). 4544 bytes result sent to driver
[2021-05-14 16:56:37,767] {docker.py:276} INFO - 21/05/14 19:56:37 INFO TaskSetManager: Starting task 140.0 in stage 2.0 (TID 145) (379a553b2355, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:37,768] {docker.py:276} INFO - 21/05/14 19:56:37 INFO TaskSetManager: Finished task 136.0 in stage 2.0 (TID 141) in 2088 ms on 379a553b2355 (executor driver) (137/200)
[2021-05-14 16:56:37,768] {docker.py:276} INFO - 21/05/14 19:56:37 INFO Executor: Running task 140.0 in stage 2.0 (TID 145)
[2021-05-14 16:56:37,776] {docker.py:276} INFO - 21/05/14 19:56:37 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:37,778] {docker.py:276} INFO - 21/05/14 19:56:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:37,778] {docker.py:276} INFO - 21/05/14 19:56:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312528922989211743546_0002_m_000140_145, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312528922989211743546_0002_m_000140_145}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312528922989211743546_0002}; taskId=attempt_202105141955312528922989211743546_0002_m_000140_145, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14ac84cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:37,779] {docker.py:276} INFO - 21/05/14 19:56:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:37 INFO StagingCommitter: Starting: Task committer attempt_202105141955312528922989211743546_0002_m_000140_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312528922989211743546_0002_m_000140_145
[2021-05-14 16:56:37,781] {docker.py:276} INFO - 21/05/14 19:56:37 INFO StagingCommitter: Task committer attempt_202105141955312528922989211743546_0002_m_000140_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312528922989211743546_0002_m_000140_145 : duration 0:00.002s
[2021-05-14 16:56:38,148] {docker.py:276} INFO - 21/05/14 19:56:38 INFO StagingCommitter: Starting: Task committer attempt_202105141955313185624153276387437_0002_m_000137_142: needsTaskCommit() Task attempt_202105141955313185624153276387437_0002_m_000137_142
[2021-05-14 16:56:38,149] {docker.py:276} INFO - 21/05/14 19:56:38 INFO StagingCommitter: Task committer attempt_202105141955313185624153276387437_0002_m_000137_142: needsTaskCommit() Task attempt_202105141955313185624153276387437_0002_m_000137_142: duration 0:00.002s
21/05/14 19:56:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313185624153276387437_0002_m_000137_142
[2021-05-14 16:56:38,151] {docker.py:276} INFO - 21/05/14 19:56:38 INFO Executor: Finished task 137.0 in stage 2.0 (TID 142). 4544 bytes result sent to driver
[2021-05-14 16:56:38,164] {docker.py:276} INFO - 21/05/14 19:56:38 INFO TaskSetManager: Starting task 141.0 in stage 2.0 (TID 146) (379a553b2355, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:38,165] {docker.py:276} INFO - 21/05/14 19:56:38 INFO Executor: Running task 141.0 in stage 2.0 (TID 146)
[2021-05-14 16:56:38,166] {docker.py:276} INFO - 21/05/14 19:56:38 INFO TaskSetManager: Finished task 137.0 in stage 2.0 (TID 142) in 2101 ms on 379a553b2355 (executor driver) (138/200)
[2021-05-14 16:56:38,173] {docker.py:276} INFO - 21/05/14 19:56:38 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:38,175] {docker.py:276} INFO - 21/05/14 19:56:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:38,176] {docker.py:276} INFO - 21/05/14 19:56:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313109527627205096351_0002_m_000141_146, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313109527627205096351_0002_m_000141_146}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313109527627205096351_0002}; taskId=attempt_202105141955313109527627205096351_0002_m_000141_146, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12df96b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:38,176] {docker.py:276} INFO - 21/05/14 19:56:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:38 INFO StagingCommitter: Starting: Task committer attempt_202105141955313109527627205096351_0002_m_000141_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313109527627205096351_0002_m_000141_146
[2021-05-14 16:56:38,179] {docker.py:276} INFO - 21/05/14 19:56:38 INFO StagingCommitter: Task committer attempt_202105141955313109527627205096351_0002_m_000141_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313109527627205096351_0002_m_000141_146 : duration 0:00.004s
[2021-05-14 16:56:39,699] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955312174924678773311756_0002_m_000139_144: needsTaskCommit() Task attempt_202105141955312174924678773311756_0002_m_000139_144
21/05/14 19:56:39 INFO StagingCommitter: Task committer attempt_202105141955312174924678773311756_0002_m_000139_144: needsTaskCommit() Task attempt_202105141955312174924678773311756_0002_m_000139_144: duration 0:00.001s
21/05/14 19:56:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312174924678773311756_0002_m_000139_144
21/05/14 19:56:39 INFO Executor: Finished task 139.0 in stage 2.0 (TID 144). 4587 bytes result sent to driver
[2021-05-14 16:56:39,700] {docker.py:276} INFO - 21/05/14 19:56:39 INFO TaskSetManager: Starting task 142.0 in stage 2.0 (TID 147) (379a553b2355, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:39,702] {docker.py:276} INFO - 21/05/14 19:56:39 INFO TaskSetManager: Finished task 139.0 in stage 2.0 (TID 144) in 2102 ms on 379a553b2355 (executor driver) (139/200)
[2021-05-14 16:56:39,703] {docker.py:276} INFO - 21/05/14 19:56:39 INFO Executor: Running task 142.0 in stage 2.0 (TID 147)
[2021-05-14 16:56:39,711] {docker.py:276} INFO - 21/05/14 19:56:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:39,713] {docker.py:276} INFO - 21/05/14 19:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315071445401744999595_0002_m_000142_147, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315071445401744999595_0002_m_000142_147}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315071445401744999595_0002}; taskId=attempt_202105141955315071445401744999595_0002_m_000142_147, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14ccea35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:39,714] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955315071445401744999595_0002_m_000142_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315071445401744999595_0002_m_000142_147
[2021-05-14 16:56:39,716] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Task committer attempt_202105141955315071445401744999595_0002_m_000142_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315071445401744999595_0002_m_000142_147 : duration 0:00.003s
[2021-05-14 16:56:39,877] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955318637741303391439404_0002_m_000138_143: needsTaskCommit() Task attempt_202105141955318637741303391439404_0002_m_000138_143
[2021-05-14 16:56:39,877] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Task committer attempt_202105141955318637741303391439404_0002_m_000138_143: needsTaskCommit() Task attempt_202105141955318637741303391439404_0002_m_000138_143: duration 0:00.001s
[2021-05-14 16:56:39,878] {docker.py:276} INFO - 21/05/14 19:56:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318637741303391439404_0002_m_000138_143
[2021-05-14 16:56:39,880] {docker.py:276} INFO - 21/05/14 19:56:39 INFO Executor: Finished task 138.0 in stage 2.0 (TID 143). 4587 bytes result sent to driver
[2021-05-14 16:56:39,882] {docker.py:276} INFO - 21/05/14 19:56:39 INFO TaskSetManager: Starting task 143.0 in stage 2.0 (TID 148) (379a553b2355, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955312528922989211743546_0002_m_000140_145: needsTaskCommit() Task attempt_202105141955312528922989211743546_0002_m_000140_145
[2021-05-14 16:56:39,884] {docker.py:276} INFO - 21/05/14 19:56:39 INFO Executor: Running task 143.0 in stage 2.0 (TID 148)
21/05/14 19:56:39 INFO StagingCommitter: Task committer attempt_202105141955312528922989211743546_0002_m_000140_145: needsTaskCommit() Task attempt_202105141955312528922989211743546_0002_m_000140_145: duration 0:00.001s
21/05/14 19:56:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312528922989211743546_0002_m_000140_145
[2021-05-14 16:56:39,885] {docker.py:276} INFO - 21/05/14 19:56:39 INFO TaskSetManager: Finished task 138.0 in stage 2.0 (TID 143) in 3148 ms on 379a553b2355 (executor driver) (140/200)
[2021-05-14 16:56:39,885] {docker.py:276} INFO - 21/05/14 19:56:39 INFO Executor: Finished task 140.0 in stage 2.0 (TID 145). 4587 bytes result sent to driver
[2021-05-14 16:56:39,888] {docker.py:276} INFO - 21/05/14 19:56:39 INFO TaskSetManager: Starting task 144.0 in stage 2.0 (TID 149) (379a553b2355, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:39,889] {docker.py:276} INFO - 21/05/14 19:56:39 INFO TaskSetManager: Finished task 140.0 in stage 2.0 (TID 145) in 2124 ms on 379a553b2355 (executor driver) (141/200)
[2021-05-14 16:56:39,890] {docker.py:276} INFO - 21/05/14 19:56:39 INFO Executor: Running task 144.0 in stage 2.0 (TID 149)
[2021-05-14 16:56:39,896] {docker.py:276} INFO - 21/05/14 19:56:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:39,896] {docker.py:276} INFO - 21/05/14 19:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:39,898] {docker.py:276} INFO - 21/05/14 19:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:39,899] {docker.py:276} INFO - 21/05/14 19:56:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:39,899] {docker.py:276} INFO - 21/05/14 19:56:39 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:39,899] {docker.py:276} INFO - 21/05/14 19:56:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315193234082741167124_0002_m_000143_148, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315193234082741167124_0002_m_000143_148}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315193234082741167124_0002}; taskId=attempt_202105141955315193234082741167124_0002_m_000143_148, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@783cf1d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:39,900] {docker.py:276} INFO - 21/05/14 19:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955315193234082741167124_0002_m_000143_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315193234082741167124_0002_m_000143_148 
21/05/14 19:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:39,901] {docker.py:276} INFO - 21/05/14 19:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:39,902] {docker.py:276} INFO - 21/05/14 19:56:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:39,902] {docker.py:276} INFO - 21/05/14 19:56:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318729371873490554016_0002_m_000144_149, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318729371873490554016_0002_m_000144_149}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318729371873490554016_0002}; taskId=attempt_202105141955318729371873490554016_0002_m_000144_149, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c87933a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:39,902] {docker.py:276} INFO - 21/05/14 19:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105141955318729371873490554016_0002_m_000144_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318729371873490554016_0002_m_000144_149
[2021-05-14 16:56:39,903] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Task committer attempt_202105141955315193234082741167124_0002_m_000143_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315193234082741167124_0002_m_000143_148 : duration 0:00.003s
[2021-05-14 16:56:39,904] {docker.py:276} INFO - 21/05/14 19:56:39 INFO StagingCommitter: Task committer attempt_202105141955318729371873490554016_0002_m_000144_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318729371873490554016_0002_m_000144_149 : duration 0:00.003s
[2021-05-14 16:56:40,256] {docker.py:276} INFO - 21/05/14 19:56:40 INFO StagingCommitter: Starting: Task committer attempt_202105141955313109527627205096351_0002_m_000141_146: needsTaskCommit() Task attempt_202105141955313109527627205096351_0002_m_000141_146
21/05/14 19:56:40 INFO StagingCommitter: Task committer attempt_202105141955313109527627205096351_0002_m_000141_146: needsTaskCommit() Task attempt_202105141955313109527627205096351_0002_m_000141_146: duration 0:00.000s
21/05/14 19:56:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313109527627205096351_0002_m_000141_146
[2021-05-14 16:56:40,256] {docker.py:276} INFO - 21/05/14 19:56:40 INFO Executor: Finished task 141.0 in stage 2.0 (TID 146). 4544 bytes result sent to driver
[2021-05-14 16:56:40,258] {docker.py:276} INFO - 21/05/14 19:56:40 INFO TaskSetManager: Starting task 145.0 in stage 2.0 (TID 150) (379a553b2355, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:40,258] {docker.py:276} INFO - 21/05/14 19:56:40 INFO Executor: Running task 145.0 in stage 2.0 (TID 150)
[2021-05-14 16:56:40,260] {docker.py:276} INFO - 21/05/14 19:56:40 INFO TaskSetManager: Finished task 141.0 in stage 2.0 (TID 146) in 2099 ms on 379a553b2355 (executor driver) (142/200)
[2021-05-14 16:56:40,268] {docker.py:276} INFO - 21/05/14 19:56:40 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:40,268] {docker.py:276} INFO - 21/05/14 19:56:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:40,269] {docker.py:276} INFO - 21/05/14 19:56:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:40,270] {docker.py:276} INFO - 21/05/14 19:56:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314209865237533520128_0002_m_000145_150, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314209865237533520128_0002_m_000145_150}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314209865237533520128_0002}; taskId=attempt_202105141955314209865237533520128_0002_m_000145_150, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18815fcd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:40,270] {docker.py:276} INFO - 21/05/14 19:56:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:40,270] {docker.py:276} INFO - 21/05/14 19:56:40 INFO StagingCommitter: Starting: Task committer attempt_202105141955314209865237533520128_0002_m_000145_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314209865237533520128_0002_m_000145_150
[2021-05-14 16:56:40,273] {docker.py:276} INFO - 21/05/14 19:56:40 INFO StagingCommitter: Task committer attempt_202105141955314209865237533520128_0002_m_000145_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314209865237533520128_0002_m_000145_150 : duration 0:00.003s
[2021-05-14 16:56:41,793] {docker.py:276} INFO - 21/05/14 19:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955315071445401744999595_0002_m_000142_147: needsTaskCommit() Task attempt_202105141955315071445401744999595_0002_m_000142_147
[2021-05-14 16:56:41,795] {docker.py:276} INFO - 21/05/14 19:56:41 INFO StagingCommitter: Task committer attempt_202105141955315071445401744999595_0002_m_000142_147: needsTaskCommit() Task attempt_202105141955315071445401744999595_0002_m_000142_147: duration 0:00.001s
[2021-05-14 16:56:41,795] {docker.py:276} INFO - 21/05/14 19:56:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315071445401744999595_0002_m_000142_147
[2021-05-14 16:56:41,797] {docker.py:276} INFO - 21/05/14 19:56:41 INFO Executor: Finished task 142.0 in stage 2.0 (TID 147). 4544 bytes result sent to driver
[2021-05-14 16:56:41,799] {docker.py:276} INFO - 21/05/14 19:56:41 INFO TaskSetManager: Starting task 146.0 in stage 2.0 (TID 151) (379a553b2355, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:41,800] {docker.py:276} INFO - 21/05/14 19:56:41 INFO TaskSetManager: Finished task 142.0 in stage 2.0 (TID 147) in 2103 ms on 379a553b2355 (executor driver) (143/200)
[2021-05-14 16:56:41,803] {docker.py:276} INFO - 21/05/14 19:56:41 INFO Executor: Running task 146.0 in stage 2.0 (TID 151)
[2021-05-14 16:56:41,811] {docker.py:276} INFO - 21/05/14 19:56:41 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:41,812] {docker.py:276} INFO - 21/05/14 19:56:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:41,814] {docker.py:276} INFO - 21/05/14 19:56:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:41,814] {docker.py:276} INFO - 21/05/14 19:56:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315156592045170403980_0002_m_000146_151, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315156592045170403980_0002_m_000146_151}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315156592045170403980_0002}; taskId=attempt_202105141955315156592045170403980_0002_m_000146_151, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68dd0357}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:41,814] {docker.py:276} INFO - 21/05/14 19:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105141955315156592045170403980_0002_m_000146_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315156592045170403980_0002_m_000146_151
[2021-05-14 16:56:41,816] {docker.py:276} INFO - 21/05/14 19:56:41 INFO StagingCommitter: Task committer attempt_202105141955315156592045170403980_0002_m_000146_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315156592045170403980_0002_m_000146_151 : duration 0:00.002s
[2021-05-14 16:56:41,970] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955314209865237533520128_0002_m_000145_150: needsTaskCommit() Task attempt_202105141955314209865237533520128_0002_m_000145_150
[2021-05-14 16:56:41,971] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Task committer attempt_202105141955314209865237533520128_0002_m_000145_150: needsTaskCommit() Task attempt_202105141955314209865237533520128_0002_m_000145_150: duration 0:00.001s
[2021-05-14 16:56:41,971] {docker.py:276} INFO - 21/05/14 19:56:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314209865237533520128_0002_m_000145_150
[2021-05-14 16:56:41,972] {docker.py:276} INFO - 21/05/14 19:56:42 INFO Executor: Finished task 145.0 in stage 2.0 (TID 150). 4544 bytes result sent to driver
[2021-05-14 16:56:41,974] {docker.py:276} INFO - 21/05/14 19:56:42 INFO TaskSetManager: Starting task 147.0 in stage 2.0 (TID 152) (379a553b2355, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:41,975] {docker.py:276} INFO - 21/05/14 19:56:42 INFO TaskSetManager: Finished task 145.0 in stage 2.0 (TID 150) in 1719 ms on 379a553b2355 (executor driver) (144/200)
[2021-05-14 16:56:41,975] {docker.py:276} INFO - 21/05/14 19:56:42 INFO Executor: Running task 147.0 in stage 2.0 (TID 152)
[2021-05-14 16:56:41,984] {docker.py:276} INFO - 21/05/14 19:56:42 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:41,985] {docker.py:276} INFO - 21/05/14 19:56:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:41,986] {docker.py:276} INFO - 21/05/14 19:56:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531339771763454102355_0002_m_000147_152, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531339771763454102355_0002_m_000147_152}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531339771763454102355_0002}; taskId=attempt_20210514195531339771763454102355_0002_m_000147_152, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7016a850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:42 INFO StagingCommitter: Starting: Task committer attempt_20210514195531339771763454102355_0002_m_000147_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531339771763454102355_0002_m_000147_152
[2021-05-14 16:56:41,989] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Task committer attempt_20210514195531339771763454102355_0002_m_000147_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531339771763454102355_0002_m_000147_152 : duration 0:00.003s
[2021-05-14 16:56:42,187] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955315193234082741167124_0002_m_000143_148: needsTaskCommit() Task attempt_202105141955315193234082741167124_0002_m_000143_148
21/05/14 19:56:42 INFO StagingCommitter: Task committer attempt_202105141955315193234082741167124_0002_m_000143_148: needsTaskCommit() Task attempt_202105141955315193234082741167124_0002_m_000143_148: duration 0:00.001s
21/05/14 19:56:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315193234082741167124_0002_m_000143_148
[2021-05-14 16:56:42,189] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955318729371873490554016_0002_m_000144_149: needsTaskCommit() Task attempt_202105141955318729371873490554016_0002_m_000144_149
[2021-05-14 16:56:42,190] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Task committer attempt_202105141955318729371873490554016_0002_m_000144_149: needsTaskCommit() Task attempt_202105141955318729371873490554016_0002_m_000144_149: duration 0:00.002s
21/05/14 19:56:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318729371873490554016_0002_m_000144_149
[2021-05-14 16:56:42,191] {docker.py:276} INFO - 21/05/14 19:56:42 INFO Executor: Finished task 143.0 in stage 2.0 (TID 148). 4544 bytes result sent to driver
[2021-05-14 16:56:42,193] {docker.py:276} INFO - 21/05/14 19:56:42 INFO TaskSetManager: Starting task 148.0 in stage 2.0 (TID 153) (379a553b2355, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:42,194] {docker.py:276} INFO - 21/05/14 19:56:42 INFO Executor: Finished task 144.0 in stage 2.0 (TID 149). 4544 bytes result sent to driver
[2021-05-14 16:56:42,194] {docker.py:276} INFO - 21/05/14 19:56:42 INFO TaskSetManager: Finished task 143.0 in stage 2.0 (TID 148) in 2315 ms on 379a553b2355 (executor driver) (145/200)
[2021-05-14 16:56:42,195] {docker.py:276} INFO - 21/05/14 19:56:42 INFO Executor: Running task 148.0 in stage 2.0 (TID 153)
[2021-05-14 16:56:42,197] {docker.py:276} INFO - 21/05/14 19:56:42 INFO TaskSetManager: Starting task 149.0 in stage 2.0 (TID 154) (379a553b2355, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:42,198] {docker.py:276} INFO - 21/05/14 19:56:42 INFO TaskSetManager: Finished task 144.0 in stage 2.0 (TID 149) in 2313 ms on 379a553b2355 (executor driver) (146/200)
[2021-05-14 16:56:42,199] {docker.py:276} INFO - 21/05/14 19:56:42 INFO Executor: Running task 149.0 in stage 2.0 (TID 154)
[2021-05-14 16:56:42,207] {docker.py:276} INFO - 21/05/14 19:56:42 INFO ShuffleBlockFetcherIterator: Getting 4 (6.2 KiB) non-empty blocks including 4 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:42,208] {docker.py:276} INFO - 21/05/14 19:56:42 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:42,208] {docker.py:276} INFO - 21/05/14 19:56:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:42,209] {docker.py:276} INFO - 21/05/14 19:56:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:42,210] {docker.py:276} INFO - 21/05/14 19:56:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315008547924797730529_0002_m_000148_153, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315008547924797730529_0002_m_000148_153}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315008547924797730529_0002}; taskId=attempt_202105141955315008547924797730529_0002_m_000148_153, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5422626c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955315008547924797730529_0002_m_000148_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315008547924797730529_0002_m_000148_153
[2021-05-14 16:56:42,211] {docker.py:276} INFO - 21/05/14 19:56:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:42,211] {docker.py:276} INFO - 21/05/14 19:56:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312796199215231086890_0002_m_000149_154, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312796199215231086890_0002_m_000149_154}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312796199215231086890_0002}; taskId=attempt_202105141955312796199215231086890_0002_m_000149_154, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c5244f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:42,212] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Starting: Task committer attempt_202105141955312796199215231086890_0002_m_000149_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312796199215231086890_0002_m_000149_154
[2021-05-14 16:56:42,214] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Task committer attempt_202105141955315008547924797730529_0002_m_000148_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315008547924797730529_0002_m_000148_153 : duration 0:00.003s
[2021-05-14 16:56:42,216] {docker.py:276} INFO - 21/05/14 19:56:42 INFO StagingCommitter: Task committer attempt_202105141955312796199215231086890_0002_m_000149_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312796199215231086890_0002_m_000149_154 : duration 0:00.004s
[2021-05-14 16:56:44,279] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955315008547924797730529_0002_m_000148_153: needsTaskCommit() Task attempt_202105141955315008547924797730529_0002_m_000148_153
[2021-05-14 16:56:44,281] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_202105141955315008547924797730529_0002_m_000148_153: needsTaskCommit() Task attempt_202105141955315008547924797730529_0002_m_000148_153: duration 0:00.001s
21/05/14 19:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315008547924797730529_0002_m_000148_153
[2021-05-14 16:56:44,283] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Finished task 148.0 in stage 2.0 (TID 153). 4544 bytes result sent to driver
[2021-05-14 16:56:44,284] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Starting task 150.0 in stage 2.0 (TID 155) (379a553b2355, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:44,286] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Finished task 148.0 in stage 2.0 (TID 153) in 2061 ms on 379a553b2355 (executor driver) (147/200)
[2021-05-14 16:56:44,287] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Running task 150.0 in stage 2.0 (TID 155)
[2021-05-14 16:56:44,295] {docker.py:276} INFO - 21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:44,297] {docker.py:276} INFO - 21/05/14 19:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312281353667717967734_0002_m_000150_155, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312281353667717967734_0002_m_000150_155}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312281353667717967734_0002}; taskId=attempt_202105141955312281353667717967734_0002_m_000150_155, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68dfe50e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:44,298] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955312281353667717967734_0002_m_000150_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312281353667717967734_0002_m_000150_155
[2021-05-14 16:56:44,301] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_202105141955312281353667717967734_0002_m_000150_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312281353667717967734_0002_m_000150_155 : duration 0:00.004s
[2021-05-14 16:56:44,445] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955315156592045170403980_0002_m_000146_151: needsTaskCommit() Task attempt_202105141955315156592045170403980_0002_m_000146_151
[2021-05-14 16:56:44,446] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_20210514195531339771763454102355_0002_m_000147_152: needsTaskCommit() Task attempt_20210514195531339771763454102355_0002_m_000147_152
[2021-05-14 16:56:44,447] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_202105141955315156592045170403980_0002_m_000146_151: needsTaskCommit() Task attempt_202105141955315156592045170403980_0002_m_000146_151: duration 0:00.002s
21/05/14 19:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315156592045170403980_0002_m_000146_151
[2021-05-14 16:56:44,447] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_20210514195531339771763454102355_0002_m_000147_152: needsTaskCommit() Task attempt_20210514195531339771763454102355_0002_m_000147_152: duration 0:00.003s
21/05/14 19:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531339771763454102355_0002_m_000147_152
[2021-05-14 16:56:44,450] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Finished task 146.0 in stage 2.0 (TID 151). 4544 bytes result sent to driver
21/05/14 19:56:44 INFO Executor: Finished task 147.0 in stage 2.0 (TID 152). 4587 bytes result sent to driver
[2021-05-14 16:56:44,451] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Starting task 151.0 in stage 2.0 (TID 156) (379a553b2355, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:44,454] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Running task 151.0 in stage 2.0 (TID 156)
[2021-05-14 16:56:44,454] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Starting task 152.0 in stage 2.0 (TID 157) (379a553b2355, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:44,455] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Finished task 146.0 in stage 2.0 (TID 151) in 2624 ms on 379a553b2355 (executor driver) (148/200)
[2021-05-14 16:56:44,455] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Running task 152.0 in stage 2.0 (TID 157)
[2021-05-14 16:56:44,456] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Finished task 147.0 in stage 2.0 (TID 152) in 2450 ms on 379a553b2355 (executor driver) (149/200)
[2021-05-14 16:56:44,464] {docker.py:276} INFO - 21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:44,464] {docker.py:276} INFO - 21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:44,466] {docker.py:276} INFO - 21/05/14 19:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313601440416220431044_0002_m_000151_156, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313601440416220431044_0002_m_000151_156}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313601440416220431044_0002}; taskId=attempt_202105141955313601440416220431044_0002_m_000151_156, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1696d032}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955313601440416220431044_0002_m_000151_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313601440416220431044_0002_m_000151_156
[2021-05-14 16:56:44,467] {docker.py:276} INFO - 21/05/14 19:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:44,468] {docker.py:276} INFO - 21/05/14 19:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:44,468] {docker.py:276} INFO - 21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531430446368725637300_0002_m_000152_157, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531430446368725637300_0002_m_000152_157}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531430446368725637300_0002}; taskId=attempt_20210514195531430446368725637300_0002_m_000152_157, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6dd3d014}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:44,468] {docker.py:276} INFO - 21/05/14 19:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_20210514195531430446368725637300_0002_m_000152_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531430446368725637300_0002_m_000152_157
[2021-05-14 16:56:44,470] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_202105141955313601440416220431044_0002_m_000151_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313601440416220431044_0002_m_000151_156 : duration 0:00.004s
[2021-05-14 16:56:44,472] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_20210514195531430446368725637300_0002_m_000152_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531430446368725637300_0002_m_000152_157 : duration 0:00.004s
[2021-05-14 16:56:44,611] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955312796199215231086890_0002_m_000149_154: needsTaskCommit() Task attempt_202105141955312796199215231086890_0002_m_000149_154
[2021-05-14 16:56:44,612] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_202105141955312796199215231086890_0002_m_000149_154: needsTaskCommit() Task attempt_202105141955312796199215231086890_0002_m_000149_154: duration 0:00.001s
21/05/14 19:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312796199215231086890_0002_m_000149_154
[2021-05-14 16:56:44,615] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Finished task 149.0 in stage 2.0 (TID 154). 4544 bytes result sent to driver
[2021-05-14 16:56:44,616] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Starting task 153.0 in stage 2.0 (TID 158) (379a553b2355, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:44,618] {docker.py:276} INFO - 21/05/14 19:56:44 INFO Executor: Running task 153.0 in stage 2.0 (TID 158)
[2021-05-14 16:56:44,618] {docker.py:276} INFO - 21/05/14 19:56:44 INFO TaskSetManager: Finished task 149.0 in stage 2.0 (TID 154) in 2390 ms on 379a553b2355 (executor driver) (150/200)
[2021-05-14 16:56:44,630] {docker.py:276} INFO - 21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:44,632] {docker.py:276} INFO - 21/05/14 19:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311598373217417923480_0002_m_000153_158, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311598373217417923480_0002_m_000153_158}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311598373217417923480_0002}; taskId=attempt_202105141955311598373217417923480_0002_m_000153_158, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a099413}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105141955311598373217417923480_0002_m_000153_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311598373217417923480_0002_m_000153_158
[2021-05-14 16:56:44,635] {docker.py:276} INFO - 21/05/14 19:56:44 INFO StagingCommitter: Task committer attempt_202105141955311598373217417923480_0002_m_000153_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311598373217417923480_0002_m_000153_158 : duration 0:00.003s
[2021-05-14 16:56:46,772] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955313601440416220431044_0002_m_000151_156: needsTaskCommit() Task attempt_202105141955313601440416220431044_0002_m_000151_156
[2021-05-14 16:56:46,773] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_202105141955313601440416220431044_0002_m_000151_156: needsTaskCommit() Task attempt_202105141955313601440416220431044_0002_m_000151_156: duration 0:00.001s
21/05/14 19:56:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313601440416220431044_0002_m_000151_156
21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_20210514195531430446368725637300_0002_m_000152_157: needsTaskCommit() Task attempt_20210514195531430446368725637300_0002_m_000152_157
[2021-05-14 16:56:46,774] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_20210514195531430446368725637300_0002_m_000152_157: needsTaskCommit() Task attempt_20210514195531430446368725637300_0002_m_000152_157: duration 0:00.001s
21/05/14 19:56:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531430446368725637300_0002_m_000152_157
[2021-05-14 16:56:46,775] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955312281353667717967734_0002_m_000150_155: needsTaskCommit() Task attempt_202105141955312281353667717967734_0002_m_000150_155
[2021-05-14 16:56:46,777] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Finished task 152.0 in stage 2.0 (TID 157). 4587 bytes result sent to driver
21/05/14 19:56:46 INFO Executor: Finished task 151.0 in stage 2.0 (TID 156). 4587 bytes result sent to driver
[2021-05-14 16:56:46,778] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955311598373217417923480_0002_m_000153_158: needsTaskCommit() Task attempt_202105141955311598373217417923480_0002_m_000153_158
[2021-05-14 16:56:46,778] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_202105141955312281353667717967734_0002_m_000150_155: needsTaskCommit() Task attempt_202105141955312281353667717967734_0002_m_000150_155: duration 0:00.002s
[2021-05-14 16:56:46,779] {docker.py:276} INFO - 21/05/14 19:56:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312281353667717967734_0002_m_000150_155
21/05/14 19:56:46 INFO TaskSetManager: Starting task 154.0 in stage 2.0 (TID 159) (379a553b2355, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:46,779] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Running task 154.0 in stage 2.0 (TID 159)
[2021-05-14 16:56:46,781] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_202105141955311598373217417923480_0002_m_000153_158: needsTaskCommit() Task attempt_202105141955311598373217417923480_0002_m_000153_158: duration 0:00.004s
21/05/14 19:56:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311598373217417923480_0002_m_000153_158
21/05/14 19:56:46 INFO TaskSetManager: Starting task 155.0 in stage 2.0 (TID 160) (379a553b2355, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:46,782] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Running task 155.0 in stage 2.0 (TID 160)
[2021-05-14 16:56:46,783] {docker.py:276} INFO - 21/05/14 19:56:46 INFO TaskSetManager: Finished task 151.0 in stage 2.0 (TID 156) in 2334 ms on 379a553b2355 (executor driver) (151/200)
[2021-05-14 16:56:46,783] {docker.py:276} INFO - 21/05/14 19:56:46 INFO TaskSetManager: Finished task 152.0 in stage 2.0 (TID 157) in 2332 ms on 379a553b2355 (executor driver) (152/200)
[2021-05-14 16:56:46,784] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Finished task 150.0 in stage 2.0 (TID 155). 4587 bytes result sent to driver
[2021-05-14 16:56:46,786] {docker.py:276} INFO - 21/05/14 19:56:46 INFO TaskSetManager: Starting task 156.0 in stage 2.0 (TID 161) (379a553b2355, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:46,787] {docker.py:276} INFO - 21/05/14 19:56:46 INFO TaskSetManager: Finished task 150.0 in stage 2.0 (TID 155) in 2506 ms on 379a553b2355 (executor driver) (153/200)
[2021-05-14 16:56:46,788] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Finished task 153.0 in stage 2.0 (TID 158). 4630 bytes result sent to driver
[2021-05-14 16:56:46,789] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Running task 156.0 in stage 2.0 (TID 161)
21/05/14 19:56:46 INFO TaskSetManager: Starting task 157.0 in stage 2.0 (TID 162) (379a553b2355, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:46,789] {docker.py:276} INFO - 21/05/14 19:56:46 INFO TaskSetManager: Finished task 153.0 in stage 2.0 (TID 158) in 2176 ms on 379a553b2355 (executor driver) (154/200)
[2021-05-14 16:56:46,790] {docker.py:276} INFO - 21/05/14 19:56:46 INFO Executor: Running task 157.0 in stage 2.0 (TID 162)
[2021-05-14 16:56:46,795] {docker.py:276} INFO - 21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:46,797] {docker.py:276} INFO - 21/05/14 19:56:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:46,798] {docker.py:276} INFO - 21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:46,798] {docker.py:276} INFO - 21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313097230068269644484_0002_m_000155_160, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313097230068269644484_0002_m_000155_160}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313097230068269644484_0002}; taskId=attempt_202105141955313097230068269644484_0002_m_000155_160, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1edd0386}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:46,798] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955313097230068269644484_0002_m_000155_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313097230068269644484_0002_m_000155_160
[2021-05-14 16:56:46,800] {docker.py:276} INFO - 21/05/14 19:56:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313531474779969409221_0002_m_000154_159, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313531474779969409221_0002_m_000154_159}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313531474779969409221_0002}; taskId=attempt_202105141955313531474779969409221_0002_m_000154_159, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b868210}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:46,801] {docker.py:276} INFO - 21/05/14 19:56:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955313531474779969409221_0002_m_000154_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313531474779969409221_0002_m_000154_159
[2021-05-14 16:56:46,801] {docker.py:276} INFO - 21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:46,801] {docker.py:276} INFO - 21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:56:46,802] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_202105141955313097230068269644484_0002_m_000155_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313097230068269644484_0002_m_000155_160 : duration 0:00.004s
[2021-05-14 16:56:46,804] {docker.py:276} INFO - 21/05/14 19:56:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:46,804] {docker.py:276} INFO - 21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531777882000315521404_0002_m_000156_161, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531777882000315521404_0002_m_000156_161}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531777882000315521404_0002}; taskId=attempt_20210514195531777882000315521404_0002_m_000156_161, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12c30e88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:46,805] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_20210514195531777882000315521404_0002_m_000156_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531777882000315521404_0002_m_000156_161
[2021-05-14 16:56:46,805] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_202105141955313531474779969409221_0002_m_000154_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313531474779969409221_0002_m_000154_159 : duration 0:00.005s
[2021-05-14 16:56:46,807] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_20210514195531777882000315521404_0002_m_000156_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531777882000315521404_0002_m_000156_161 : duration 0:00.003s
[2021-05-14 16:56:46,810] {docker.py:276} INFO - 21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:46,810] {docker.py:276} INFO - 21/05/14 19:56:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:46,811] {docker.py:276} INFO - 21/05/14 19:56:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:46,812] {docker.py:276} INFO - 21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:46,812] {docker.py:276} INFO - 21/05/14 19:56:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317275660053860335830_0002_m_000157_162, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317275660053860335830_0002_m_000157_162}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317275660053860335830_0002}; taskId=attempt_202105141955317275660053860335830_0002_m_000157_162, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68326a5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:46,813] {docker.py:276} INFO - 21/05/14 19:56:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:46,813] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Starting: Task committer attempt_202105141955317275660053860335830_0002_m_000157_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317275660053860335830_0002_m_000157_162
[2021-05-14 16:56:46,815] {docker.py:276} INFO - 21/05/14 19:56:46 INFO StagingCommitter: Task committer attempt_202105141955317275660053860335830_0002_m_000157_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317275660053860335830_0002_m_000157_162 : duration 0:00.003s
[2021-05-14 16:56:49,133] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105141955313097230068269644484_0002_m_000155_160: needsTaskCommit() Task attempt_202105141955313097230068269644484_0002_m_000155_160
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_20210514195531777882000315521404_0002_m_000156_161: needsTaskCommit() Task attempt_20210514195531777882000315521404_0002_m_000156_161
[2021-05-14 16:56:49,134] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_20210514195531777882000315521404_0002_m_000156_161: needsTaskCommit() Task attempt_20210514195531777882000315521404_0002_m_000156_161: duration 0:00.001s
21/05/14 19:56:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531777882000315521404_0002_m_000156_161
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105141955313531474779969409221_0002_m_000154_159: needsTaskCommit() Task attempt_202105141955313531474779969409221_0002_m_000154_159
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105141955317275660053860335830_0002_m_000157_162: needsTaskCommit() Task attempt_202105141955317275660053860335830_0002_m_000157_162
[2021-05-14 16:56:49,135] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_202105141955313531474779969409221_0002_m_000154_159: needsTaskCommit() Task attempt_202105141955313531474779969409221_0002_m_000154_159: duration 0:00.001s
21/05/14 19:56:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313531474779969409221_0002_m_000154_159
[2021-05-14 16:56:49,136] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_202105141955313097230068269644484_0002_m_000155_160: needsTaskCommit() Task attempt_202105141955313097230068269644484_0002_m_000155_160: duration 0:00.001s
21/05/14 19:56:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313097230068269644484_0002_m_000155_160
21/05/14 19:56:49 INFO Executor: Finished task 154.0 in stage 2.0 (TID 159). 4544 bytes result sent to driver
21/05/14 19:56:49 INFO Executor: Finished task 156.0 in stage 2.0 (TID 161). 4544 bytes result sent to driver
21/05/14 19:56:49 INFO Executor: Finished task 155.0 in stage 2.0 (TID 160). 4544 bytes result sent to driver
[2021-05-14 16:56:49,140] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_202105141955317275660053860335830_0002_m_000157_162: needsTaskCommit() Task attempt_202105141955317275660053860335830_0002_m_000157_162: duration 0:00.003s
21/05/14 19:56:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317275660053860335830_0002_m_000157_162
21/05/14 19:56:49 INFO TaskSetManager: Starting task 158.0 in stage 2.0 (TID 163) (379a553b2355, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:56:49 INFO Executor: Running task 158.0 in stage 2.0 (TID 163)
21/05/14 19:56:49 INFO TaskSetManager: Starting task 159.0 in stage 2.0 (TID 164) (379a553b2355, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:49,141] {docker.py:276} INFO - 21/05/14 19:56:49 INFO Executor: Running task 159.0 in stage 2.0 (TID 164)
[2021-05-14 16:56:49,141] {docker.py:276} INFO - 21/05/14 19:56:49 INFO Executor: Finished task 157.0 in stage 2.0 (TID 162). 4544 bytes result sent to driver
21/05/14 19:56:49 INFO TaskSetManager: Starting task 160.0 in stage 2.0 (TID 165) (379a553b2355, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:49,142] {docker.py:276} INFO - 21/05/14 19:56:49 INFO TaskSetManager: Finished task 154.0 in stage 2.0 (TID 159) in 2366 ms on 379a553b2355 (executor driver) (155/200)
[2021-05-14 16:56:49,142] {docker.py:276} INFO - 21/05/14 19:56:49 INFO TaskSetManager: Finished task 155.0 in stage 2.0 (TID 160) in 2363 ms on 379a553b2355 (executor driver) (156/200)
[2021-05-14 16:56:49,143] {docker.py:276} INFO - 21/05/14 19:56:49 INFO TaskSetManager: Starting task 161.0 in stage 2.0 (TID 166) (379a553b2355, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:49,143] {docker.py:276} INFO - 21/05/14 19:56:49 INFO TaskSetManager: Finished task 156.0 in stage 2.0 (TID 161) in 2360 ms on 379a553b2355 (executor driver) (157/200)
[2021-05-14 16:56:49,143] {docker.py:276} INFO - 21/05/14 19:56:49 INFO TaskSetManager: Finished task 157.0 in stage 2.0 (TID 162) in 2358 ms on 379a553b2355 (executor driver) (158/200)
[2021-05-14 16:56:49,144] {docker.py:276} INFO - 21/05/14 19:56:49 INFO Executor: Running task 161.0 in stage 2.0 (TID 166)
[2021-05-14 16:56:49,151] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:49,152] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:49,152] {docker.py:276} INFO - 21/05/14 19:56:49 INFO Executor: Running task 160.0 in stage 2.0 (TID 165)
[2021-05-14 16:56:49,153] {docker.py:276} INFO - 21/05/14 19:56:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:49,154] {docker.py:276} INFO - 21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317709268759623743243_0002_m_000159_164, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317709268759623743243_0002_m_000159_164}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317709268759623743243_0002}; taskId=attempt_202105141955317709268759623743243_0002_m_000159_164, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@746b1171}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105141955317709268759623743243_0002_m_000159_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317709268759623743243_0002_m_000159_164
[2021-05-14 16:56:49,157] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_202105141955317709268759623743243_0002_m_000159_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317709268759623743243_0002_m_000159_164 : duration 0:00.003s
[2021-05-14 16:56:49,160] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-14 16:56:49,163] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Getting 4 (5.1 KiB) non-empty blocks including 4 (5.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:49,164] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:56:49,165] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:49,165] {docker.py:276} INFO - 21/05/14 19:56:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:49,166] {docker.py:276} INFO - 21/05/14 19:56:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:49,167] {docker.py:276} INFO - 21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:49,167] {docker.py:276} INFO - 21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318967299077856644005_0002_m_000158_163, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318967299077856644005_0002_m_000158_163}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318967299077856644005_0002}; taskId=attempt_202105141955318967299077856644005_0002_m_000158_163, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17dc28c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:49,168] {docker.py:276} INFO - 21/05/14 19:56:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105141955318967299077856644005_0002_m_000158_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318967299077856644005_0002_m_000158_163
[2021-05-14 16:56:49,168] {docker.py:276} INFO - 21/05/14 19:56:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:49,169] {docker.py:276} INFO - 21/05/14 19:56:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:49,170] {docker.py:276} INFO - 21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:49,171] {docker.py:276} INFO - 21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318885734663582598491_0002_m_000160_165, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318885734663582598491_0002_m_000160_165}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318885734663582598491_0002}; taskId=attempt_202105141955318885734663582598491_0002_m_000160_165, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7dd8c90d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:49,171] {docker.py:276} INFO - 21/05/14 19:56:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105141955318885734663582598491_0002_m_000160_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318885734663582598491_0002_m_000160_165
[2021-05-14 16:56:49,172] {docker.py:276} INFO - 21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531158462367696396624_0002_m_000161_166, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531158462367696396624_0002_m_000161_166}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531158462367696396624_0002}; taskId=attempt_20210514195531158462367696396624_0002_m_000161_166, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55f8d070}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:49,172] {docker.py:276} INFO - 21/05/14 19:56:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:49 INFO StagingCommitter: Starting: Task committer attempt_20210514195531158462367696396624_0002_m_000161_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531158462367696396624_0002_m_000161_166
[2021-05-14 16:56:49,175] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_202105141955318967299077856644005_0002_m_000158_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318967299077856644005_0002_m_000158_163 : duration 0:00.008s
[2021-05-14 16:56:49,176] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_20210514195531158462367696396624_0002_m_000161_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531158462367696396624_0002_m_000161_166 : duration 0:00.005s
[2021-05-14 16:56:49,181] {docker.py:276} INFO - 21/05/14 19:56:49 INFO StagingCommitter: Task committer attempt_202105141955318885734663582598491_0002_m_000160_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318885734663582598491_0002_m_000160_165 : duration 0:00.010s
[2021-05-14 16:56:51,177] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955317709268759623743243_0002_m_000159_164: needsTaskCommit() Task attempt_202105141955317709268759623743243_0002_m_000159_164
[2021-05-14 16:56:51,178] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955317709268759623743243_0002_m_000159_164: needsTaskCommit() Task attempt_202105141955317709268759623743243_0002_m_000159_164: duration 0:00.000s
[2021-05-14 16:56:51,179] {docker.py:276} INFO - 21/05/14 19:56:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317709268759623743243_0002_m_000159_164
[2021-05-14 16:56:51,181] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Finished task 159.0 in stage 2.0 (TID 164). 4544 bytes result sent to driver
[2021-05-14 16:56:51,182] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Starting task 162.0 in stage 2.0 (TID 167) (379a553b2355, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:51,183] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Finished task 159.0 in stage 2.0 (TID 164) in 2048 ms on 379a553b2355 (executor driver) (159/200)
[2021-05-14 16:56:51,184] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Running task 162.0 in stage 2.0 (TID 167)
[2021-05-14 16:56:51,188] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955318967299077856644005_0002_m_000158_163: needsTaskCommit() Task attempt_202105141955318967299077856644005_0002_m_000158_163
[2021-05-14 16:56:51,189] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955318967299077856644005_0002_m_000158_163: needsTaskCommit() Task attempt_202105141955318967299077856644005_0002_m_000158_163: duration 0:00.000s
[2021-05-14 16:56:51,189] {docker.py:276} INFO - 21/05/14 19:56:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318967299077856644005_0002_m_000158_163
[2021-05-14 16:56:51,192] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Finished task 158.0 in stage 2.0 (TID 163). 4544 bytes result sent to driver
[2021-05-14 16:56:51,192] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Starting task 163.0 in stage 2.0 (TID 168) (379a553b2355, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:51,193] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Running task 163.0 in stage 2.0 (TID 168)
[2021-05-14 16:56:51,194] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Finished task 158.0 in stage 2.0 (TID 163) in 2060 ms on 379a553b2355 (executor driver) (160/200)
[2021-05-14 16:56:51,195] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955318885734663582598491_0002_m_000160_165: needsTaskCommit() Task attempt_202105141955318885734663582598491_0002_m_000160_165
[2021-05-14 16:56:51,196] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955318885734663582598491_0002_m_000160_165: needsTaskCommit() Task attempt_202105141955318885734663582598491_0002_m_000160_165: duration 0:00.001s
21/05/14 19:56:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318885734663582598491_0002_m_000160_165
[2021-05-14 16:56:51,197] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Finished task 160.0 in stage 2.0 (TID 165). 4544 bytes result sent to driver
[2021-05-14 16:56:51,198] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_20210514195531158462367696396624_0002_m_000161_166: needsTaskCommit() Task attempt_20210514195531158462367696396624_0002_m_000161_166
21/05/14 19:56:51 INFO TaskSetManager: Starting task 164.0 in stage 2.0 (TID 169) (379a553b2355, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:51,198] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Finished task 160.0 in stage 2.0 (TID 165) in 2061 ms on 379a553b2355 (executor driver) (161/200)
[2021-05-14 16:56:51,199] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Running task 164.0 in stage 2.0 (TID 169)
[2021-05-14 16:56:51,200] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_20210514195531158462367696396624_0002_m_000161_166: needsTaskCommit() Task attempt_20210514195531158462367696396624_0002_m_000161_166: duration 0:00.001s
[2021-05-14 16:56:51,201] {docker.py:276} INFO - 21/05/14 19:56:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531158462367696396624_0002_m_000161_166
[2021-05-14 16:56:51,201] {docker.py:276} INFO - 21/05/14 19:56:51 INFO Executor: Finished task 161.0 in stage 2.0 (TID 166). 4544 bytes result sent to driver
[2021-05-14 16:56:51,202] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Starting task 165.0 in stage 2.0 (TID 170) (379a553b2355, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:51,203] {docker.py:276} INFO - 21/05/14 19:56:51 INFO TaskSetManager: Finished task 161.0 in stage 2.0 (TID 166) in 2064 ms on 379a553b2355 (executor driver) (162/200)
21/05/14 19:56:51 INFO Executor: Running task 165.0 in stage 2.0 (TID 170)
[2021-05-14 16:56:51,206] {docker.py:276} INFO - 21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Getting 4 (4.9 KiB) non-empty blocks including 4 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:51,210] {docker.py:276} INFO - 21/05/14 19:56:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:51,210] {docker.py:276} INFO - 21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317058140742837394580_0002_m_000162_167, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317058140742837394580_0002_m_000162_167}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317058140742837394580_0002}; taskId=attempt_202105141955317058140742837394580_0002_m_000162_167, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@241ddda4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:51,211] {docker.py:276} INFO - 21/05/14 19:56:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:51,211] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955317058140742837394580_0002_m_000162_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317058140742837394580_0002_m_000162_167
[2021-05-14 16:56:51,211] {docker.py:276} INFO - 21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:51,212] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955317058140742837394580_0002_m_000162_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317058140742837394580_0002_m_000162_167 : duration 0:00.003s
[2021-05-14 16:56:51,212] {docker.py:276} INFO - 21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:56:51,215] {docker.py:276} INFO - 21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:51,216] {docker.py:276} INFO - 21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:51,217] {docker.py:276} INFO - 21/05/14 19:56:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:51,218] {docker.py:276} INFO - 21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312819103093809613283_0002_m_000163_168, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312819103093809613283_0002_m_000163_168}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312819103093809613283_0002}; taskId=attempt_202105141955312819103093809613283_0002_m_000163_168, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a4d8d2a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955312819103093809613283_0002_m_000163_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312819103093809613283_0002_m_000163_168
[2021-05-14 16:56:51,218] {docker.py:276} INFO - 21/05/14 19:56:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:51,218] {docker.py:276} INFO - 21/05/14 19:56:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:51,219] {docker.py:276} INFO - 21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:51,220] {docker.py:276} INFO - 21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313685809577566673839_0002_m_000164_169, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313685809577566673839_0002_m_000164_169}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313685809577566673839_0002}; taskId=attempt_202105141955313685809577566673839_0002_m_000164_169, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8f2bb82}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:51,220] {docker.py:276} INFO - 21/05/14 19:56:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:51,221] {docker.py:276} INFO - 21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:56:51,222] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955313685809577566673839_0002_m_000164_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313685809577566673839_0002_m_000164_169
[2021-05-14 16:56:51,222] {docker.py:276} INFO - 21/05/14 19:56:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:51,223] {docker.py:276} INFO - 21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:51,223] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955312819103093809613283_0002_m_000163_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312819103093809613283_0002_m_000163_168 : duration 0:00.007s
[2021-05-14 16:56:51,224] {docker.py:276} INFO - 21/05/14 19:56:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318039411113092598479_0002_m_000165_170, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318039411113092598479_0002_m_000165_170}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318039411113092598479_0002}; taskId=attempt_202105141955318039411113092598479_0002_m_000165_170, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51be7d59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:51,224] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955313685809577566673839_0002_m_000164_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313685809577566673839_0002_m_000164_169 : duration 0:00.005s
[2021-05-14 16:56:51,224] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Starting: Task committer attempt_202105141955318039411113092598479_0002_m_000165_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318039411113092598479_0002_m_000165_170
[2021-05-14 16:56:51,226] {docker.py:276} INFO - 21/05/14 19:56:51 INFO StagingCommitter: Task committer attempt_202105141955318039411113092598479_0002_m_000165_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318039411113092598479_0002_m_000165_170 : duration 0:00.005s
[2021-05-14 16:56:52,392] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955312819103093809613283_0002_m_000163_168: needsTaskCommit() Task attempt_202105141955312819103093809613283_0002_m_000163_168
[2021-05-14 16:56:52,394] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955312819103093809613283_0002_m_000163_168: needsTaskCommit() Task attempt_202105141955312819103093809613283_0002_m_000163_168: duration 0:00.001s
21/05/14 19:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312819103093809613283_0002_m_000163_168
[2021-05-14 16:56:52,395] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Finished task 163.0 in stage 2.0 (TID 168). 4544 bytes result sent to driver
[2021-05-14 16:56:52,396] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Starting task 166.0 in stage 2.0 (TID 171) (379a553b2355, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:52,398] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Finished task 163.0 in stage 2.0 (TID 168) in 1208 ms on 379a553b2355 (executor driver) (163/200)
[2021-05-14 16:56:52,399] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Running task 166.0 in stage 2.0 (TID 171)
[2021-05-14 16:56:52,408] {docker.py:276} INFO - 21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Getting 4 (4.9 KiB) non-empty blocks including 4 (4.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:52,410] {docker.py:276} INFO - 21/05/14 19:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312440007685544322701_0002_m_000166_171, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312440007685544322701_0002_m_000166_171}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312440007685544322701_0002}; taskId=attempt_202105141955312440007685544322701_0002_m_000166_171, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26d7da88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:52,410] {docker.py:276} INFO - 21/05/14 19:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955312440007685544322701_0002_m_000166_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312440007685544322701_0002_m_000166_171
[2021-05-14 16:56:52,413] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955312440007685544322701_0002_m_000166_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312440007685544322701_0002_m_000166_171 : duration 0:00.003s
[2021-05-14 16:56:52,753] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955313685809577566673839_0002_m_000164_169: needsTaskCommit() Task attempt_202105141955313685809577566673839_0002_m_000164_169
[2021-05-14 16:56:52,754] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955313685809577566673839_0002_m_000164_169: needsTaskCommit() Task attempt_202105141955313685809577566673839_0002_m_000164_169: duration 0:00.000s
[2021-05-14 16:56:52,754] {docker.py:276} INFO - 21/05/14 19:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313685809577566673839_0002_m_000164_169
[2021-05-14 16:56:52,756] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Finished task 164.0 in stage 2.0 (TID 169). 4544 bytes result sent to driver
[2021-05-14 16:56:52,756] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955318039411113092598479_0002_m_000165_170: needsTaskCommit() Task attempt_202105141955318039411113092598479_0002_m_000165_170
[2021-05-14 16:56:52,757] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955318039411113092598479_0002_m_000165_170: needsTaskCommit() Task attempt_202105141955318039411113092598479_0002_m_000165_170: duration 0:00.001s
21/05/14 19:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318039411113092598479_0002_m_000165_170
[2021-05-14 16:56:52,758] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Starting task 167.0 in stage 2.0 (TID 172) (379a553b2355, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:52,758] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Finished task 164.0 in stage 2.0 (TID 169) in 1562 ms on 379a553b2355 (executor driver) (164/200)
[2021-05-14 16:56:52,759] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Running task 167.0 in stage 2.0 (TID 172)
[2021-05-14 16:56:52,760] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Finished task 165.0 in stage 2.0 (TID 170). 4544 bytes result sent to driver
[2021-05-14 16:56:52,761] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Starting task 168.0 in stage 2.0 (TID 173) (379a553b2355, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:52,761] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Finished task 165.0 in stage 2.0 (TID 170) in 1561 ms on 379a553b2355 (executor driver) (165/200)
21/05/14 19:56:52 INFO Executor: Running task 168.0 in stage 2.0 (TID 173)
[2021-05-14 16:56:52,778] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955317058140742837394580_0002_m_000162_167: needsTaskCommit() Task attempt_202105141955317058140742837394580_0002_m_000162_167
21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955317058140742837394580_0002_m_000162_167: needsTaskCommit() Task attempt_202105141955317058140742837394580_0002_m_000162_167: duration 0:00.001s
21/05/14 19:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317058140742837394580_0002_m_000162_167
[2021-05-14 16:56:52,779] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Finished task 162.0 in stage 2.0 (TID 167). 4587 bytes result sent to driver
[2021-05-14 16:56:52,780] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Starting task 169.0 in stage 2.0 (TID 174) (379a553b2355, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:52,781] {docker.py:276} INFO - 21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:52,781] {docker.py:276} INFO - 21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:52,782] {docker.py:276} INFO - 21/05/14 19:56:52 INFO Executor: Running task 169.0 in stage 2.0 (TID 174)
[2021-05-14 16:56:52,782] {docker.py:276} INFO - 21/05/14 19:56:52 INFO TaskSetManager: Finished task 162.0 in stage 2.0 (TID 167) in 1602 ms on 379a553b2355 (executor driver) (166/200)
[2021-05-14 16:56:52,782] {docker.py:276} INFO - 21/05/14 19:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:52,783] {docker.py:276} INFO - 21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:52,784] {docker.py:276} INFO - 21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316702932409349234664_0002_m_000167_172, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316702932409349234664_0002_m_000167_172}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316702932409349234664_0002}; taskId=attempt_202105141955316702932409349234664_0002_m_000167_172, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a34f435}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:52,784] {docker.py:276} INFO - 21/05/14 19:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:52,785] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955316702932409349234664_0002_m_000167_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316702932409349234664_0002_m_000167_172
[2021-05-14 16:56:52,785] {docker.py:276} INFO - 21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:52,786] {docker.py:276} INFO - 21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:52,787] {docker.py:276} INFO - 21/05/14 19:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:52,788] {docker.py:276} INFO - 21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:52,788] {docker.py:276} INFO - 21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311080062540375696242_0002_m_000168_173, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311080062540375696242_0002_m_000168_173}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311080062540375696242_0002}; taskId=attempt_202105141955311080062540375696242_0002_m_000168_173, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@174b309}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:52,789] {docker.py:276} INFO - 21/05/14 19:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955311080062540375696242_0002_m_000168_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311080062540375696242_0002_m_000168_173
[2021-05-14 16:56:52,789] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955316702932409349234664_0002_m_000167_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316702932409349234664_0002_m_000167_172 : duration 0:00.004s
[2021-05-14 16:56:52,794] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955311080062540375696242_0002_m_000168_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311080062540375696242_0002_m_000168_173 : duration 0:00.004s
[2021-05-14 16:56:52,794] {docker.py:276} INFO - 21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:52,795] {docker.py:276} INFO - 21/05/14 19:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:52,795] {docker.py:276} INFO - 21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312977308992944646439_0002_m_000169_174, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312977308992944646439_0002_m_000169_174}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312977308992944646439_0002}; taskId=attempt_202105141955312977308992944646439_0002_m_000169_174, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e2351a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:52,796] {docker.py:276} INFO - 21/05/14 19:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105141955312977308992944646439_0002_m_000169_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312977308992944646439_0002_m_000169_174
[2021-05-14 16:56:52,800] {docker.py:276} INFO - 21/05/14 19:56:52 INFO StagingCommitter: Task committer attempt_202105141955312977308992944646439_0002_m_000169_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312977308992944646439_0002_m_000169_174 : duration 0:00.006s
[2021-05-14 16:56:53,920] {docker.py:276} INFO - 21/05/14 19:56:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955312440007685544322701_0002_m_000166_171: needsTaskCommit() Task attempt_202105141955312440007685544322701_0002_m_000166_171
[2021-05-14 16:56:53,921] {docker.py:276} INFO - 21/05/14 19:56:53 INFO StagingCommitter: Task committer attempt_202105141955312440007685544322701_0002_m_000166_171: needsTaskCommit() Task attempt_202105141955312440007685544322701_0002_m_000166_171: duration 0:00.000s
[2021-05-14 16:56:53,921] {docker.py:276} INFO - 21/05/14 19:56:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312440007685544322701_0002_m_000166_171
[2021-05-14 16:56:53,923] {docker.py:276} INFO - 21/05/14 19:56:53 INFO Executor: Finished task 166.0 in stage 2.0 (TID 171). 4587 bytes result sent to driver
[2021-05-14 16:56:53,926] {docker.py:276} INFO - 21/05/14 19:56:53 INFO TaskSetManager: Starting task 170.0 in stage 2.0 (TID 175) (379a553b2355, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:56:53 INFO TaskSetManager: Finished task 166.0 in stage 2.0 (TID 171) in 1531 ms on 379a553b2355 (executor driver) (167/200)
21/05/14 19:56:53 INFO Executor: Running task 170.0 in stage 2.0 (TID 175)
[2021-05-14 16:56:53,933] {docker.py:276} INFO - 21/05/14 19:56:53 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:53,935] {docker.py:276} INFO - 21/05/14 19:56:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:53,935] {docker.py:276} INFO - 21/05/14 19:56:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:53,936] {docker.py:276} INFO - 21/05/14 19:56:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317500166443803378899_0002_m_000170_175, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317500166443803378899_0002_m_000170_175}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317500166443803378899_0002}; taskId=attempt_202105141955317500166443803378899_0002_m_000170_175, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e868ce7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:53,936] {docker.py:276} INFO - 21/05/14 19:56:53 INFO StagingCommitter: Starting: Task committer attempt_202105141955317500166443803378899_0002_m_000170_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317500166443803378899_0002_m_000170_175
[2021-05-14 16:56:53,939] {docker.py:276} INFO - 21/05/14 19:56:53 INFO StagingCommitter: Task committer attempt_202105141955317500166443803378899_0002_m_000170_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317500166443803378899_0002_m_000170_175 : duration 0:00.003s
[2021-05-14 16:56:54,335] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955312977308992944646439_0002_m_000169_174: needsTaskCommit() Task attempt_202105141955312977308992944646439_0002_m_000169_174
21/05/14 19:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955316702932409349234664_0002_m_000167_172: needsTaskCommit() Task attempt_202105141955316702932409349234664_0002_m_000167_172
[2021-05-14 16:56:54,337] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Task committer attempt_202105141955316702932409349234664_0002_m_000167_172: needsTaskCommit() Task attempt_202105141955316702932409349234664_0002_m_000167_172: duration 0:00.001s
21/05/14 19:56:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316702932409349234664_0002_m_000167_172
[2021-05-14 16:56:54,337] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Task committer attempt_202105141955312977308992944646439_0002_m_000169_174: needsTaskCommit() Task attempt_202105141955312977308992944646439_0002_m_000169_174: duration 0:00.002s
[2021-05-14 16:56:54,338] {docker.py:276} INFO - 21/05/14 19:56:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312977308992944646439_0002_m_000169_174
[2021-05-14 16:56:54,339] {docker.py:276} INFO - 21/05/14 19:56:54 INFO Executor: Finished task 167.0 in stage 2.0 (TID 172). 4587 bytes result sent to driver
[2021-05-14 16:56:54,342] {docker.py:276} INFO - 21/05/14 19:56:54 INFO TaskSetManager: Starting task 171.0 in stage 2.0 (TID 176) (379a553b2355, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:54,343] {docker.py:276} INFO - 21/05/14 19:56:54 INFO Executor: Running task 171.0 in stage 2.0 (TID 176)
21/05/14 19:56:54 INFO TaskSetManager: Finished task 167.0 in stage 2.0 (TID 172) in 1587 ms on 379a553b2355 (executor driver) (168/200)
[2021-05-14 16:56:54,344] {docker.py:276} INFO - 21/05/14 19:56:54 INFO Executor: Finished task 169.0 in stage 2.0 (TID 174). 4544 bytes result sent to driver
[2021-05-14 16:56:54,344] {docker.py:276} INFO - 21/05/14 19:56:54 INFO TaskSetManager: Starting task 172.0 in stage 2.0 (TID 177) (379a553b2355, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:54,345] {docker.py:276} INFO - 21/05/14 19:56:54 INFO TaskSetManager: Finished task 169.0 in stage 2.0 (TID 174) in 1567 ms on 379a553b2355 (executor driver) (169/200)
[2021-05-14 16:56:54,346] {docker.py:276} INFO - 21/05/14 19:56:54 INFO Executor: Running task 172.0 in stage 2.0 (TID 177)
[2021-05-14 16:56:54,347] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955311080062540375696242_0002_m_000168_173: needsTaskCommit() Task attempt_202105141955311080062540375696242_0002_m_000168_173
[2021-05-14 16:56:54,348] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Task committer attempt_202105141955311080062540375696242_0002_m_000168_173: needsTaskCommit() Task attempt_202105141955311080062540375696242_0002_m_000168_173: duration 0:00.000s
21/05/14 19:56:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311080062540375696242_0002_m_000168_173
[2021-05-14 16:56:54,350] {docker.py:276} INFO - 21/05/14 19:56:54 INFO Executor: Finished task 168.0 in stage 2.0 (TID 173). 4587 bytes result sent to driver
[2021-05-14 16:56:54,351] {docker.py:276} INFO - 21/05/14 19:56:54 INFO TaskSetManager: Starting task 173.0 in stage 2.0 (TID 178) (379a553b2355, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:54,351] {docker.py:276} INFO - 21/05/14 19:56:54 INFO TaskSetManager: Finished task 168.0 in stage 2.0 (TID 173) in 1593 ms on 379a553b2355 (executor driver) (170/200)
[2021-05-14 16:56:54,352] {docker.py:276} INFO - 21/05/14 19:56:54 INFO Executor: Running task 173.0 in stage 2.0 (TID 178)
[2021-05-14 16:56:54,360] {docker.py:276} INFO - 21/05/14 19:56:54 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:54,361] {docker.py:276} INFO - 21/05/14 19:56:54 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:54,362] {docker.py:276} INFO - 21/05/14 19:56:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:56:54,363] {docker.py:276} INFO - 21/05/14 19:56:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312843241480872654482_0002_m_000173_178, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312843241480872654482_0002_m_000173_178}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312843241480872654482_0002}; taskId=attempt_202105141955312843241480872654482_0002_m_000173_178, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@595c7ccc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:54,364] {docker.py:276} INFO - 21/05/14 19:56:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:54,364] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955312843241480872654482_0002_m_000173_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312843241480872654482_0002_m_000173_178 
21/05/14 19:56:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:54,364] {docker.py:276} INFO - 21/05/14 19:56:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317387073046943234640_0002_m_000171_176, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317387073046943234640_0002_m_000171_176}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317387073046943234640_0002}; taskId=attempt_202105141955317387073046943234640_0002_m_000171_176, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@352438d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:54,365] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955317387073046943234640_0002_m_000171_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317387073046943234640_0002_m_000171_176
[2021-05-14 16:56:54,366] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Task committer attempt_202105141955312843241480872654482_0002_m_000173_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312843241480872654482_0002_m_000173_178 : duration 0:00.003s
[2021-05-14 16:56:54,369] {docker.py:276} INFO - 21/05/14 19:56:54 INFO ShuffleBlockFetcherIterator: Getting 4 (5.4 KiB) non-empty blocks including 4 (5.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:54,371] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Task committer attempt_202105141955317387073046943234640_0002_m_000171_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317387073046943234640_0002_m_000171_176 : duration 0:00.007s
21/05/14 19:56:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:56:54,371] {docker.py:276} INFO - 21/05/14 19:56:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:54,372] {docker.py:276} INFO - 21/05/14 19:56:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313373886823119679758_0002_m_000172_177, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313373886823119679758_0002_m_000172_177}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313373886823119679758_0002}; taskId=attempt_202105141955313373886823119679758_0002_m_000172_177, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47fae06d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:54,372] {docker.py:276} INFO - 21/05/14 19:56:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:54,373] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105141955313373886823119679758_0002_m_000172_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313373886823119679758_0002_m_000172_177
[2021-05-14 16:56:54,377] {docker.py:276} INFO - 21/05/14 19:56:54 INFO StagingCommitter: Task committer attempt_202105141955313373886823119679758_0002_m_000172_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313373886823119679758_0002_m_000172_177 : duration 0:00.005s
[2021-05-14 16:56:55,456] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955317500166443803378899_0002_m_000170_175: needsTaskCommit() Task attempt_202105141955317500166443803378899_0002_m_000170_175
[2021-05-14 16:56:55,458] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955317500166443803378899_0002_m_000170_175: needsTaskCommit() Task attempt_202105141955317500166443803378899_0002_m_000170_175: duration 0:00.001s
21/05/14 19:56:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317500166443803378899_0002_m_000170_175
[2021-05-14 16:56:55,463] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Finished task 170.0 in stage 2.0 (TID 175). 4544 bytes result sent to driver
[2021-05-14 16:56:55,464] {docker.py:276} INFO - 21/05/14 19:56:55 INFO TaskSetManager: Starting task 174.0 in stage 2.0 (TID 179) (379a553b2355, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:55,465] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Running task 174.0 in stage 2.0 (TID 179)
21/05/14 19:56:55 INFO TaskSetManager: Finished task 170.0 in stage 2.0 (TID 175) in 1543 ms on 379a553b2355 (executor driver) (171/200)
[2021-05-14 16:56:55,475] {docker.py:276} INFO - 21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Getting 4 (5.7 KiB) non-empty blocks including 4 (5.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:55,477] {docker.py:276} INFO - 21/05/14 19:56:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:55,478] {docker.py:276} INFO - 21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955317921884503560921780_0002_m_000174_179, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317921884503560921780_0002_m_000174_179}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955317921884503560921780_0002}; taskId=attempt_202105141955317921884503560921780_0002_m_000174_179, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7272bfeb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:55,479] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955317921884503560921780_0002_m_000174_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317921884503560921780_0002_m_000174_179
[2021-05-14 16:56:55,481] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955317921884503560921780_0002_m_000174_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955317921884503560921780_0002_m_000174_179 : duration 0:00.004s
[2021-05-14 16:56:55,912] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955312843241480872654482_0002_m_000173_178: needsTaskCommit() Task attempt_202105141955312843241480872654482_0002_m_000173_178
[2021-05-14 16:56:55,913] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955312843241480872654482_0002_m_000173_178: needsTaskCommit() Task attempt_202105141955312843241480872654482_0002_m_000173_178: duration 0:00.001s
21/05/14 19:56:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312843241480872654482_0002_m_000173_178
[2021-05-14 16:56:55,914] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Finished task 173.0 in stage 2.0 (TID 178). 4544 bytes result sent to driver
[2021-05-14 16:56:55,916] {docker.py:276} INFO - 21/05/14 19:56:55 INFO TaskSetManager: Starting task 175.0 in stage 2.0 (TID 180) (379a553b2355, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:55,917] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955313373886823119679758_0002_m_000172_177: needsTaskCommit() Task attempt_202105141955313373886823119679758_0002_m_000172_177
[2021-05-14 16:56:55,919] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955313373886823119679758_0002_m_000172_177: needsTaskCommit() Task attempt_202105141955313373886823119679758_0002_m_000172_177: duration 0:00.002s
21/05/14 19:56:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313373886823119679758_0002_m_000172_177
21/05/14 19:56:55 INFO TaskSetManager: Finished task 173.0 in stage 2.0 (TID 178) in 1569 ms on 379a553b2355 (executor driver) (172/200)
[2021-05-14 16:56:55,919] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Running task 175.0 in stage 2.0 (TID 180)
[2021-05-14 16:56:55,921] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Finished task 172.0 in stage 2.0 (TID 177). 4544 bytes result sent to driver
[2021-05-14 16:56:55,921] {docker.py:276} INFO - 21/05/14 19:56:55 INFO TaskSetManager: Starting task 176.0 in stage 2.0 (TID 181) (379a553b2355, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:55,922] {docker.py:276} INFO - 21/05/14 19:56:55 INFO TaskSetManager: Finished task 172.0 in stage 2.0 (TID 177) in 1581 ms on 379a553b2355 (executor driver) (173/200)
[2021-05-14 16:56:55,923] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Running task 176.0 in stage 2.0 (TID 181)
[2021-05-14 16:56:55,930] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955317387073046943234640_0002_m_000171_176: needsTaskCommit() Task attempt_202105141955317387073046943234640_0002_m_000171_176
[2021-05-14 16:56:55,930] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955317387073046943234640_0002_m_000171_176: needsTaskCommit() Task attempt_202105141955317387073046943234640_0002_m_000171_176: duration 0:00.000s
21/05/14 19:56:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317387073046943234640_0002_m_000171_176
[2021-05-14 16:56:55,930] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Finished task 171.0 in stage 2.0 (TID 176). 4544 bytes result sent to driver
[2021-05-14 16:56:55,931] {docker.py:276} INFO - 21/05/14 19:56:55 INFO TaskSetManager: Starting task 177.0 in stage 2.0 (TID 182) (379a553b2355, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:55,933] {docker.py:276} INFO - 21/05/14 19:56:55 INFO TaskSetManager: Finished task 171.0 in stage 2.0 (TID 176) in 1595 ms on 379a553b2355 (executor driver) (174/200)
[2021-05-14 16:56:55,933] {docker.py:276} INFO - 21/05/14 19:56:55 INFO Executor: Running task 177.0 in stage 2.0 (TID 182)
[2021-05-14 16:56:55,934] {docker.py:276} INFO - 21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:55,935] {docker.py:276} INFO - 21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:55,936] {docker.py:276} INFO - 21/05/14 19:56:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318181118991295220422_0002_m_000175_180, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318181118991295220422_0002_m_000175_180}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318181118991295220422_0002}; taskId=attempt_202105141955318181118991295220422_0002_m_000175_180, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a00f44d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:55,936] {docker.py:276} INFO - 21/05/14 19:56:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955318181118991295220422_0002_m_000175_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318181118991295220422_0002_m_000175_180
[2021-05-14 16:56:55,937] {docker.py:276} INFO - 21/05/14 19:56:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:55,937] {docker.py:276} INFO - 21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:55,938] {docker.py:276} INFO - 21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311329245156604377603_0002_m_000176_181, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311329245156604377603_0002_m_000176_181}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311329245156604377603_0002}; taskId=attempt_202105141955311329245156604377603_0002_m_000176_181, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5819527d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:55,938] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955311329245156604377603_0002_m_000176_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311329245156604377603_0002_m_000176_181
[2021-05-14 16:56:55,940] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955318181118991295220422_0002_m_000175_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318181118991295220422_0002_m_000175_180 : duration 0:00.005s
[2021-05-14 16:56:55,942] {docker.py:276} INFO - 21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:55,944] {docker.py:276} INFO - 21/05/14 19:56:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:55,944] {docker.py:276} INFO - 21/05/14 19:56:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313785163789495744731_0002_m_000177_182, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313785163789495744731_0002_m_000177_182}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313785163789495744731_0002}; taskId=attempt_202105141955313785163789495744731_0002_m_000177_182, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64c6f0e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105141955313785163789495744731_0002_m_000177_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313785163789495744731_0002_m_000177_182
[2021-05-14 16:56:55,947] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955311329245156604377603_0002_m_000176_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311329245156604377603_0002_m_000176_181 : duration 0:00.009s
[2021-05-14 16:56:55,949] {docker.py:276} INFO - 21/05/14 19:56:55 INFO StagingCommitter: Task committer attempt_202105141955313785163789495744731_0002_m_000177_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313785163789495744731_0002_m_000177_182 : duration 0:00.005s
[2021-05-14 16:56:57,037] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955318181118991295220422_0002_m_000175_180: needsTaskCommit() Task attempt_202105141955318181118991295220422_0002_m_000175_180
[2021-05-14 16:56:57,038] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955318181118991295220422_0002_m_000175_180: needsTaskCommit() Task attempt_202105141955318181118991295220422_0002_m_000175_180: duration 0:00.001s
[2021-05-14 16:56:57,039] {docker.py:276} INFO - 21/05/14 19:56:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318181118991295220422_0002_m_000175_180
[2021-05-14 16:56:57,040] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Finished task 175.0 in stage 2.0 (TID 180). 4544 bytes result sent to driver
[2021-05-14 16:56:57,041] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Starting task 178.0 in stage 2.0 (TID 183) (379a553b2355, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:57,042] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Finished task 175.0 in stage 2.0 (TID 180) in 1128 ms on 379a553b2355 (executor driver) (175/200)
[2021-05-14 16:56:57,043] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Running task 178.0 in stage 2.0 (TID 183)
[2021-05-14 16:56:57,052] {docker.py:276} INFO - 21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:56:57,052] {docker.py:276} INFO - 21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:57,054] {docker.py:276} INFO - 21/05/14 19:56:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:57,054] {docker.py:276} INFO - 21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314945574151588150303_0002_m_000178_183, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314945574151588150303_0002_m_000178_183}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314945574151588150303_0002}; taskId=attempt_202105141955314945574151588150303_0002_m_000178_183, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11e6d709}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955314945574151588150303_0002_m_000178_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314945574151588150303_0002_m_000178_183
[2021-05-14 16:56:57,055] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955313785163789495744731_0002_m_000177_182: needsTaskCommit() Task attempt_202105141955313785163789495744731_0002_m_000177_182
[2021-05-14 16:56:57,055] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955313785163789495744731_0002_m_000177_182: needsTaskCommit() Task attempt_202105141955313785163789495744731_0002_m_000177_182: duration 0:00.000s
21/05/14 19:56:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313785163789495744731_0002_m_000177_182
[2021-05-14 16:56:57,057] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Finished task 177.0 in stage 2.0 (TID 182). 4544 bytes result sent to driver
[2021-05-14 16:56:57,058] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955314945574151588150303_0002_m_000178_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314945574151588150303_0002_m_000178_183 : duration 0:00.004s
[2021-05-14 16:56:57,059] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Starting task 179.0 in stage 2.0 (TID 184) (379a553b2355, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:57,059] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Running task 179.0 in stage 2.0 (TID 184)
[2021-05-14 16:56:57,060] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Finished task 177.0 in stage 2.0 (TID 182) in 1130 ms on 379a553b2355 (executor driver) (176/200)
[2021-05-14 16:56:57,069] {docker.py:276} INFO - 21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:57,071] {docker.py:276} INFO - 21/05/14 19:56:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955315982201777888793702_0002_m_000179_184, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315982201777888793702_0002_m_000179_184}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955315982201777888793702_0002}; taskId=attempt_202105141955315982201777888793702_0002_m_000179_184, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ac14f8b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:57,072] {docker.py:276} INFO - 21/05/14 19:56:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:57,072] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955315982201777888793702_0002_m_000179_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315982201777888793702_0002_m_000179_184
[2021-05-14 16:56:57,074] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955315982201777888793702_0002_m_000179_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955315982201777888793702_0002_m_000179_184 : duration 0:00.003s
[2021-05-14 16:56:57,163] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955317921884503560921780_0002_m_000174_179: needsTaskCommit() Task attempt_202105141955317921884503560921780_0002_m_000174_179
[2021-05-14 16:56:57,164] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955317921884503560921780_0002_m_000174_179: needsTaskCommit() Task attempt_202105141955317921884503560921780_0002_m_000174_179: duration 0:00.001s
21/05/14 19:56:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955317921884503560921780_0002_m_000174_179
[2021-05-14 16:56:57,165] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Finished task 174.0 in stage 2.0 (TID 179). 4544 bytes result sent to driver
[2021-05-14 16:56:57,167] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Starting task 180.0 in stage 2.0 (TID 185) (379a553b2355, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:57,168] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Running task 180.0 in stage 2.0 (TID 185)
[2021-05-14 16:56:57,169] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Finished task 174.0 in stage 2.0 (TID 179) in 1708 ms on 379a553b2355 (executor driver) (177/200)
[2021-05-14 16:56:57,178] {docker.py:276} INFO - 21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:57,180] {docker.py:276} INFO - 21/05/14 19:56:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:57,181] {docker.py:276} INFO - 21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313534086907246944778_0002_m_000180_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313534086907246944778_0002_m_000180_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313534086907246944778_0002}; taskId=attempt_202105141955313534086907246944778_0002_m_000180_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d7af76a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955313534086907246944778_0002_m_000180_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313534086907246944778_0002_m_000180_185
[2021-05-14 16:56:57,183] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955313534086907246944778_0002_m_000180_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313534086907246944778_0002_m_000180_185 : duration 0:00.003s
[2021-05-14 16:56:57,520] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105141955311329245156604377603_0002_m_000176_181: needsTaskCommit() Task attempt_202105141955311329245156604377603_0002_m_000176_181
[2021-05-14 16:56:57,522] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_202105141955311329245156604377603_0002_m_000176_181: needsTaskCommit() Task attempt_202105141955311329245156604377603_0002_m_000176_181: duration 0:00.002s
21/05/14 19:56:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311329245156604377603_0002_m_000176_181
[2021-05-14 16:56:57,524] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Finished task 176.0 in stage 2.0 (TID 181). 4544 bytes result sent to driver
[2021-05-14 16:56:57,525] {docker.py:276} INFO - 21/05/14 19:56:57 INFO TaskSetManager: Starting task 181.0 in stage 2.0 (TID 186) (379a553b2355, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:57,527] {docker.py:276} INFO - 21/05/14 19:56:57 INFO Executor: Running task 181.0 in stage 2.0 (TID 186)
21/05/14 19:56:57 INFO TaskSetManager: Finished task 176.0 in stage 2.0 (TID 181) in 1606 ms on 379a553b2355 (executor driver) (178/200)
[2021-05-14 16:56:57,538] {docker.py:276} INFO - 21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:57,539] {docker.py:276} INFO - 21/05/14 19:56:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051419553115553089595926228_0002_m_000181_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553115553089595926228_0002_m_000181_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051419553115553089595926228_0002}; taskId=attempt_2021051419553115553089595926228_0002_m_000181_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74d87403}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:57,540] {docker.py:276} INFO - 21/05/14 19:56:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:57 INFO StagingCommitter: Starting: Task committer attempt_2021051419553115553089595926228_0002_m_000181_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553115553089595926228_0002_m_000181_186
[2021-05-14 16:56:57,542] {docker.py:276} INFO - 21/05/14 19:56:57 INFO StagingCommitter: Task committer attempt_2021051419553115553089595926228_0002_m_000181_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_2021051419553115553089595926228_0002_m_000181_186 : duration 0:00.003s
[2021-05-14 16:56:58,620] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955314945574151588150303_0002_m_000178_183: needsTaskCommit() Task attempt_202105141955314945574151588150303_0002_m_000178_183
[2021-05-14 16:56:58,620] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Task committer attempt_202105141955314945574151588150303_0002_m_000178_183: needsTaskCommit() Task attempt_202105141955314945574151588150303_0002_m_000178_183: duration 0:00.001s
21/05/14 19:56:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314945574151588150303_0002_m_000178_183
[2021-05-14 16:56:58,621] {docker.py:276} INFO - 21/05/14 19:56:58 INFO Executor: Finished task 178.0 in stage 2.0 (TID 183). 4587 bytes result sent to driver
[2021-05-14 16:56:58,623] {docker.py:276} INFO - 21/05/14 19:56:58 INFO TaskSetManager: Starting task 182.0 in stage 2.0 (TID 187) (379a553b2355, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:58,624] {docker.py:276} INFO - 21/05/14 19:56:58 INFO Executor: Running task 182.0 in stage 2.0 (TID 187)
21/05/14 19:56:58 INFO TaskSetManager: Finished task 178.0 in stage 2.0 (TID 183) in 1585 ms on 379a553b2355 (executor driver) (179/200)
[2021-05-14 16:56:58,636] {docker.py:276} INFO - 21/05/14 19:56:58 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:58,638] {docker.py:276} INFO - 21/05/14 19:56:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:58,639] {docker.py:276} INFO - 21/05/14 19:56:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318133819881588584570_0002_m_000182_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318133819881588584570_0002_m_000182_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318133819881588584570_0002}; taskId=attempt_202105141955318133819881588584570_0002_m_000182_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4045bf8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955318133819881588584570_0002_m_000182_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318133819881588584570_0002_m_000182_187
[2021-05-14 16:56:58,643] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Task committer attempt_202105141955318133819881588584570_0002_m_000182_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318133819881588584570_0002_m_000182_187 : duration 0:00.004s
[2021-05-14 16:56:58,664] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955315982201777888793702_0002_m_000179_184: needsTaskCommit() Task attempt_202105141955315982201777888793702_0002_m_000179_184
[2021-05-14 16:56:58,665] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Task committer attempt_202105141955315982201777888793702_0002_m_000179_184: needsTaskCommit() Task attempt_202105141955315982201777888793702_0002_m_000179_184: duration 0:00.000s
21/05/14 19:56:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955315982201777888793702_0002_m_000179_184
[2021-05-14 16:56:58,666] {docker.py:276} INFO - 21/05/14 19:56:58 INFO Executor: Finished task 179.0 in stage 2.0 (TID 184). 4587 bytes result sent to driver
[2021-05-14 16:56:58,667] {docker.py:276} INFO - 21/05/14 19:56:58 INFO TaskSetManager: Starting task 183.0 in stage 2.0 (TID 188) (379a553b2355, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:58,668] {docker.py:276} INFO - 21/05/14 19:56:58 INFO TaskSetManager: Finished task 179.0 in stage 2.0 (TID 184) in 1612 ms on 379a553b2355 (executor driver) (180/200)
[2021-05-14 16:56:58,668] {docker.py:276} INFO - 21/05/14 19:56:58 INFO Executor: Running task 183.0 in stage 2.0 (TID 188)
[2021-05-14 16:56:58,675] {docker.py:276} INFO - 21/05/14 19:56:58 INFO ShuffleBlockFetcherIterator: Getting 4 (6.2 KiB) non-empty blocks including 4 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:58,677] {docker.py:276} INFO - 21/05/14 19:56:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531413719770935485290_0002_m_000183_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531413719770935485290_0002_m_000183_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531413719770935485290_0002}; taskId=attempt_20210514195531413719770935485290_0002_m_000183_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65a43fd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:58,677] {docker.py:276} INFO - 21/05/14 19:56:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:56:58 INFO StagingCommitter: Starting: Task committer attempt_20210514195531413719770935485290_0002_m_000183_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531413719770935485290_0002_m_000183_188
[2021-05-14 16:56:58,680] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Task committer attempt_20210514195531413719770935485290_0002_m_000183_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531413719770935485290_0002_m_000183_188 : duration 0:00.003s
[2021-05-14 16:56:58,732] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955313534086907246944778_0002_m_000180_185: needsTaskCommit() Task attempt_202105141955313534086907246944778_0002_m_000180_185
[2021-05-14 16:56:58,733] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Task committer attempt_202105141955313534086907246944778_0002_m_000180_185: needsTaskCommit() Task attempt_202105141955313534086907246944778_0002_m_000180_185: duration 0:00.000s
21/05/14 19:56:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313534086907246944778_0002_m_000180_185
[2021-05-14 16:56:58,734] {docker.py:276} INFO - 21/05/14 19:56:58 INFO Executor: Finished task 180.0 in stage 2.0 (TID 185). 4587 bytes result sent to driver
[2021-05-14 16:56:58,734] {docker.py:276} INFO - 21/05/14 19:56:58 INFO TaskSetManager: Starting task 184.0 in stage 2.0 (TID 189) (379a553b2355, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:58,735] {docker.py:276} INFO - 21/05/14 19:56:58 INFO Executor: Running task 184.0 in stage 2.0 (TID 189)
21/05/14 19:56:58 INFO TaskSetManager: Finished task 180.0 in stage 2.0 (TID 185) in 1571 ms on 379a553b2355 (executor driver) (181/200)
[2021-05-14 16:56:58,742] {docker.py:276} INFO - 21/05/14 19:56:58 INFO ShuffleBlockFetcherIterator: Getting 4 (6.7 KiB) non-empty blocks including 4 (6.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:58,744] {docker.py:276} INFO - 21/05/14 19:56:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:56:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:56:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955312209553619084209871_0002_m_000184_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312209553619084209871_0002_m_000184_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955312209553619084209871_0002}; taskId=attempt_202105141955312209553619084209871_0002_m_000184_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@719a5b79}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:56:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:58,744] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105141955312209553619084209871_0002_m_000184_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312209553619084209871_0002_m_000184_189
[2021-05-14 16:56:58,747] {docker.py:276} INFO - 21/05/14 19:56:58 INFO StagingCommitter: Task committer attempt_202105141955312209553619084209871_0002_m_000184_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955312209553619084209871_0002_m_000184_189 : duration 0:00.003s
[2021-05-14 16:56:59,132] {docker.py:276} INFO - 21/05/14 19:56:59 INFO StagingCommitter: Starting: Task committer attempt_2021051419553115553089595926228_0002_m_000181_186: needsTaskCommit() Task attempt_2021051419553115553089595926228_0002_m_000181_186
[2021-05-14 16:56:59,133] {docker.py:276} INFO - 21/05/14 19:56:59 INFO StagingCommitter: Task committer attempt_2021051419553115553089595926228_0002_m_000181_186: needsTaskCommit() Task attempt_2021051419553115553089595926228_0002_m_000181_186: duration 0:00.002s
[2021-05-14 16:56:59,134] {docker.py:276} INFO - 21/05/14 19:56:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051419553115553089595926228_0002_m_000181_186
[2021-05-14 16:56:59,135] {docker.py:276} INFO - 21/05/14 19:56:59 INFO Executor: Finished task 181.0 in stage 2.0 (TID 186). 4587 bytes result sent to driver
[2021-05-14 16:56:59,136] {docker.py:276} INFO - 21/05/14 19:56:59 INFO TaskSetManager: Starting task 185.0 in stage 2.0 (TID 190) (379a553b2355, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:56:59,137] {docker.py:276} INFO - 21/05/14 19:56:59 INFO TaskSetManager: Finished task 181.0 in stage 2.0 (TID 186) in 1615 ms on 379a553b2355 (executor driver) (182/200)
[2021-05-14 16:56:59,138] {docker.py:276} INFO - 21/05/14 19:56:59 INFO Executor: Running task 185.0 in stage 2.0 (TID 190)
[2021-05-14 16:56:59,147] {docker.py:276} INFO - 21/05/14 19:56:59 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:56:59,149] {docker.py:276} INFO - 21/05/14 19:56:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:56:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:56:59,150] {docker.py:276} INFO - 21/05/14 19:56:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:59,150] {docker.py:276} INFO - 21/05/14 19:56:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318727549206806007993_0002_m_000185_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318727549206806007993_0002_m_000185_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318727549206806007993_0002}; taskId=attempt_202105141955318727549206806007993_0002_m_000185_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f9dda1e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:56:59,150] {docker.py:276} INFO - 21/05/14 19:56:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:56:59,151] {docker.py:276} INFO - 21/05/14 19:56:59 INFO StagingCommitter: Starting: Task committer attempt_202105141955318727549206806007993_0002_m_000185_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318727549206806007993_0002_m_000185_190
[2021-05-14 16:56:59,153] {docker.py:276} INFO - 21/05/14 19:56:59 INFO StagingCommitter: Task committer attempt_202105141955318727549206806007993_0002_m_000185_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318727549206806007993_0002_m_000185_190 : duration 0:00.003s
[2021-05-14 16:57:00,222] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955318133819881588584570_0002_m_000182_187: needsTaskCommit() Task attempt_202105141955318133819881588584570_0002_m_000182_187
[2021-05-14 16:57:00,223] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_202105141955318133819881588584570_0002_m_000182_187: needsTaskCommit() Task attempt_202105141955318133819881588584570_0002_m_000182_187: duration 0:00.001s
21/05/14 19:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318133819881588584570_0002_m_000182_187
[2021-05-14 16:57:00,223] {docker.py:276} INFO - 21/05/14 19:57:00 INFO Executor: Finished task 182.0 in stage 2.0 (TID 187). 4544 bytes result sent to driver
[2021-05-14 16:57:00,225] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Starting task 186.0 in stage 2.0 (TID 191) (379a553b2355, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:00,226] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Finished task 182.0 in stage 2.0 (TID 187) in 1605 ms on 379a553b2355 (executor driver) (183/200)
21/05/14 19:57:00 INFO Executor: Running task 186.0 in stage 2.0 (TID 191)
[2021-05-14 16:57:00,236] {docker.py:276} INFO - 21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:00,238] {docker.py:276} INFO - 21/05/14 19:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531654286313583804872_0002_m_000186_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531654286313583804872_0002_m_000186_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531654286313583804872_0002}; taskId=attempt_20210514195531654286313583804872_0002_m_000186_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20ca8a4d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_20210514195531654286313583804872_0002_m_000186_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531654286313583804872_0002_m_000186_191
[2021-05-14 16:57:00,241] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_20210514195531654286313583804872_0002_m_000186_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531654286313583804872_0002_m_000186_191 : duration 0:00.003s
[2021-05-14 16:57:00,318] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_20210514195531413719770935485290_0002_m_000183_188: needsTaskCommit() Task attempt_20210514195531413719770935485290_0002_m_000183_188
[2021-05-14 16:57:00,319] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_20210514195531413719770935485290_0002_m_000183_188: needsTaskCommit() Task attempt_20210514195531413719770935485290_0002_m_000183_188: duration 0:00.000s
21/05/14 19:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531413719770935485290_0002_m_000183_188
[2021-05-14 16:57:00,320] {docker.py:276} INFO - 21/05/14 19:57:00 INFO Executor: Finished task 183.0 in stage 2.0 (TID 188). 4544 bytes result sent to driver
[2021-05-14 16:57:00,322] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Starting task 187.0 in stage 2.0 (TID 192) (379a553b2355, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:00,323] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Finished task 183.0 in stage 2.0 (TID 188) in 1657 ms on 379a553b2355 (executor driver) (184/200)
[2021-05-14 16:57:00,324] {docker.py:276} INFO - 21/05/14 19:57:00 INFO Executor: Running task 187.0 in stage 2.0 (TID 192)
[2021-05-14 16:57:00,334] {docker.py:276} INFO - 21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:00,335] {docker.py:276} INFO - 21/05/14 19:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311431855102435897870_0002_m_000187_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311431855102435897870_0002_m_000187_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311431855102435897870_0002}; taskId=attempt_202105141955311431855102435897870_0002_m_000187_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13847041}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:57:00,336] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955311431855102435897870_0002_m_000187_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311431855102435897870_0002_m_000187_192
[2021-05-14 16:57:00,339] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_202105141955311431855102435897870_0002_m_000187_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311431855102435897870_0002_m_000187_192 : duration 0:00.003s
[2021-05-14 16:57:00,340] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955312209553619084209871_0002_m_000184_189: needsTaskCommit() Task attempt_202105141955312209553619084209871_0002_m_000184_189
[2021-05-14 16:57:00,340] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_202105141955312209553619084209871_0002_m_000184_189: needsTaskCommit() Task attempt_202105141955312209553619084209871_0002_m_000184_189: duration 0:00.001s
21/05/14 19:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955312209553619084209871_0002_m_000184_189
[2021-05-14 16:57:00,341] {docker.py:276} INFO - 21/05/14 19:57:00 INFO Executor: Finished task 184.0 in stage 2.0 (TID 189). 4544 bytes result sent to driver
[2021-05-14 16:57:00,342] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Starting task 188.0 in stage 2.0 (TID 193) (379a553b2355, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:00,343] {docker.py:276} INFO - 21/05/14 19:57:00 INFO Executor: Running task 188.0 in stage 2.0 (TID 193)
[2021-05-14 16:57:00,344] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Finished task 184.0 in stage 2.0 (TID 189) in 1611 ms on 379a553b2355 (executor driver) (185/200)
[2021-05-14 16:57:00,351] {docker.py:276} INFO - 21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:00,353] {docker.py:276} INFO - 21/05/14 19:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:00,353] {docker.py:276} INFO - 21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531170327263820037383_0002_m_000188_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531170327263820037383_0002_m_000188_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531170327263820037383_0002}; taskId=attempt_20210514195531170327263820037383_0002_m_000188_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38eb3f4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_20210514195531170327263820037383_0002_m_000188_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531170327263820037383_0002_m_000188_193
[2021-05-14 16:57:00,356] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_20210514195531170327263820037383_0002_m_000188_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531170327263820037383_0002_m_000188_193 : duration 0:00.003s
[2021-05-14 16:57:00,824] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955318727549206806007993_0002_m_000185_190: needsTaskCommit() Task attempt_202105141955318727549206806007993_0002_m_000185_190
[2021-05-14 16:57:00,824] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_202105141955318727549206806007993_0002_m_000185_190: needsTaskCommit() Task attempt_202105141955318727549206806007993_0002_m_000185_190: duration 0:00.000s
[2021-05-14 16:57:00,825] {docker.py:276} INFO - 21/05/14 19:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318727549206806007993_0002_m_000185_190
[2021-05-14 16:57:00,826] {docker.py:276} INFO - 21/05/14 19:57:00 INFO Executor: Finished task 185.0 in stage 2.0 (TID 190). 4544 bytes result sent to driver
[2021-05-14 16:57:00,828] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Starting task 189.0 in stage 2.0 (TID 194) (379a553b2355, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:00,829] {docker.py:276} INFO - 21/05/14 19:57:00 INFO TaskSetManager: Finished task 185.0 in stage 2.0 (TID 190) in 1695 ms on 379a553b2355 (executor driver) (186/200)
21/05/14 19:57:00 INFO Executor: Running task 189.0 in stage 2.0 (TID 194)
[2021-05-14 16:57:00,840] {docker.py:276} INFO - 21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Getting 4 (5.3 KiB) non-empty blocks including 4 (5.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:00,842] {docker.py:276} INFO - 21/05/14 19:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955313135229846814011603_0002_m_000189_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313135229846814011603_0002_m_000189_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955313135229846814011603_0002}; taskId=attempt_202105141955313135229846814011603_0002_m_000189_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19e6ba80}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:57:00,843] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105141955313135229846814011603_0002_m_000189_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313135229846814011603_0002_m_000189_194
[2021-05-14 16:57:00,846] {docker.py:276} INFO - 21/05/14 19:57:00 INFO StagingCommitter: Task committer attempt_202105141955313135229846814011603_0002_m_000189_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955313135229846814011603_0002_m_000189_194 : duration 0:00.004s
[2021-05-14 16:57:01,908] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Starting: Task committer attempt_20210514195531654286313583804872_0002_m_000186_191: needsTaskCommit() Task attempt_20210514195531654286313583804872_0002_m_000186_191
[2021-05-14 16:57:01,909] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Task committer attempt_20210514195531654286313583804872_0002_m_000186_191: needsTaskCommit() Task attempt_20210514195531654286313583804872_0002_m_000186_191: duration 0:00.001s
21/05/14 19:57:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531654286313583804872_0002_m_000186_191
[2021-05-14 16:57:01,909] {docker.py:276} INFO - 21/05/14 19:57:01 INFO Executor: Finished task 186.0 in stage 2.0 (TID 191). 4544 bytes result sent to driver
[2021-05-14 16:57:01,911] {docker.py:276} INFO - 21/05/14 19:57:01 INFO TaskSetManager: Starting task 190.0 in stage 2.0 (TID 195) (379a553b2355, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:01,912] {docker.py:276} INFO - 21/05/14 19:57:01 INFO TaskSetManager: Finished task 186.0 in stage 2.0 (TID 191) in 1688 ms on 379a553b2355 (executor driver) (187/200)
[2021-05-14 16:57:01,912] {docker.py:276} INFO - 21/05/14 19:57:01 INFO Executor: Running task 190.0 in stage 2.0 (TID 195)
[2021-05-14 16:57:01,918] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Starting: Task committer attempt_20210514195531170327263820037383_0002_m_000188_193: needsTaskCommit() Task attempt_20210514195531170327263820037383_0002_m_000188_193
[2021-05-14 16:57:01,919] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Task committer attempt_20210514195531170327263820037383_0002_m_000188_193: needsTaskCommit() Task attempt_20210514195531170327263820037383_0002_m_000188_193: duration 0:00.001s
[2021-05-14 16:57:01,920] {docker.py:276} INFO - 21/05/14 19:57:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531170327263820037383_0002_m_000188_193
[2021-05-14 16:57:01,921] {docker.py:276} INFO - 21/05/14 19:57:01 INFO Executor: Finished task 188.0 in stage 2.0 (TID 193). 4544 bytes result sent to driver
[2021-05-14 16:57:01,922] {docker.py:276} INFO - 21/05/14 19:57:01 INFO ShuffleBlockFetcherIterator: Getting 4 (6.3 KiB) non-empty blocks including 4 (6.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:57:01,923] {docker.py:276} INFO - 21/05/14 19:57:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 16:57:01,923] {docker.py:276} INFO - 21/05/14 19:57:01 INFO TaskSetManager: Starting task 191.0 in stage 2.0 (TID 196) (379a553b2355, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:01,925] {docker.py:276} INFO - 21/05/14 19:57:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:57:01,925] {docker.py:276} INFO - 21/05/14 19:57:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314164693712578282776_0002_m_000190_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314164693712578282776_0002_m_000190_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314164693712578282776_0002}; taskId=attempt_202105141955314164693712578282776_0002_m_000190_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a9bba0c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955314164693712578282776_0002_m_000190_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314164693712578282776_0002_m_000190_195
[2021-05-14 16:57:01,926] {docker.py:276} INFO - 21/05/14 19:57:01 INFO Executor: Running task 191.0 in stage 2.0 (TID 196)
[2021-05-14 16:57:01,926] {docker.py:276} INFO - 21/05/14 19:57:01 INFO TaskSetManager: Finished task 188.0 in stage 2.0 (TID 193) in 1584 ms on 379a553b2355 (executor driver) (188/200)
[2021-05-14 16:57:01,928] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Task committer attempt_202105141955314164693712578282776_0002_m_000190_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314164693712578282776_0002_m_000190_195 : duration 0:00.003s
[2021-05-14 16:57:01,935] {docker.py:276} INFO - 21/05/14 19:57:01 INFO ShuffleBlockFetcherIterator: Getting 4 (5.5 KiB) non-empty blocks including 4 (5.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:57:01,935] {docker.py:276} INFO - 21/05/14 19:57:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:01,937] {docker.py:276} INFO - 21/05/14 19:57:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:57:01,938] {docker.py:276} INFO - 21/05/14 19:57:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:57:01,939] {docker.py:276} INFO - 21/05/14 19:57:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314321959717261981749_0002_m_000191_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314321959717261981749_0002_m_000191_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314321959717261981749_0002}; taskId=attempt_202105141955314321959717261981749_0002_m_000191_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f1e78ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 16:57:01,939] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Starting: Task committer attempt_202105141955314321959717261981749_0002_m_000191_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314321959717261981749_0002_m_000191_196
[2021-05-14 16:57:01,942] {docker.py:276} INFO - 21/05/14 19:57:01 INFO StagingCommitter: Task committer attempt_202105141955314321959717261981749_0002_m_000191_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314321959717261981749_0002_m_000191_196 : duration 0:00.004s
[2021-05-14 16:57:02,170] {docker.py:276} INFO - 21/05/14 19:57:02 INFO StagingCommitter: Starting: Task committer attempt_202105141955311431855102435897870_0002_m_000187_192: needsTaskCommit() Task attempt_202105141955311431855102435897870_0002_m_000187_192
[2021-05-14 16:57:02,171] {docker.py:276} INFO - 21/05/14 19:57:02 INFO StagingCommitter: Task committer attempt_202105141955311431855102435897870_0002_m_000187_192: needsTaskCommit() Task attempt_202105141955311431855102435897870_0002_m_000187_192: duration 0:00.001s
21/05/14 19:57:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311431855102435897870_0002_m_000187_192
[2021-05-14 16:57:02,173] {docker.py:276} INFO - 21/05/14 19:57:02 INFO Executor: Finished task 187.0 in stage 2.0 (TID 192). 4544 bytes result sent to driver
[2021-05-14 16:57:02,174] {docker.py:276} INFO - 21/05/14 19:57:02 INFO TaskSetManager: Starting task 192.0 in stage 2.0 (TID 197) (379a553b2355, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:02,175] {docker.py:276} INFO - 21/05/14 19:57:02 INFO TaskSetManager: Finished task 187.0 in stage 2.0 (TID 192) in 1856 ms on 379a553b2355 (executor driver) (189/200)
[2021-05-14 16:57:02,179] {docker.py:276} INFO - 21/05/14 19:57:02 INFO Executor: Running task 192.0 in stage 2.0 (TID 197)
[2021-05-14 16:57:02,188] {docker.py:276} INFO - 21/05/14 19:57:02 INFO ShuffleBlockFetcherIterator: Getting 4 (5.2 KiB) non-empty blocks including 4 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:02,190] {docker.py:276} INFO - 21/05/14 19:57:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955316712246529052954033_0002_m_000192_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316712246529052954033_0002_m_000192_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955316712246529052954033_0002}; taskId=attempt_202105141955316712246529052954033_0002_m_000192_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3df2fa6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:02,190] {docker.py:276} INFO - 21/05/14 19:57:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:02 INFO StagingCommitter: Starting: Task committer attempt_202105141955316712246529052954033_0002_m_000192_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316712246529052954033_0002_m_000192_197
[2021-05-14 16:57:02,193] {docker.py:276} INFO - 21/05/14 19:57:02 INFO StagingCommitter: Task committer attempt_202105141955316712246529052954033_0002_m_000192_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955316712246529052954033_0002_m_000192_197 : duration 0:00.002s
[2021-05-14 16:57:02,755] {docker.py:276} INFO - 21/05/14 19:57:02 INFO StagingCommitter: Starting: Task committer attempt_202105141955313135229846814011603_0002_m_000189_194: needsTaskCommit() Task attempt_202105141955313135229846814011603_0002_m_000189_194
21/05/14 19:57:02 INFO StagingCommitter: Task committer attempt_202105141955313135229846814011603_0002_m_000189_194: needsTaskCommit() Task attempt_202105141955313135229846814011603_0002_m_000189_194: duration 0:00.000s
21/05/14 19:57:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955313135229846814011603_0002_m_000189_194
[2021-05-14 16:57:02,756] {docker.py:276} INFO - 21/05/14 19:57:02 INFO Executor: Finished task 189.0 in stage 2.0 (TID 194). 4544 bytes result sent to driver
[2021-05-14 16:57:02,756] {docker.py:276} INFO - 21/05/14 19:57:02 INFO TaskSetManager: Starting task 193.0 in stage 2.0 (TID 198) (379a553b2355, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 19:57:02 INFO TaskSetManager: Finished task 189.0 in stage 2.0 (TID 194) in 1929 ms on 379a553b2355 (executor driver) (190/200)
21/05/14 19:57:02 INFO Executor: Running task 193.0 in stage 2.0 (TID 198)
[2021-05-14 16:57:02,766] {docker.py:276} INFO - 21/05/14 19:57:02 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:02,771] {docker.py:276} INFO - 21/05/14 19:57:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:57:02,772] {docker.py:276} INFO - 21/05/14 19:57:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955311233869367276935417_0002_m_000193_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311233869367276935417_0002_m_000193_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955311233869367276935417_0002}; taskId=attempt_202105141955311233869367276935417_0002_m_000193_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@239d96d2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:02 INFO StagingCommitter: Starting: Task committer attempt_202105141955311233869367276935417_0002_m_000193_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311233869367276935417_0002_m_000193_198
[2021-05-14 16:57:02,773] {docker.py:276} INFO - 21/05/14 19:57:02 INFO StagingCommitter: Task committer attempt_202105141955311233869367276935417_0002_m_000193_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955311233869367276935417_0002_m_000193_198 : duration 0:00.005s
[2021-05-14 16:57:03,493] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955314164693712578282776_0002_m_000190_195: needsTaskCommit() Task attempt_202105141955314164693712578282776_0002_m_000190_195
[2021-05-14 16:57:03,493] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Task committer attempt_202105141955314164693712578282776_0002_m_000190_195: needsTaskCommit() Task attempt_202105141955314164693712578282776_0002_m_000190_195: duration 0:00.001s
[2021-05-14 16:57:03,494] {docker.py:276} INFO - 21/05/14 19:57:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314164693712578282776_0002_m_000190_195
[2021-05-14 16:57:03,496] {docker.py:276} INFO - 21/05/14 19:57:03 INFO Executor: Finished task 190.0 in stage 2.0 (TID 195). 4544 bytes result sent to driver
[2021-05-14 16:57:03,497] {docker.py:276} INFO - 21/05/14 19:57:03 INFO TaskSetManager: Starting task 194.0 in stage 2.0 (TID 199) (379a553b2355, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:03,498] {docker.py:276} INFO - 21/05/14 19:57:03 INFO TaskSetManager: Finished task 190.0 in stage 2.0 (TID 195) in 1590 ms on 379a553b2355 (executor driver) (191/200)
[2021-05-14 16:57:03,499] {docker.py:276} INFO - 21/05/14 19:57:03 INFO Executor: Running task 194.0 in stage 2.0 (TID 199)
[2021-05-14 16:57:03,507] {docker.py:276} INFO - 21/05/14 19:57:03 INFO ShuffleBlockFetcherIterator: Getting 4 (6.2 KiB) non-empty blocks including 4 (6.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 16:57:03,508] {docker.py:276} INFO - 21/05/14 19:57:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:03,509] {docker.py:276} INFO - 21/05/14 19:57:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 16:57:03,510] {docker.py:276} INFO - 21/05/14 19:57:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 16:57:03,510] {docker.py:276} INFO - 21/05/14 19:57:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318853678570066278459_0002_m_000194_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318853678570066278459_0002_m_000194_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318853678570066278459_0002}; taskId=attempt_202105141955318853678570066278459_0002_m_000194_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76b6687}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955318853678570066278459_0002_m_000194_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318853678570066278459_0002_m_000194_199
[2021-05-14 16:57:03,514] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Task committer attempt_202105141955318853678570066278459_0002_m_000194_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318853678570066278459_0002_m_000194_199 : duration 0:00.003s
[2021-05-14 16:57:03,599] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955314321959717261981749_0002_m_000191_196: needsTaskCommit() Task attempt_202105141955314321959717261981749_0002_m_000191_196
[2021-05-14 16:57:03,600] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Task committer attempt_202105141955314321959717261981749_0002_m_000191_196: needsTaskCommit() Task attempt_202105141955314321959717261981749_0002_m_000191_196: duration 0:00.001s
[2021-05-14 16:57:03,600] {docker.py:276} INFO - 21/05/14 19:57:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314321959717261981749_0002_m_000191_196
[2021-05-14 16:57:03,603] {docker.py:276} INFO - 21/05/14 19:57:03 INFO Executor: Finished task 191.0 in stage 2.0 (TID 196). 4544 bytes result sent to driver
[2021-05-14 16:57:03,604] {docker.py:276} INFO - 21/05/14 19:57:03 INFO TaskSetManager: Starting task 195.0 in stage 2.0 (TID 200) (379a553b2355, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:03,605] {docker.py:276} INFO - 21/05/14 19:57:03 INFO Executor: Running task 195.0 in stage 2.0 (TID 200)
[2021-05-14 16:57:03,606] {docker.py:276} INFO - 21/05/14 19:57:03 INFO TaskSetManager: Finished task 191.0 in stage 2.0 (TID 196) in 1685 ms on 379a553b2355 (executor driver) (192/200)
[2021-05-14 16:57:03,627] {docker.py:276} INFO - 21/05/14 19:57:03 INFO ShuffleBlockFetcherIterator: Getting 4 (6.0 KiB) non-empty blocks including 4 (6.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:03,629] {docker.py:276} INFO - 21/05/14 19:57:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:03,629] {docker.py:276} INFO - 21/05/14 19:57:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531340141893247024101_0002_m_000195_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531340141893247024101_0002_m_000195_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531340141893247024101_0002}; taskId=attempt_20210514195531340141893247024101_0002_m_000195_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f65a193}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:03 INFO StagingCommitter: Starting: Task committer attempt_20210514195531340141893247024101_0002_m_000195_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531340141893247024101_0002_m_000195_200
[2021-05-14 16:57:03,633] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Task committer attempt_20210514195531340141893247024101_0002_m_000195_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531340141893247024101_0002_m_000195_200 : duration 0:00.003s
[2021-05-14 16:57:03,899] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955316712246529052954033_0002_m_000192_197: needsTaskCommit() Task attempt_202105141955316712246529052954033_0002_m_000192_197
[2021-05-14 16:57:03,901] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Task committer attempt_202105141955316712246529052954033_0002_m_000192_197: needsTaskCommit() Task attempt_202105141955316712246529052954033_0002_m_000192_197: duration 0:00.001s
21/05/14 19:57:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955316712246529052954033_0002_m_000192_197
[2021-05-14 16:57:03,904] {docker.py:276} INFO - 21/05/14 19:57:03 INFO Executor: Finished task 192.0 in stage 2.0 (TID 197). 4587 bytes result sent to driver
[2021-05-14 16:57:03,906] {docker.py:276} INFO - 21/05/14 19:57:03 INFO TaskSetManager: Starting task 196.0 in stage 2.0 (TID 201) (379a553b2355, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:03,907] {docker.py:276} INFO - 21/05/14 19:57:03 INFO Executor: Running task 196.0 in stage 2.0 (TID 201)
21/05/14 19:57:03 INFO TaskSetManager: Finished task 192.0 in stage 2.0 (TID 197) in 1735 ms on 379a553b2355 (executor driver) (193/200)
[2021-05-14 16:57:03,917] {docker.py:276} INFO - 21/05/14 19:57:03 INFO ShuffleBlockFetcherIterator: Getting 4 (5.8 KiB) non-empty blocks including 4 (5.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:03,919] {docker.py:276} INFO - 21/05/14 19:57:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314089670224302216848_0002_m_000196_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314089670224302216848_0002_m_000196_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314089670224302216848_0002}; taskId=attempt_202105141955314089670224302216848_0002_m_000196_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5de29e6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105141955314089670224302216848_0002_m_000196_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314089670224302216848_0002_m_000196_201
[2021-05-14 16:57:03,922] {docker.py:276} INFO - 21/05/14 19:57:03 INFO StagingCommitter: Task committer attempt_202105141955314089670224302216848_0002_m_000196_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314089670224302216848_0002_m_000196_201 : duration 0:00.003s
[2021-05-14 16:57:03,989] {docker.py:276} INFO - 21/05/14 19:57:04 INFO StagingCommitter: Starting: Task committer attempt_202105141955311233869367276935417_0002_m_000193_198: needsTaskCommit() Task attempt_202105141955311233869367276935417_0002_m_000193_198
[2021-05-14 16:57:03,990] {docker.py:276} INFO - 21/05/14 19:57:04 INFO StagingCommitter: Task committer attempt_202105141955311233869367276935417_0002_m_000193_198: needsTaskCommit() Task attempt_202105141955311233869367276935417_0002_m_000193_198: duration 0:00.000s
[2021-05-14 16:57:03,991] {docker.py:276} INFO - 21/05/14 19:57:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955311233869367276935417_0002_m_000193_198
[2021-05-14 16:57:03,992] {docker.py:276} INFO - 21/05/14 19:57:04 INFO Executor: Finished task 193.0 in stage 2.0 (TID 198). 4587 bytes result sent to driver
[2021-05-14 16:57:03,994] {docker.py:276} INFO - 21/05/14 19:57:04 INFO TaskSetManager: Starting task 197.0 in stage 2.0 (TID 202) (379a553b2355, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:03,995] {docker.py:276} INFO - 21/05/14 19:57:04 INFO TaskSetManager: Finished task 193.0 in stage 2.0 (TID 198) in 1243 ms on 379a553b2355 (executor driver) (194/200)
[2021-05-14 16:57:03,995] {docker.py:276} INFO - 21/05/14 19:57:04 INFO Executor: Running task 197.0 in stage 2.0 (TID 202)
[2021-05-14 16:57:04,004] {docker.py:276} INFO - 21/05/14 19:57:04 INFO ShuffleBlockFetcherIterator: Getting 4 (5.6 KiB) non-empty blocks including 4 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:04,006] {docker.py:276} INFO - 21/05/14 19:57:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 19:57:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514195531296827635969712228_0002_m_000197_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531296827635969712228_0002_m_000197_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514195531296827635969712228_0002}; taskId=attempt_20210514195531296827635969712228_0002_m_000197_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62f20f95}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:04,006] {docker.py:276} INFO - 21/05/14 19:57:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:04 INFO StagingCommitter: Starting: Task committer attempt_20210514195531296827635969712228_0002_m_000197_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531296827635969712228_0002_m_000197_202
[2021-05-14 16:57:04,010] {docker.py:276} INFO - 21/05/14 19:57:04 INFO StagingCommitter: Task committer attempt_20210514195531296827635969712228_0002_m_000197_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_20210514195531296827635969712228_0002_m_000197_202 : duration 0:00.003s
[2021-05-14 16:57:05,144] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955318853678570066278459_0002_m_000194_199: needsTaskCommit() Task attempt_202105141955318853678570066278459_0002_m_000194_199
[2021-05-14 16:57:05,145] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Task committer attempt_202105141955318853678570066278459_0002_m_000194_199: needsTaskCommit() Task attempt_202105141955318853678570066278459_0002_m_000194_199: duration 0:00.000s
[2021-05-14 16:57:05,145] {docker.py:276} INFO - 21/05/14 19:57:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318853678570066278459_0002_m_000194_199
[2021-05-14 16:57:05,146] {docker.py:276} INFO - 21/05/14 19:57:05 INFO Executor: Finished task 194.0 in stage 2.0 (TID 199). 4587 bytes result sent to driver
[2021-05-14 16:57:05,148] {docker.py:276} INFO - 21/05/14 19:57:05 INFO TaskSetManager: Starting task 198.0 in stage 2.0 (TID 203) (379a553b2355, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:05,149] {docker.py:276} INFO - 21/05/14 19:57:05 INFO Executor: Running task 198.0 in stage 2.0 (TID 203)
[2021-05-14 16:57:05,150] {docker.py:276} INFO - 21/05/14 19:57:05 INFO TaskSetManager: Finished task 194.0 in stage 2.0 (TID 199) in 1655 ms on 379a553b2355 (executor driver) (195/200)
[2021-05-14 16:57:05,160] {docker.py:276} INFO - 21/05/14 19:57:05 INFO ShuffleBlockFetcherIterator: Getting 4 (5.9 KiB) non-empty blocks including 4 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:05,162] {docker.py:276} INFO - 21/05/14 19:57:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:05,162] {docker.py:276} INFO - 21/05/14 19:57:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955314072593329038890502_0002_m_000198_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314072593329038890502_0002_m_000198_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955314072593329038890502_0002}; taskId=attempt_202105141955314072593329038890502_0002_m_000198_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22e04c0d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955314072593329038890502_0002_m_000198_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314072593329038890502_0002_m_000198_203
[2021-05-14 16:57:05,165] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Task committer attempt_202105141955314072593329038890502_0002_m_000198_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955314072593329038890502_0002_m_000198_203 : duration 0:00.003s
[2021-05-14 16:57:05,302] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Starting: Task committer attempt_20210514195531340141893247024101_0002_m_000195_200: needsTaskCommit() Task attempt_20210514195531340141893247024101_0002_m_000195_200
[2021-05-14 16:57:05,303] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Task committer attempt_20210514195531340141893247024101_0002_m_000195_200: needsTaskCommit() Task attempt_20210514195531340141893247024101_0002_m_000195_200: duration 0:00.000s
[2021-05-14 16:57:05,303] {docker.py:276} INFO - 21/05/14 19:57:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531340141893247024101_0002_m_000195_200
[2021-05-14 16:57:05,304] {docker.py:276} INFO - 21/05/14 19:57:05 INFO Executor: Finished task 195.0 in stage 2.0 (TID 200). 4587 bytes result sent to driver
[2021-05-14 16:57:05,305] {docker.py:276} INFO - 21/05/14 19:57:05 INFO TaskSetManager: Starting task 199.0 in stage 2.0 (TID 204) (379a553b2355, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 16:57:05,306] {docker.py:276} INFO - 21/05/14 19:57:05 INFO Executor: Running task 199.0 in stage 2.0 (TID 204)
21/05/14 19:57:05 INFO TaskSetManager: Finished task 195.0 in stage 2.0 (TID 200) in 1704 ms on 379a553b2355 (executor driver) (196/200)
[2021-05-14 16:57:05,313] {docker.py:276} INFO - 21/05/14 19:57:05 INFO ShuffleBlockFetcherIterator: Getting 4 (6.1 KiB) non-empty blocks including 4 (6.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 19:57:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 16:57:05,315] {docker.py:276} INFO - 21/05/14 19:57:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 19:57:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 19:57:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:05,315] {docker.py:276} INFO - 21/05/14 19:57:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141955318799689004763827767_0002_m_000199_204, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318799689004763827767_0002_m_000199_204}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141955318799689004763827767_0002}; taskId=attempt_202105141955318799689004763827767_0002_m_000199_204, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bd134b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/82b8cc11-1921-48bc-8fc4-74386e7235e1/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 19:57:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 19:57:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955318799689004763827767_0002_m_000199_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318799689004763827767_0002_m_000199_204
[2021-05-14 16:57:05,318] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Task committer attempt_202105141955318799689004763827767_0002_m_000199_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/82b8cc11-1921-48bc-8fc4-74386e7235e1/_temporary/0/_temporary/attempt_202105141955318799689004763827767_0002_m_000199_204 : duration 0:00.003s
[2021-05-14 16:57:05,591] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Starting: Task committer attempt_20210514195531296827635969712228_0002_m_000197_202: needsTaskCommit() Task attempt_20210514195531296827635969712228_0002_m_000197_202
[2021-05-14 16:57:05,592] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Task committer attempt_20210514195531296827635969712228_0002_m_000197_202: needsTaskCommit() Task attempt_20210514195531296827635969712228_0002_m_000197_202: duration 0:00.001s
[2021-05-14 16:57:05,592] {docker.py:276} INFO - 21/05/14 19:57:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514195531296827635969712228_0002_m_000197_202
[2021-05-14 16:57:05,593] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Starting: Task committer attempt_202105141955314089670224302216848_0002_m_000196_201: needsTaskCommit() Task attempt_202105141955314089670224302216848_0002_m_000196_201
[2021-05-14 16:57:05,593] {docker.py:276} INFO - 21/05/14 19:57:05 INFO StagingCommitter: Task committer attempt_202105141955314089670224302216848_0002_m_000196_201: needsTaskCommit() Task attempt_202105141955314089670224302216848_0002_m_000196_201: duration 0:00.000s
21/05/14 19:57:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314089670224302216848_0002_m_000196_201
[2021-05-14 16:57:05,595] {docker.py:276} INFO - 21/05/14 19:57:05 INFO Executor: Finished task 197.0 in stage 2.0 (TID 202). 4544 bytes result sent to driver
[2021-05-14 16:57:05,595] {docker.py:276} INFO - 21/05/14 19:57:05 INFO Executor: Finished task 196.0 in stage 2.0 (TID 201). 4544 bytes result sent to driver
[2021-05-14 16:57:05,596] {docker.py:276} INFO - 21/05/14 19:57:05 INFO TaskSetManager: Finished task 197.0 in stage 2.0 (TID 202) in 1605 ms on 379a553b2355 (executor driver) (197/200)
[2021-05-14 16:57:05,597] {docker.py:276} INFO - 21/05/14 19:57:05 INFO TaskSetManager: Finished task 196.0 in stage 2.0 (TID 201) in 1694 ms on 379a553b2355 (executor driver) (198/200)
[2021-05-14 16:57:06,370] {docker.py:276} INFO - 21/05/14 19:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955314072593329038890502_0002_m_000198_203: needsTaskCommit() Task attempt_202105141955314072593329038890502_0002_m_000198_203
[2021-05-14 16:57:06,371] {docker.py:276} INFO - 21/05/14 19:57:06 INFO StagingCommitter: Task committer attempt_202105141955314072593329038890502_0002_m_000198_203: needsTaskCommit() Task attempt_202105141955314072593329038890502_0002_m_000198_203: duration 0:00.000s
[2021-05-14 16:57:06,371] {docker.py:276} INFO - 21/05/14 19:57:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955314072593329038890502_0002_m_000198_203
[2021-05-14 16:57:06,372] {docker.py:276} INFO - 21/05/14 19:57:06 INFO Executor: Finished task 198.0 in stage 2.0 (TID 203). 4544 bytes result sent to driver
[2021-05-14 16:57:06,373] {docker.py:276} INFO - 21/05/14 19:57:06 INFO TaskSetManager: Finished task 198.0 in stage 2.0 (TID 203) in 1227 ms on 379a553b2355 (executor driver) (199/200)
[2021-05-14 16:57:06,475] {docker.py:276} INFO - 21/05/14 19:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105141955318799689004763827767_0002_m_000199_204: needsTaskCommit() Task attempt_202105141955318799689004763827767_0002_m_000199_204
[2021-05-14 16:57:06,476] {docker.py:276} INFO - 21/05/14 19:57:06 INFO StagingCommitter: Task committer attempt_202105141955318799689004763827767_0002_m_000199_204: needsTaskCommit() Task attempt_202105141955318799689004763827767_0002_m_000199_204: duration 0:00.001s
21/05/14 19:57:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141955318799689004763827767_0002_m_000199_204
[2021-05-14 16:57:06,477] {docker.py:276} INFO - 21/05/14 19:57:06 INFO Executor: Finished task 199.0 in stage 2.0 (TID 204). 4544 bytes result sent to driver
[2021-05-14 16:57:06,479] {docker.py:276} INFO - 21/05/14 19:57:06 INFO TaskSetManager: Finished task 199.0 in stage 2.0 (TID 204) in 1175 ms on 379a553b2355 (executor driver) (200/200)
21/05/14 19:57:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-14 16:57:06,480] {docker.py:276} INFO - 21/05/14 19:57:06 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 91.116 s
[2021-05-14 16:57:06,480] {docker.py:276} INFO - 21/05/14 19:57:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 19:57:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-14 16:57:06,481] {docker.py:276} INFO - 21/05/14 19:57:06 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 94.709655 s
[2021-05-14 16:57:06,484] {docker.py:276} INFO - 21/05/14 19:57:06 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105141955313798401951849224908_0000_m_000000_0: commitJob((no job ID))
[2021-05-14 16:57:06,502] {docker.py:276} INFO - 21/05/14 19:57:06 WARN AbstractS3ACommitter: Task committer attempt_202105141955313798401951849224908_0000_m_000000_0: No pending uploads to commit
[2021-05-14 16:57:06,976] {docker.py:276} INFO - 21/05/14 19:57:07 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/14 19:57:07 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-14 16:57:07,144] {docker.py:276} INFO - 21/05/14 19:57:07 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.168s
21/05/14 19:57:07 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.168s
[2021-05-14 16:57:07,145] {docker.py:276} INFO - 21/05/14 19:57:07 INFO AbstractS3ACommitter: Task committer attempt_202105141955313798401951849224908_0000_m_000000_0: commitJob((no job ID)): duration 0:00.661s
[2021-05-14 16:57:07,954] {docker.py:276} INFO - 21/05/14 19:57:07 INFO FileFormatWriter: Write Job 82b8cc11-1921-48bc-8fc4-74386e7235e1 committed.
[2021-05-14 16:57:07,963] {docker.py:276} INFO - 21/05/14 19:57:07 INFO FileFormatWriter: Finished processing stats for write job 82b8cc11-1921-48bc-8fc4-74386e7235e1.
[2021-05-14 16:57:08,075] {docker.py:276} INFO - 21/05/14 19:57:08 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-14 16:57:08,094] {docker.py:276} INFO - 21/05/14 19:57:08 INFO SparkUI: Stopped Spark web UI at http://379a553b2355:4040
[2021-05-14 16:57:08,114] {docker.py:276} INFO - 21/05/14 19:57:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-14 16:57:08,130] {docker.py:276} INFO - 21/05/14 19:57:08 INFO MemoryStore: MemoryStore cleared
[2021-05-14 16:57:08,131] {docker.py:276} INFO - 21/05/14 19:57:08 INFO BlockManager: BlockManager stopped
[2021-05-14 16:57:08,135] {docker.py:276} INFO - 21/05/14 19:57:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-14 16:57:08,140] {docker.py:276} INFO - 21/05/14 19:57:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-14 16:57:08,147] {docker.py:276} INFO - 21/05/14 19:57:08 INFO SparkContext: Successfully stopped SparkContext
[2021-05-14 16:57:08,148] {docker.py:276} INFO - 21/05/14 19:57:08 INFO ShutdownHookManager: Shutdown hook called
[2021-05-14 16:57:08,149] {docker.py:276} INFO - 21/05/14 19:57:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-a202d404-8b66-43b3-b12d-1e0530bccf2f
[2021-05-14 16:57:08,151] {docker.py:276} INFO - 21/05/14 19:57:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-2acefd47-af70-42ff-85c0-7f757e0b34c8
[2021-05-14 16:57:08,153] {docker.py:276} INFO - 21/05/14 19:57:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-2acefd47-af70-42ff-85c0-7f757e0b34c8/pyspark-f694686b-5a74-4589-90bf-9c4301551d14
[2021-05-14 16:57:08,159] {docker.py:276} INFO - 21/05/14 19:57:08 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-14 16:57:08,160] {docker.py:276} INFO - 21/05/14 19:57:08 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-14 16:57:08,161] {docker.py:276} INFO - 21/05/14 19:57:08 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-14 16:57:08,413] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210514T195430, start_date=20210514T195500, end_date=20210514T195708
[2021-05-14 16:57:08,462] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-14 16:57:08,499] {local_task_job.py:146} INFO - Task exited with return code 0
