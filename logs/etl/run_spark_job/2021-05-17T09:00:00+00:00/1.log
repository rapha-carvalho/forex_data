[2021-05-17 11:08:48,829] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T09:00:00+00:00 [queued]>
[2021-05-17 11:08:48,834] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T09:00:00+00:00 [queued]>
[2021-05-17 11:08:48,834] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 11:08:48,834] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-17 11:08:48,834] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 11:08:48,839] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-17T09:00:00+00:00
[2021-05-17 11:08:48,842] {standard_task_runner.py:52} INFO - Started process 35447 to run task
[2021-05-17 11:08:48,849] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-17T09:00:00+00:00', '--job-id', '925', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpwbf3ounc', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpiu085xfk']
[2021-05-17 11:08:48,851] {standard_task_runner.py:77} INFO - Job 925: Subtask run_spark_job
[2021-05-17 11:08:48,884] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-17T09:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-17 11:08:48,909] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-17T09:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-17T09:00:00+00:00
[2021-05-17 11:08:48,912] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-17 11:08:52,083] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-17 11:08:52,086] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-17 11:08:52,087] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-17 11:08:52,093] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-17 11:08:54,767] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-17 11:08:55,329] {docker.py:276} INFO - 21/05/17 14:08:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-17 11:08:57,830] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-17 11:08:57,844] {docker.py:276} INFO - 21/05/17 14:08:57 INFO SparkContext: Running Spark version 3.1.1
[2021-05-17 11:08:57,912] {docker.py:276} INFO - 21/05/17 14:08:57 INFO ResourceUtils: ==============================================================
[2021-05-17 11:08:57,913] {docker.py:276} INFO - 21/05/17 14:08:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-17 11:08:57,913] {docker.py:276} INFO - 21/05/17 14:08:57 INFO ResourceUtils: ==============================================================
[2021-05-17 11:08:57,914] {docker.py:276} INFO - 21/05/17 14:08:57 INFO SparkContext: Submitted application: spark.py
[2021-05-17 11:08:57,949] {docker.py:276} INFO - 21/05/17 14:08:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-17 11:08:57,965] {docker.py:276} INFO - 21/05/17 14:08:57 INFO ResourceProfile: Limiting resource is cpu
[2021-05-17 11:08:57,965] {docker.py:276} INFO - 21/05/17 14:08:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-17 11:08:58,036] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SecurityManager: Changing view acls to: jovyan
21/05/17 14:08:58 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-17 11:08:58,037] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SecurityManager: Changing view acls groups to: 
21/05/17 14:08:58 INFO SecurityManager: Changing modify acls groups to:
[2021-05-17 11:08:58,037] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-17 11:08:58,391] {docker.py:276} INFO - 21/05/17 14:08:58 INFO Utils: Successfully started service 'sparkDriver' on port 38475.
[2021-05-17 11:08:58,429] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SparkEnv: Registering MapOutputTracker
[2021-05-17 11:08:58,495] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-17 11:08:58,528] {docker.py:276} INFO - 21/05/17 14:08:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-17 11:08:58,529] {docker.py:276} INFO - 21/05/17 14:08:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-17 11:08:58,537] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-17 11:08:58,560] {docker.py:276} INFO - 21/05/17 14:08:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-32382f5e-4f7f-4227-af4d-1871fc253e61
[2021-05-17 11:08:58,591] {docker.py:276} INFO - 21/05/17 14:08:58 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-17 11:08:58,615] {docker.py:276} INFO - 21/05/17 14:08:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-17 11:08:58,904] {docker.py:276} INFO - 21/05/17 14:08:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-17 11:08:59,010] {docker.py:276} INFO - 21/05/17 14:08:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://d733d8da4350:4040
[2021-05-17 11:08:59,282] {docker.py:276} INFO - 21/05/17 14:08:59 INFO Executor: Starting executor ID driver on host d733d8da4350
[2021-05-17 11:08:59,324] {docker.py:276} INFO - 21/05/17 14:08:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35777.
[2021-05-17 11:08:59,325] {docker.py:276} INFO - 21/05/17 14:08:59 INFO NettyBlockTransferService: Server created on d733d8da4350:35777
[2021-05-17 11:08:59,327] {docker.py:276} INFO - 21/05/17 14:08:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-17 11:08:59,340] {docker.py:276} INFO - 21/05/17 14:08:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d733d8da4350, 35777, None)
[2021-05-17 11:08:59,353] {docker.py:276} INFO - 21/05/17 14:08:59 INFO BlockManagerMasterEndpoint: Registering block manager d733d8da4350:35777 with 934.4 MiB RAM, BlockManagerId(driver, d733d8da4350, 35777, None)
[2021-05-17 11:08:59,357] {docker.py:276} INFO - 21/05/17 14:08:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d733d8da4350, 35777, None)
[2021-05-17 11:08:59,359] {docker.py:276} INFO - 21/05/17 14:08:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d733d8da4350, 35777, None)
[2021-05-17 11:08:59,928] {docker.py:276} INFO - 21/05/17 14:08:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-17 11:08:59,928] {docker.py:276} INFO - 21/05/17 14:08:59 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-17 11:09:01,044] {docker.py:276} INFO - 21/05/17 14:09:01 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-17 11:09:01,114] {docker.py:276} INFO - 21/05/17 14:09:01 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-17 11:09:01,115] {docker.py:276} INFO - 21/05/17 14:09:01 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-17 11:09:06,796] {docker.py:276} INFO - 21/05/17 14:09:06 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 75 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621215193_to_1621215506.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621215506_to_1621217306.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621217306_to_1621219106.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621219106_to_1621220906.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621220906_to_1621222706.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621222706_to_1621224506.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621224506_to_1621226306.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621226306_to_1621228106.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621228106_to_1621229906.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621229906_to_1621231706.csv.
[2021-05-17 11:09:07,319] {docker.py:276} INFO - 21/05/17 14:09:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:07,347] {docker.py:276} INFO - 21/05/17 14:09:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 75 output partitions
[2021-05-17 11:09:07,347] {docker.py:276} INFO - 21/05/17 14:09:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 11:09:07,348] {docker.py:276} INFO - 21/05/17 14:09:07 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 11:09:07,350] {docker.py:276} INFO - 21/05/17 14:09:07 INFO DAGScheduler: Missing parents: List()
[2021-05-17 11:09:07,359] {docker.py:276} INFO - 21/05/17 14:09:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 11:09:07,462] {docker.py:276} INFO - 21/05/17 14:09:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-17 11:09:07,527] {docker.py:276} INFO - 21/05/17 14:09:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-17 11:09:07,531] {docker.py:276} INFO - 21/05/17 14:09:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d733d8da4350:35777 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-17 11:09:07,538] {docker.py:276} INFO - 21/05/17 14:09:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-17 11:09:07,572] {docker.py:276} INFO - 21/05/17 14:09:07 INFO DAGScheduler: Submitting 75 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-17 11:09:07,583] {docker.py:276} INFO - 21/05/17 14:09:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 75 tasks resource profile 0
[2021-05-17 11:09:07,672] {docker.py:276} INFO - 21/05/17 14:09:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (d733d8da4350, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:07,678] {docker.py:276} INFO - 21/05/17 14:09:07 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (d733d8da4350, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:07,682] {docker.py:276} INFO - 21/05/17 14:09:07 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (d733d8da4350, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:07,683] {docker.py:276} INFO - 21/05/17 14:09:07 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (d733d8da4350, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:07,699] {docker.py:276} INFO - 21/05/17 14:09:07 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
21/05/17 14:09:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-17 11:09:07,699] {docker.py:276} INFO - 21/05/17 14:09:07 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-17 11:09:07,700] {docker.py:276} INFO - 21/05/17 14:09:07 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-17 11:09:08,129] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1929 bytes result sent to driver
[2021-05-17 11:09:08,135] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (d733d8da4350, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,137] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-17 11:09:08,150] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 461 ms on d733d8da4350 (executor driver) (1/75)
[2021-05-17 11:09:08,312] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-17 11:09:08,314] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (d733d8da4350, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,315] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-17 11:09:08,326] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 193 ms on d733d8da4350 (executor driver) (2/75)
[2021-05-17 11:09:08,497] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-17 11:09:08,501] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (d733d8da4350, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,502] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-17 11:09:08,504] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 190 ms on d733d8da4350 (executor driver) (3/75)
[2021-05-17 11:09:08,679] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-17 11:09:08,681] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (d733d8da4350, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,683] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-17 11:09:08,683] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 184 ms on d733d8da4350 (executor driver) (4/75)
[2021-05-17 11:09:08,691] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-17 11:09:08,692] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (d733d8da4350, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,693] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1015 ms on d733d8da4350 (executor driver) (5/75)
[2021-05-17 11:09:08,695] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-17 11:09:08,699] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-17 11:09:08,700] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1929 bytes result sent to driver
[2021-05-17 11:09:08,701] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (d733d8da4350, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,701] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
21/05/17 14:09:08 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (d733d8da4350, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,702] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1025 ms on d733d8da4350 (executor driver) (6/75)
[2021-05-17 11:09:08,702] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1061 ms on d733d8da4350 (executor driver) (7/75)
[2021-05-17 11:09:08,703] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-17 11:09:08,880] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-17 11:09:08,883] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (d733d8da4350, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,885] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-17 11:09:08,886] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 205 ms on d733d8da4350 (executor driver) (8/75)
[2021-05-17 11:09:08,890] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-17 11:09:08,891] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-17 11:09:08,893] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (d733d8da4350, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,894] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 195 ms on d733d8da4350 (executor driver) (9/75)
[2021-05-17 11:09:08,896] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (d733d8da4350, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,898] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 199 ms on d733d8da4350 (executor driver) (10/75)
[2021-05-17 11:09:08,898] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-17 11:09:08,900] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-17 11:09:08,905] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-17 11:09:08,906] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (d733d8da4350, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:08,908] {docker.py:276} INFO - 21/05/17 14:09:08 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 217 ms on d733d8da4350 (executor driver) (11/75)
[2021-05-17 11:09:08,910] {docker.py:276} INFO - 21/05/17 14:09:08 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-17 11:09:09,077] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1886 bytes result sent to driver
[2021-05-17 11:09:09,080] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1886 bytes result sent to driver
[2021-05-17 11:09:09,081] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (d733d8da4350, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,083] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-17 11:09:09,084] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (d733d8da4350, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,085] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-17 11:09:09,086] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 204 ms on d733d8da4350 (executor driver) (12/75)
[2021-05-17 11:09:09,087] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 194 ms on d733d8da4350 (executor driver) (13/75)
[2021-05-17 11:09:09,095] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1886 bytes result sent to driver
[2021-05-17 11:09:09,096] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (d733d8da4350, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,097] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 202 ms on d733d8da4350 (executor driver) (14/75)
21/05/17 14:09:09 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-17 11:09:09,101] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1886 bytes result sent to driver
[2021-05-17 11:09:09,103] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (d733d8da4350, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,103] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 198 ms on d733d8da4350 (executor driver) (15/75)
[2021-05-17 11:09:09,104] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-17 11:09:09,279] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-17 11:09:09,280] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-17 11:09:09,281] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (d733d8da4350, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,283] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-17 11:09:09,283] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (d733d8da4350, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,284] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-17 11:09:09,284] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 188 ms on d733d8da4350 (executor driver) (16/75)
[2021-05-17 11:09:09,285] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 182 ms on d733d8da4350 (executor driver) (17/75)
[2021-05-17 11:09:09,288] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1886 bytes result sent to driver
[2021-05-17 11:09:09,289] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (d733d8da4350, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,291] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-17 11:09:09,292] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 213 ms on d733d8da4350 (executor driver) (18/75)
[2021-05-17 11:09:09,375] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
[2021-05-17 11:09:09,377] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (d733d8da4350, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,379] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 296 ms on d733d8da4350 (executor driver) (19/75)
[2021-05-17 11:09:09,379] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-17 11:09:09,460] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
[2021-05-17 11:09:09,463] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (d733d8da4350, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,465] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 183 ms on d733d8da4350 (executor driver) (20/75)
[2021-05-17 11:09:09,466] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-17 11:09:09,468] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-17 11:09:09,470] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (d733d8da4350, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,471] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-17 11:09:09,472] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 192 ms on d733d8da4350 (executor driver) (21/75)
[2021-05-17 11:09:09,478] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-17 11:09:09,480] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (d733d8da4350, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,481] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-17 11:09:09,485] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 192 ms on d733d8da4350 (executor driver) (22/75)
[2021-05-17 11:09:09,557] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-17 11:09:09,559] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (d733d8da4350, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,561] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 184 ms on d733d8da4350 (executor driver) (23/75)
21/05/17 14:09:09 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-17 11:09:09,645] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-17 11:09:09,649] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (d733d8da4350, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,651] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-17 11:09:09,652] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 189 ms on d733d8da4350 (executor driver) (24/75)
[2021-05-17 11:09:09,654] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-17 11:09:09,656] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (d733d8da4350, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,666] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 197 ms on d733d8da4350 (executor driver) (25/75)
[2021-05-17 11:09:09,668] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1886 bytes result sent to driver
[2021-05-17 11:09:09,669] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-17 11:09:09,670] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (d733d8da4350, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,679] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-17 11:09:09,679] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 199 ms on d733d8da4350 (executor driver) (26/75)
[2021-05-17 11:09:09,738] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1886 bytes result sent to driver
[2021-05-17 11:09:09,741] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (d733d8da4350, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,742] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 184 ms on d733d8da4350 (executor driver) (27/75)
[2021-05-17 11:09:09,742] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-17 11:09:09,846] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-17 11:09:09,847] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (d733d8da4350, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,849] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 179 ms on d733d8da4350 (executor driver) (28/75)
[2021-05-17 11:09:09,851] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-17 11:09:09,852] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (d733d8da4350, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,853] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1886 bytes result sent to driver
[2021-05-17 11:09:09,854] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-17 11:09:09,856] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 199 ms on d733d8da4350 (executor driver) (29/75)
21/05/17 14:09:09 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-17 11:09:09,858] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (d733d8da4350, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,860] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
21/05/17 14:09:09 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 213 ms on d733d8da4350 (executor driver) (30/75)
[2021-05-17 11:09:09,914] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-17 11:09:09,916] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (d733d8da4350, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:09,917] {docker.py:276} INFO - 21/05/17 14:09:09 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 176 ms on d733d8da4350 (executor driver) (31/75)
[2021-05-17 11:09:09,918] {docker.py:276} INFO - 21/05/17 14:09:09 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-17 11:09:10,029] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-17 11:09:10,031] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (d733d8da4350, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,032] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-17 11:09:10,033] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 176 ms on d733d8da4350 (executor driver) (32/75)
[2021-05-17 11:09:10,035] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 184 ms on d733d8da4350 (executor driver) (33/75)
[2021-05-17 11:09:10,036] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-17 11:09:10,038] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (d733d8da4350, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,042] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-17 11:09:10,044] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-17 11:09:10,045] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (d733d8da4350, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,046] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 199 ms on d733d8da4350 (executor driver) (34/75)
[2021-05-17 11:09:10,046] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-17 11:09:10,090] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-17 11:09:10,091] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 175 ms on d733d8da4350 (executor driver) (35/75)
[2021-05-17 11:09:10,093] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (d733d8da4350, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,094] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-17 11:09:10,222] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-17 11:09:10,224] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (d733d8da4350, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,225] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 188 ms on d733d8da4350 (executor driver) (36/75)
[2021-05-17 11:09:10,226] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-17 11:09:10,228] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-17 11:09:10,229] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 185 ms on d733d8da4350 (executor driver) (37/75)
[2021-05-17 11:09:10,231] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (d733d8da4350, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,232] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-17 11:09:10,235] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-17 11:09:10,235] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (d733d8da4350, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,237] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 206 ms on d733d8da4350 (executor driver) (38/75)
[2021-05-17 11:09:10,238] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-17 11:09:10,265] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1886 bytes result sent to driver
[2021-05-17 11:09:10,267] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (d733d8da4350, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,268] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-17 11:09:10,269] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 176 ms on d733d8da4350 (executor driver) (39/75)
[2021-05-17 11:09:10,402] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1886 bytes result sent to driver
[2021-05-17 11:09:10,404] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (d733d8da4350, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,405] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-17 11:09:10,406] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 183 ms on d733d8da4350 (executor driver) (40/75)
[2021-05-17 11:09:10,419] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1886 bytes result sent to driver
21/05/17 14:09:10 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1886 bytes result sent to driver
[2021-05-17 11:09:10,421] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (d733d8da4350, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,422] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 187 ms on d733d8da4350 (executor driver) (41/75)
[2021-05-17 11:09:10,424] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (d733d8da4350, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,425] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 195 ms on d733d8da4350 (executor driver) (42/75)
[2021-05-17 11:09:10,425] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-17 11:09:10,431] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-17 11:09:10,443] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-17 11:09:10,445] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (d733d8da4350, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,446] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-17 11:09:10,446] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 180 ms on d733d8da4350 (executor driver) (43/75)
[2021-05-17 11:09:10,581] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-17 11:09:10,584] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (d733d8da4350, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,585] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-17 11:09:10,586] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 182 ms on d733d8da4350 (executor driver) (44/75)
[2021-05-17 11:09:10,599] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-17 11:09:10,601] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (d733d8da4350, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,602] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-17 11:09:10,603] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 182 ms on d733d8da4350 (executor driver) (45/75)
[2021-05-17 11:09:10,607] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-17 11:09:10,608] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (d733d8da4350, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,609] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 186 ms on d733d8da4350 (executor driver) (46/75)
[2021-05-17 11:09:10,610] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-17 11:09:10,617] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-17 11:09:10,619] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (d733d8da4350, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,621] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 176 ms on d733d8da4350 (executor driver) (47/75)
21/05/17 14:09:10 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-17 11:09:10,761] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-17 11:09:10,763] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (d733d8da4350, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,765] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 182 ms on d733d8da4350 (executor driver) (48/75)
[2021-05-17 11:09:10,766] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-17 11:09:10,777] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-17 11:09:10,778] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (d733d8da4350, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,780] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 179 ms on d733d8da4350 (executor driver) (49/75)
[2021-05-17 11:09:10,781] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-17 11:09:10,782] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-17 11:09:10,784] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (d733d8da4350, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,785] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 177 ms on d733d8da4350 (executor driver) (50/75)
21/05/17 14:09:10 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-17 11:09:10,790] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-17 11:09:10,791] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (d733d8da4350, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,792] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-17 11:09:10,792] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 173 ms on d733d8da4350 (executor driver) (51/75)
[2021-05-17 11:09:10,941] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-17 11:09:10,943] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (d733d8da4350, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,944] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-17 11:09:10,945] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 183 ms on d733d8da4350 (executor driver) (52/75)
[2021-05-17 11:09:10,958] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1886 bytes result sent to driver
[2021-05-17 11:09:10,959] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (d733d8da4350, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,961] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 177 ms on d733d8da4350 (executor driver) (53/75)
[2021-05-17 11:09:10,962] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-17 11:09:10,967] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1886 bytes result sent to driver
[2021-05-17 11:09:10,968] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1886 bytes result sent to driver
21/05/17 14:09:10 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (d733d8da4350, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,969] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (d733d8da4350, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:10,970] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
21/05/17 14:09:10 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 192 ms on d733d8da4350 (executor driver) (54/75)
[2021-05-17 11:09:10,970] {docker.py:276} INFO - 21/05/17 14:09:10 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-17 11:09:10,971] {docker.py:276} INFO - 21/05/17 14:09:10 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 180 ms on d733d8da4350 (executor driver) (55/75)
[2021-05-17 11:09:11,147] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-17 11:09:11,148] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1886 bytes result sent to driver
[2021-05-17 11:09:11,149] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
[2021-05-17 11:09:11,151] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-17 11:09:11,151] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (d733d8da4350, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,152] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-17 11:09:11,153] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (d733d8da4350, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,154] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 186 ms on d733d8da4350 (executor driver) (56/75)
[2021-05-17 11:09:11,155] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-17 11:09:11,156] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (d733d8da4350, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,157] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 216 ms on d733d8da4350 (executor driver) (57/75)
[2021-05-17 11:09:11,158] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-17 11:09:11,158] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 200 ms on d733d8da4350 (executor driver) (58/75)
[2021-05-17 11:09:11,160] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (d733d8da4350, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,163] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-17 11:09:11,163] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 194 ms on d733d8da4350 (executor driver) (59/75)
[2021-05-17 11:09:11,334] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-17 11:09:11,336] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (d733d8da4350, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,338] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 182 ms on d733d8da4350 (executor driver) (60/75)
[2021-05-17 11:09:11,339] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-17 11:09:11,340] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-17 11:09:11,341] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-17 11:09:11,342] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-17 11:09:11,343] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (d733d8da4350, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,345] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
21/05/17 14:09:11 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 196 ms on d733d8da4350 (executor driver) (61/75)
[2021-05-17 11:09:11,347] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (d733d8da4350, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,348] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 189 ms on d733d8da4350 (executor driver) (62/75)
[2021-05-17 11:09:11,350] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (d733d8da4350, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,351] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-17 11:09:11,352] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 200 ms on d733d8da4350 (executor driver) (63/75)
[2021-05-17 11:09:11,353] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-17 11:09:11,537] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
21/05/17 14:09:11 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-17 11:09:11,538] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1843 bytes result sent to driver
[2021-05-17 11:09:11,538] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-17 11:09:11,539] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (d733d8da4350, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,539] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-17 11:09:11,540] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (d733d8da4350, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,542] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-17 11:09:11,543] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (d733d8da4350, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,544] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 198 ms on d733d8da4350 (executor driver) (64/75)
[2021-05-17 11:09:11,545] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-17 11:09:11,546] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (d733d8da4350, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,546] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 211 ms on d733d8da4350 (executor driver) (65/75)
[2021-05-17 11:09:11,547] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 197 ms on d733d8da4350 (executor driver) (66/75)
[2021-05-17 11:09:11,548] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 206 ms on d733d8da4350 (executor driver) (67/75)
[2021-05-17 11:09:11,551] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-17 11:09:11,719] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1886 bytes result sent to driver
[2021-05-17 11:09:11,720] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (d733d8da4350, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,721] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-17 11:09:11,722] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 183 ms on d733d8da4350 (executor driver) (68/75)
[2021-05-17 11:09:11,729] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1886 bytes result sent to driver
[2021-05-17 11:09:11,730] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1886 bytes result sent to driver
[2021-05-17 11:09:11,731] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (d733d8da4350, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,732] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-17 11:09:11,734] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (d733d8da4350, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,734] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 192 ms on d733d8da4350 (executor driver) (69/75)
[2021-05-17 11:09:11,735] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-17 11:09:11,735] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 190 ms on d733d8da4350 (executor driver) (70/75)
[2021-05-17 11:09:11,740] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1886 bytes result sent to driver
[2021-05-17 11:09:11,742] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (d733d8da4350, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:11,742] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 207 ms on d733d8da4350 (executor driver) (71/75)
[2021-05-17 11:09:11,744] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-17 11:09:11,897] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-17 11:09:11,906] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-17 11:09:11,907] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 189 ms on d733d8da4350 (executor driver) (72/75)
[2021-05-17 11:09:11,908] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 175 ms on d733d8da4350 (executor driver) (73/75)
[2021-05-17 11:09:11,912] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-17 11:09:11,914] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 183 ms on d733d8da4350 (executor driver) (74/75)
[2021-05-17 11:09:11,919] {docker.py:276} INFO - 21/05/17 14:09:11 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1843 bytes result sent to driver
[2021-05-17 11:09:11,920] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 176 ms on d733d8da4350 (executor driver) (75/75)
[2021-05-17 11:09:11,920] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-17 11:09:11,922] {docker.py:276} INFO - 21/05/17 14:09:11 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 4.541 s
[2021-05-17 11:09:11,929] {docker.py:276} INFO - 21/05/17 14:09:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-17 11:09:11,930] {docker.py:276} INFO - 21/05/17 14:09:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-17 11:09:11,934] {docker.py:276} INFO - 21/05/17 14:09:11 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 4.618332 s
[2021-05-17 11:09:11,970] {docker.py:276} INFO - 21/05/17 14:09:11 INFO InMemoryFileIndex: It took 5192 ms to list leaf files for 75 paths.
[2021-05-17 11:09:12,087] {docker.py:276} INFO - 21/05/17 14:09:12 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 75 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621215193_to_1621215506.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621215506_to_1621217306.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621217306_to_1621219106.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621219106_to_1621220906.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621220906_to_1621222706.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621222706_to_1621224506.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621224506_to_1621226306.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621226306_to_1621228106.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621228106_to_1621229906.csv, s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621229906_to_1621231706.csv.
[2021-05-17 11:09:12,125] {docker.py:276} INFO - 21/05/17 14:09:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:12,128] {docker.py:276} INFO - 21/05/17 14:09:12 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 75 output partitions
[2021-05-17 11:09:12,129] {docker.py:276} INFO - 21/05/17 14:09:12 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 11:09:12,129] {docker.py:276} INFO - 21/05/17 14:09:12 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 11:09:12,130] {docker.py:276} INFO - 21/05/17 14:09:12 INFO DAGScheduler: Missing parents: List()
[2021-05-17 11:09:12,131] {docker.py:276} INFO - 21/05/17 14:09:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 11:09:12,143] {docker.py:276} INFO - 21/05/17 14:09:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-17 11:09:12,154] {docker.py:276} INFO - 21/05/17 14:09:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-17 11:09:12,155] {docker.py:276} INFO - 21/05/17 14:09:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d733d8da4350:35777 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-17 11:09:12,157] {docker.py:276} INFO - 21/05/17 14:09:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-17 11:09:12,160] {docker.py:276} INFO - 21/05/17 14:09:12 INFO DAGScheduler: Submitting 75 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-17 11:09:12,160] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 75 tasks resource profile 0
[2021-05-17 11:09:12,163] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 75) (d733d8da4350, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,163] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 76) (d733d8da4350, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,164] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 77) (d733d8da4350, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,165] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 78) (d733d8da4350, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,166] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 1.0 in stage 1.0 (TID 76)
[2021-05-17 11:09:12,167] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 3.0 in stage 1.0 (TID 78)
[2021-05-17 11:09:12,167] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 75)
[2021-05-17 11:09:12,168] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 2.0 in stage 1.0 (TID 77)
[2021-05-17 11:09:12,242] {docker.py:276} INFO - 21/05/17 14:09:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d733d8da4350:35777 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-17 11:09:12,337] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 76). 1843 bytes result sent to driver
[2021-05-17 11:09:12,338] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 79) (d733d8da4350, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,339] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 76) in 176 ms on d733d8da4350 (executor driver) (1/75)
[2021-05-17 11:09:12,339] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 4.0 in stage 1.0 (TID 79)
[2021-05-17 11:09:12,340] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 75). 1843 bytes result sent to driver
[2021-05-17 11:09:12,341] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 80) (d733d8da4350, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,342] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 75) in 180 ms on d733d8da4350 (executor driver) (2/75)
[2021-05-17 11:09:12,342] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 5.0 in stage 1.0 (TID 80)
[2021-05-17 11:09:12,344] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 2.0 in stage 1.0 (TID 77). 1843 bytes result sent to driver
[2021-05-17 11:09:12,345] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 81) (d733d8da4350, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,346] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 77) in 182 ms on d733d8da4350 (executor driver) (3/75)
[2021-05-17 11:09:12,347] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 3.0 in stage 1.0 (TID 78). 1843 bytes result sent to driver
[2021-05-17 11:09:12,348] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 82) (d733d8da4350, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,349] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 6.0 in stage 1.0 (TID 81)
[2021-05-17 11:09:12,349] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 78) in 184 ms on d733d8da4350 (executor driver) (4/75)
[2021-05-17 11:09:12,351] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 7.0 in stage 1.0 (TID 82)
[2021-05-17 11:09:12,518] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 5.0 in stage 1.0 (TID 80). 1843 bytes result sent to driver
[2021-05-17 11:09:12,520] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 4.0 in stage 1.0 (TID 79). 1843 bytes result sent to driver
[2021-05-17 11:09:12,521] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 80) in 180 ms on d733d8da4350 (executor driver) (5/75)
[2021-05-17 11:09:12,522] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 83) (d733d8da4350, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,523] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 84) (d733d8da4350, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,524] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 79) in 186 ms on d733d8da4350 (executor driver) (6/75)
[2021-05-17 11:09:12,525] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 7.0 in stage 1.0 (TID 82). 1843 bytes result sent to driver
[2021-05-17 11:09:12,526] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 85) (d733d8da4350, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,527] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 82) in 180 ms on d733d8da4350 (executor driver) (7/75)
[2021-05-17 11:09:12,528] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 8.0 in stage 1.0 (TID 83)
[2021-05-17 11:09:12,529] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 6.0 in stage 1.0 (TID 81). 1843 bytes result sent to driver
[2021-05-17 11:09:12,530] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 10.0 in stage 1.0 (TID 85)
[2021-05-17 11:09:12,530] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 86) (d733d8da4350, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,532] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 81) in 187 ms on d733d8da4350 (executor driver) (8/75)
[2021-05-17 11:09:12,533] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 11.0 in stage 1.0 (TID 86)
[2021-05-17 11:09:12,534] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 9.0 in stage 1.0 (TID 84)
[2021-05-17 11:09:12,707] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 11.0 in stage 1.0 (TID 86). 1843 bytes result sent to driver
[2021-05-17 11:09:12,708] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 10.0 in stage 1.0 (TID 85). 1843 bytes result sent to driver
[2021-05-17 11:09:12,709] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 9.0 in stage 1.0 (TID 84). 1843 bytes result sent to driver
[2021-05-17 11:09:12,710] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 87) (d733d8da4350, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,711] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 86) in 180 ms on d733d8da4350 (executor driver) (9/75)
[2021-05-17 11:09:12,713] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 88) (d733d8da4350, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,714] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 12.0 in stage 1.0 (TID 87)
[2021-05-17 11:09:12,714] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 8.0 in stage 1.0 (TID 83). 1843 bytes result sent to driver
[2021-05-17 11:09:12,715] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 13.0 in stage 1.0 (TID 88)
[2021-05-17 11:09:12,716] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 89) (d733d8da4350, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,717] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 85) in 189 ms on d733d8da4350 (executor driver) (10/75)
21/05/17 14:09:12 INFO Executor: Running task 14.0 in stage 1.0 (TID 89)
[2021-05-17 11:09:12,718] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 90) (d733d8da4350, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,718] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 84) in 195 ms on d733d8da4350 (executor driver) (11/75)
21/05/17 14:09:12 INFO Executor: Running task 15.0 in stage 1.0 (TID 90)
[2021-05-17 11:09:12,719] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 83) in 198 ms on d733d8da4350 (executor driver) (12/75)
[2021-05-17 11:09:12,895] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 14.0 in stage 1.0 (TID 89). 1886 bytes result sent to driver
[2021-05-17 11:09:12,897] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 91) (d733d8da4350, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,898] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 89) in 184 ms on d733d8da4350 (executor driver) (13/75)
[2021-05-17 11:09:12,899] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 15.0 in stage 1.0 (TID 90). 1886 bytes result sent to driver
[2021-05-17 11:09:12,900] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 16.0 in stage 1.0 (TID 91)
[2021-05-17 11:09:12,903] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 12.0 in stage 1.0 (TID 87). 1886 bytes result sent to driver
[2021-05-17 11:09:12,903] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Finished task 13.0 in stage 1.0 (TID 88). 1886 bytes result sent to driver
[2021-05-17 11:09:12,904] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 92) (d733d8da4350, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,906] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 17.0 in stage 1.0 (TID 92)
[2021-05-17 11:09:12,908] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 93) (d733d8da4350, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,909] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 94) (d733d8da4350, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:12,910] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 18.0 in stage 1.0 (TID 93)
[2021-05-17 11:09:12,913] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 90) in 196 ms on d733d8da4350 (executor driver) (14/75)
[2021-05-17 11:09:12,913] {docker.py:276} INFO - 21/05/17 14:09:12 INFO Executor: Running task 19.0 in stage 1.0 (TID 94)
[2021-05-17 11:09:12,914] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 88) in 201 ms on d733d8da4350 (executor driver) (15/75)
[2021-05-17 11:09:12,915] {docker.py:276} INFO - 21/05/17 14:09:12 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 87) in 207 ms on d733d8da4350 (executor driver) (16/75)
[2021-05-17 11:09:13,070] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 16.0 in stage 1.0 (TID 91). 1843 bytes result sent to driver
[2021-05-17 11:09:13,072] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 95) (d733d8da4350, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,073] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 20.0 in stage 1.0 (TID 95)
21/05/17 14:09:13 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 91) in 178 ms on d733d8da4350 (executor driver) (17/75)
[2021-05-17 11:09:13,076] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 17.0 in stage 1.0 (TID 92). 1843 bytes result sent to driver
[2021-05-17 11:09:13,076] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 96) (d733d8da4350, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,078] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 21.0 in stage 1.0 (TID 96)
[2021-05-17 11:09:13,079] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 92) in 174 ms on d733d8da4350 (executor driver) (18/75)
[2021-05-17 11:09:13,084] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 18.0 in stage 1.0 (TID 93). 1843 bytes result sent to driver
[2021-05-17 11:09:13,085] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 19.0 in stage 1.0 (TID 94). 1843 bytes result sent to driver
[2021-05-17 11:09:13,085] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 97) (d733d8da4350, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,086] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 22.0 in stage 1.0 (TID 97)
[2021-05-17 11:09:13,087] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 98) (d733d8da4350, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,088] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 23.0 in stage 1.0 (TID 98)
[2021-05-17 11:09:13,089] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 94) in 180 ms on d733d8da4350 (executor driver) (19/75)
[2021-05-17 11:09:13,090] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 93) in 182 ms on d733d8da4350 (executor driver) (20/75)
[2021-05-17 11:09:13,242] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 20.0 in stage 1.0 (TID 95). 1843 bytes result sent to driver
[2021-05-17 11:09:13,244] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 99) (d733d8da4350, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,246] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 24.0 in stage 1.0 (TID 99)
21/05/17 14:09:13 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 95) in 175 ms on d733d8da4350 (executor driver) (21/75)
[2021-05-17 11:09:13,260] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 22.0 in stage 1.0 (TID 97). 1843 bytes result sent to driver
[2021-05-17 11:09:13,260] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 23.0 in stage 1.0 (TID 98). 1843 bytes result sent to driver
[2021-05-17 11:09:13,261] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 21.0 in stage 1.0 (TID 96). 1843 bytes result sent to driver
[2021-05-17 11:09:13,261] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 100) (d733d8da4350, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,262] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 25.0 in stage 1.0 (TID 100)
[2021-05-17 11:09:13,263] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 97) in 177 ms on d733d8da4350 (executor driver) (22/75)
[2021-05-17 11:09:13,266] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 101) (d733d8da4350, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,266] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 98) in 180 ms on d733d8da4350 (executor driver) (23/75)
[2021-05-17 11:09:13,268] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 26.0 in stage 1.0 (TID 101)
[2021-05-17 11:09:13,269] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 102) (d733d8da4350, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,270] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 96) in 195 ms on d733d8da4350 (executor driver) (24/75)
[2021-05-17 11:09:13,271] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 27.0 in stage 1.0 (TID 102)
[2021-05-17 11:09:13,420] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 24.0 in stage 1.0 (TID 99). 1843 bytes result sent to driver
[2021-05-17 11:09:13,422] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 103) (d733d8da4350, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,423] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 28.0 in stage 1.0 (TID 103)
21/05/17 14:09:13 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 99) in 180 ms on d733d8da4350 (executor driver) (25/75)
[2021-05-17 11:09:13,429] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 25.0 in stage 1.0 (TID 100). 1843 bytes result sent to driver
[2021-05-17 11:09:13,430] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 104) (d733d8da4350, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,431] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 100) in 170 ms on d733d8da4350 (executor driver) (26/75)
[2021-05-17 11:09:13,432] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 29.0 in stage 1.0 (TID 104)
[2021-05-17 11:09:13,442] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 27.0 in stage 1.0 (TID 102). 1843 bytes result sent to driver
[2021-05-17 11:09:13,443] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 105) (d733d8da4350, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,443] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 102) in 175 ms on d733d8da4350 (executor driver) (27/75)
[2021-05-17 11:09:13,444] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 30.0 in stage 1.0 (TID 105)
[2021-05-17 11:09:13,468] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 26.0 in stage 1.0 (TID 101). 1886 bytes result sent to driver
[2021-05-17 11:09:13,469] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 106) (d733d8da4350, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,470] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 101) in 205 ms on d733d8da4350 (executor driver) (28/75)
[2021-05-17 11:09:13,471] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 31.0 in stage 1.0 (TID 106)
[2021-05-17 11:09:13,594] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 28.0 in stage 1.0 (TID 103). 1886 bytes result sent to driver
[2021-05-17 11:09:13,595] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 107) (d733d8da4350, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,596] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 103) in 176 ms on d733d8da4350 (executor driver) (29/75)
[2021-05-17 11:09:13,597] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 32.0 in stage 1.0 (TID 107)
[2021-05-17 11:09:13,599] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 29.0 in stage 1.0 (TID 104). 1886 bytes result sent to driver
[2021-05-17 11:09:13,601] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 108) (d733d8da4350, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,602] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 33.0 in stage 1.0 (TID 108)
[2021-05-17 11:09:13,603] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 104) in 173 ms on d733d8da4350 (executor driver) (30/75)
[2021-05-17 11:09:13,624] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 30.0 in stage 1.0 (TID 105). 1886 bytes result sent to driver
[2021-05-17 11:09:13,625] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 109) (d733d8da4350, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,626] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 34.0 in stage 1.0 (TID 109)
21/05/17 14:09:13 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 105) in 184 ms on d733d8da4350 (executor driver) (31/75)
[2021-05-17 11:09:13,636] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 31.0 in stage 1.0 (TID 106). 1843 bytes result sent to driver
[2021-05-17 11:09:13,637] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 110) (d733d8da4350, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,637] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 106) in 169 ms on d733d8da4350 (executor driver) (32/75)
[2021-05-17 11:09:13,638] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 35.0 in stage 1.0 (TID 110)
[2021-05-17 11:09:13,773] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 32.0 in stage 1.0 (TID 107). 1843 bytes result sent to driver
[2021-05-17 11:09:13,775] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 111) (d733d8da4350, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,777] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 33.0 in stage 1.0 (TID 108). 1843 bytes result sent to driver
[2021-05-17 11:09:13,778] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 107) in 182 ms on d733d8da4350 (executor driver) (33/75)
[2021-05-17 11:09:13,779] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 36.0 in stage 1.0 (TID 111)
[2021-05-17 11:09:13,779] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 112) (d733d8da4350, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,780] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 108) in 180 ms on d733d8da4350 (executor driver) (34/75)
[2021-05-17 11:09:13,781] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 37.0 in stage 1.0 (TID 112)
[2021-05-17 11:09:13,798] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 34.0 in stage 1.0 (TID 109). 1843 bytes result sent to driver
[2021-05-17 11:09:13,799] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 113) (d733d8da4350, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,800] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 109) in 176 ms on d733d8da4350 (executor driver) (35/75)
[2021-05-17 11:09:13,801] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 38.0 in stage 1.0 (TID 113)
[2021-05-17 11:09:13,803] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 35.0 in stage 1.0 (TID 110). 1843 bytes result sent to driver
[2021-05-17 11:09:13,804] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 114) (d733d8da4350, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,805] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 39.0 in stage 1.0 (TID 114)
[2021-05-17 11:09:13,807] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 110) in 171 ms on d733d8da4350 (executor driver) (36/75)
[2021-05-17 11:09:13,948] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 36.0 in stage 1.0 (TID 111). 1843 bytes result sent to driver
[2021-05-17 11:09:13,949] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 37.0 in stage 1.0 (TID 112). 1843 bytes result sent to driver
[2021-05-17 11:09:13,950] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 115) (d733d8da4350, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,951] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 111) in 177 ms on d733d8da4350 (executor driver) (37/75)
[2021-05-17 11:09:13,952] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 116) (d733d8da4350, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/17 14:09:13 INFO Executor: Running task 40.0 in stage 1.0 (TID 115)
[2021-05-17 11:09:13,953] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 41.0 in stage 1.0 (TID 116)
21/05/17 14:09:13 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 112) in 175 ms on d733d8da4350 (executor driver) (38/75)
[2021-05-17 11:09:13,971] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 38.0 in stage 1.0 (TID 113). 1843 bytes result sent to driver
[2021-05-17 11:09:13,972] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 117) (d733d8da4350, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,973] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 113) in 174 ms on d733d8da4350 (executor driver) (39/75)
[2021-05-17 11:09:13,973] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 42.0 in stage 1.0 (TID 117)
[2021-05-17 11:09:13,976] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Finished task 39.0 in stage 1.0 (TID 114). 1843 bytes result sent to driver
[2021-05-17 11:09:13,977] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 118) (d733d8da4350, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:13,978] {docker.py:276} INFO - 21/05/17 14:09:13 INFO Executor: Running task 43.0 in stage 1.0 (TID 118)
[2021-05-17 11:09:13,979] {docker.py:276} INFO - 21/05/17 14:09:13 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 114) in 174 ms on d733d8da4350 (executor driver) (40/75)
[2021-05-17 11:09:14,122] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 40.0 in stage 1.0 (TID 115). 1843 bytes result sent to driver
[2021-05-17 11:09:14,124] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 119) (d733d8da4350, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,125] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 115) in 176 ms on d733d8da4350 (executor driver) (41/75)
[2021-05-17 11:09:14,126] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 44.0 in stage 1.0 (TID 119)
[2021-05-17 11:09:14,127] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 41.0 in stage 1.0 (TID 116). 1843 bytes result sent to driver
[2021-05-17 11:09:14,128] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 120) (d733d8da4350, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,130] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 116) in 178 ms on d733d8da4350 (executor driver) (42/75)
[2021-05-17 11:09:14,131] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 45.0 in stage 1.0 (TID 120)
[2021-05-17 11:09:14,143] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 43.0 in stage 1.0 (TID 118). 1886 bytes result sent to driver
[2021-05-17 11:09:14,144] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 121) (d733d8da4350, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,145] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 118) in 168 ms on d733d8da4350 (executor driver) (43/75)
[2021-05-17 11:09:14,146] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 46.0 in stage 1.0 (TID 121)
[2021-05-17 11:09:14,147] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 42.0 in stage 1.0 (TID 117). 1886 bytes result sent to driver
[2021-05-17 11:09:14,148] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 122) (d733d8da4350, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,149] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 117) in 178 ms on d733d8da4350 (executor driver) (44/75)
21/05/17 14:09:14 INFO Executor: Running task 47.0 in stage 1.0 (TID 122)
[2021-05-17 11:09:14,301] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 44.0 in stage 1.0 (TID 119). 1886 bytes result sent to driver
[2021-05-17 11:09:14,303] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 123) (d733d8da4350, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,304] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 119) in 181 ms on d733d8da4350 (executor driver) (45/75)
[2021-05-17 11:09:14,305] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 45.0 in stage 1.0 (TID 120). 1886 bytes result sent to driver
[2021-05-17 11:09:14,306] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 48.0 in stage 1.0 (TID 123)
21/05/17 14:09:14 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 124) (d733d8da4350, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,307] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 49.0 in stage 1.0 (TID 124)
[2021-05-17 11:09:14,308] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 120) in 180 ms on d733d8da4350 (executor driver) (46/75)
[2021-05-17 11:09:14,316] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 47.0 in stage 1.0 (TID 122). 1843 bytes result sent to driver
[2021-05-17 11:09:14,317] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 125) (d733d8da4350, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,318] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 122) in 170 ms on d733d8da4350 (executor driver) (47/75)
21/05/17 14:09:14 INFO Executor: Running task 50.0 in stage 1.0 (TID 125)
[2021-05-17 11:09:14,320] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 46.0 in stage 1.0 (TID 121). 1843 bytes result sent to driver
[2021-05-17 11:09:14,321] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 126) (d733d8da4350, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,323] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 121) in 179 ms on d733d8da4350 (executor driver) (48/75)
[2021-05-17 11:09:14,323] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 51.0 in stage 1.0 (TID 126)
[2021-05-17 11:09:14,476] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 49.0 in stage 1.0 (TID 124). 1843 bytes result sent to driver
[2021-05-17 11:09:14,478] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 48.0 in stage 1.0 (TID 123). 1843 bytes result sent to driver
[2021-05-17 11:09:14,479] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 127) (d733d8da4350, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,481] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 124) in 175 ms on d733d8da4350 (executor driver) (49/75)
[2021-05-17 11:09:14,482] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 52.0 in stage 1.0 (TID 127)
[2021-05-17 11:09:14,483] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 123) in 182 ms on d733d8da4350 (executor driver) (50/75)
[2021-05-17 11:09:14,485] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 128) (d733d8da4350, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,486] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 53.0 in stage 1.0 (TID 128)
[2021-05-17 11:09:14,487] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 51.0 in stage 1.0 (TID 126). 1843 bytes result sent to driver
[2021-05-17 11:09:14,488] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 129) (d733d8da4350, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,489] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 50.0 in stage 1.0 (TID 125). 1843 bytes result sent to driver
[2021-05-17 11:09:14,490] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 54.0 in stage 1.0 (TID 129)
[2021-05-17 11:09:14,491] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 126) in 169 ms on d733d8da4350 (executor driver) (51/75)
[2021-05-17 11:09:14,492] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 130) (d733d8da4350, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,493] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 125) in 176 ms on d733d8da4350 (executor driver) (52/75)
[2021-05-17 11:09:14,493] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 55.0 in stage 1.0 (TID 130)
[2021-05-17 11:09:14,663] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 52.0 in stage 1.0 (TID 127). 1843 bytes result sent to driver
[2021-05-17 11:09:14,665] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 131) (d733d8da4350, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,666] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 55.0 in stage 1.0 (TID 130). 1843 bytes result sent to driver
[2021-05-17 11:09:14,667] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 56.0 in stage 1.0 (TID 131)
[2021-05-17 11:09:14,668] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 54.0 in stage 1.0 (TID 129). 1843 bytes result sent to driver
21/05/17 14:09:14 INFO Executor: Finished task 53.0 in stage 1.0 (TID 128). 1843 bytes result sent to driver
[2021-05-17 11:09:14,669] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 127) in 189 ms on d733d8da4350 (executor driver) (53/75)
[2021-05-17 11:09:14,671] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 132) (d733d8da4350, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,673] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 133) (d733d8da4350, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,673] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 129) in 185 ms on d733d8da4350 (executor driver) (54/75)
[2021-05-17 11:09:14,674] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 58.0 in stage 1.0 (TID 133)
[2021-05-17 11:09:14,675] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 130) in 184 ms on d733d8da4350 (executor driver) (55/75)
[2021-05-17 11:09:14,676] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 57.0 in stage 1.0 (TID 132)
[2021-05-17 11:09:14,676] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 128) in 191 ms on d733d8da4350 (executor driver) (56/75)
[2021-05-17 11:09:14,677] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 134) (d733d8da4350, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,681] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 59.0 in stage 1.0 (TID 134)
[2021-05-17 11:09:14,850] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 56.0 in stage 1.0 (TID 131). 1843 bytes result sent to driver
[2021-05-17 11:09:14,851] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 57.0 in stage 1.0 (TID 132). 1843 bytes result sent to driver
[2021-05-17 11:09:14,853] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 58.0 in stage 1.0 (TID 133). 1843 bytes result sent to driver
[2021-05-17 11:09:14,853] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 135) (d733d8da4350, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,854] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 131) in 190 ms on d733d8da4350 (executor driver) (57/75)
[2021-05-17 11:09:14,855] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 60.0 in stage 1.0 (TID 135)
[2021-05-17 11:09:14,856] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 136) (d733d8da4350, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,858] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 132) in 187 ms on d733d8da4350 (executor driver) (58/75)
[2021-05-17 11:09:14,858] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 61.0 in stage 1.0 (TID 136)
[2021-05-17 11:09:14,859] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 137) (d733d8da4350, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,860] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 133) in 188 ms on d733d8da4350 (executor driver) (59/75)
[2021-05-17 11:09:14,861] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 62.0 in stage 1.0 (TID 137)
[2021-05-17 11:09:14,877] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Finished task 59.0 in stage 1.0 (TID 134). 1929 bytes result sent to driver
[2021-05-17 11:09:14,878] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 138) (d733d8da4350, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:14,879] {docker.py:276} INFO - 21/05/17 14:09:14 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 134) in 203 ms on d733d8da4350 (executor driver) (60/75)
[2021-05-17 11:09:14,879] {docker.py:276} INFO - 21/05/17 14:09:14 INFO Executor: Running task 63.0 in stage 1.0 (TID 138)
[2021-05-17 11:09:15,040] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 61.0 in stage 1.0 (TID 136). 1886 bytes result sent to driver
[2021-05-17 11:09:15,041] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 139) (d733d8da4350, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,042] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 136) in 187 ms on d733d8da4350 (executor driver) (61/75)
[2021-05-17 11:09:15,044] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 64.0 in stage 1.0 (TID 139)
[2021-05-17 11:09:15,045] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 62.0 in stage 1.0 (TID 137). 1886 bytes result sent to driver
[2021-05-17 11:09:15,046] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 140) (d733d8da4350, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,047] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 65.0 in stage 1.0 (TID 140)
[2021-05-17 11:09:15,048] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 137) in 190 ms on d733d8da4350 (executor driver) (62/75)
[2021-05-17 11:09:15,048] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 60.0 in stage 1.0 (TID 135). 1886 bytes result sent to driver
[2021-05-17 11:09:15,049] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 141) (d733d8da4350, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,050] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 63.0 in stage 1.0 (TID 138). 1843 bytes result sent to driver
[2021-05-17 11:09:15,050] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 135) in 198 ms on d733d8da4350 (executor driver) (63/75)
21/05/17 14:09:15 INFO Executor: Running task 66.0 in stage 1.0 (TID 141)
[2021-05-17 11:09:15,052] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 142) (d733d8da4350, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,052] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 138) in 175 ms on d733d8da4350 (executor driver) (64/75)
[2021-05-17 11:09:15,062] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 67.0 in stage 1.0 (TID 142)
[2021-05-17 11:09:15,220] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 65.0 in stage 1.0 (TID 140). 1843 bytes result sent to driver
21/05/17 14:09:15 INFO Executor: Finished task 64.0 in stage 1.0 (TID 139). 1843 bytes result sent to driver
[2021-05-17 11:09:15,221] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 143) (d733d8da4350, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,223] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 140) in 176 ms on d733d8da4350 (executor driver) (65/75)
[2021-05-17 11:09:15,224] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 68.0 in stage 1.0 (TID 143)
[2021-05-17 11:09:15,225] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 144) (d733d8da4350, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,226] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 69.0 in stage 1.0 (TID 144)
[2021-05-17 11:09:15,227] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 66.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-17 11:09:15,228] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 139) in 186 ms on d733d8da4350 (executor driver) (66/75)
[2021-05-17 11:09:15,229] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 67.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-17 11:09:15,230] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 145) (d733d8da4350, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,231] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 70.0 in stage 1.0 (TID 145)
[2021-05-17 11:09:15,232] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 142) in 179 ms on d733d8da4350 (executor driver) (67/75)
[2021-05-17 11:09:15,233] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 141) in 184 ms on d733d8da4350 (executor driver) (68/75)
[2021-05-17 11:09:15,235] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 146) (d733d8da4350, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,237] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 71.0 in stage 1.0 (TID 146)
[2021-05-17 11:09:15,404] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 69.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-17 11:09:15,406] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 147) (d733d8da4350, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,407] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 71.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-17 11:09:15,408] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 72.0 in stage 1.0 (TID 147)
[2021-05-17 11:09:15,409] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 148) (d733d8da4350, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,410] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 146) in 175 ms on d733d8da4350 (executor driver) (69/75)
[2021-05-17 11:09:15,411] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 73.0 in stage 1.0 (TID 148)
21/05/17 14:09:15 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 144) in 187 ms on d733d8da4350 (executor driver) (70/75)
[2021-05-17 11:09:15,413] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 68.0 in stage 1.0 (TID 143). 1886 bytes result sent to driver
[2021-05-17 11:09:15,414] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 149) (d733d8da4350, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:15,415] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Running task 74.0 in stage 1.0 (TID 149)
[2021-05-17 11:09:15,417] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 143) in 196 ms on d733d8da4350 (executor driver) (71/75)
[2021-05-17 11:09:15,476] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 70.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
[2021-05-17 11:09:15,479] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 145) in 249 ms on d733d8da4350 (executor driver) (72/75)
[2021-05-17 11:09:15,584] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 72.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-17 11:09:15,585] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 147) in 181 ms on d733d8da4350 (executor driver) (73/75)
[2021-05-17 11:09:15,588] {docker.py:276} INFO - 21/05/17 14:09:15 INFO Executor: Finished task 74.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
21/05/17 14:09:15 INFO Executor: Finished task 73.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-17 11:09:15,589] {docker.py:276} INFO - 21/05/17 14:09:15 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 149) in 174 ms on d733d8da4350 (executor driver) (74/75)
21/05/17 14:09:15 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 148) in 181 ms on d733d8da4350 (executor driver) (75/75)
21/05/17 14:09:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-17 11:09:15,591] {docker.py:276} INFO - 21/05/17 14:09:15 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 3.458 s
[2021-05-17 11:09:15,591] {docker.py:276} INFO - 21/05/17 14:09:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 14:09:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-17 11:09:15,592] {docker.py:276} INFO - 21/05/17 14:09:15 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 3.470165 s
[2021-05-17 11:09:15,604] {docker.py:276} INFO - 21/05/17 14:09:15 INFO InMemoryFileIndex: It took 3523 ms to list leaf files for 75 paths.
[2021-05-17 11:09:15,669] {docker.py:276} INFO - 21/05/17 14:09:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on d733d8da4350:35777 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-17 11:09:18,449] {docker.py:276} INFO - 21/05/17 14:09:18 INFO FileSourceStrategy: Pushed Filters:
[2021-05-17 11:09:18,455] {docker.py:276} INFO - 21/05/17 14:09:18 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-17 11:09:18,459] {docker.py:276} INFO - 21/05/17 14:09:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-17 11:09:18,995] {docker.py:276} INFO - 21/05/17 14:09:19 INFO CodeGenerator: Code generated in 273.0235 ms
[2021-05-17 11:09:19,010] {docker.py:276} INFO - 21/05/17 14:09:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-17 11:09:19,030] {docker.py:276} INFO - 21/05/17 14:09:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-17 11:09:19,031] {docker.py:276} INFO - 21/05/17 14:09:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d733d8da4350:35777 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 11:09:19,032] {docker.py:276} INFO - 21/05/17 14:09:19 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:19,048] {docker.py:276} INFO - 21/05/17 14:09:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 80530548 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 11:09:19,162] {docker.py:276} INFO - 21/05/17 14:09:19 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:19,164] {docker.py:276} INFO - 21/05/17 14:09:19 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-17 11:09:19,164] {docker.py:276} INFO - 21/05/17 14:09:19 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 11:09:19,165] {docker.py:276} INFO - 21/05/17 14:09:19 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 11:09:19,166] {docker.py:276} INFO - 21/05/17 14:09:19 INFO DAGScheduler: Missing parents: List()
[2021-05-17 11:09:19,167] {docker.py:276} INFO - 21/05/17 14:09:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 11:09:19,194] {docker.py:276} INFO - 21/05/17 14:09:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-17 11:09:19,216] {docker.py:276} INFO - 21/05/17 14:09:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-17 11:09:19,217] {docker.py:276} INFO - 21/05/17 14:09:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on d733d8da4350:35777 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-17 11:09:19,217] {docker.py:276} INFO - 21/05/17 14:09:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-17 11:09:19,218] {docker.py:276} INFO - 21/05/17 14:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/17 14:09:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-17 11:09:19,222] {docker.py:276} INFO - 21/05/17 14:09:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 150) (d733d8da4350, executor driver, partition 0, PROCESS_LOCAL, 6885 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:19,224] {docker.py:276} INFO - 21/05/17 14:09:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 150)
[2021-05-17 11:09:19,331] {docker.py:276} INFO - 21/05/17 14:09:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621228106_to_1621229906.csv, range: 0-104308, partition values: [empty row]
[2021-05-17 11:09:19,358] {docker.py:276} INFO - 21/05/17 14:09:19 INFO CodeGenerator: Code generated in 18.1762 ms
[2021-05-17 11:09:20,112] {docker.py:276} INFO - 21/05/17 14:09:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 150). 1564 bytes result sent to driver
[2021-05-17 11:09:20,113] {docker.py:276} INFO - 21/05/17 14:09:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 150) in 894 ms on d733d8da4350 (executor driver) (1/1)
21/05/17 14:09:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-17 11:09:20,114] {docker.py:276} INFO - 21/05/17 14:09:20 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.944 s
[2021-05-17 11:09:20,115] {docker.py:276} INFO - 21/05/17 14:09:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-17 11:09:20,115] {docker.py:276} INFO - 21/05/17 14:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-17 11:09:20,116] {docker.py:276} INFO - 21/05/17 14:09:20 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.954092 s
[2021-05-17 11:09:20,147] {docker.py:276} INFO - 21/05/17 14:09:20 INFO CodeGenerator: Code generated in 14.1373 ms
[2021-05-17 11:09:20,215] {docker.py:276} INFO - 21/05/17 14:09:20 INFO FileSourceStrategy: Pushed Filters: 
21/05/17 14:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-17 11:09:20,215] {docker.py:276} INFO - 21/05/17 14:09:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-17 11:09:20,222] {docker.py:276} INFO - 21/05/17 14:09:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-17 11:09:20,255] {docker.py:276} INFO - 21/05/17 14:09:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on d733d8da4350:35777 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-17 11:09:20,256] {docker.py:276} INFO - 21/05/17 14:09:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-17 11:09:20,257] {docker.py:276} INFO - 21/05/17 14:09:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on d733d8da4350:35777 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 11:09:20,258] {docker.py:276} INFO - 21/05/17 14:09:20 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:20,262] {docker.py:276} INFO - 21/05/17 14:09:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 80530548 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 11:09:20,876] {docker.py:276} INFO - 21/05/17 14:09:20 INFO FileSourceStrategy: Pushed Filters: 
21/05/17 14:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-17 11:09:20,877] {docker.py:276} INFO - 21/05/17 14:09:20 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-17 11:09:21,512] {docker.py:276} INFO - 21/05/17 14:09:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:21,515] {docker.py:276} INFO - 21/05/17 14:09:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:21,516] {docker.py:276} INFO - 21/05/17 14:09:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217135854710666598768_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217135854710666598768_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217135854710666598768_0000}; taskId=attempt_202105171409217135854710666598768_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c11f5fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:21,516] {docker.py:276} INFO - 21/05/17 14:09:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:21,552] {docker.py:276} INFO - 21/05/17 14:09:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-17 11:09:21,646] {docker.py:276} INFO - 21/05/17 14:09:21 INFO CodeGenerator: Code generated in 62.2505 ms
[2021-05-17 11:09:21,648] {docker.py:276} INFO - 21/05/17 14:09:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-17 11:09:21,714] {docker.py:276} INFO - 21/05/17 14:09:21 INFO CodeGenerator: Code generated in 38.3908 ms
[2021-05-17 11:09:21,720] {docker.py:276} INFO - 21/05/17 14:09:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-17 11:09:21,725] {docker.py:276} INFO - 21/05/17 14:09:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-17 11:09:21,727] {docker.py:276} INFO - 21/05/17 14:09:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on d733d8da4350:35777 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 11:09:21,728] {docker.py:276} INFO - 21/05/17 14:09:21 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:21,736] {docker.py:276} INFO - 21/05/17 14:09:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 80530548 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 11:09:21,819] {docker.py:276} INFO - 21/05/17 14:09:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on d733d8da4350:35777 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 11:09:21,872] {docker.py:276} INFO - 21/05/17 14:09:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 11:09:21,877] {docker.py:276} INFO - 21/05/17 14:09:21 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-17 11:09:21,880] {docker.py:276} INFO - 21/05/17 14:09:21 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/17 14:09:21 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
21/05/17 14:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-17 11:09:21,882] {docker.py:276} INFO - 21/05/17 14:09:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-17 11:09:21,884] {docker.py:276} INFO - 21/05/17 14:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 11:09:21,897] {docker.py:276} INFO - 21/05/17 14:09:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-17 11:09:21,909] {docker.py:276} INFO - 21/05/17 14:09:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-17 11:09:21,910] {docker.py:276} INFO - 21/05/17 14:09:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on d733d8da4350:35777 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-17 11:09:21,911] {docker.py:276} INFO - 21/05/17 14:09:21 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-17 11:09:21,913] {docker.py:276} INFO - 21/05/17 14:09:21 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21/05/17 14:09:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0
[2021-05-17 11:09:21,916] {docker.py:276} INFO - 21/05/17 14:09:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 151) (d733d8da4350, executor driver, partition 0, PROCESS_LOCAL, 6874 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:21,917] {docker.py:276} INFO - 21/05/17 14:09:21 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 152) (d733d8da4350, executor driver, partition 1, PROCESS_LOCAL, 6874 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:21,918] {docker.py:276} INFO - 21/05/17 14:09:21 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 153) (d733d8da4350, executor driver, partition 2, PROCESS_LOCAL, 6874 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:21,919] {docker.py:276} INFO - 21/05/17 14:09:21 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 154) (d733d8da4350, executor driver, partition 3, PROCESS_LOCAL, 6764 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:21,920] {docker.py:276} INFO - 21/05/17 14:09:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 151)
[2021-05-17 11:09:21,921] {docker.py:276} INFO - 21/05/17 14:09:21 INFO Executor: Running task 2.0 in stage 3.0 (TID 153)
[2021-05-17 11:09:21,922] {docker.py:276} INFO - 21/05/17 14:09:21 INFO Executor: Running task 3.0 in stage 3.0 (TID 154)
[2021-05-17 11:09:21,923] {docker.py:276} INFO - 21/05/17 14:09:21 INFO Executor: Running task 1.0 in stage 3.0 (TID 152)
[2021-05-17 11:09:22,076] {docker.py:276} INFO - 21/05/17 14:09:22 INFO BlockManagerInfo: Removed broadcast_4_piece0 on d733d8da4350:35777 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 11:09:22,120] {docker.py:276} INFO - 21/05/17 14:09:22 INFO CodeGenerator: Code generated in 96.6908 ms
[2021-05-17 11:09:22,165] {docker.py:276} INFO - 21/05/17 14:09:22 INFO CodeGenerator: Code generated in 17.5078 ms
[2021-05-17 11:09:22,211] {docker.py:276} INFO - 21/05/17 14:09:22 INFO CodeGenerator: Code generated in 35.1956 ms
[2021-05-17 11:09:22,237] {docker.py:276} INFO - 21/05/17 14:09:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621249706_to_1621251506.csv, range: 0-103913, partition values: [empty row]
[2021-05-17 11:09:22,238] {docker.py:276} INFO - 21/05/17 14:09:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621238906_to_1621240706.csv, range: 0-104229, partition values: [empty row]
[2021-05-17 11:09:22,245] {docker.py:276} INFO - 21/05/17 14:09:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621228106_to_1621229906.csv, range: 0-104308, partition values: [empty row]
[2021-05-17 11:09:22,245] {docker.py:276} INFO - 21/05/17 14:09:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621246106_to_1621247906.csv, range: 0-104151, partition values: [empty row]
[2021-05-17 11:09:23,482] {docker.py:276} INFO - 21/05/17 14:09:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621244306_to_1621246106.csv, range: 0-104226, partition values: [empty row]
[2021-05-17 11:09:24,031] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621242506_to_1621244306.csv, range: 0-104133, partition values: [empty row]
[2021-05-17 11:09:24,032] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621231706_to_1621233506.csv, range: 0-104303, partition values: [empty row]
[2021-05-17 11:09:24,043] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621215506_to_1621217306.csv, range: 0-104223, partition values: [empty row]
[2021-05-17 11:09:24,208] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621220906_to_1621222706.csv, range: 0-103910, partition values: [empty row]
[2021-05-17 11:09:24,548] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621253306_to_1621255106.csv, range: 0-104300, partition values: [empty row]
[2021-05-17 11:09:24,559] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621255106_to_1621256906.csv, range: 0-104223, partition values: [empty row]
[2021-05-17 11:09:24,578] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621228106_to_1621229906.csv, range: 0-104123, partition values: [empty row]
[2021-05-17 11:09:24,903] {docker.py:276} INFO - 21/05/17 14:09:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621255106_to_1621256906.csv, range: 0-103878, partition values: [empty row]
[2021-05-17 11:09:24,980] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621219106_to_1621220906.csv, range: 0-104294, partition values: [empty row]
[2021-05-17 11:09:24,985] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621222706_to_1621224506.csv, range: 0-104119, partition values: [empty row]
[2021-05-17 11:09:24,986] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621215506_to_1621217306.csv, range: 0-104208, partition values: [empty row]
[2021-05-17 11:09:25,447] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621219106_to_1621220906.csv, range: 0-103849, partition values: [empty row]
[2021-05-17 11:09:25,482] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621249706_to_1621251506.csv, range: 0-104206, partition values: [empty row]
[2021-05-17 11:09:25,483] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621217306_to_1621219106.csv, range: 0-104113, partition values: [empty row]
[2021-05-17 11:09:25,493] {docker.py:276} INFO - 21/05/17 14:09:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621249706_to_1621251506.csv, range: 0-104292, partition values: [empty row]
[2021-05-17 11:09:26,017] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621237106_to_1621238906.csv, range: 0-103834, partition values: [empty row]
[2021-05-17 11:09:26,030] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621242506_to_1621244306.csv, range: 0-104204, partition values: [empty row]
[2021-05-17 11:09:26,036] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621247906_to_1621249706.csv, range: 0-104292, partition values: [empty row]
[2021-05-17 11:09:26,044] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621229906_to_1621231706.csv, range: 0-104107, partition values: [empty row]
[2021-05-17 11:09:26,438] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621231706_to_1621233506.csv, range: 0-104202, partition values: [empty row]
[2021-05-17 11:09:26,443] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621240706_to_1621242506.csv, range: 0-104284, partition values: [empty row]
[2021-05-17 11:09:26,455] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621224506_to_1621226306.csv, range: 0-104098, partition values: [empty row]
[2021-05-17 11:09:26,531] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621253306_to_1621255106.csv, range: 0-103834, partition values: [empty row]
[2021-05-17 11:09:26,804] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621251506_to_1621253306.csv, range: 0-104201, partition values: [empty row]
[2021-05-17 11:09:26,809] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621238906_to_1621240706.csv, range: 0-104278, partition values: [empty row]
[2021-05-17 11:09:26,814] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621253306_to_1621255106.csv, range: 0-104093, partition values: [empty row]
[2021-05-17 11:09:26,891] {docker.py:276} INFO - 21/05/17 14:09:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621251506_to_1621253306.csv, range: 0-103833, partition values: [empty row]
[2021-05-17 11:09:27,172] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621217306_to_1621219106.csv, range: 0-104073, partition values: [empty row]
[2021-05-17 11:09:27,177] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621235306_to_1621237106.csv, range: 0-104276, partition values: [empty row]
[2021-05-17 11:09:27,177] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621255106_to_1621256906.csv, range: 0-104187, partition values: [empty row]
[2021-05-17 11:09:27,249] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621246106_to_1621247906.csv, range: 0-103823, partition values: [empty row]
[2021-05-17 11:09:27,537] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621228106_to_1621229906.csv, range: 0-104063, partition values: [empty row]
[2021-05-17 11:09:27,542] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621240706_to_1621242506.csv, range: 0-104274, partition values: [empty row]
[2021-05-17 11:09:27,548] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621256906_to_1621258706.csv, range: 0-104185, partition values: [empty row]
[2021-05-17 11:09:27,601] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621244306_to_1621246106.csv, range: 0-103810, partition values: [empty row]
[2021-05-17 11:09:27,916] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621224506_to_1621226306.csv, range: 0-104041, partition values: [empty row]
[2021-05-17 11:09:27,917] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621244306_to_1621246106.csv, range: 0-104264, partition values: [empty row]
[2021-05-17 11:09:27,919] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621222706_to_1621224506.csv, range: 0-104182, partition values: [empty row]
[2021-05-17 11:09:27,950] {docker.py:276} INFO - 21/05/17 14:09:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621256906_to_1621258706.csv, range: 0-103796, partition values: [empty row]
[2021-05-17 11:09:28,275] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621256906_to_1621258706.csv, range: 0-104034, partition values: [empty row]
[2021-05-17 11:09:28,278] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621233506_to_1621235306.csv, range: 0-104181, partition values: [empty row]
[2021-05-17 11:09:28,283] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621226306_to_1621228106.csv, range: 0-104264, partition values: [empty row]
[2021-05-17 11:09:28,301] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621231706_to_1621233506.csv, range: 0-103779, partition values: [empty row]
[2021-05-17 11:09:28,640] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621229906_to_1621231706.csv, range: 0-104180, partition values: [empty row]
[2021-05-17 11:09:28,641] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621215506_to_1621217306.csv, range: 0-104014, partition values: [empty row]
[2021-05-17 11:09:28,646] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621246106_to_1621247906.csv, range: 0-104263, partition values: [empty row]
[2021-05-17 11:09:28,648] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621242506_to_1621244306.csv, range: 0-103764, partition values: [empty row]
[2021-05-17 11:09:28,992] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621233506_to_1621235306.csv, range: 0-104262, partition values: [empty row]
[2021-05-17 11:09:28,995] {docker.py:276} INFO - 21/05/17 14:09:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621247906_to_1621249706.csv, range: 0-104178, partition values: [empty row]
[2021-05-17 11:09:28,997] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621226306_to_1621228106.csv, range: 0-104003, partition values: [empty row]
[2021-05-17 11:09:29,017] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621233506_to_1621235306.csv, range: 0-103757, partition values: [empty row]
[2021-05-17 11:09:29,334] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621235306_to_1621237106.csv, range: 0-104251, partition values: [empty row]
[2021-05-17 11:09:29,344] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621222706_to_1621224506.csv, range: 0-104177, partition values: [empty row]
[2021-05-17 11:09:29,349] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621247906_to_1621249706.csv, range: 0-103968, partition values: [empty row]
[2021-05-17 11:09:29,377] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621238906_to_1621240706.csv, range: 0-103648, partition values: [empty row]
[2021-05-17 11:09:29,698] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621237106_to_1621238906.csv, range: 0-104173, partition values: [empty row]
[2021-05-17 11:09:29,699] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621240706_to_1621242506.csv, range: 0-103953, partition values: [empty row]
21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621251506_to_1621253306.csv, range: 0-104250, partition values: [empty row]
[2021-05-17 11:09:29,732] {docker.py:276} INFO - 21/05/17 14:09:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621217306_to_1621219106.csv, range: 0-103641, partition values: [empty row]
[2021-05-17 11:09:30,042] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621220906_to_1621222706.csv, range: 0-104243, partition values: [empty row]
[2021-05-17 11:09:30,050] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621229906_to_1621231706.csv, range: 0-103950, partition values: [empty row]
[2021-05-17 11:09:30,052] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621237106_to_1621238906.csv, range: 0-104171, partition values: [empty row]
[2021-05-17 11:09:30,098] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621215193_to_1621215506.csv, range: 0-18209, partition values: [empty row]
[2021-05-17 11:09:30,395] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621226306_to_1621228106.csv, range: 0-104231, partition values: [empty row]
[2021-05-17 11:09:30,403] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_11_08_26/from_1621219106_to_1621220906.csv, range: 0-104171, partition values: [empty row]
[2021-05-17 11:09:30,404] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621235306_to_1621237106.csv, range: 0-103928, partition values: [empty row]
[2021-05-17 11:09:30,444] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621215193_to_1621215506.csv, range: 0-18187, partition values: [empty row]
[2021-05-17 11:09:30,761] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621224506_to_1621226306.csv, range: 0-103914, partition values: [empty row]
[2021-05-17 11:09:30,799] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_11_08_26/from_1621215193_to_1621215506.csv, range: 0-18186, partition values: [empty row]
[2021-05-17 11:09:30,805] {docker.py:276} INFO - 21/05/17 14:09:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_11_08_26/from_1621220906_to_1621222706.csv, range: 0-104229, partition values: [empty row]
[2021-05-17 11:09:31,433] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Finished task 1.0 in stage 3.0 (TID 152). 2722 bytes result sent to driver
[2021-05-17 11:09:31,437] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 152) in 9496 ms on d733d8da4350 (executor driver) (1/4)
[2021-05-17 11:09:31,608] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Finished task 0.0 in stage 3.0 (TID 151). 2679 bytes result sent to driver
[2021-05-17 11:09:31,609] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 151) in 9671 ms on d733d8da4350 (executor driver) (2/4)
[2021-05-17 11:09:31,627] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Finished task 3.0 in stage 3.0 (TID 154). 2679 bytes result sent to driver
[2021-05-17 11:09:31,628] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 154) in 9686 ms on d733d8da4350 (executor driver) (3/4)
[2021-05-17 11:09:31,645] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Finished task 2.0 in stage 3.0 (TID 153). 2679 bytes result sent to driver
[2021-05-17 11:09:31,646] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 153) in 9705 ms on d733d8da4350 (executor driver) (4/4)
[2021-05-17 11:09:31,647] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-17 11:09:31,648] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 9.735 s
[2021-05-17 11:09:31,649] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: looking for newly runnable stages
[2021-05-17 11:09:31,650] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: running: Set()
[2021-05-17 11:09:31,651] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-17 11:09:31,652] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: failed: Set()
[2021-05-17 11:09:31,658] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 11:09:31,705] {docker.py:276} INFO - 21/05/17 14:09:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-17 11:09:31,730] {docker.py:276} INFO - 21/05/17 14:09:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-17 11:09:31,731] {docker.py:276} INFO - 21/05/17 14:09:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on d733d8da4350:35777 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-17 11:09:31,732] {docker.py:276} INFO - 21/05/17 14:09:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-17 11:09:31,734] {docker.py:276} INFO - 21/05/17 14:09:31 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/17 14:09:31 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-17 11:09:31,748] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 155) (d733d8da4350, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:31,748] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 156) (d733d8da4350, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:31,748] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 157) (d733d8da4350, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:31,749] {docker.py:276} INFO - 21/05/17 14:09:31 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 158) (d733d8da4350, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:31,749] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Running task 0.0 in stage 4.0 (TID 155)
[2021-05-17 11:09:31,749] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Running task 1.0 in stage 4.0 (TID 156)
[2021-05-17 11:09:31,749] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Running task 3.0 in stage 4.0 (TID 158)
[2021-05-17 11:09:31,750] {docker.py:276} INFO - 21/05/17 14:09:31 INFO Executor: Running task 2.0 in stage 4.0 (TID 157)
[2021-05-17 11:09:31,836] {docker.py:276} INFO - 21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:31,840] {docker.py:276} INFO - 21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:31,840] {docker.py:276} INFO - 21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:31,859] {docker.py:276} INFO - 21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 35 ms
[2021-05-17 11:09:31,860] {docker.py:276} INFO - 21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms
21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms
[2021-05-17 11:09:31,861] {docker.py:276} INFO - 21/05/17 14:09:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms
[2021-05-17 11:09:31,882] {docker.py:276} INFO - 21/05/17 14:09:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:31,883] {docker.py:276} INFO - 21/05/17 14:09:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:31,886] {docker.py:276} INFO - 21/05/17 14:09:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:31,886] {docker.py:276} INFO - 21/05/17 14:09:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:31,886] {docker.py:276} INFO - 21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:31,887] {docker.py:276} INFO - 21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214874097418910248788_0004_m_000003_158, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214874097418910248788_0004_m_000003_158}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214874097418910248788_0004}; taskId=attempt_202105171409214874097418910248788_0004_m_000003_158, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4037d0cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:31,887] {docker.py:276} INFO - 21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212124450413317342240_0004_m_000001_156, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212124450413317342240_0004_m_000001_156}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212124450413317342240_0004}; taskId=attempt_202105171409212124450413317342240_0004_m_000001_156, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5716c1fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:31,887] {docker.py:276} INFO - 21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216995866713162025348_0004_m_000002_157, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216995866713162025348_0004_m_000002_157}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216995866713162025348_0004}; taskId=attempt_202105171409216995866713162025348_0004_m_000002_157, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7535f9f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:31,888] {docker.py:276} INFO - 21/05/17 14:09:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:31,889] {docker.py:276} INFO - 21/05/17 14:09:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:31,890] {docker.py:276} INFO - 21/05/17 14:09:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:31,890] {docker.py:276} INFO - 21/05/17 14:09:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216842911629282455468_0004_m_000000_155, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216842911629282455468_0004_m_000000_155}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216842911629282455468_0004}; taskId=attempt_202105171409216842911629282455468_0004_m_000000_155, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d40186b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:31,891] {docker.py:276} INFO - 21/05/17 14:09:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:31,891] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409216842911629282455468_0004_m_000000_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216842911629282455468_0004_m_000000_155
[2021-05-17 11:09:31,892] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409212124450413317342240_0004_m_000001_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212124450413317342240_0004_m_000001_156
[2021-05-17 11:09:31,892] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409214874097418910248788_0004_m_000003_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214874097418910248788_0004_m_000003_158 
21/05/17 14:09:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409216995866713162025348_0004_m_000002_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216995866713162025348_0004_m_000002_157
[2021-05-17 11:09:31,951] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Task committer attempt_202105171409216842911629282455468_0004_m_000000_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216842911629282455468_0004_m_000000_155 : duration 0:00.061s
[2021-05-17 11:09:31,959] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Task committer attempt_202105171409216995866713162025348_0004_m_000002_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216995866713162025348_0004_m_000002_157 : duration 0:00.068s
[2021-05-17 11:09:31,960] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Task committer attempt_202105171409212124450413317342240_0004_m_000001_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212124450413317342240_0004_m_000001_156 : duration 0:00.069s
[2021-05-17 11:09:31,961] {docker.py:276} INFO - 21/05/17 14:09:31 INFO StagingCommitter: Task committer attempt_202105171409214874097418910248788_0004_m_000003_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214874097418910248788_0004_m_000003_158 : duration 0:00.070s
[2021-05-17 11:09:33,916] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409214874097418910248788_0004_m_000003_158: needsTaskCommit() Task attempt_202105171409214874097418910248788_0004_m_000003_158
[2021-05-17 11:09:33,916] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Task committer attempt_202105171409214874097418910248788_0004_m_000003_158: needsTaskCommit() Task attempt_202105171409214874097418910248788_0004_m_000003_158: duration 0:00.001s
[2021-05-17 11:09:33,917] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409212124450413317342240_0004_m_000001_156: needsTaskCommit() Task attempt_202105171409212124450413317342240_0004_m_000001_156
[2021-05-17 11:09:33,917] {docker.py:276} INFO - 21/05/17 14:09:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214874097418910248788_0004_m_000003_158
21/05/17 14:09:33 INFO StagingCommitter: Task committer attempt_202105171409212124450413317342240_0004_m_000001_156: needsTaskCommit() Task attempt_202105171409212124450413317342240_0004_m_000001_156: duration 0:00.000s
[2021-05-17 11:09:33,918] {docker.py:276} INFO - 21/05/17 14:09:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212124450413317342240_0004_m_000001_156
[2021-05-17 11:09:33,925] {docker.py:276} INFO - 21/05/17 14:09:33 INFO Executor: Finished task 1.0 in stage 4.0 (TID 156). 4587 bytes result sent to driver
[2021-05-17 11:09:33,926] {docker.py:276} INFO - 21/05/17 14:09:33 INFO Executor: Finished task 3.0 in stage 4.0 (TID 158). 4587 bytes result sent to driver
[2021-05-17 11:09:33,928] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409216842911629282455468_0004_m_000000_155: needsTaskCommit() Task attempt_202105171409216842911629282455468_0004_m_000000_155
[2021-05-17 11:09:33,928] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Task committer attempt_202105171409216842911629282455468_0004_m_000000_155: needsTaskCommit() Task attempt_202105171409216842911629282455468_0004_m_000000_155: duration 0:00.001s
21/05/17 14:09:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216842911629282455468_0004_m_000000_155
[2021-05-17 11:09:33,928] {docker.py:276} INFO - 21/05/17 14:09:33 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 159) (d733d8da4350, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:33,931] {docker.py:276} INFO - 21/05/17 14:09:33 INFO Executor: Finished task 0.0 in stage 4.0 (TID 155). 4544 bytes result sent to driver
[2021-05-17 11:09:33,933] {docker.py:276} INFO - 21/05/17 14:09:33 INFO Executor: Running task 4.0 in stage 4.0 (TID 159)
21/05/17 14:09:33 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 160) (d733d8da4350, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:33,934] {docker.py:276} INFO - 21/05/17 14:09:33 INFO Executor: Running task 5.0 in stage 4.0 (TID 160)
[2021-05-17 11:09:33,935] {docker.py:276} INFO - 21/05/17 14:09:33 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 161) (d733d8da4350, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:33,936] {docker.py:276} INFO - 21/05/17 14:09:33 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 156) in 2197 ms on d733d8da4350 (executor driver) (1/200)
[2021-05-17 11:09:33,937] {docker.py:276} INFO - 21/05/17 14:09:33 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 158) in 2197 ms on d733d8da4350 (executor driver) (2/200)
[2021-05-17 11:09:33,937] {docker.py:276} INFO - 21/05/17 14:09:33 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 155) in 2200 ms on d733d8da4350 (executor driver) (3/200)
[2021-05-17 11:09:33,945] {docker.py:276} INFO - 21/05/17 14:09:33 INFO Executor: Running task 6.0 in stage 4.0 (TID 161)
[2021-05-17 11:09:33,945] {docker.py:276} INFO - 21/05/17 14:09:33 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:33,946] {docker.py:276} INFO - 21/05/17 14:09:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:33,948] {docker.py:276} INFO - 21/05/17 14:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:33 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:33,949] {docker.py:276} INFO - 21/05/17 14:09:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:33,949] {docker.py:276} INFO - 21/05/17 14:09:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 14:09:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921986891754377481636_0004_m_000004_159, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921986891754377481636_0004_m_000004_159}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921986891754377481636_0004}; taskId=attempt_20210517140921986891754377481636_0004_m_000004_159, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27dc7780}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:33,950] {docker.py:276} INFO - 21/05/17 14:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:33,950] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_20210517140921986891754377481636_0004_m_000004_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921986891754377481636_0004_m_000004_159
[2021-05-17 11:09:33,953] {docker.py:276} INFO - 21/05/17 14:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:33,956] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Task committer attempt_20210517140921986891754377481636_0004_m_000004_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921986891754377481636_0004_m_000004_159 : duration 0:00.008s
[2021-05-17 11:09:33,958] {docker.py:276} INFO - 21/05/17 14:09:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:33,959] {docker.py:276} INFO - 21/05/17 14:09:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217073750404616175606_0004_m_000005_160, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217073750404616175606_0004_m_000005_160}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217073750404616175606_0004}; taskId=attempt_202105171409217073750404616175606_0004_m_000005_160, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22803e02}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:33,960] {docker.py:276} INFO - 21/05/17 14:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:33,962] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409217073750404616175606_0004_m_000005_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217073750404616175606_0004_m_000005_160
[2021-05-17 11:09:33,965] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Task committer attempt_202105171409217073750404616175606_0004_m_000005_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217073750404616175606_0004_m_000005_160 : duration 0:00.005s
[2021-05-17 11:09:33,968] {docker.py:276} INFO - 21/05/17 14:09:33 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:33,968] {docker.py:276} INFO - 21/05/17 14:09:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:33,970] {docker.py:276} INFO - 21/05/17 14:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:33,971] {docker.py:276} INFO - 21/05/17 14:09:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212741741090813666679_0004_m_000006_161, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212741741090813666679_0004_m_000006_161}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212741741090813666679_0004}; taskId=attempt_202105171409212741741090813666679_0004_m_000006_161, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c79cff4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:33,971] {docker.py:276} INFO - 21/05/17 14:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409212741741090813666679_0004_m_000006_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212741741090813666679_0004_m_000006_161
[2021-05-17 11:09:33,980] {docker.py:276} INFO - 21/05/17 14:09:33 INFO StagingCommitter: Task committer attempt_202105171409212741741090813666679_0004_m_000006_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212741741090813666679_0004_m_000006_161 : duration 0:00.008s
[2021-05-17 11:09:34,211] {docker.py:276} INFO - 21/05/17 14:09:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409216995866713162025348_0004_m_000002_157: needsTaskCommit() Task attempt_202105171409216995866713162025348_0004_m_000002_157
[2021-05-17 11:09:34,212] {docker.py:276} INFO - 21/05/17 14:09:34 INFO StagingCommitter: Task committer attempt_202105171409216995866713162025348_0004_m_000002_157: needsTaskCommit() Task attempt_202105171409216995866713162025348_0004_m_000002_157: duration 0:00.002s
[2021-05-17 11:09:34,213] {docker.py:276} INFO - 21/05/17 14:09:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216995866713162025348_0004_m_000002_157
[2021-05-17 11:09:34,215] {docker.py:276} INFO - 21/05/17 14:09:34 INFO Executor: Finished task 2.0 in stage 4.0 (TID 157). 4544 bytes result sent to driver
[2021-05-17 11:09:34,216] {docker.py:276} INFO - 21/05/17 14:09:34 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 162) (d733d8da4350, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:34,217] {docker.py:276} INFO - 21/05/17 14:09:34 INFO Executor: Running task 7.0 in stage 4.0 (TID 162)
21/05/17 14:09:34 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 157) in 2478 ms on d733d8da4350 (executor driver) (4/200)
[2021-05-17 11:09:34,230] {docker.py:276} INFO - 21/05/17 14:09:34 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:34,230] {docker.py:276} INFO - 21/05/17 14:09:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:34,234] {docker.py:276} INFO - 21/05/17 14:09:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:34,235] {docker.py:276} INFO - 21/05/17 14:09:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215788918897167857425_0004_m_000007_162, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215788918897167857425_0004_m_000007_162}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215788918897167857425_0004}; taskId=attempt_202105171409215788918897167857425_0004_m_000007_162, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c2f5936}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409215788918897167857425_0004_m_000007_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215788918897167857425_0004_m_000007_162
[2021-05-17 11:09:34,239] {docker.py:276} INFO - 21/05/17 14:09:34 INFO StagingCommitter: Task committer attempt_202105171409215788918897167857425_0004_m_000007_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215788918897167857425_0004_m_000007_162 : duration 0:00.006s
[2021-05-17 11:09:36,258] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409212741741090813666679_0004_m_000006_161: needsTaskCommit() Task attempt_202105171409212741741090813666679_0004_m_000006_161
[2021-05-17 11:09:36,259] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_202105171409212741741090813666679_0004_m_000006_161: needsTaskCommit() Task attempt_202105171409212741741090813666679_0004_m_000006_161: duration 0:00.002s
[2021-05-17 11:09:36,259] {docker.py:276} INFO - 21/05/17 14:09:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212741741090813666679_0004_m_000006_161
[2021-05-17 11:09:36,261] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Finished task 6.0 in stage 4.0 (TID 161). 4587 bytes result sent to driver
[2021-05-17 11:09:36,262] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 163) (d733d8da4350, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:36,263] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 161) in 2332 ms on d733d8da4350 (executor driver) (5/200)
[2021-05-17 11:09:36,264] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Running task 8.0 in stage 4.0 (TID 163)
[2021-05-17 11:09:36,280] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_20210517140921986891754377481636_0004_m_000004_159: needsTaskCommit() Task attempt_20210517140921986891754377481636_0004_m_000004_159
[2021-05-17 11:09:36,280] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_20210517140921986891754377481636_0004_m_000004_159: needsTaskCommit() Task attempt_20210517140921986891754377481636_0004_m_000004_159: duration 0:00.001s
21/05/17 14:09:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921986891754377481636_0004_m_000004_159
[2021-05-17 11:09:36,281] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Getting 4 (21.6 KiB) non-empty blocks including 4 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:36,282] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:36,283] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Finished task 4.0 in stage 4.0 (TID 159). 4587 bytes result sent to driver
[2021-05-17 11:09:36,283] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 164) (d733d8da4350, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:36,284] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 159) in 2359 ms on d733d8da4350 (executor driver) (6/200)
[2021-05-17 11:09:36,285] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Running task 9.0 in stage 4.0 (TID 164)
[2021-05-17 11:09:36,286] {docker.py:276} INFO - 21/05/17 14:09:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:36,287] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:36,287] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211128272282767164408_0004_m_000008_163, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211128272282767164408_0004_m_000008_163}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211128272282767164408_0004}; taskId=attempt_202105171409211128272282767164408_0004_m_000008_163, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30a28c57}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:36,287] {docker.py:276} INFO - 21/05/17 14:09:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:36,288] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409211128272282767164408_0004_m_000008_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211128272282767164408_0004_m_000008_163
[2021-05-17 11:09:36,293] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_202105171409211128272282767164408_0004_m_000008_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211128272282767164408_0004_m_000008_163 : duration 0:00.005s
[2021-05-17 11:09:36,297] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409217073750404616175606_0004_m_000005_160: needsTaskCommit() Task attempt_202105171409217073750404616175606_0004_m_000005_160
[2021-05-17 11:09:36,298] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_202105171409217073750404616175606_0004_m_000005_160: needsTaskCommit() Task attempt_202105171409217073750404616175606_0004_m_000005_160: duration 0:00.003s
21/05/17 14:09:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217073750404616175606_0004_m_000005_160
[2021-05-17 11:09:36,298] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Finished task 5.0 in stage 4.0 (TID 160). 4587 bytes result sent to driver
[2021-05-17 11:09:36,299] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:36,300] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:36,301] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 165) (d733d8da4350, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:36,302] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Running task 10.0 in stage 4.0 (TID 165)
[2021-05-17 11:09:36,303] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 160) in 2373 ms on d733d8da4350 (executor driver) (7/200)
[2021-05-17 11:09:36,303] {docker.py:276} INFO - 21/05/17 14:09:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:36,304] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:36,304] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213928207437458838450_0004_m_000009_164, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213928207437458838450_0004_m_000009_164}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213928207437458838450_0004}; taskId=attempt_202105171409213928207437458838450_0004_m_000009_164, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b4072fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:36,305] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409213928207437458838450_0004_m_000009_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213928207437458838450_0004_m_000009_164
[2021-05-17 11:09:36,310] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_202105171409213928207437458838450_0004_m_000009_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213928207437458838450_0004_m_000009_164 : duration 0:00.006s
[2021-05-17 11:09:36,332] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:36,335] {docker.py:276} INFO - 21/05/17 14:09:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:36,335] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217285743471009145079_0004_m_000010_165, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217285743471009145079_0004_m_000010_165}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217285743471009145079_0004}; taskId=attempt_202105171409217285743471009145079_0004_m_000010_165, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3372d72e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:36,335] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409217285743471009145079_0004_m_000010_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217285743471009145079_0004_m_000010_165
[2021-05-17 11:09:36,339] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_202105171409217285743471009145079_0004_m_000010_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217285743471009145079_0004_m_000010_165 : duration 0:00.004s
[2021-05-17 11:09:36,589] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409215788918897167857425_0004_m_000007_162: needsTaskCommit() Task attempt_202105171409215788918897167857425_0004_m_000007_162
[2021-05-17 11:09:36,589] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_202105171409215788918897167857425_0004_m_000007_162: needsTaskCommit() Task attempt_202105171409215788918897167857425_0004_m_000007_162: duration 0:00.002s
21/05/17 14:09:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215788918897167857425_0004_m_000007_162
[2021-05-17 11:09:36,591] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Finished task 7.0 in stage 4.0 (TID 162). 4587 bytes result sent to driver
[2021-05-17 11:09:36,592] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 166) (d733d8da4350, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:36,592] {docker.py:276} INFO - 21/05/17 14:09:36 INFO Executor: Running task 11.0 in stage 4.0 (TID 166)
[2021-05-17 11:09:36,593] {docker.py:276} INFO - 21/05/17 14:09:36 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 162) in 2380 ms on d733d8da4350 (executor driver) (8/200)
[2021-05-17 11:09:36,603] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Getting 4 (21.7 KiB) non-empty blocks including 4 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:36,603] {docker.py:276} INFO - 21/05/17 14:09:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:36,605] {docker.py:276} INFO - 21/05/17 14:09:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:36,606] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:36,606] {docker.py:276} INFO - 21/05/17 14:09:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921331352307012749917_0004_m_000011_166, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921331352307012749917_0004_m_000011_166}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921331352307012749917_0004}; taskId=attempt_20210517140921331352307012749917_0004_m_000011_166, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c1c58c3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:36,606] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Starting: Task committer attempt_20210517140921331352307012749917_0004_m_000011_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921331352307012749917_0004_m_000011_166
[2021-05-17 11:09:36,619] {docker.py:276} INFO - 21/05/17 14:09:36 INFO StagingCommitter: Task committer attempt_20210517140921331352307012749917_0004_m_000011_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921331352307012749917_0004_m_000011_166 : duration 0:00.013s
[2021-05-17 11:09:38,579] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409211128272282767164408_0004_m_000008_163: needsTaskCommit() Task attempt_202105171409211128272282767164408_0004_m_000008_163
[2021-05-17 11:09:38,579] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_202105171409211128272282767164408_0004_m_000008_163: needsTaskCommit() Task attempt_202105171409211128272282767164408_0004_m_000008_163: duration 0:00.001s
21/05/17 14:09:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211128272282767164408_0004_m_000008_163
[2021-05-17 11:09:38,580] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Finished task 8.0 in stage 4.0 (TID 163). 4587 bytes result sent to driver
[2021-05-17 11:09:38,584] {docker.py:276} INFO - 21/05/17 14:09:38 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 167) (d733d8da4350, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 14:09:38 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 163) in 2323 ms on d733d8da4350 (executor driver) (9/200)
21/05/17 14:09:38 INFO Executor: Running task 12.0 in stage 4.0 (TID 167)
[2021-05-17 11:09:38,592] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:38,593] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:38,594] {docker.py:276} INFO - 21/05/17 14:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:38,595] {docker.py:276} INFO - 21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217087979178166025700_0004_m_000012_167, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217087979178166025700_0004_m_000012_167}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217087979178166025700_0004}; taskId=attempt_202105171409217087979178166025700_0004_m_000012_167, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2942f617}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409217087979178166025700_0004_m_000012_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217087979178166025700_0004_m_000012_167
[2021-05-17 11:09:38,600] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_202105171409217087979178166025700_0004_m_000012_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217087979178166025700_0004_m_000012_167 : duration 0:00.006s
[2021-05-17 11:09:38,668] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409217285743471009145079_0004_m_000010_165: needsTaskCommit() Task attempt_202105171409217285743471009145079_0004_m_000010_165
[2021-05-17 11:09:38,668] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_202105171409217285743471009145079_0004_m_000010_165: needsTaskCommit() Task attempt_202105171409217285743471009145079_0004_m_000010_165: duration 0:00.001s
[2021-05-17 11:09:38,669] {docker.py:276} INFO - 21/05/17 14:09:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217285743471009145079_0004_m_000010_165
[2021-05-17 11:09:38,669] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Finished task 10.0 in stage 4.0 (TID 165). 4587 bytes result sent to driver
[2021-05-17 11:09:38,671] {docker.py:276} INFO - 21/05/17 14:09:38 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 168) (d733d8da4350, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:38,672] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Running task 13.0 in stage 4.0 (TID 168)
21/05/17 14:09:38 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 165) in 2375 ms on d733d8da4350 (executor driver) (10/200)
[2021-05-17 11:09:38,680] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:38,683] {docker.py:276} INFO - 21/05/17 14:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:38,683] {docker.py:276} INFO - 21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:38,684] {docker.py:276} INFO - 21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216439544767364258739_0004_m_000013_168, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216439544767364258739_0004_m_000013_168}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216439544767364258739_0004}; taskId=attempt_202105171409216439544767364258739_0004_m_000013_168, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5714317c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:38,684] {docker.py:276} INFO - 21/05/17 14:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:38,684] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409216439544767364258739_0004_m_000013_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216439544767364258739_0004_m_000013_168
[2021-05-17 11:09:38,691] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_202105171409216439544767364258739_0004_m_000013_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216439544767364258739_0004_m_000013_168 : duration 0:00.006s
[2021-05-17 11:09:38,750] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409213928207437458838450_0004_m_000009_164: needsTaskCommit() Task attempt_202105171409213928207437458838450_0004_m_000009_164
[2021-05-17 11:09:38,751] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_202105171409213928207437458838450_0004_m_000009_164: needsTaskCommit() Task attempt_202105171409213928207437458838450_0004_m_000009_164: duration 0:00.001s
[2021-05-17 11:09:38,751] {docker.py:276} INFO - 21/05/17 14:09:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213928207437458838450_0004_m_000009_164
[2021-05-17 11:09:38,752] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Finished task 9.0 in stage 4.0 (TID 164). 4587 bytes result sent to driver
[2021-05-17 11:09:38,753] {docker.py:276} INFO - 21/05/17 14:09:38 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 169) (d733d8da4350, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:38,754] {docker.py:276} INFO - 21/05/17 14:09:38 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 164) in 2474 ms on d733d8da4350 (executor driver) (11/200)
[2021-05-17 11:09:38,755] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Running task 14.0 in stage 4.0 (TID 169)
[2021-05-17 11:09:38,763] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Getting 4 (20.3 KiB) non-empty blocks including 4 (20.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:38,764] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:38,767] {docker.py:276} INFO - 21/05/17 14:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:38,768] {docker.py:276} INFO - 21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:38,768] {docker.py:276} INFO - 21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051714092178973004568369944_0004_m_000014_169, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092178973004568369944_0004_m_000014_169}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051714092178973004568369944_0004}; taskId=attempt_2021051714092178973004568369944_0004_m_000014_169, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34a10dd8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:38,768] {docker.py:276} INFO - 21/05/17 14:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:38,769] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_2021051714092178973004568369944_0004_m_000014_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092178973004568369944_0004_m_000014_169
[2021-05-17 11:09:38,774] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_2021051714092178973004568369944_0004_m_000014_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092178973004568369944_0004_m_000014_169 : duration 0:00.006s
[2021-05-17 11:09:38,873] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_20210517140921331352307012749917_0004_m_000011_166: needsTaskCommit() Task attempt_20210517140921331352307012749917_0004_m_000011_166
[2021-05-17 11:09:38,874] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_20210517140921331352307012749917_0004_m_000011_166: needsTaskCommit() Task attempt_20210517140921331352307012749917_0004_m_000011_166: duration 0:00.002s
[2021-05-17 11:09:38,874] {docker.py:276} INFO - 21/05/17 14:09:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921331352307012749917_0004_m_000011_166
[2021-05-17 11:09:38,879] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Finished task 11.0 in stage 4.0 (TID 166). 4587 bytes result sent to driver
[2021-05-17 11:09:38,880] {docker.py:276} INFO - 21/05/17 14:09:38 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 170) (d733d8da4350, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:38,881] {docker.py:276} INFO - 21/05/17 14:09:38 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 166) in 2292 ms on d733d8da4350 (executor driver) (12/200)
[2021-05-17 11:09:38,883] {docker.py:276} INFO - 21/05/17 14:09:38 INFO Executor: Running task 15.0 in stage 4.0 (TID 170)
[2021-05-17 11:09:38,892] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:38,893] {docker.py:276} INFO - 21/05/17 14:09:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:38,895] {docker.py:276} INFO - 21/05/17 14:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:38,896] {docker.py:276} INFO - 21/05/17 14:09:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218273578940517190515_0004_m_000015_170, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218273578940517190515_0004_m_000015_170}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218273578940517190515_0004}; taskId=attempt_202105171409218273578940517190515_0004_m_000015_170, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@529401ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:38,896] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409218273578940517190515_0004_m_000015_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218273578940517190515_0004_m_000015_170
[2021-05-17 11:09:38,901] {docker.py:276} INFO - 21/05/17 14:09:38 INFO StagingCommitter: Task committer attempt_202105171409218273578940517190515_0004_m_000015_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218273578940517190515_0004_m_000015_170 : duration 0:00.005s
[2021-05-17 11:09:40,845] {docker.py:276} INFO - 21/05/17 14:09:40 INFO StagingCommitter: Starting: Task committer attempt_202105171409217087979178166025700_0004_m_000012_167: needsTaskCommit() Task attempt_202105171409217087979178166025700_0004_m_000012_167
[2021-05-17 11:09:40,846] {docker.py:276} INFO - 21/05/17 14:09:40 INFO StagingCommitter: Task committer attempt_202105171409217087979178166025700_0004_m_000012_167: needsTaskCommit() Task attempt_202105171409217087979178166025700_0004_m_000012_167: duration 0:00.004s
21/05/17 14:09:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217087979178166025700_0004_m_000012_167
[2021-05-17 11:09:40,848] {docker.py:276} INFO - 21/05/17 14:09:40 INFO Executor: Finished task 12.0 in stage 4.0 (TID 167). 4544 bytes result sent to driver
[2021-05-17 11:09:40,852] {docker.py:276} INFO - 21/05/17 14:09:40 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 171) (d733d8da4350, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:40,853] {docker.py:276} INFO - 21/05/17 14:09:40 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 167) in 2274 ms on d733d8da4350 (executor driver) (13/200)
[2021-05-17 11:09:40,854] {docker.py:276} INFO - 21/05/17 14:09:40 INFO Executor: Running task 16.0 in stage 4.0 (TID 171)
[2021-05-17 11:09:40,866] {docker.py:276} INFO - 21/05/17 14:09:40 INFO ShuffleBlockFetcherIterator: Getting 4 (22.5 KiB) non-empty blocks including 4 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:40,868] {docker.py:276} INFO - 21/05/17 14:09:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:09:40,869] {docker.py:276} INFO - 21/05/17 14:09:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:40,869] {docker.py:276} INFO - 21/05/17 14:09:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:40,870] {docker.py:276} INFO - 21/05/17 14:09:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211845710413701084047_0004_m_000016_171, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211845710413701084047_0004_m_000016_171}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211845710413701084047_0004}; taskId=attempt_202105171409211845710413701084047_0004_m_000016_171, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66a36ea7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:40,870] {docker.py:276} INFO - 21/05/17 14:09:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:40,871] {docker.py:276} INFO - 21/05/17 14:09:40 INFO StagingCommitter: Starting: Task committer attempt_202105171409211845710413701084047_0004_m_000016_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211845710413701084047_0004_m_000016_171
[2021-05-17 11:09:40,875] {docker.py:276} INFO - 21/05/17 14:09:40 INFO StagingCommitter: Task committer attempt_202105171409211845710413701084047_0004_m_000016_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211845710413701084047_0004_m_000016_171 : duration 0:00.004s
[2021-05-17 11:09:41,034] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_2021051714092178973004568369944_0004_m_000014_169: needsTaskCommit() Task attempt_2021051714092178973004568369944_0004_m_000014_169
[2021-05-17 11:09:41,036] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Task committer attempt_2021051714092178973004568369944_0004_m_000014_169: needsTaskCommit() Task attempt_2021051714092178973004568369944_0004_m_000014_169: duration 0:00.002s
21/05/17 14:09:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051714092178973004568369944_0004_m_000014_169
[2021-05-17 11:09:41,037] {docker.py:276} INFO - 21/05/17 14:09:41 INFO Executor: Finished task 14.0 in stage 4.0 (TID 169). 4544 bytes result sent to driver
[2021-05-17 11:09:41,039] {docker.py:276} INFO - 21/05/17 14:09:41 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 172) (d733d8da4350, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:41,040] {docker.py:276} INFO - 21/05/17 14:09:41 INFO Executor: Running task 17.0 in stage 4.0 (TID 172)
[2021-05-17 11:09:41,041] {docker.py:276} INFO - 21/05/17 14:09:41 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 169) in 2290 ms on d733d8da4350 (executor driver) (14/200)
[2021-05-17 11:09:41,051] {docker.py:276} INFO - 21/05/17 14:09:41 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:41,054] {docker.py:276} INFO - 21/05/17 14:09:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:09:41,054] {docker.py:276} INFO - 21/05/17 14:09:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:41,055] {docker.py:276} INFO - 21/05/17 14:09:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214137513933262279562_0004_m_000017_172, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214137513933262279562_0004_m_000017_172}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214137513933262279562_0004}; taskId=attempt_202105171409214137513933262279562_0004_m_000017_172, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42363059}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:41,055] {docker.py:276} INFO - 21/05/17 14:09:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409214137513933262279562_0004_m_000017_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214137513933262279562_0004_m_000017_172
[2021-05-17 11:09:41,059] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Task committer attempt_202105171409214137513933262279562_0004_m_000017_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214137513933262279562_0004_m_000017_172 : duration 0:00.005s
[2021-05-17 11:09:41,126] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409218273578940517190515_0004_m_000015_170: needsTaskCommit() Task attempt_202105171409218273578940517190515_0004_m_000015_170
[2021-05-17 11:09:41,126] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Task committer attempt_202105171409218273578940517190515_0004_m_000015_170: needsTaskCommit() Task attempt_202105171409218273578940517190515_0004_m_000015_170: duration 0:00.000s
[2021-05-17 11:09:41,126] {docker.py:276} INFO - 21/05/17 14:09:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218273578940517190515_0004_m_000015_170
[2021-05-17 11:09:41,127] {docker.py:276} INFO - 21/05/17 14:09:41 INFO Executor: Finished task 15.0 in stage 4.0 (TID 170). 4544 bytes result sent to driver
[2021-05-17 11:09:41,127] {docker.py:276} INFO - 21/05/17 14:09:41 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 173) (d733d8da4350, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:41,128] {docker.py:276} INFO - 21/05/17 14:09:41 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 170) in 2251 ms on d733d8da4350 (executor driver) (15/200)
21/05/17 14:09:41 INFO Executor: Running task 18.0 in stage 4.0 (TID 173)
[2021-05-17 11:09:41,138] {docker.py:276} INFO - 21/05/17 14:09:41 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:41,140] {docker.py:276} INFO - 21/05/17 14:09:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213337040721186001190_0004_m_000018_173, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213337040721186001190_0004_m_000018_173}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213337040721186001190_0004}; taskId=attempt_202105171409213337040721186001190_0004_m_000018_173, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@288c0441}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:41,140] {docker.py:276} INFO - 21/05/17 14:09:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409213337040721186001190_0004_m_000018_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213337040721186001190_0004_m_000018_173
[2021-05-17 11:09:41,143] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Task committer attempt_202105171409213337040721186001190_0004_m_000018_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213337040721186001190_0004_m_000018_173 : duration 0:00.004s
[2021-05-17 11:09:41,191] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409216439544767364258739_0004_m_000013_168: needsTaskCommit() Task attempt_202105171409216439544767364258739_0004_m_000013_168
[2021-05-17 11:09:41,192] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Task committer attempt_202105171409216439544767364258739_0004_m_000013_168: needsTaskCommit() Task attempt_202105171409216439544767364258739_0004_m_000013_168: duration 0:00.001s
21/05/17 14:09:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216439544767364258739_0004_m_000013_168
[2021-05-17 11:09:41,193] {docker.py:276} INFO - 21/05/17 14:09:41 INFO Executor: Finished task 13.0 in stage 4.0 (TID 168). 4544 bytes result sent to driver
[2021-05-17 11:09:41,194] {docker.py:276} INFO - 21/05/17 14:09:41 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 174) (d733d8da4350, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:41,196] {docker.py:276} INFO - 21/05/17 14:09:41 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 168) in 2528 ms on d733d8da4350 (executor driver) (16/200)
[2021-05-17 11:09:41,196] {docker.py:276} INFO - 21/05/17 14:09:41 INFO Executor: Running task 19.0 in stage 4.0 (TID 174)
[2021-05-17 11:09:41,205] {docker.py:276} INFO - 21/05/17 14:09:41 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:41,208] {docker.py:276} INFO - 21/05/17 14:09:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:41,208] {docker.py:276} INFO - 21/05/17 14:09:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:41,208] {docker.py:276} INFO - 21/05/17 14:09:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211329286636283371659_0004_m_000019_174, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211329286636283371659_0004_m_000019_174}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211329286636283371659_0004}; taskId=attempt_202105171409211329286636283371659_0004_m_000019_174, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29f272a6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:41,209] {docker.py:276} INFO - 21/05/17 14:09:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:41,209] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409211329286636283371659_0004_m_000019_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211329286636283371659_0004_m_000019_174
[2021-05-17 11:09:41,213] {docker.py:276} INFO - 21/05/17 14:09:41 INFO StagingCommitter: Task committer attempt_202105171409211329286636283371659_0004_m_000019_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211329286636283371659_0004_m_000019_174 : duration 0:00.004s
[2021-05-17 11:09:43,178] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409211845710413701084047_0004_m_000016_171: needsTaskCommit() Task attempt_202105171409211845710413701084047_0004_m_000016_171
[2021-05-17 11:09:43,178] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409211845710413701084047_0004_m_000016_171: needsTaskCommit() Task attempt_202105171409211845710413701084047_0004_m_000016_171: duration 0:00.001s
[2021-05-17 11:09:43,179] {docker.py:276} INFO - 21/05/17 14:09:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211845710413701084047_0004_m_000016_171
[2021-05-17 11:09:43,179] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Finished task 16.0 in stage 4.0 (TID 171). 4544 bytes result sent to driver
[2021-05-17 11:09:43,180] {docker.py:276} INFO - 21/05/17 14:09:43 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 175) (d733d8da4350, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:43,181] {docker.py:276} INFO - 21/05/17 14:09:43 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 171) in 2335 ms on d733d8da4350 (executor driver) (17/200)
[2021-05-17 11:09:43,182] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Running task 20.0 in stage 4.0 (TID 175)
[2021-05-17 11:09:43,194] {docker.py:276} INFO - 21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Getting 4 (22.5 KiB) non-empty blocks including 4 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:43,196] {docker.py:276} INFO - 21/05/17 14:09:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:43,197] {docker.py:276} INFO - 21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214188187311505391251_0004_m_000020_175, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214188187311505391251_0004_m_000020_175}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214188187311505391251_0004}; taskId=attempt_202105171409214188187311505391251_0004_m_000020_175, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2335b815}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409214188187311505391251_0004_m_000020_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214188187311505391251_0004_m_000020_175
[2021-05-17 11:09:43,201] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409214188187311505391251_0004_m_000020_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214188187311505391251_0004_m_000020_175 : duration 0:00.004s
[2021-05-17 11:09:43,339] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409214137513933262279562_0004_m_000017_172: needsTaskCommit() Task attempt_202105171409214137513933262279562_0004_m_000017_172
[2021-05-17 11:09:43,340] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409214137513933262279562_0004_m_000017_172: needsTaskCommit() Task attempt_202105171409214137513933262279562_0004_m_000017_172: duration 0:00.004s
[2021-05-17 11:09:43,341] {docker.py:276} INFO - 21/05/17 14:09:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214137513933262279562_0004_m_000017_172
[2021-05-17 11:09:43,343] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Finished task 17.0 in stage 4.0 (TID 172). 4544 bytes result sent to driver
[2021-05-17 11:09:43,345] {docker.py:276} INFO - 21/05/17 14:09:43 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 176) (d733d8da4350, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:43,347] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Running task 21.0 in stage 4.0 (TID 176)
21/05/17 14:09:43 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 172) in 2311 ms on d733d8da4350 (executor driver) (18/200)
[2021-05-17 11:09:43,369] {docker.py:276} INFO - 21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:43,372] {docker.py:276} INFO - 21/05/17 14:09:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:43,372] {docker.py:276} INFO - 21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212819876089344719378_0004_m_000021_176, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212819876089344719378_0004_m_000021_176}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212819876089344719378_0004}; taskId=attempt_202105171409212819876089344719378_0004_m_000021_176, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51c9567a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409212819876089344719378_0004_m_000021_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212819876089344719378_0004_m_000021_176
[2021-05-17 11:09:43,378] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409212819876089344719378_0004_m_000021_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212819876089344719378_0004_m_000021_176 : duration 0:00.006s
[2021-05-17 11:09:43,474] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409211329286636283371659_0004_m_000019_174: needsTaskCommit() Task attempt_202105171409211329286636283371659_0004_m_000019_174
[2021-05-17 11:09:43,474] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409211329286636283371659_0004_m_000019_174: needsTaskCommit() Task attempt_202105171409211329286636283371659_0004_m_000019_174: duration 0:00.001s
21/05/17 14:09:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211329286636283371659_0004_m_000019_174
[2021-05-17 11:09:43,475] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Finished task 19.0 in stage 4.0 (TID 174). 4587 bytes result sent to driver
[2021-05-17 11:09:43,477] {docker.py:276} INFO - 21/05/17 14:09:43 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 177) (d733d8da4350, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:43,478] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Running task 22.0 in stage 4.0 (TID 177)
21/05/17 14:09:43 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 174) in 2286 ms on d733d8da4350 (executor driver) (19/200)
[2021-05-17 11:09:43,485] {docker.py:276} INFO - 21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:43,486] {docker.py:276} INFO - 21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:43,488] {docker.py:276} INFO - 21/05/17 14:09:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:43,488] {docker.py:276} INFO - 21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211574175765437565782_0004_m_000022_177, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211574175765437565782_0004_m_000022_177}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211574175765437565782_0004}; taskId=attempt_202105171409211574175765437565782_0004_m_000022_177, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@341a5f20}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:43,489] {docker.py:276} INFO - 21/05/17 14:09:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409211574175765437565782_0004_m_000022_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211574175765437565782_0004_m_000022_177
[2021-05-17 11:09:43,494] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409211574175765437565782_0004_m_000022_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211574175765437565782_0004_m_000022_177 : duration 0:00.005s
[2021-05-17 11:09:43,639] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409213337040721186001190_0004_m_000018_173: needsTaskCommit() Task attempt_202105171409213337040721186001190_0004_m_000018_173
[2021-05-17 11:09:43,640] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409213337040721186001190_0004_m_000018_173: needsTaskCommit() Task attempt_202105171409213337040721186001190_0004_m_000018_173: duration 0:00.003s
21/05/17 14:09:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213337040721186001190_0004_m_000018_173
[2021-05-17 11:09:43,642] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Finished task 18.0 in stage 4.0 (TID 173). 4587 bytes result sent to driver
[2021-05-17 11:09:43,647] {docker.py:276} INFO - 21/05/17 14:09:43 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 178) (d733d8da4350, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:43,649] {docker.py:276} INFO - 21/05/17 14:09:43 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 173) in 2524 ms on d733d8da4350 (executor driver) (20/200)
[2021-05-17 11:09:43,650] {docker.py:276} INFO - 21/05/17 14:09:43 INFO Executor: Running task 23.0 in stage 4.0 (TID 178)
[2021-05-17 11:09:43,659] {docker.py:276} INFO - 21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:43,662] {docker.py:276} INFO - 21/05/17 14:09:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:43,662] {docker.py:276} INFO - 21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212638845906576055131_0004_m_000023_178, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212638845906576055131_0004_m_000023_178}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212638845906576055131_0004}; taskId=attempt_202105171409212638845906576055131_0004_m_000023_178, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40657d1f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:43,662] {docker.py:276} INFO - 21/05/17 14:09:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:43,663] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409212638845906576055131_0004_m_000023_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212638845906576055131_0004_m_000023_178
[2021-05-17 11:09:43,667] {docker.py:276} INFO - 21/05/17 14:09:43 INFO StagingCommitter: Task committer attempt_202105171409212638845906576055131_0004_m_000023_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212638845906576055131_0004_m_000023_178 : duration 0:00.005s
[2021-05-17 11:09:45,704] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105171409214188187311505391251_0004_m_000020_175: needsTaskCommit() Task attempt_202105171409214188187311505391251_0004_m_000020_175
[2021-05-17 11:09:45,704] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Task committer attempt_202105171409214188187311505391251_0004_m_000020_175: needsTaskCommit() Task attempt_202105171409214188187311505391251_0004_m_000020_175: duration 0:00.001s
21/05/17 14:09:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214188187311505391251_0004_m_000020_175
[2021-05-17 11:09:45,706] {docker.py:276} INFO - 21/05/17 14:09:45 INFO Executor: Finished task 20.0 in stage 4.0 (TID 175). 4587 bytes result sent to driver
[2021-05-17 11:09:45,708] {docker.py:276} INFO - 21/05/17 14:09:45 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 179) (d733d8da4350, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:45,709] {docker.py:276} INFO - 21/05/17 14:09:45 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 175) in 2532 ms on d733d8da4350 (executor driver) (21/200)
21/05/17 14:09:45 INFO Executor: Running task 24.0 in stage 4.0 (TID 179)
[2021-05-17 11:09:45,717] {docker.py:276} INFO - 21/05/17 14:09:45 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:45,719] {docker.py:276} INFO - 21/05/17 14:09:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:45,720] {docker.py:276} INFO - 21/05/17 14:09:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213792307727033665828_0004_m_000024_179, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213792307727033665828_0004_m_000024_179}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213792307727033665828_0004}; taskId=attempt_202105171409213792307727033665828_0004_m_000024_179, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@629c6bc8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105171409213792307727033665828_0004_m_000024_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213792307727033665828_0004_m_000024_179
[2021-05-17 11:09:45,725] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Task committer attempt_202105171409213792307727033665828_0004_m_000024_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213792307727033665828_0004_m_000024_179 : duration 0:00.004s
[2021-05-17 11:09:45,768] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105171409211574175765437565782_0004_m_000022_177: needsTaskCommit() Task attempt_202105171409211574175765437565782_0004_m_000022_177
[2021-05-17 11:09:45,768] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Task committer attempt_202105171409211574175765437565782_0004_m_000022_177: needsTaskCommit() Task attempt_202105171409211574175765437565782_0004_m_000022_177: duration 0:00.001s
21/05/17 14:09:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211574175765437565782_0004_m_000022_177
[2021-05-17 11:09:45,769] {docker.py:276} INFO - 21/05/17 14:09:45 INFO Executor: Finished task 22.0 in stage 4.0 (TID 177). 4544 bytes result sent to driver
[2021-05-17 11:09:45,771] {docker.py:276} INFO - 21/05/17 14:09:45 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 180) (d733d8da4350, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:45,771] {docker.py:276} INFO - 21/05/17 14:09:45 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 177) in 2297 ms on d733d8da4350 (executor driver) (22/200)
[2021-05-17 11:09:45,772] {docker.py:276} INFO - 21/05/17 14:09:45 INFO Executor: Running task 25.0 in stage 4.0 (TID 180)
[2021-05-17 11:09:45,780] {docker.py:276} INFO - 21/05/17 14:09:45 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:45,781] {docker.py:276} INFO - 21/05/17 14:09:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:45,783] {docker.py:276} INFO - 21/05/17 14:09:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:45,784] {docker.py:276} INFO - 21/05/17 14:09:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211891574531459968956_0004_m_000025_180, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211891574531459968956_0004_m_000025_180}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211891574531459968956_0004}; taskId=attempt_202105171409211891574531459968956_0004_m_000025_180, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@117adf6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:45,784] {docker.py:276} INFO - 21/05/17 14:09:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:45,784] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105171409211891574531459968956_0004_m_000025_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211891574531459968956_0004_m_000025_180
[2021-05-17 11:09:45,788] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Task committer attempt_202105171409211891574531459968956_0004_m_000025_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211891574531459968956_0004_m_000025_180 : duration 0:00.004s
[2021-05-17 11:09:45,916] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105171409212819876089344719378_0004_m_000021_176: needsTaskCommit() Task attempt_202105171409212819876089344719378_0004_m_000021_176
[2021-05-17 11:09:45,916] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Task committer attempt_202105171409212819876089344719378_0004_m_000021_176: needsTaskCommit() Task attempt_202105171409212819876089344719378_0004_m_000021_176: duration 0:00.001s
21/05/17 14:09:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212819876089344719378_0004_m_000021_176
[2021-05-17 11:09:45,919] {docker.py:276} INFO - 21/05/17 14:09:45 INFO Executor: Finished task 21.0 in stage 4.0 (TID 176). 4587 bytes result sent to driver
[2021-05-17 11:09:45,920] {docker.py:276} INFO - 21/05/17 14:09:45 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 181) (d733d8da4350, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:45,921] {docker.py:276} INFO - 21/05/17 14:09:45 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 176) in 2580 ms on d733d8da4350 (executor driver) (23/200)
[2021-05-17 11:09:45,922] {docker.py:276} INFO - 21/05/17 14:09:45 INFO Executor: Running task 26.0 in stage 4.0 (TID 181)
[2021-05-17 11:09:45,930] {docker.py:276} INFO - 21/05/17 14:09:45 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:45,930] {docker.py:276} INFO - 21/05/17 14:09:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:45,932] {docker.py:276} INFO - 21/05/17 14:09:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:45,932] {docker.py:276} INFO - 21/05/17 14:09:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:45,933] {docker.py:276} INFO - 21/05/17 14:09:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215661483302260950685_0004_m_000026_181, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215661483302260950685_0004_m_000026_181}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215661483302260950685_0004}; taskId=attempt_202105171409215661483302260950685_0004_m_000026_181, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6541836d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:45,933] {docker.py:276} INFO - 21/05/17 14:09:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:45,933] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Starting: Task committer attempt_202105171409215661483302260950685_0004_m_000026_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215661483302260950685_0004_m_000026_181
[2021-05-17 11:09:45,937] {docker.py:276} INFO - 21/05/17 14:09:45 INFO StagingCommitter: Task committer attempt_202105171409215661483302260950685_0004_m_000026_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215661483302260950685_0004_m_000026_181 : duration 0:00.004s
[2021-05-17 11:09:46,160] {docker.py:276} INFO - 21/05/17 14:09:46 INFO StagingCommitter: Starting: Task committer attempt_202105171409212638845906576055131_0004_m_000023_178: needsTaskCommit() Task attempt_202105171409212638845906576055131_0004_m_000023_178
[2021-05-17 11:09:46,161] {docker.py:276} INFO - 21/05/17 14:09:46 INFO StagingCommitter: Task committer attempt_202105171409212638845906576055131_0004_m_000023_178: needsTaskCommit() Task attempt_202105171409212638845906576055131_0004_m_000023_178: duration 0:00.003s
21/05/17 14:09:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212638845906576055131_0004_m_000023_178
[2021-05-17 11:09:46,163] {docker.py:276} INFO - 21/05/17 14:09:46 INFO Executor: Finished task 23.0 in stage 4.0 (TID 178). 4544 bytes result sent to driver
[2021-05-17 11:09:46,165] {docker.py:276} INFO - 21/05/17 14:09:46 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 182) (d733d8da4350, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:46,165] {docker.py:276} INFO - 21/05/17 14:09:46 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 178) in 2523 ms on d733d8da4350 (executor driver) (24/200)
[2021-05-17 11:09:46,167] {docker.py:276} INFO - 21/05/17 14:09:46 INFO Executor: Running task 27.0 in stage 4.0 (TID 182)
[2021-05-17 11:09:46,177] {docker.py:276} INFO - 21/05/17 14:09:46 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:46,179] {docker.py:276} INFO - 21/05/17 14:09:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218228396520035654260_0004_m_000027_182, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218228396520035654260_0004_m_000027_182}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218228396520035654260_0004}; taskId=attempt_202105171409218228396520035654260_0004_m_000027_182, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55622cd3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:46,179] {docker.py:276} INFO - 21/05/17 14:09:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:46 INFO StagingCommitter: Starting: Task committer attempt_202105171409218228396520035654260_0004_m_000027_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218228396520035654260_0004_m_000027_182
[2021-05-17 11:09:46,182] {docker.py:276} INFO - 21/05/17 14:09:46 INFO StagingCommitter: Task committer attempt_202105171409218228396520035654260_0004_m_000027_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218228396520035654260_0004_m_000027_182 : duration 0:00.003s
[2021-05-17 11:09:47,986] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409213792307727033665828_0004_m_000024_179: needsTaskCommit() Task attempt_202105171409213792307727033665828_0004_m_000024_179
[2021-05-17 11:09:47,987] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409213792307727033665828_0004_m_000024_179: needsTaskCommit() Task attempt_202105171409213792307727033665828_0004_m_000024_179: duration 0:00.001s
21/05/17 14:09:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213792307727033665828_0004_m_000024_179
[2021-05-17 11:09:47,987] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Finished task 24.0 in stage 4.0 (TID 179). 4544 bytes result sent to driver
[2021-05-17 11:09:47,989] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 183) (d733d8da4350, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:47,990] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Running task 28.0 in stage 4.0 (TID 183)
[2021-05-17 11:09:47,992] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 179) in 2286 ms on d733d8da4350 (executor driver) (25/200)
[2021-05-17 11:09:48,011] {docker.py:276} INFO - 21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:48,013] {docker.py:276} INFO - 21/05/17 14:09:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217480323217938550303_0004_m_000028_183, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217480323217938550303_0004_m_000028_183}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217480323217938550303_0004}; taskId=attempt_202105171409217480323217938550303_0004_m_000028_183, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1050a220}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:48,013] {docker.py:276} INFO - 21/05/17 14:09:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409217480323217938550303_0004_m_000028_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217480323217938550303_0004_m_000028_183
[2021-05-17 11:09:48,016] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409217480323217938550303_0004_m_000028_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217480323217938550303_0004_m_000028_183 : duration 0:00.004s
[2021-05-17 11:09:48,143] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409211891574531459968956_0004_m_000025_180: needsTaskCommit() Task attempt_202105171409211891574531459968956_0004_m_000025_180
[2021-05-17 11:09:48,144] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409211891574531459968956_0004_m_000025_180: needsTaskCommit() Task attempt_202105171409211891574531459968956_0004_m_000025_180: duration 0:00.002s
21/05/17 14:09:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211891574531459968956_0004_m_000025_180
[2021-05-17 11:09:48,145] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Finished task 25.0 in stage 4.0 (TID 180). 4544 bytes result sent to driver
[2021-05-17 11:09:48,147] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 184) (d733d8da4350, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:48,148] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 180) in 2380 ms on d733d8da4350 (executor driver) (26/200)
[2021-05-17 11:09:48,149] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Running task 29.0 in stage 4.0 (TID 184)
[2021-05-17 11:09:48,158] {docker.py:276} INFO - 21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:48,162] {docker.py:276} INFO - 21/05/17 14:09:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:48,162] {docker.py:276} INFO - 21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:48,162] {docker.py:276} INFO - 21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216844576863920670910_0004_m_000029_184, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216844576863920670910_0004_m_000029_184}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216844576863920670910_0004}; taskId=attempt_202105171409216844576863920670910_0004_m_000029_184, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36d3064e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:48,163] {docker.py:276} INFO - 21/05/17 14:09:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409216844576863920670910_0004_m_000029_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216844576863920670910_0004_m_000029_184
[2021-05-17 11:09:48,165] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409216844576863920670910_0004_m_000029_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216844576863920670910_0004_m_000029_184 : duration 0:00.003s
[2021-05-17 11:09:48,174] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409218228396520035654260_0004_m_000027_182: needsTaskCommit() Task attempt_202105171409218228396520035654260_0004_m_000027_182
[2021-05-17 11:09:48,175] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409218228396520035654260_0004_m_000027_182: needsTaskCommit() Task attempt_202105171409218228396520035654260_0004_m_000027_182: duration 0:00.003s
21/05/17 14:09:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218228396520035654260_0004_m_000027_182
[2021-05-17 11:09:48,176] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Finished task 27.0 in stage 4.0 (TID 182). 4544 bytes result sent to driver
[2021-05-17 11:09:48,177] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 185) (d733d8da4350, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:48,178] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Running task 30.0 in stage 4.0 (TID 185)
[2021-05-17 11:09:48,178] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 182) in 2016 ms on d733d8da4350 (executor driver) (27/200)
[2021-05-17 11:09:48,186] {docker.py:276} INFO - 21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Getting 4 (20.5 KiB) non-empty blocks including 4 (20.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:48,194] {docker.py:276} INFO - 21/05/17 14:09:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409219213846806056337915_0004_m_000030_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219213846806056337915_0004_m_000030_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409219213846806056337915_0004}; taskId=attempt_202105171409219213846806056337915_0004_m_000030_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@628b3e81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:48,196] {docker.py:276} INFO - 21/05/17 14:09:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409219213846806056337915_0004_m_000030_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219213846806056337915_0004_m_000030_185
[2021-05-17 11:09:48,199] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409219213846806056337915_0004_m_000030_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219213846806056337915_0004_m_000030_185 : duration 0:00.003s
[2021-05-17 11:09:48,234] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409215661483302260950685_0004_m_000026_181: needsTaskCommit() Task attempt_202105171409215661483302260950685_0004_m_000026_181
[2021-05-17 11:09:48,234] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409215661483302260950685_0004_m_000026_181: needsTaskCommit() Task attempt_202105171409215661483302260950685_0004_m_000026_181: duration 0:00.001s
21/05/17 14:09:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215661483302260950685_0004_m_000026_181
[2021-05-17 11:09:48,235] {docker.py:276} INFO - 21/05/17 14:09:48 INFO Executor: Finished task 26.0 in stage 4.0 (TID 181). 4544 bytes result sent to driver
[2021-05-17 11:09:48,237] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 186) (d733d8da4350, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:48,238] {docker.py:276} INFO - 21/05/17 14:09:48 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 181) in 2320 ms on d733d8da4350 (executor driver) (28/200)
21/05/17 14:09:48 INFO Executor: Running task 31.0 in stage 4.0 (TID 186)
[2021-05-17 11:09:48,247] {docker.py:276} INFO - 21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:48,249] {docker.py:276} INFO - 21/05/17 14:09:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211483018390395483963_0004_m_000031_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211483018390395483963_0004_m_000031_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211483018390395483963_0004}; taskId=attempt_202105171409211483018390395483963_0004_m_000031_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e5e0451}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409211483018390395483963_0004_m_000031_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211483018390395483963_0004_m_000031_186
[2021-05-17 11:09:48,254] {docker.py:276} INFO - 21/05/17 14:09:48 INFO StagingCommitter: Task committer attempt_202105171409211483018390395483963_0004_m_000031_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211483018390395483963_0004_m_000031_186 : duration 0:00.005s
[2021-05-17 11:09:50,535] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409211483018390395483963_0004_m_000031_186: needsTaskCommit() Task attempt_202105171409211483018390395483963_0004_m_000031_186
[2021-05-17 11:09:50,539] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409216844576863920670910_0004_m_000029_184: needsTaskCommit() Task attempt_202105171409216844576863920670910_0004_m_000029_184
21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409211483018390395483963_0004_m_000031_186: needsTaskCommit() Task attempt_202105171409211483018390395483963_0004_m_000031_186: duration 0:00.005s
21/05/17 14:09:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211483018390395483963_0004_m_000031_186
[2021-05-17 11:09:50,541] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409216844576863920670910_0004_m_000029_184: needsTaskCommit() Task attempt_202105171409216844576863920670910_0004_m_000029_184: duration 0:00.004s
[2021-05-17 11:09:50,541] {docker.py:276} INFO - 21/05/17 14:09:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216844576863920670910_0004_m_000029_184
[2021-05-17 11:09:50,542] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Finished task 31.0 in stage 4.0 (TID 186). 4544 bytes result sent to driver
[2021-05-17 11:09:50,544] {docker.py:276} INFO - 21/05/17 14:09:50 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 187) (d733d8da4350, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:50,545] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Running task 32.0 in stage 4.0 (TID 187)
[2021-05-17 11:09:50,546] {docker.py:276} INFO - 21/05/17 14:09:50 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 186) in 2312 ms on d733d8da4350 (executor driver) (29/200)
[2021-05-17 11:09:50,547] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Finished task 29.0 in stage 4.0 (TID 184). 4544 bytes result sent to driver
[2021-05-17 11:09:50,548] {docker.py:276} INFO - 21/05/17 14:09:50 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 184) in 2404 ms on d733d8da4350 (executor driver) (30/200)
[2021-05-17 11:09:50,549] {docker.py:276} INFO - 21/05/17 14:09:50 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 188) (d733d8da4350, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:50,550] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Running task 33.0 in stage 4.0 (TID 188)
[2021-05-17 11:09:50,558] {docker.py:276} INFO - 21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:50,561] {docker.py:276} INFO - 21/05/17 14:09:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211619667442498873870_0004_m_000032_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211619667442498873870_0004_m_000032_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211619667442498873870_0004}; taskId=attempt_202105171409211619667442498873870_0004_m_000032_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48672a5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:50,561] {docker.py:276} INFO - 21/05/17 14:09:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409211619667442498873870_0004_m_000032_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211619667442498873870_0004_m_000032_187
[2021-05-17 11:09:50,564] {docker.py:276} INFO - 21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:50,565] {docker.py:276} INFO - 21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:50,565] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409211619667442498873870_0004_m_000032_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211619667442498873870_0004_m_000032_187 : duration 0:00.004s
[2021-05-17 11:09:50,566] {docker.py:276} INFO - 21/05/17 14:09:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:50,567] {docker.py:276} INFO - 21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:50,568] {docker.py:276} INFO - 21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217973497595612606610_0004_m_000033_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217973497595612606610_0004_m_000033_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217973497595612606610_0004}; taskId=attempt_202105171409217973497595612606610_0004_m_000033_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e7b1982}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:50,568] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409217973497595612606610_0004_m_000033_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217973497595612606610_0004_m_000033_188
[2021-05-17 11:09:50,570] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409217973497595612606610_0004_m_000033_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217973497595612606610_0004_m_000033_188 : duration 0:00.004s
[2021-05-17 11:09:50,712] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409219213846806056337915_0004_m_000030_185: needsTaskCommit() Task attempt_202105171409219213846806056337915_0004_m_000030_185
[2021-05-17 11:09:50,713] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409219213846806056337915_0004_m_000030_185: needsTaskCommit() Task attempt_202105171409219213846806056337915_0004_m_000030_185: duration 0:00.002s
[2021-05-17 11:09:50,713] {docker.py:276} INFO - 21/05/17 14:09:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409219213846806056337915_0004_m_000030_185
[2021-05-17 11:09:50,714] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Finished task 30.0 in stage 4.0 (TID 185). 4587 bytes result sent to driver
[2021-05-17 11:09:50,716] {docker.py:276} INFO - 21/05/17 14:09:50 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 189) (d733d8da4350, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:50,716] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Running task 34.0 in stage 4.0 (TID 189)
21/05/17 14:09:50 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 185) in 2542 ms on d733d8da4350 (executor driver) (31/200)
[2021-05-17 11:09:50,724] {docker.py:276} INFO - 21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:50,726] {docker.py:276} INFO - 21/05/17 14:09:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:50,727] {docker.py:276} INFO - 21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216036446211064047546_0004_m_000034_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216036446211064047546_0004_m_000034_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216036446211064047546_0004}; taskId=attempt_202105171409216036446211064047546_0004_m_000034_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44ea6a78}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:50,727] {docker.py:276} INFO - 21/05/17 14:09:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:50,728] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409216036446211064047546_0004_m_000034_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216036446211064047546_0004_m_000034_189
[2021-05-17 11:09:50,730] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409216036446211064047546_0004_m_000034_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216036446211064047546_0004_m_000034_189 : duration 0:00.003s
[2021-05-17 11:09:50,887] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409217480323217938550303_0004_m_000028_183: needsTaskCommit() Task attempt_202105171409217480323217938550303_0004_m_000028_183
[2021-05-17 11:09:50,888] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409217480323217938550303_0004_m_000028_183: needsTaskCommit() Task attempt_202105171409217480323217938550303_0004_m_000028_183: duration 0:00.004s
21/05/17 14:09:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217480323217938550303_0004_m_000028_183
[2021-05-17 11:09:50,890] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Finished task 28.0 in stage 4.0 (TID 183). 4587 bytes result sent to driver
[2021-05-17 11:09:50,893] {docker.py:276} INFO - 21/05/17 14:09:50 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 190) (d733d8da4350, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:50,895] {docker.py:276} INFO - 21/05/17 14:09:50 INFO Executor: Running task 35.0 in stage 4.0 (TID 190)
21/05/17 14:09:50 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 183) in 2908 ms on d733d8da4350 (executor driver) (32/200)
[2021-05-17 11:09:50,905] {docker.py:276} INFO - 21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Getting 4 (22.8 KiB) non-empty blocks including 4 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:50,908] {docker.py:276} INFO - 21/05/17 14:09:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218664389588819397911_0004_m_000035_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218664389588819397911_0004_m_000035_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218664389588819397911_0004}; taskId=attempt_202105171409218664389588819397911_0004_m_000035_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29d51577}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:50,908] {docker.py:276} INFO - 21/05/17 14:09:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409218664389588819397911_0004_m_000035_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218664389588819397911_0004_m_000035_190
[2021-05-17 11:09:50,913] {docker.py:276} INFO - 21/05/17 14:09:50 INFO StagingCommitter: Task committer attempt_202105171409218664389588819397911_0004_m_000035_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218664389588819397911_0004_m_000035_190 : duration 0:00.005s
[2021-05-17 11:09:52,857] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Starting: Task committer attempt_202105171409217973497595612606610_0004_m_000033_188: needsTaskCommit() Task attempt_202105171409217973497595612606610_0004_m_000033_188
[2021-05-17 11:09:52,858] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Task committer attempt_202105171409217973497595612606610_0004_m_000033_188: needsTaskCommit() Task attempt_202105171409217973497595612606610_0004_m_000033_188: duration 0:00.001s
[2021-05-17 11:09:52,858] {docker.py:276} INFO - 21/05/17 14:09:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217973497595612606610_0004_m_000033_188
[2021-05-17 11:09:52,861] {docker.py:276} INFO - 21/05/17 14:09:52 INFO Executor: Finished task 33.0 in stage 4.0 (TID 188). 4587 bytes result sent to driver
[2021-05-17 11:09:52,868] {docker.py:276} INFO - 21/05/17 14:09:52 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 191) (d733d8da4350, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:52,868] {docker.py:276} INFO - 21/05/17 14:09:52 INFO Executor: Running task 36.0 in stage 4.0 (TID 191)
[2021-05-17 11:09:52,869] {docker.py:276} INFO - 21/05/17 14:09:52 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 188) in 2320 ms on d733d8da4350 (executor driver) (33/200)
[2021-05-17 11:09:52,873] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Starting: Task committer attempt_202105171409211619667442498873870_0004_m_000032_187: needsTaskCommit() Task attempt_202105171409211619667442498873870_0004_m_000032_187
[2021-05-17 11:09:52,874] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Task committer attempt_202105171409211619667442498873870_0004_m_000032_187: needsTaskCommit() Task attempt_202105171409211619667442498873870_0004_m_000032_187: duration 0:00.000s
21/05/17 14:09:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211619667442498873870_0004_m_000032_187
[2021-05-17 11:09:52,874] {docker.py:276} INFO - 21/05/17 14:09:52 INFO Executor: Finished task 32.0 in stage 4.0 (TID 187). 4587 bytes result sent to driver
[2021-05-17 11:09:52,876] {docker.py:276} INFO - 21/05/17 14:09:52 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 192) (d733d8da4350, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:52,877] {docker.py:276} INFO - 21/05/17 14:09:52 INFO Executor: Running task 37.0 in stage 4.0 (TID 192)
21/05/17 14:09:52 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 187) in 2336 ms on d733d8da4350 (executor driver) (34/200)
[2021-05-17 11:09:52,884] {docker.py:276} INFO - 21/05/17 14:09:52 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:52,885] {docker.py:276} INFO - 21/05/17 14:09:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:52,888] {docker.py:276} INFO - 21/05/17 14:09:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:52,889] {docker.py:276} INFO - 21/05/17 14:09:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:52,890] {docker.py:276} INFO - 21/05/17 14:09:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218559977782039531876_0004_m_000036_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218559977782039531876_0004_m_000036_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218559977782039531876_0004}; taskId=attempt_202105171409218559977782039531876_0004_m_000036_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a36b709}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:52,890] {docker.py:276} INFO - 21/05/17 14:09:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:52 INFO StagingCommitter: Starting: Task committer attempt_202105171409218559977782039531876_0004_m_000036_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218559977782039531876_0004_m_000036_191
[2021-05-17 11:09:52,894] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Task committer attempt_202105171409218559977782039531876_0004_m_000036_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218559977782039531876_0004_m_000036_191 : duration 0:00.005s
[2021-05-17 11:09:52,898] {docker.py:276} INFO - 21/05/17 14:09:52 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:09:52,900] {docker.py:276} INFO - 21/05/17 14:09:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:52,901] {docker.py:276} INFO - 21/05/17 14:09:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215726936194165597902_0004_m_000037_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215726936194165597902_0004_m_000037_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215726936194165597902_0004}; taskId=attempt_202105171409215726936194165597902_0004_m_000037_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6138d464}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:52,901] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Starting: Task committer attempt_202105171409215726936194165597902_0004_m_000037_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215726936194165597902_0004_m_000037_192
[2021-05-17 11:09:52,908] {docker.py:276} INFO - 21/05/17 14:09:52 INFO StagingCommitter: Task committer attempt_202105171409215726936194165597902_0004_m_000037_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215726936194165597902_0004_m_000037_192 : duration 0:00.008s
[2021-05-17 11:09:53,049] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409216036446211064047546_0004_m_000034_189: needsTaskCommit() Task attempt_202105171409216036446211064047546_0004_m_000034_189
[2021-05-17 11:09:53,050] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Task committer attempt_202105171409216036446211064047546_0004_m_000034_189: needsTaskCommit() Task attempt_202105171409216036446211064047546_0004_m_000034_189: duration 0:00.001s
21/05/17 14:09:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216036446211064047546_0004_m_000034_189
[2021-05-17 11:09:53,052] {docker.py:276} INFO - 21/05/17 14:09:53 INFO Executor: Finished task 34.0 in stage 4.0 (TID 189). 4544 bytes result sent to driver
[2021-05-17 11:09:53,053] {docker.py:276} INFO - 21/05/17 14:09:53 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 193) (d733d8da4350, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:53,054] {docker.py:276} INFO - 21/05/17 14:09:53 INFO Executor: Running task 38.0 in stage 4.0 (TID 193)
21/05/17 14:09:53 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 189) in 2341 ms on d733d8da4350 (executor driver) (35/200)
[2021-05-17 11:09:53,066] {docker.py:276} INFO - 21/05/17 14:09:53 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:53,069] {docker.py:276} INFO - 21/05/17 14:09:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:53,070] {docker.py:276} INFO - 21/05/17 14:09:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218051473921678294530_0004_m_000038_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218051473921678294530_0004_m_000038_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218051473921678294530_0004}; taskId=attempt_202105171409218051473921678294530_0004_m_000038_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25d7d560}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409218051473921678294530_0004_m_000038_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218051473921678294530_0004_m_000038_193
[2021-05-17 11:09:53,074] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Task committer attempt_202105171409218051473921678294530_0004_m_000038_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218051473921678294530_0004_m_000038_193 : duration 0:00.004s
[2021-05-17 11:09:53,254] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409218664389588819397911_0004_m_000035_190: needsTaskCommit() Task attempt_202105171409218664389588819397911_0004_m_000035_190
[2021-05-17 11:09:53,255] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Task committer attempt_202105171409218664389588819397911_0004_m_000035_190: needsTaskCommit() Task attempt_202105171409218664389588819397911_0004_m_000035_190: duration 0:00.001s
21/05/17 14:09:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218664389588819397911_0004_m_000035_190
[2021-05-17 11:09:53,256] {docker.py:276} INFO - 21/05/17 14:09:53 INFO Executor: Finished task 35.0 in stage 4.0 (TID 190). 4544 bytes result sent to driver
[2021-05-17 11:09:53,258] {docker.py:276} INFO - 21/05/17 14:09:53 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 194) (d733d8da4350, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:53,258] {docker.py:276} INFO - 21/05/17 14:09:53 INFO Executor: Running task 39.0 in stage 4.0 (TID 194)
[2021-05-17 11:09:53,259] {docker.py:276} INFO - 21/05/17 14:09:53 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 190) in 2371 ms on d733d8da4350 (executor driver) (36/200)
[2021-05-17 11:09:53,269] {docker.py:276} INFO - 21/05/17 14:09:53 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:53,271] {docker.py:276} INFO - 21/05/17 14:09:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:09:53,272] {docker.py:276} INFO - 21/05/17 14:09:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:53,272] {docker.py:276} INFO - 21/05/17 14:09:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:53,272] {docker.py:276} INFO - 21/05/17 14:09:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921394184801738779024_0004_m_000039_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921394184801738779024_0004_m_000039_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921394184801738779024_0004}; taskId=attempt_20210517140921394184801738779024_0004_m_000039_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5491ec77}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:53,272] {docker.py:276} INFO - 21/05/17 14:09:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:53,273] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Starting: Task committer attempt_20210517140921394184801738779024_0004_m_000039_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921394184801738779024_0004_m_000039_194
[2021-05-17 11:09:53,277] {docker.py:276} INFO - 21/05/17 14:09:53 INFO StagingCommitter: Task committer attempt_20210517140921394184801738779024_0004_m_000039_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921394184801738779024_0004_m_000039_194 : duration 0:00.004s
[2021-05-17 11:09:55,159] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409215726936194165597902_0004_m_000037_192: needsTaskCommit() Task attempt_202105171409215726936194165597902_0004_m_000037_192
[2021-05-17 11:09:55,160] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_202105171409215726936194165597902_0004_m_000037_192: needsTaskCommit() Task attempt_202105171409215726936194165597902_0004_m_000037_192: duration 0:00.001s
21/05/17 14:09:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215726936194165597902_0004_m_000037_192
[2021-05-17 11:09:55,163] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Finished task 37.0 in stage 4.0 (TID 192). 4544 bytes result sent to driver
[2021-05-17 11:09:55,166] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 195) (d733d8da4350, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:55,166] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Running task 40.0 in stage 4.0 (TID 195)
[2021-05-17 11:09:55,167] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 192) in 2292 ms on d733d8da4350 (executor driver) (37/200)
[2021-05-17 11:09:55,176] {docker.py:276} INFO - 21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 4 (23.5 KiB) non-empty blocks including 4 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:55,178] {docker.py:276} INFO - 21/05/17 14:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:55,179] {docker.py:276} INFO - 21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211866654732301064232_0004_m_000040_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211866654732301064232_0004_m_000040_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211866654732301064232_0004}; taskId=attempt_202105171409211866654732301064232_0004_m_000040_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31f8dd10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:55,179] {docker.py:276} INFO - 21/05/17 14:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409211866654732301064232_0004_m_000040_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211866654732301064232_0004_m_000040_195
[2021-05-17 11:09:55,181] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409218559977782039531876_0004_m_000036_191: needsTaskCommit() Task attempt_202105171409218559977782039531876_0004_m_000036_191
[2021-05-17 11:09:55,182] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_202105171409218559977782039531876_0004_m_000036_191: needsTaskCommit() Task attempt_202105171409218559977782039531876_0004_m_000036_191: duration 0:00.000s
21/05/17 14:09:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218559977782039531876_0004_m_000036_191
[2021-05-17 11:09:55,183] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Finished task 36.0 in stage 4.0 (TID 191). 4544 bytes result sent to driver
[2021-05-17 11:09:55,184] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 196) (d733d8da4350, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:55,185] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Running task 41.0 in stage 4.0 (TID 196)
[2021-05-17 11:09:55,185] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 191) in 2326 ms on d733d8da4350 (executor driver) (38/200)
[2021-05-17 11:09:55,188] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_202105171409211866654732301064232_0004_m_000040_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211866654732301064232_0004_m_000040_195 : duration 0:00.009s
[2021-05-17 11:09:55,193] {docker.py:276} INFO - 21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:55,195] {docker.py:276} INFO - 21/05/17 14:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921676606539544961991_0004_m_000041_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921676606539544961991_0004_m_000041_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921676606539544961991_0004}; taskId=attempt_20210517140921676606539544961991_0004_m_000041_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9a1821c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:55,195] {docker.py:276} INFO - 21/05/17 14:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:55,195] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_20210517140921676606539544961991_0004_m_000041_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921676606539544961991_0004_m_000041_196
[2021-05-17 11:09:55,198] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_20210517140921676606539544961991_0004_m_000041_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921676606539544961991_0004_m_000041_196 : duration 0:00.002s
[2021-05-17 11:09:55,298] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409218051473921678294530_0004_m_000038_193: needsTaskCommit() Task attempt_202105171409218051473921678294530_0004_m_000038_193
[2021-05-17 11:09:55,299] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_202105171409218051473921678294530_0004_m_000038_193: needsTaskCommit() Task attempt_202105171409218051473921678294530_0004_m_000038_193: duration 0:00.001s
21/05/17 14:09:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218051473921678294530_0004_m_000038_193
[2021-05-17 11:09:55,302] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Finished task 38.0 in stage 4.0 (TID 193). 4544 bytes result sent to driver
[2021-05-17 11:09:55,304] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 197) (d733d8da4350, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:55,305] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 193) in 2254 ms on d733d8da4350 (executor driver) (39/200)
[2021-05-17 11:09:55,306] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Running task 42.0 in stage 4.0 (TID 197)
[2021-05-17 11:09:55,314] {docker.py:276} INFO - 21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 4 (20.1 KiB) non-empty blocks including 4 (20.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:55,316] {docker.py:276} INFO - 21/05/17 14:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:55,317] {docker.py:276} INFO - 21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218982174386104432838_0004_m_000042_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218982174386104432838_0004_m_000042_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218982174386104432838_0004}; taskId=attempt_202105171409218982174386104432838_0004_m_000042_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a4953cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:55,317] {docker.py:276} INFO - 21/05/17 14:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409218982174386104432838_0004_m_000042_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218982174386104432838_0004_m_000042_197
[2021-05-17 11:09:55,322] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_202105171409218982174386104432838_0004_m_000042_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218982174386104432838_0004_m_000042_197 : duration 0:00.005s
[2021-05-17 11:09:55,622] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_20210517140921394184801738779024_0004_m_000039_194: needsTaskCommit() Task attempt_20210517140921394184801738779024_0004_m_000039_194
21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_20210517140921394184801738779024_0004_m_000039_194: needsTaskCommit() Task attempt_20210517140921394184801738779024_0004_m_000039_194: duration 0:00.001s
[2021-05-17 11:09:55,623] {docker.py:276} INFO - 21/05/17 14:09:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921394184801738779024_0004_m_000039_194
[2021-05-17 11:09:55,627] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Finished task 39.0 in stage 4.0 (TID 194). 4544 bytes result sent to driver
[2021-05-17 11:09:55,628] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 198) (d733d8da4350, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:55,629] {docker.py:276} INFO - 21/05/17 14:09:55 INFO Executor: Running task 43.0 in stage 4.0 (TID 198)
[2021-05-17 11:09:55,629] {docker.py:276} INFO - 21/05/17 14:09:55 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 194) in 2375 ms on d733d8da4350 (executor driver) (40/200)
[2021-05-17 11:09:55,638] {docker.py:276} INFO - 21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:55,641] {docker.py:276} INFO - 21/05/17 14:09:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:55,641] {docker.py:276} INFO - 21/05/17 14:09:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213461221180481598669_0004_m_000043_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213461221180481598669_0004_m_000043_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213461221180481598669_0004}; taskId=attempt_202105171409213461221180481598669_0004_m_000043_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b3a7918}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:55,641] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409213461221180481598669_0004_m_000043_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213461221180481598669_0004_m_000043_198
[2021-05-17 11:09:55,644] {docker.py:276} INFO - 21/05/17 14:09:55 INFO StagingCommitter: Task committer attempt_202105171409213461221180481598669_0004_m_000043_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213461221180481598669_0004_m_000043_198 : duration 0:00.003s
[2021-05-17 11:09:57,470] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_20210517140921676606539544961991_0004_m_000041_196: needsTaskCommit() Task attempt_20210517140921676606539544961991_0004_m_000041_196
[2021-05-17 11:09:57,471] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_20210517140921676606539544961991_0004_m_000041_196: needsTaskCommit() Task attempt_20210517140921676606539544961991_0004_m_000041_196: duration 0:00.001s
[2021-05-17 11:09:57,472] {docker.py:276} INFO - 21/05/17 14:09:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921676606539544961991_0004_m_000041_196
[2021-05-17 11:09:57,474] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Finished task 41.0 in stage 4.0 (TID 196). 4544 bytes result sent to driver
[2021-05-17 11:09:57,476] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 199) (d733d8da4350, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:57,477] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 196) in 2260 ms on d733d8da4350 (executor driver) (41/200)
[2021-05-17 11:09:57,479] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Running task 44.0 in stage 4.0 (TID 199)
[2021-05-17 11:09:57,488] {docker.py:276} INFO - 21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Getting 4 (22.8 KiB) non-empty blocks including 4 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:57,491] {docker.py:276} INFO - 21/05/17 14:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212378139001318384868_0004_m_000044_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212378139001318384868_0004_m_000044_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212378139001318384868_0004}; taskId=attempt_202105171409212378139001318384868_0004_m_000044_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41106dd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:57,491] {docker.py:276} INFO - 21/05/17 14:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409212378139001318384868_0004_m_000044_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212378139001318384868_0004_m_000044_199
[2021-05-17 11:09:57,493] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_202105171409212378139001318384868_0004_m_000044_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212378139001318384868_0004_m_000044_199 : duration 0:00.002s
[2021-05-17 11:09:57,525] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409211866654732301064232_0004_m_000040_195: needsTaskCommit() Task attempt_202105171409211866654732301064232_0004_m_000040_195
21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_202105171409211866654732301064232_0004_m_000040_195: needsTaskCommit() Task attempt_202105171409211866654732301064232_0004_m_000040_195: duration 0:00.000s
21/05/17 14:09:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211866654732301064232_0004_m_000040_195
[2021-05-17 11:09:57,526] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Finished task 40.0 in stage 4.0 (TID 195). 4544 bytes result sent to driver
[2021-05-17 11:09:57,527] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 200) (d733d8da4350, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:57,529] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Running task 45.0 in stage 4.0 (TID 200)
21/05/17 14:09:57 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 195) in 2333 ms on d733d8da4350 (executor driver) (42/200)
[2021-05-17 11:09:57,538] {docker.py:276} INFO - 21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Getting 4 (21.1 KiB) non-empty blocks including 4 (21.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:57,540] {docker.py:276} INFO - 21/05/17 14:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921245186468061085807_0004_m_000045_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921245186468061085807_0004_m_000045_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921245186468061085807_0004}; taskId=attempt_20210517140921245186468061085807_0004_m_000045_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e2ce853}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:57,541] {docker.py:276} INFO - 21/05/17 14:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_20210517140921245186468061085807_0004_m_000045_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921245186468061085807_0004_m_000045_200
[2021-05-17 11:09:57,545] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_20210517140921245186468061085807_0004_m_000045_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921245186468061085807_0004_m_000045_200 : duration 0:00.004s
[2021-05-17 11:09:57,575] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409218982174386104432838_0004_m_000042_197: needsTaskCommit() Task attempt_202105171409218982174386104432838_0004_m_000042_197
[2021-05-17 11:09:57,576] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_202105171409218982174386104432838_0004_m_000042_197: needsTaskCommit() Task attempt_202105171409218982174386104432838_0004_m_000042_197: duration 0:00.000s
21/05/17 14:09:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218982174386104432838_0004_m_000042_197
[2021-05-17 11:09:57,578] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Finished task 42.0 in stage 4.0 (TID 197). 4544 bytes result sent to driver
[2021-05-17 11:09:57,579] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 201) (d733d8da4350, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:57,580] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Running task 46.0 in stage 4.0 (TID 201)
[2021-05-17 11:09:57,582] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 197) in 2246 ms on d733d8da4350 (executor driver) (43/200)
[2021-05-17 11:09:57,590] {docker.py:276} INFO - 21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:57,591] {docker.py:276} INFO - 21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:57,593] {docker.py:276} INFO - 21/05/17 14:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:09:57,593] {docker.py:276} INFO - 21/05/17 14:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:57,594] {docker.py:276} INFO - 21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:57,594] {docker.py:276} INFO - 21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215644334190131926010_0004_m_000046_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215644334190131926010_0004_m_000046_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215644334190131926010_0004}; taskId=attempt_202105171409215644334190131926010_0004_m_000046_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e643a08}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:57,594] {docker.py:276} INFO - 21/05/17 14:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409215644334190131926010_0004_m_000046_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215644334190131926010_0004_m_000046_201
[2021-05-17 11:09:57,598] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_202105171409215644334190131926010_0004_m_000046_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215644334190131926010_0004_m_000046_201 : duration 0:00.004s
[2021-05-17 11:09:57,966] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409213461221180481598669_0004_m_000043_198: needsTaskCommit() Task attempt_202105171409213461221180481598669_0004_m_000043_198
[2021-05-17 11:09:57,967] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_202105171409213461221180481598669_0004_m_000043_198: needsTaskCommit() Task attempt_202105171409213461221180481598669_0004_m_000043_198: duration 0:00.001s
21/05/17 14:09:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213461221180481598669_0004_m_000043_198
[2021-05-17 11:09:57,969] {docker.py:276} INFO - 21/05/17 14:09:57 INFO Executor: Finished task 43.0 in stage 4.0 (TID 198). 4544 bytes result sent to driver
[2021-05-17 11:09:57,970] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 202) (d733d8da4350, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:57,972] {docker.py:276} INFO - 21/05/17 14:09:57 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 198) in 2313 ms on d733d8da4350 (executor driver) (44/200)
21/05/17 14:09:57 INFO Executor: Running task 47.0 in stage 4.0 (TID 202)
[2021-05-17 11:09:57,986] {docker.py:276} INFO - 21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Getting 4 (20.3 KiB) non-empty blocks including 4 (20.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:09:57,986] {docker.py:276} INFO - 21/05/17 14:09:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:57,991] {docker.py:276} INFO - 21/05/17 14:09:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214923505145477527203_0004_m_000047_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214923505145477527203_0004_m_000047_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214923505145477527203_0004}; taskId=attempt_202105171409214923505145477527203_0004_m_000047_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@726238c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:57,991] {docker.py:276} INFO - 21/05/17 14:09:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409214923505145477527203_0004_m_000047_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214923505145477527203_0004_m_000047_202
[2021-05-17 11:09:57,994] {docker.py:276} INFO - 21/05/17 14:09:57 INFO StagingCommitter: Task committer attempt_202105171409214923505145477527203_0004_m_000047_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214923505145477527203_0004_m_000047_202 : duration 0:00.002s
[2021-05-17 11:09:59,440] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Starting: Task committer attempt_202105171409212378139001318384868_0004_m_000044_199: needsTaskCommit() Task attempt_202105171409212378139001318384868_0004_m_000044_199
[2021-05-17 11:09:59,441] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Task committer attempt_202105171409212378139001318384868_0004_m_000044_199: needsTaskCommit() Task attempt_202105171409212378139001318384868_0004_m_000044_199: duration 0:00.000s
21/05/17 14:09:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212378139001318384868_0004_m_000044_199
[2021-05-17 11:09:59,442] {docker.py:276} INFO - 21/05/17 14:09:59 INFO Executor: Finished task 44.0 in stage 4.0 (TID 199). 4587 bytes result sent to driver
[2021-05-17 11:09:59,443] {docker.py:276} INFO - 21/05/17 14:09:59 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 203) (d733d8da4350, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:59,444] {docker.py:276} INFO - 21/05/17 14:09:59 INFO Executor: Running task 48.0 in stage 4.0 (TID 203)
[2021-05-17 11:09:59,445] {docker.py:276} INFO - 21/05/17 14:09:59 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 199) in 1971 ms on d733d8da4350 (executor driver) (45/200)
[2021-05-17 11:09:59,453] {docker.py:276} INFO - 21/05/17 14:09:59 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:59,455] {docker.py:276} INFO - 21/05/17 14:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:09:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:09:59,455] {docker.py:276} INFO - 21/05/17 14:09:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214814410070026668105_0004_m_000048_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214814410070026668105_0004_m_000048_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214814410070026668105_0004}; taskId=attempt_202105171409214814410070026668105_0004_m_000048_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e9687e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:09:59 INFO StagingCommitter: Starting: Task committer attempt_202105171409214814410070026668105_0004_m_000048_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214814410070026668105_0004_m_000048_203
[2021-05-17 11:09:59,458] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Task committer attempt_202105171409214814410070026668105_0004_m_000048_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214814410070026668105_0004_m_000048_203 : duration 0:00.003s
[2021-05-17 11:09:59,957] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Starting: Task committer attempt_202105171409215644334190131926010_0004_m_000046_201: needsTaskCommit() Task attempt_202105171409215644334190131926010_0004_m_000046_201
[2021-05-17 11:09:59,958] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Task committer attempt_202105171409215644334190131926010_0004_m_000046_201: needsTaskCommit() Task attempt_202105171409215644334190131926010_0004_m_000046_201: duration 0:00.000s
21/05/17 14:09:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215644334190131926010_0004_m_000046_201
[2021-05-17 11:09:59,959] {docker.py:276} INFO - 21/05/17 14:09:59 INFO Executor: Finished task 46.0 in stage 4.0 (TID 201). 4587 bytes result sent to driver
[2021-05-17 11:09:59,960] {docker.py:276} INFO - 21/05/17 14:09:59 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 204) (d733d8da4350, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:09:59,962] {docker.py:276} INFO - 21/05/17 14:09:59 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 201) in 2386 ms on d733d8da4350 (executor driver) (46/200)
[2021-05-17 11:09:59,963] {docker.py:276} INFO - 21/05/17 14:09:59 INFO Executor: Running task 49.0 in stage 4.0 (TID 204)
[2021-05-17 11:09:59,971] {docker.py:276} INFO - 21/05/17 14:09:59 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:09:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:09:59,973] {docker.py:276} INFO - 21/05/17 14:09:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:09:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:09:59,974] {docker.py:276} INFO - 21/05/17 14:09:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:09:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212433033217812017173_0004_m_000049_204, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212433033217812017173_0004_m_000049_204}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212433033217812017173_0004}; taskId=attempt_202105171409212433033217812017173_0004_m_000049_204, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@126fad4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:09:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:09:59,974] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Starting: Task committer attempt_202105171409212433033217812017173_0004_m_000049_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212433033217812017173_0004_m_000049_204
[2021-05-17 11:09:59,977] {docker.py:276} INFO - 21/05/17 14:09:59 INFO StagingCommitter: Task committer attempt_202105171409212433033217812017173_0004_m_000049_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212433033217812017173_0004_m_000049_204 : duration 0:00.004s
[2021-05-17 11:10:00,027] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Starting: Task committer attempt_20210517140921245186468061085807_0004_m_000045_200: needsTaskCommit() Task attempt_20210517140921245186468061085807_0004_m_000045_200
[2021-05-17 11:10:00,028] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Task committer attempt_20210517140921245186468061085807_0004_m_000045_200: needsTaskCommit() Task attempt_20210517140921245186468061085807_0004_m_000045_200: duration 0:00.001s
[2021-05-17 11:10:00,028] {docker.py:276} INFO - 21/05/17 14:10:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921245186468061085807_0004_m_000045_200
[2021-05-17 11:10:00,029] {docker.py:276} INFO - 21/05/17 14:10:00 INFO Executor: Finished task 45.0 in stage 4.0 (TID 200). 4587 bytes result sent to driver
[2021-05-17 11:10:00,029] {docker.py:276} INFO - 21/05/17 14:10:00 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 205) (d733d8da4350, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:00,030] {docker.py:276} INFO - 21/05/17 14:10:00 INFO Executor: Running task 50.0 in stage 4.0 (TID 205)
[2021-05-17 11:10:00,031] {docker.py:276} INFO - 21/05/17 14:10:00 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 200) in 2506 ms on d733d8da4350 (executor driver) (47/200)
[2021-05-17 11:10:00,038] {docker.py:276} INFO - 21/05/17 14:10:00 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:00,040] {docker.py:276} INFO - 21/05/17 14:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:00,040] {docker.py:276} INFO - 21/05/17 14:10:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218424159388414931758_0004_m_000050_205, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218424159388414931758_0004_m_000050_205}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218424159388414931758_0004}; taskId=attempt_202105171409218424159388414931758_0004_m_000050_205, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ad29283}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:00,040] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409218424159388414931758_0004_m_000050_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218424159388414931758_0004_m_000050_205
[2021-05-17 11:10:00,043] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Task committer attempt_202105171409218424159388414931758_0004_m_000050_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218424159388414931758_0004_m_000050_205 : duration 0:00.003s
[2021-05-17 11:10:00,443] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409214923505145477527203_0004_m_000047_202: needsTaskCommit() Task attempt_202105171409214923505145477527203_0004_m_000047_202
[2021-05-17 11:10:00,444] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Task committer attempt_202105171409214923505145477527203_0004_m_000047_202: needsTaskCommit() Task attempt_202105171409214923505145477527203_0004_m_000047_202: duration 0:00.001s
21/05/17 14:10:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214923505145477527203_0004_m_000047_202
[2021-05-17 11:10:00,446] {docker.py:276} INFO - 21/05/17 14:10:00 INFO Executor: Finished task 47.0 in stage 4.0 (TID 202). 4587 bytes result sent to driver
[2021-05-17 11:10:00,447] {docker.py:276} INFO - 21/05/17 14:10:00 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 206) (d733d8da4350, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:00,448] {docker.py:276} INFO - 21/05/17 14:10:00 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 202) in 2481 ms on d733d8da4350 (executor driver) (48/200)
[2021-05-17 11:10:00,449] {docker.py:276} INFO - 21/05/17 14:10:00 INFO Executor: Running task 51.0 in stage 4.0 (TID 206)
[2021-05-17 11:10:00,459] {docker.py:276} INFO - 21/05/17 14:10:00 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:00,461] {docker.py:276} INFO - 21/05/17 14:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:00,461] {docker.py:276} INFO - 21/05/17 14:10:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212958805463398929735_0004_m_000051_206, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212958805463398929735_0004_m_000051_206}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212958805463398929735_0004}; taskId=attempt_202105171409212958805463398929735_0004_m_000051_206, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7153248f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409212958805463398929735_0004_m_000051_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212958805463398929735_0004_m_000051_206
[2021-05-17 11:10:00,466] {docker.py:276} INFO - 21/05/17 14:10:00 INFO StagingCommitter: Task committer attempt_202105171409212958805463398929735_0004_m_000051_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212958805463398929735_0004_m_000051_206 : duration 0:00.005s
[2021-05-17 11:10:01,392] {docker.py:276} INFO - 21/05/17 14:10:01 INFO StagingCommitter: Starting: Task committer attempt_202105171409214814410070026668105_0004_m_000048_203: needsTaskCommit() Task attempt_202105171409214814410070026668105_0004_m_000048_203
[2021-05-17 11:10:01,392] {docker.py:276} INFO - 21/05/17 14:10:01 INFO StagingCommitter: Task committer attempt_202105171409214814410070026668105_0004_m_000048_203: needsTaskCommit() Task attempt_202105171409214814410070026668105_0004_m_000048_203: duration 0:00.001s
21/05/17 14:10:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214814410070026668105_0004_m_000048_203
[2021-05-17 11:10:01,394] {docker.py:276} INFO - 21/05/17 14:10:01 INFO Executor: Finished task 48.0 in stage 4.0 (TID 203). 4544 bytes result sent to driver
[2021-05-17 11:10:01,395] {docker.py:276} INFO - 21/05/17 14:10:01 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 207) (d733d8da4350, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:01,396] {docker.py:276} INFO - 21/05/17 14:10:01 INFO Executor: Running task 52.0 in stage 4.0 (TID 207)
21/05/17 14:10:01 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 203) in 1957 ms on d733d8da4350 (executor driver) (49/200)
[2021-05-17 11:10:01,407] {docker.py:276} INFO - 21/05/17 14:10:01 INFO ShuffleBlockFetcherIterator: Getting 4 (20.4 KiB) non-empty blocks including 4 (20.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:01,409] {docker.py:276} INFO - 21/05/17 14:10:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215719086194517123447_0004_m_000052_207, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215719086194517123447_0004_m_000052_207}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215719086194517123447_0004}; taskId=attempt_202105171409215719086194517123447_0004_m_000052_207, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@380c98a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:01,410] {docker.py:276} INFO - 21/05/17 14:10:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:01 INFO StagingCommitter: Starting: Task committer attempt_202105171409215719086194517123447_0004_m_000052_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215719086194517123447_0004_m_000052_207
[2021-05-17 11:10:01,413] {docker.py:276} INFO - 21/05/17 14:10:01 INFO StagingCommitter: Task committer attempt_202105171409215719086194517123447_0004_m_000052_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215719086194517123447_0004_m_000052_207 : duration 0:00.003s
[2021-05-17 11:10:02,013] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409212433033217812017173_0004_m_000049_204: needsTaskCommit() Task attempt_202105171409212433033217812017173_0004_m_000049_204
[2021-05-17 11:10:02,014] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Task committer attempt_202105171409212433033217812017173_0004_m_000049_204: needsTaskCommit() Task attempt_202105171409212433033217812017173_0004_m_000049_204: duration 0:00.001s
21/05/17 14:10:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212433033217812017173_0004_m_000049_204
[2021-05-17 11:10:02,016] {docker.py:276} INFO - 21/05/17 14:10:02 INFO Executor: Finished task 49.0 in stage 4.0 (TID 204). 4544 bytes result sent to driver
[2021-05-17 11:10:02,017] {docker.py:276} INFO - 21/05/17 14:10:02 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 208) (d733d8da4350, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:02,018] {docker.py:276} INFO - 21/05/17 14:10:02 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 204) in 2060 ms on d733d8da4350 (executor driver) (50/200)
21/05/17 14:10:02 INFO Executor: Running task 53.0 in stage 4.0 (TID 208)
[2021-05-17 11:10:02,029] {docker.py:276} INFO - 21/05/17 14:10:02 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:02,030] {docker.py:276} INFO - 21/05/17 14:10:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:02,033] {docker.py:276} INFO - 21/05/17 14:10:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:02,033] {docker.py:276} INFO - 21/05/17 14:10:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218191899947351272498_0004_m_000053_208, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218191899947351272498_0004_m_000053_208}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218191899947351272498_0004}; taskId=attempt_202105171409218191899947351272498_0004_m_000053_208, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fccdcbc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409218191899947351272498_0004_m_000053_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218191899947351272498_0004_m_000053_208
[2021-05-17 11:10:02,036] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Task committer attempt_202105171409218191899947351272498_0004_m_000053_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218191899947351272498_0004_m_000053_208 : duration 0:00.003s
[2021-05-17 11:10:02,442] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409218424159388414931758_0004_m_000050_205: needsTaskCommit() Task attempt_202105171409218424159388414931758_0004_m_000050_205
[2021-05-17 11:10:02,443] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Task committer attempt_202105171409218424159388414931758_0004_m_000050_205: needsTaskCommit() Task attempt_202105171409218424159388414931758_0004_m_000050_205: duration 0:00.000s
21/05/17 14:10:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218424159388414931758_0004_m_000050_205
[2021-05-17 11:10:02,444] {docker.py:276} INFO - 21/05/17 14:10:02 INFO Executor: Finished task 50.0 in stage 4.0 (TID 205). 4544 bytes result sent to driver
[2021-05-17 11:10:02,445] {docker.py:276} INFO - 21/05/17 14:10:02 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 209) (d733d8da4350, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:02,446] {docker.py:276} INFO - 21/05/17 14:10:02 INFO Executor: Running task 54.0 in stage 4.0 (TID 209)
21/05/17 14:10:02 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 205) in 2420 ms on d733d8da4350 (executor driver) (51/200)
[2021-05-17 11:10:02,453] {docker.py:276} INFO - 21/05/17 14:10:02 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:02,455] {docker.py:276} INFO - 21/05/17 14:10:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:02,456] {docker.py:276} INFO - 21/05/17 14:10:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215661121399802859686_0004_m_000054_209, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215661121399802859686_0004_m_000054_209}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215661121399802859686_0004}; taskId=attempt_202105171409215661121399802859686_0004_m_000054_209, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71121c18}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409215661121399802859686_0004_m_000054_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215661121399802859686_0004_m_000054_209
[2021-05-17 11:10:02,458] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Task committer attempt_202105171409215661121399802859686_0004_m_000054_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215661121399802859686_0004_m_000054_209 : duration 0:00.003s
[2021-05-17 11:10:02,750] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409212958805463398929735_0004_m_000051_206: needsTaskCommit() Task attempt_202105171409212958805463398929735_0004_m_000051_206
[2021-05-17 11:10:02,751] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Task committer attempt_202105171409212958805463398929735_0004_m_000051_206: needsTaskCommit() Task attempt_202105171409212958805463398929735_0004_m_000051_206: duration 0:00.001s
21/05/17 14:10:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212958805463398929735_0004_m_000051_206
[2021-05-17 11:10:02,753] {docker.py:276} INFO - 21/05/17 14:10:02 INFO Executor: Finished task 51.0 in stage 4.0 (TID 206). 4544 bytes result sent to driver
[2021-05-17 11:10:02,755] {docker.py:276} INFO - 21/05/17 14:10:02 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 210) (d733d8da4350, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:02,757] {docker.py:276} INFO - 21/05/17 14:10:02 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 206) in 2312 ms on d733d8da4350 (executor driver) (52/200)
[2021-05-17 11:10:02,758] {docker.py:276} INFO - 21/05/17 14:10:02 INFO Executor: Running task 55.0 in stage 4.0 (TID 210)
[2021-05-17 11:10:02,768] {docker.py:276} INFO - 21/05/17 14:10:02 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:02,769] {docker.py:276} INFO - 21/05/17 14:10:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214059530663776508297_0004_m_000055_210, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214059530663776508297_0004_m_000055_210}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214059530663776508297_0004}; taskId=attempt_202105171409214059530663776508297_0004_m_000055_210, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d607184}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:02,770] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409214059530663776508297_0004_m_000055_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214059530663776508297_0004_m_000055_210
[2021-05-17 11:10:02,773] {docker.py:276} INFO - 21/05/17 14:10:02 INFO StagingCommitter: Task committer attempt_202105171409214059530663776508297_0004_m_000055_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214059530663776508297_0004_m_000055_210 : duration 0:00.003s
[2021-05-17 11:10:03,432] {docker.py:276} INFO - 21/05/17 14:10:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409218191899947351272498_0004_m_000053_208: needsTaskCommit() Task attempt_202105171409218191899947351272498_0004_m_000053_208
21/05/17 14:10:03 INFO StagingCommitter: Task committer attempt_202105171409218191899947351272498_0004_m_000053_208: needsTaskCommit() Task attempt_202105171409218191899947351272498_0004_m_000053_208: duration 0:00.001s
21/05/17 14:10:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218191899947351272498_0004_m_000053_208
[2021-05-17 11:10:03,435] {docker.py:276} INFO - 21/05/17 14:10:03 INFO Executor: Finished task 53.0 in stage 4.0 (TID 208). 4544 bytes result sent to driver
[2021-05-17 11:10:03,437] {docker.py:276} INFO - 21/05/17 14:10:03 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 211) (d733d8da4350, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:03,439] {docker.py:276} INFO - 21/05/17 14:10:03 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 208) in 1422 ms on d733d8da4350 (executor driver) (53/200)
21/05/17 14:10:03 INFO Executor: Running task 56.0 in stage 4.0 (TID 211)
[2021-05-17 11:10:03,447] {docker.py:276} INFO - 21/05/17 14:10:03 INFO ShuffleBlockFetcherIterator: Getting 4 (21.0 KiB) non-empty blocks including 4 (21.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:03,450] {docker.py:276} INFO - 21/05/17 14:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:03,450] {docker.py:276} INFO - 21/05/17 14:10:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217637632866084279660_0004_m_000056_211, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217637632866084279660_0004_m_000056_211}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217637632866084279660_0004}; taskId=attempt_202105171409217637632866084279660_0004_m_000056_211, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e544769}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:03,451] {docker.py:276} INFO - 21/05/17 14:10:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409217637632866084279660_0004_m_000056_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217637632866084279660_0004_m_000056_211
[2021-05-17 11:10:03,454] {docker.py:276} INFO - 21/05/17 14:10:03 INFO StagingCommitter: Task committer attempt_202105171409217637632866084279660_0004_m_000056_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217637632866084279660_0004_m_000056_211 : duration 0:00.003s
[2021-05-17 11:10:03,487] {docker.py:276} INFO - 21/05/17 14:10:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409215719086194517123447_0004_m_000052_207: needsTaskCommit() Task attempt_202105171409215719086194517123447_0004_m_000052_207
[2021-05-17 11:10:03,488] {docker.py:276} INFO - 21/05/17 14:10:03 INFO StagingCommitter: Task committer attempt_202105171409215719086194517123447_0004_m_000052_207: needsTaskCommit() Task attempt_202105171409215719086194517123447_0004_m_000052_207: duration 0:00.001s
21/05/17 14:10:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215719086194517123447_0004_m_000052_207
[2021-05-17 11:10:03,489] {docker.py:276} INFO - 21/05/17 14:10:03 INFO Executor: Finished task 52.0 in stage 4.0 (TID 207). 4544 bytes result sent to driver
[2021-05-17 11:10:03,489] {docker.py:276} INFO - 21/05/17 14:10:03 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 212) (d733d8da4350, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:03,490] {docker.py:276} INFO - 21/05/17 14:10:03 INFO Executor: Running task 57.0 in stage 4.0 (TID 212)
21/05/17 14:10:03 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 207) in 2098 ms on d733d8da4350 (executor driver) (54/200)
[2021-05-17 11:10:03,498] {docker.py:276} INFO - 21/05/17 14:10:03 INFO ShuffleBlockFetcherIterator: Getting 4 (20.1 KiB) non-empty blocks including 4 (20.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:03,500] {docker.py:276} INFO - 21/05/17 14:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:03,500] {docker.py:276} INFO - 21/05/17 14:10:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213627799769292210868_0004_m_000057_212, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213627799769292210868_0004_m_000057_212}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213627799769292210868_0004}; taskId=attempt_202105171409213627799769292210868_0004_m_000057_212, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ff538da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409213627799769292210868_0004_m_000057_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213627799769292210868_0004_m_000057_212
[2021-05-17 11:10:03,503] {docker.py:276} INFO - 21/05/17 14:10:03 INFO StagingCommitter: Task committer attempt_202105171409213627799769292210868_0004_m_000057_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213627799769292210868_0004_m_000057_212 : duration 0:00.003s
[2021-05-17 11:10:04,937] {docker.py:276} INFO - 21/05/17 14:10:04 INFO StagingCommitter: Starting: Task committer attempt_202105171409215661121399802859686_0004_m_000054_209: needsTaskCommit() Task attempt_202105171409215661121399802859686_0004_m_000054_209
[2021-05-17 11:10:04,938] {docker.py:276} INFO - 21/05/17 14:10:04 INFO StagingCommitter: Task committer attempt_202105171409215661121399802859686_0004_m_000054_209: needsTaskCommit() Task attempt_202105171409215661121399802859686_0004_m_000054_209: duration 0:00.001s
[2021-05-17 11:10:04,939] {docker.py:276} INFO - 21/05/17 14:10:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215661121399802859686_0004_m_000054_209
[2021-05-17 11:10:04,941] {docker.py:276} INFO - 21/05/17 14:10:04 INFO Executor: Finished task 54.0 in stage 4.0 (TID 209). 4544 bytes result sent to driver
[2021-05-17 11:10:04,943] {docker.py:276} INFO - 21/05/17 14:10:04 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 213) (d733d8da4350, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:04,943] {docker.py:276} INFO - 21/05/17 14:10:04 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 209) in 2501 ms on d733d8da4350 (executor driver) (55/200)
[2021-05-17 11:10:04,944] {docker.py:276} INFO - 21/05/17 14:10:04 INFO Executor: Running task 58.0 in stage 4.0 (TID 213)
[2021-05-17 11:10:04,954] {docker.py:276} INFO - 21/05/17 14:10:04 INFO ShuffleBlockFetcherIterator: Getting 4 (20.4 KiB) non-empty blocks including 4 (20.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:04,956] {docker.py:276} INFO - 21/05/17 14:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:04,956] {docker.py:276} INFO - 21/05/17 14:10:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214731136489863686824_0004_m_000058_213, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214731136489863686824_0004_m_000058_213}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214731136489863686824_0004}; taskId=attempt_202105171409214731136489863686824_0004_m_000058_213, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2fd3eae2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:04,957] {docker.py:276} INFO - 21/05/17 14:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:04,957] {docker.py:276} INFO - 21/05/17 14:10:04 INFO StagingCommitter: Starting: Task committer attempt_202105171409214731136489863686824_0004_m_000058_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214731136489863686824_0004_m_000058_213
[2021-05-17 11:10:04,959] {docker.py:276} INFO - 21/05/17 14:10:04 INFO StagingCommitter: Task committer attempt_202105171409214731136489863686824_0004_m_000058_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214731136489863686824_0004_m_000058_213 : duration 0:00.003s
[2021-05-17 11:10:05,591] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409217637632866084279660_0004_m_000056_211: needsTaskCommit() Task attempt_202105171409217637632866084279660_0004_m_000056_211
21/05/17 14:10:05 INFO StagingCommitter: Task committer attempt_202105171409217637632866084279660_0004_m_000056_211: needsTaskCommit() Task attempt_202105171409217637632866084279660_0004_m_000056_211: duration 0:00.001s
21/05/17 14:10:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217637632866084279660_0004_m_000056_211
[2021-05-17 11:10:05,594] {docker.py:276} INFO - 21/05/17 14:10:05 INFO Executor: Finished task 56.0 in stage 4.0 (TID 211). 4544 bytes result sent to driver
[2021-05-17 11:10:05,594] {docker.py:276} INFO - 21/05/17 14:10:05 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 214) (d733d8da4350, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:05,596] {docker.py:276} INFO - 21/05/17 14:10:05 INFO Executor: Running task 59.0 in stage 4.0 (TID 214)
[2021-05-17 11:10:05,597] {docker.py:276} INFO - 21/05/17 14:10:05 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 211) in 2163 ms on d733d8da4350 (executor driver) (56/200)
[2021-05-17 11:10:05,608] {docker.py:276} INFO - 21/05/17 14:10:05 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:05,608] {docker.py:276} INFO - 21/05/17 14:10:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:05,610] {docker.py:276} INFO - 21/05/17 14:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:05,611] {docker.py:276} INFO - 21/05/17 14:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:05,611] {docker.py:276} INFO - 21/05/17 14:10:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214720686185023518317_0004_m_000059_214, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214720686185023518317_0004_m_000059_214}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214720686185023518317_0004}; taskId=attempt_202105171409214720686185023518317_0004_m_000059_214, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d751be4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:05,612] {docker.py:276} INFO - 21/05/17 14:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:05,612] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409214720686185023518317_0004_m_000059_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214720686185023518317_0004_m_000059_214
[2021-05-17 11:10:05,615] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Task committer attempt_202105171409214720686185023518317_0004_m_000059_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214720686185023518317_0004_m_000059_214 : duration 0:00.003s
[2021-05-17 11:10:05,713] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409214059530663776508297_0004_m_000055_210: needsTaskCommit() Task attempt_202105171409214059530663776508297_0004_m_000055_210
[2021-05-17 11:10:05,714] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Task committer attempt_202105171409214059530663776508297_0004_m_000055_210: needsTaskCommit() Task attempt_202105171409214059530663776508297_0004_m_000055_210: duration 0:00.001s
21/05/17 14:10:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214059530663776508297_0004_m_000055_210
[2021-05-17 11:10:05,717] {docker.py:276} INFO - 21/05/17 14:10:05 INFO Executor: Finished task 55.0 in stage 4.0 (TID 210). 4544 bytes result sent to driver
[2021-05-17 11:10:05,718] {docker.py:276} INFO - 21/05/17 14:10:05 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 215) (d733d8da4350, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:05,719] {docker.py:276} INFO - 21/05/17 14:10:05 INFO Executor: Running task 60.0 in stage 4.0 (TID 215)
[2021-05-17 11:10:05,721] {docker.py:276} INFO - 21/05/17 14:10:05 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 210) in 2970 ms on d733d8da4350 (executor driver) (57/200)
[2021-05-17 11:10:05,730] {docker.py:276} INFO - 21/05/17 14:10:05 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:05,732] {docker.py:276} INFO - 21/05/17 14:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:05,733] {docker.py:276} INFO - 21/05/17 14:10:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921725416681766477700_0004_m_000060_215, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921725416681766477700_0004_m_000060_215}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921725416681766477700_0004}; taskId=attempt_20210517140921725416681766477700_0004_m_000060_215, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ced8d41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:05,733] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Starting: Task committer attempt_20210517140921725416681766477700_0004_m_000060_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921725416681766477700_0004_m_000060_215
[2021-05-17 11:10:05,735] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Task committer attempt_20210517140921725416681766477700_0004_m_000060_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921725416681766477700_0004_m_000060_215 : duration 0:00.002s
[2021-05-17 11:10:05,935] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409213627799769292210868_0004_m_000057_212: needsTaskCommit() Task attempt_202105171409213627799769292210868_0004_m_000057_212
[2021-05-17 11:10:05,936] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Task committer attempt_202105171409213627799769292210868_0004_m_000057_212: needsTaskCommit() Task attempt_202105171409213627799769292210868_0004_m_000057_212: duration 0:00.002s
21/05/17 14:10:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213627799769292210868_0004_m_000057_212
[2021-05-17 11:10:05,940] {docker.py:276} INFO - 21/05/17 14:10:05 INFO Executor: Finished task 57.0 in stage 4.0 (TID 212). 4544 bytes result sent to driver
[2021-05-17 11:10:05,941] {docker.py:276} INFO - 21/05/17 14:10:05 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 216) (d733d8da4350, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:05,942] {docker.py:276} INFO - 21/05/17 14:10:05 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 212) in 2455 ms on d733d8da4350 (executor driver) (58/200)
21/05/17 14:10:05 INFO Executor: Running task 61.0 in stage 4.0 (TID 216)
[2021-05-17 11:10:05,952] {docker.py:276} INFO - 21/05/17 14:10:05 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:05,955] {docker.py:276} INFO - 21/05/17 14:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:05,956] {docker.py:276} INFO - 21/05/17 14:10:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212599207007939839721_0004_m_000061_216, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212599207007939839721_0004_m_000061_216}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212599207007939839721_0004}; taskId=attempt_202105171409212599207007939839721_0004_m_000061_216, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d3a9a3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:05,956] {docker.py:276} INFO - 21/05/17 14:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:05,956] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409212599207007939839721_0004_m_000061_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212599207007939839721_0004_m_000061_216
[2021-05-17 11:10:05,959] {docker.py:276} INFO - 21/05/17 14:10:05 INFO StagingCommitter: Task committer attempt_202105171409212599207007939839721_0004_m_000061_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212599207007939839721_0004_m_000061_216 : duration 0:00.003s
[2021-05-17 11:10:07,310] {docker.py:276} INFO - 21/05/17 14:10:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409214731136489863686824_0004_m_000058_213: needsTaskCommit() Task attempt_202105171409214731136489863686824_0004_m_000058_213
[2021-05-17 11:10:07,311] {docker.py:276} INFO - 21/05/17 14:10:07 INFO StagingCommitter: Task committer attempt_202105171409214731136489863686824_0004_m_000058_213: needsTaskCommit() Task attempt_202105171409214731136489863686824_0004_m_000058_213: duration 0:00.002s
21/05/17 14:10:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214731136489863686824_0004_m_000058_213
[2021-05-17 11:10:07,313] {docker.py:276} INFO - 21/05/17 14:10:07 INFO Executor: Finished task 58.0 in stage 4.0 (TID 213). 4544 bytes result sent to driver
[2021-05-17 11:10:07,315] {docker.py:276} INFO - 21/05/17 14:10:07 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 217) (d733d8da4350, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:07,316] {docker.py:276} INFO - 21/05/17 14:10:07 INFO Executor: Running task 62.0 in stage 4.0 (TID 217)
21/05/17 14:10:07 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 213) in 2377 ms on d733d8da4350 (executor driver) (59/200)
[2021-05-17 11:10:07,336] {docker.py:276} INFO - 21/05/17 14:10:07 INFO ShuffleBlockFetcherIterator: Getting 4 (20.4 KiB) non-empty blocks including 4 (20.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:07,338] {docker.py:276} INFO - 21/05/17 14:10:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216703181429897726939_0004_m_000062_217, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216703181429897726939_0004_m_000062_217}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216703181429897726939_0004}; taskId=attempt_202105171409216703181429897726939_0004_m_000062_217, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40ddc717}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:07,339] {docker.py:276} INFO - 21/05/17 14:10:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409216703181429897726939_0004_m_000062_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216703181429897726939_0004_m_000062_217
[2021-05-17 11:10:07,342] {docker.py:276} INFO - 21/05/17 14:10:07 INFO StagingCommitter: Task committer attempt_202105171409216703181429897726939_0004_m_000062_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216703181429897726939_0004_m_000062_217 : duration 0:00.003s
[2021-05-17 11:10:08,098] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409214720686185023518317_0004_m_000059_214: needsTaskCommit() Task attempt_202105171409214720686185023518317_0004_m_000059_214
[2021-05-17 11:10:08,099] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Task committer attempt_202105171409214720686185023518317_0004_m_000059_214: needsTaskCommit() Task attempt_202105171409214720686185023518317_0004_m_000059_214: duration 0:00.002s
21/05/17 14:10:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214720686185023518317_0004_m_000059_214
[2021-05-17 11:10:08,100] {docker.py:276} INFO - 21/05/17 14:10:08 INFO Executor: Finished task 59.0 in stage 4.0 (TID 214). 4587 bytes result sent to driver
[2021-05-17 11:10:08,102] {docker.py:276} INFO - 21/05/17 14:10:08 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 218) (d733d8da4350, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:08,103] {docker.py:276} INFO - 21/05/17 14:10:08 INFO Executor: Running task 63.0 in stage 4.0 (TID 218)
[2021-05-17 11:10:08,104] {docker.py:276} INFO - 21/05/17 14:10:08 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 214) in 2512 ms on d733d8da4350 (executor driver) (60/200)
[2021-05-17 11:10:08,115] {docker.py:276} INFO - 21/05/17 14:10:08 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:08,115] {docker.py:276} INFO - 21/05/17 14:10:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:08,117] {docker.py:276} INFO - 21/05/17 14:10:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:08,118] {docker.py:276} INFO - 21/05/17 14:10:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:08,119] {docker.py:276} INFO - 21/05/17 14:10:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:08,119] {docker.py:276} INFO - 21/05/17 14:10:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216702340999768236339_0004_m_000063_218, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216702340999768236339_0004_m_000063_218}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216702340999768236339_0004}; taskId=attempt_202105171409216702340999768236339_0004_m_000063_218, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51cd93d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:08,119] {docker.py:276} INFO - 21/05/17 14:10:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:08,119] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409216702340999768236339_0004_m_000063_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216702340999768236339_0004_m_000063_218
[2021-05-17 11:10:08,122] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Task committer attempt_202105171409216702340999768236339_0004_m_000063_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216702340999768236339_0004_m_000063_218 : duration 0:00.003s
[2021-05-17 11:10:08,177] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Starting: Task committer attempt_20210517140921725416681766477700_0004_m_000060_215: needsTaskCommit() Task attempt_20210517140921725416681766477700_0004_m_000060_215
[2021-05-17 11:10:08,178] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Task committer attempt_20210517140921725416681766477700_0004_m_000060_215: needsTaskCommit() Task attempt_20210517140921725416681766477700_0004_m_000060_215: duration 0:00.001s
[2021-05-17 11:10:08,178] {docker.py:276} INFO - 21/05/17 14:10:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921725416681766477700_0004_m_000060_215
[2021-05-17 11:10:08,179] {docker.py:276} INFO - 21/05/17 14:10:08 INFO Executor: Finished task 60.0 in stage 4.0 (TID 215). 4587 bytes result sent to driver
[2021-05-17 11:10:08,180] {docker.py:276} INFO - 21/05/17 14:10:08 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 219) (d733d8da4350, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:08,180] {docker.py:276} INFO - 21/05/17 14:10:08 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 215) in 2466 ms on d733d8da4350 (executor driver) (61/200)
[2021-05-17 11:10:08,181] {docker.py:276} INFO - 21/05/17 14:10:08 INFO Executor: Running task 64.0 in stage 4.0 (TID 219)
[2021-05-17 11:10:08,188] {docker.py:276} INFO - 21/05/17 14:10:08 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:08,189] {docker.py:276} INFO - 21/05/17 14:10:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:08,190] {docker.py:276} INFO - 21/05/17 14:10:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:08,190] {docker.py:276} INFO - 21/05/17 14:10:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:08,191] {docker.py:276} INFO - 21/05/17 14:10:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:08,191] {docker.py:276} INFO - 21/05/17 14:10:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215241129743984930184_0004_m_000064_219, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215241129743984930184_0004_m_000064_219}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215241129743984930184_0004}; taskId=attempt_202105171409215241129743984930184_0004_m_000064_219, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c93ddc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:08,191] {docker.py:276} INFO - 21/05/17 14:10:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:08,191] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409215241129743984930184_0004_m_000064_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215241129743984930184_0004_m_000064_219
[2021-05-17 11:10:08,194] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Task committer attempt_202105171409215241129743984930184_0004_m_000064_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215241129743984930184_0004_m_000064_219 : duration 0:00.003s
[2021-05-17 11:10:08,279] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409212599207007939839721_0004_m_000061_216: needsTaskCommit() Task attempt_202105171409212599207007939839721_0004_m_000061_216
21/05/17 14:10:08 INFO StagingCommitter: Task committer attempt_202105171409212599207007939839721_0004_m_000061_216: needsTaskCommit() Task attempt_202105171409212599207007939839721_0004_m_000061_216: duration 0:00.000s
21/05/17 14:10:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212599207007939839721_0004_m_000061_216
[2021-05-17 11:10:08,280] {docker.py:276} INFO - 21/05/17 14:10:08 INFO Executor: Finished task 61.0 in stage 4.0 (TID 216). 4587 bytes result sent to driver
[2021-05-17 11:10:08,282] {docker.py:276} INFO - 21/05/17 14:10:08 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 220) (d733d8da4350, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:08,283] {docker.py:276} INFO - 21/05/17 14:10:08 INFO Executor: Running task 65.0 in stage 4.0 (TID 220)
[2021-05-17 11:10:08,283] {docker.py:276} INFO - 21/05/17 14:10:08 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 216) in 2346 ms on d733d8da4350 (executor driver) (62/200)
[2021-05-17 11:10:08,320] {docker.py:276} INFO - 21/05/17 14:10:08 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:08,321] {docker.py:276} INFO - 21/05/17 14:10:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215123426702040278522_0004_m_000065_220, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215123426702040278522_0004_m_000065_220}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215123426702040278522_0004}; taskId=attempt_202105171409215123426702040278522_0004_m_000065_220, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a3ce79c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409215123426702040278522_0004_m_000065_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215123426702040278522_0004_m_000065_220
[2021-05-17 11:10:08,322] {docker.py:276} INFO - 21/05/17 14:10:08 INFO StagingCommitter: Task committer attempt_202105171409215123426702040278522_0004_m_000065_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215123426702040278522_0004_m_000065_220 : duration 0:00.006s
[2021-05-17 11:10:09,949] {docker.py:276} INFO - 21/05/17 14:10:09 INFO StagingCommitter: Starting: Task committer attempt_202105171409216703181429897726939_0004_m_000062_217: needsTaskCommit() Task attempt_202105171409216703181429897726939_0004_m_000062_217
[2021-05-17 11:10:09,950] {docker.py:276} INFO - 21/05/17 14:10:09 INFO StagingCommitter: Task committer attempt_202105171409216703181429897726939_0004_m_000062_217: needsTaskCommit() Task attempt_202105171409216703181429897726939_0004_m_000062_217: duration 0:00.001s
21/05/17 14:10:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216703181429897726939_0004_m_000062_217
[2021-05-17 11:10:09,952] {docker.py:276} INFO - 21/05/17 14:10:09 INFO Executor: Finished task 62.0 in stage 4.0 (TID 217). 4587 bytes result sent to driver
[2021-05-17 11:10:09,953] {docker.py:276} INFO - 21/05/17 14:10:09 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 221) (d733d8da4350, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:09,955] {docker.py:276} INFO - 21/05/17 14:10:09 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 217) in 2643 ms on d733d8da4350 (executor driver) (63/200)
[2021-05-17 11:10:09,956] {docker.py:276} INFO - 21/05/17 14:10:09 INFO Executor: Running task 66.0 in stage 4.0 (TID 221)
[2021-05-17 11:10:09,965] {docker.py:276} INFO - 21/05/17 14:10:09 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:09,966] {docker.py:276} INFO - 21/05/17 14:10:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:09,968] {docker.py:276} INFO - 21/05/17 14:10:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:09,968] {docker.py:276} INFO - 21/05/17 14:10:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:09,968] {docker.py:276} INFO - 21/05/17 14:10:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:09,969] {docker.py:276} INFO - 21/05/17 14:10:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215875637635943938708_0004_m_000066_221, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215875637635943938708_0004_m_000066_221}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215875637635943938708_0004}; taskId=attempt_202105171409215875637635943938708_0004_m_000066_221, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27039427}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:09,969] {docker.py:276} INFO - 21/05/17 14:10:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:09,969] {docker.py:276} INFO - 21/05/17 14:10:09 INFO StagingCommitter: Starting: Task committer attempt_202105171409215875637635943938708_0004_m_000066_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215875637635943938708_0004_m_000066_221
[2021-05-17 11:10:09,972] {docker.py:276} INFO - 21/05/17 14:10:09 INFO StagingCommitter: Task committer attempt_202105171409215875637635943938708_0004_m_000066_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215875637635943938708_0004_m_000066_221 : duration 0:00.003s
[2021-05-17 11:10:10,479] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409216702340999768236339_0004_m_000063_218: needsTaskCommit() Task attempt_202105171409216702340999768236339_0004_m_000063_218
[2021-05-17 11:10:10,480] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Task committer attempt_202105171409216702340999768236339_0004_m_000063_218: needsTaskCommit() Task attempt_202105171409216702340999768236339_0004_m_000063_218: duration 0:00.000s
21/05/17 14:10:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216702340999768236339_0004_m_000063_218
[2021-05-17 11:10:10,481] {docker.py:276} INFO - 21/05/17 14:10:10 INFO Executor: Finished task 63.0 in stage 4.0 (TID 218). 4544 bytes result sent to driver
[2021-05-17 11:10:10,483] {docker.py:276} INFO - 21/05/17 14:10:10 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 222) (d733d8da4350, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:10,483] {docker.py:276} INFO - 21/05/17 14:10:10 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 218) in 2384 ms on d733d8da4350 (executor driver) (64/200)
[2021-05-17 11:10:10,484] {docker.py:276} INFO - 21/05/17 14:10:10 INFO Executor: Running task 67.0 in stage 4.0 (TID 222)
[2021-05-17 11:10:10,492] {docker.py:276} INFO - 21/05/17 14:10:10 INFO ShuffleBlockFetcherIterator: Getting 4 (22.6 KiB) non-empty blocks including 4 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:10,494] {docker.py:276} INFO - 21/05/17 14:10:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:10,494] {docker.py:276} INFO - 21/05/17 14:10:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213597799277556405268_0004_m_000067_222, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213597799277556405268_0004_m_000067_222}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213597799277556405268_0004}; taskId=attempt_202105171409213597799277556405268_0004_m_000067_222, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ec59162}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:10,495] {docker.py:276} INFO - 21/05/17 14:10:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409213597799277556405268_0004_m_000067_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213597799277556405268_0004_m_000067_222
[2021-05-17 11:10:10,498] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Task committer attempt_202105171409213597799277556405268_0004_m_000067_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213597799277556405268_0004_m_000067_222 : duration 0:00.003s
[2021-05-17 11:10:10,515] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409215241129743984930184_0004_m_000064_219: needsTaskCommit() Task attempt_202105171409215241129743984930184_0004_m_000064_219
[2021-05-17 11:10:10,515] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Task committer attempt_202105171409215241129743984930184_0004_m_000064_219: needsTaskCommit() Task attempt_202105171409215241129743984930184_0004_m_000064_219: duration 0:00.001s
[2021-05-17 11:10:10,515] {docker.py:276} INFO - 21/05/17 14:10:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215241129743984930184_0004_m_000064_219
[2021-05-17 11:10:10,516] {docker.py:276} INFO - 21/05/17 14:10:10 INFO Executor: Finished task 64.0 in stage 4.0 (TID 219). 4544 bytes result sent to driver
[2021-05-17 11:10:10,516] {docker.py:276} INFO - 21/05/17 14:10:10 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 223) (d733d8da4350, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:10,517] {docker.py:276} INFO - 21/05/17 14:10:10 INFO Executor: Running task 68.0 in stage 4.0 (TID 223)
[2021-05-17 11:10:10,517] {docker.py:276} INFO - 21/05/17 14:10:10 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 219) in 2340 ms on d733d8da4350 (executor driver) (65/200)
[2021-05-17 11:10:10,524] {docker.py:276} INFO - 21/05/17 14:10:10 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:10,525] {docker.py:276} INFO - 21/05/17 14:10:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:10,526] {docker.py:276} INFO - 21/05/17 14:10:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:10,526] {docker.py:276} INFO - 21/05/17 14:10:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218369729747206149851_0004_m_000068_223, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218369729747206149851_0004_m_000068_223}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218369729747206149851_0004}; taskId=attempt_202105171409218369729747206149851_0004_m_000068_223, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d10efd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:10,527] {docker.py:276} INFO - 21/05/17 14:10:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:10,527] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409218369729747206149851_0004_m_000068_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218369729747206149851_0004_m_000068_223
[2021-05-17 11:10:10,528] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Task committer attempt_202105171409218369729747206149851_0004_m_000068_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218369729747206149851_0004_m_000068_223 : duration 0:00.002s
[2021-05-17 11:10:10,580] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409215123426702040278522_0004_m_000065_220: needsTaskCommit() Task attempt_202105171409215123426702040278522_0004_m_000065_220
[2021-05-17 11:10:10,582] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Task committer attempt_202105171409215123426702040278522_0004_m_000065_220: needsTaskCommit() Task attempt_202105171409215123426702040278522_0004_m_000065_220: duration 0:00.000s
21/05/17 14:10:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215123426702040278522_0004_m_000065_220
[2021-05-17 11:10:10,584] {docker.py:276} INFO - 21/05/17 14:10:10 INFO Executor: Finished task 65.0 in stage 4.0 (TID 220). 4544 bytes result sent to driver
[2021-05-17 11:10:10,585] {docker.py:276} INFO - 21/05/17 14:10:10 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 224) (d733d8da4350, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:10,586] {docker.py:276} INFO - 21/05/17 14:10:10 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 220) in 2308 ms on d733d8da4350 (executor driver) (66/200)
[2021-05-17 11:10:10,587] {docker.py:276} INFO - 21/05/17 14:10:10 INFO Executor: Running task 69.0 in stage 4.0 (TID 224)
[2021-05-17 11:10:10,595] {docker.py:276} INFO - 21/05/17 14:10:10 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:10,596] {docker.py:276} INFO - 21/05/17 14:10:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:10,597] {docker.py:276} INFO - 21/05/17 14:10:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217292024098588392540_0004_m_000069_224, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217292024098588392540_0004_m_000069_224}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217292024098588392540_0004}; taskId=attempt_202105171409217292024098588392540_0004_m_000069_224, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5909af32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:10,597] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409217292024098588392540_0004_m_000069_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217292024098588392540_0004_m_000069_224
[2021-05-17 11:10:10,600] {docker.py:276} INFO - 21/05/17 14:10:10 INFO StagingCommitter: Task committer attempt_202105171409217292024098588392540_0004_m_000069_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217292024098588392540_0004_m_000069_224 : duration 0:00.003s
[2021-05-17 11:10:12,336] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409215875637635943938708_0004_m_000066_221: needsTaskCommit() Task attempt_202105171409215875637635943938708_0004_m_000066_221
[2021-05-17 11:10:12,337] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409215875637635943938708_0004_m_000066_221: needsTaskCommit() Task attempt_202105171409215875637635943938708_0004_m_000066_221: duration 0:00.001s
21/05/17 14:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215875637635943938708_0004_m_000066_221
[2021-05-17 11:10:12,339] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Finished task 66.0 in stage 4.0 (TID 221). 4544 bytes result sent to driver
[2021-05-17 11:10:12,341] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 225) (d733d8da4350, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:12,342] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Running task 70.0 in stage 4.0 (TID 225)
[2021-05-17 11:10:12,343] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 221) in 2392 ms on d733d8da4350 (executor driver) (67/200)
[2021-05-17 11:10:12,352] {docker.py:276} INFO - 21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:12,354] {docker.py:276} INFO - 21/05/17 14:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:12,354] {docker.py:276} INFO - 21/05/17 14:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:12,355] {docker.py:276} INFO - 21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:12,355] {docker.py:276} INFO - 21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212301634059466275464_0004_m_000070_225, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212301634059466275464_0004_m_000070_225}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212301634059466275464_0004}; taskId=attempt_202105171409212301634059466275464_0004_m_000070_225, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ee70f06}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:12,355] {docker.py:276} INFO - 21/05/17 14:10:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:12,356] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409212301634059466275464_0004_m_000070_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212301634059466275464_0004_m_000070_225
[2021-05-17 11:10:12,358] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409212301634059466275464_0004_m_000070_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212301634059466275464_0004_m_000070_225 : duration 0:00.003s
[2021-05-17 11:10:12,756] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409218369729747206149851_0004_m_000068_223: needsTaskCommit() Task attempt_202105171409218369729747206149851_0004_m_000068_223
[2021-05-17 11:10:12,756] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409218369729747206149851_0004_m_000068_223: needsTaskCommit() Task attempt_202105171409218369729747206149851_0004_m_000068_223: duration 0:00.001s
21/05/17 14:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218369729747206149851_0004_m_000068_223
[2021-05-17 11:10:12,758] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Finished task 68.0 in stage 4.0 (TID 223). 4544 bytes result sent to driver
[2021-05-17 11:10:12,760] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 226) (d733d8da4350, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:12,761] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 223) in 2246 ms on d733d8da4350 (executor driver) (68/200)
[2021-05-17 11:10:12,762] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Running task 71.0 in stage 4.0 (TID 226)
[2021-05-17 11:10:12,771] {docker.py:276} INFO - 21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Getting 4 (21.6 KiB) non-empty blocks including 4 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:12,773] {docker.py:276} INFO - 21/05/17 14:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:12,774] {docker.py:276} INFO - 21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218240396696264781115_0004_m_000071_226, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218240396696264781115_0004_m_000071_226}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218240396696264781115_0004}; taskId=attempt_202105171409218240396696264781115_0004_m_000071_226, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@346f8bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:12,774] {docker.py:276} INFO - 21/05/17 14:10:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:12,774] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409218240396696264781115_0004_m_000071_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218240396696264781115_0004_m_000071_226
[2021-05-17 11:10:12,777] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409218240396696264781115_0004_m_000071_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218240396696264781115_0004_m_000071_226 : duration 0:00.003s
[2021-05-17 11:10:12,808] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409213597799277556405268_0004_m_000067_222: needsTaskCommit() Task attempt_202105171409213597799277556405268_0004_m_000067_222
[2021-05-17 11:10:12,809] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409213597799277556405268_0004_m_000067_222: needsTaskCommit() Task attempt_202105171409213597799277556405268_0004_m_000067_222: duration 0:00.001s
21/05/17 14:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213597799277556405268_0004_m_000067_222
[2021-05-17 11:10:12,811] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Finished task 67.0 in stage 4.0 (TID 222). 4544 bytes result sent to driver
[2021-05-17 11:10:12,812] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 227) (d733d8da4350, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:12,813] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Running task 72.0 in stage 4.0 (TID 227)
[2021-05-17 11:10:12,814] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 222) in 2334 ms on d733d8da4350 (executor driver) (69/200)
[2021-05-17 11:10:12,824] {docker.py:276} INFO - 21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:12,826] {docker.py:276} INFO - 21/05/17 14:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216437158077920095448_0004_m_000072_227, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216437158077920095448_0004_m_000072_227}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216437158077920095448_0004}; taskId=attempt_202105171409216437158077920095448_0004_m_000072_227, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e4dbc9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:12,826] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409216437158077920095448_0004_m_000072_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216437158077920095448_0004_m_000072_227
[2021-05-17 11:10:12,828] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409216437158077920095448_0004_m_000072_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216437158077920095448_0004_m_000072_227 : duration 0:00.003s
[2021-05-17 11:10:12,954] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409217292024098588392540_0004_m_000069_224: needsTaskCommit() Task attempt_202105171409217292024098588392540_0004_m_000069_224
[2021-05-17 11:10:12,955] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409217292024098588392540_0004_m_000069_224: needsTaskCommit() Task attempt_202105171409217292024098588392540_0004_m_000069_224: duration 0:00.001s
21/05/17 14:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217292024098588392540_0004_m_000069_224
[2021-05-17 11:10:12,957] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Finished task 69.0 in stage 4.0 (TID 224). 4544 bytes result sent to driver
[2021-05-17 11:10:12,959] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 228) (d733d8da4350, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:12,960] {docker.py:276} INFO - 21/05/17 14:10:12 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 224) in 2379 ms on d733d8da4350 (executor driver) (70/200)
[2021-05-17 11:10:12,961] {docker.py:276} INFO - 21/05/17 14:10:12 INFO Executor: Running task 73.0 in stage 4.0 (TID 228)
[2021-05-17 11:10:12,970] {docker.py:276} INFO - 21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:12,971] {docker.py:276} INFO - 21/05/17 14:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:12,971] {docker.py:276} INFO - 21/05/17 14:10:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212729386795277752893_0004_m_000073_228, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212729386795277752893_0004_m_000073_228}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212729386795277752893_0004}; taskId=attempt_202105171409212729386795277752893_0004_m_000073_228, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78388a4e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409212729386795277752893_0004_m_000073_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212729386795277752893_0004_m_000073_228
[2021-05-17 11:10:12,974] {docker.py:276} INFO - 21/05/17 14:10:12 INFO StagingCommitter: Task committer attempt_202105171409212729386795277752893_0004_m_000073_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212729386795277752893_0004_m_000073_228 : duration 0:00.003s
[2021-05-17 11:10:14,617] {docker.py:276} INFO - 21/05/17 14:10:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409212301634059466275464_0004_m_000070_225: needsTaskCommit() Task attempt_202105171409212301634059466275464_0004_m_000070_225
[2021-05-17 11:10:14,618] {docker.py:276} INFO - 21/05/17 14:10:14 INFO StagingCommitter: Task committer attempt_202105171409212301634059466275464_0004_m_000070_225: needsTaskCommit() Task attempt_202105171409212301634059466275464_0004_m_000070_225: duration 0:00.001s
21/05/17 14:10:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212301634059466275464_0004_m_000070_225
[2021-05-17 11:10:14,621] {docker.py:276} INFO - 21/05/17 14:10:14 INFO Executor: Finished task 70.0 in stage 4.0 (TID 225). 4544 bytes result sent to driver
[2021-05-17 11:10:14,623] {docker.py:276} INFO - 21/05/17 14:10:14 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 229) (d733d8da4350, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:14,623] {docker.py:276} INFO - 21/05/17 14:10:14 INFO Executor: Running task 74.0 in stage 4.0 (TID 229)
[2021-05-17 11:10:14,624] {docker.py:276} INFO - 21/05/17 14:10:14 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 225) in 2287 ms on d733d8da4350 (executor driver) (71/200)
[2021-05-17 11:10:14,632] {docker.py:276} INFO - 21/05/17 14:10:14 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:14,634] {docker.py:276} INFO - 21/05/17 14:10:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921945421374311448239_0004_m_000074_229, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921945421374311448239_0004_m_000074_229}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921945421374311448239_0004}; taskId=attempt_20210517140921945421374311448239_0004_m_000074_229, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@261b9d25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:14,635] {docker.py:276} INFO - 21/05/17 14:10:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:14 INFO StagingCommitter: Starting: Task committer attempt_20210517140921945421374311448239_0004_m_000074_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921945421374311448239_0004_m_000074_229
[2021-05-17 11:10:14,637] {docker.py:276} INFO - 21/05/17 14:10:14 INFO StagingCommitter: Task committer attempt_20210517140921945421374311448239_0004_m_000074_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921945421374311448239_0004_m_000074_229 : duration 0:00.003s
[2021-05-17 11:10:15,085] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Starting: Task committer attempt_202105171409218240396696264781115_0004_m_000071_226: needsTaskCommit() Task attempt_202105171409218240396696264781115_0004_m_000071_226
[2021-05-17 11:10:15,085] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Task committer attempt_202105171409218240396696264781115_0004_m_000071_226: needsTaskCommit() Task attempt_202105171409218240396696264781115_0004_m_000071_226: duration 0:00.000s
21/05/17 14:10:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218240396696264781115_0004_m_000071_226
[2021-05-17 11:10:15,087] {docker.py:276} INFO - 21/05/17 14:10:15 INFO Executor: Finished task 71.0 in stage 4.0 (TID 226). 4544 bytes result sent to driver
[2021-05-17 11:10:15,088] {docker.py:276} INFO - 21/05/17 14:10:15 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 230) (d733d8da4350, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:15,089] {docker.py:276} INFO - 21/05/17 14:10:15 INFO Executor: Running task 75.0 in stage 4.0 (TID 230)
[2021-05-17 11:10:15,090] {docker.py:276} INFO - 21/05/17 14:10:15 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 226) in 2334 ms on d733d8da4350 (executor driver) (72/200)
[2021-05-17 11:10:15,097] {docker.py:276} INFO - 21/05/17 14:10:15 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:15,099] {docker.py:276} INFO - 21/05/17 14:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:15,100] {docker.py:276} INFO - 21/05/17 14:10:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921661318172306907509_0004_m_000075_230, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921661318172306907509_0004_m_000075_230}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921661318172306907509_0004}; taskId=attempt_20210517140921661318172306907509_0004_m_000075_230, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61b8a06}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:15 INFO StagingCommitter: Starting: Task committer attempt_20210517140921661318172306907509_0004_m_000075_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921661318172306907509_0004_m_000075_230
[2021-05-17 11:10:15,102] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Task committer attempt_20210517140921661318172306907509_0004_m_000075_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921661318172306907509_0004_m_000075_230 : duration 0:00.002s
[2021-05-17 11:10:15,128] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Starting: Task committer attempt_202105171409216437158077920095448_0004_m_000072_227: needsTaskCommit() Task attempt_202105171409216437158077920095448_0004_m_000072_227
21/05/17 14:10:15 INFO StagingCommitter: Task committer attempt_202105171409216437158077920095448_0004_m_000072_227: needsTaskCommit() Task attempt_202105171409216437158077920095448_0004_m_000072_227: duration 0:00.000s
21/05/17 14:10:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216437158077920095448_0004_m_000072_227
[2021-05-17 11:10:15,130] {docker.py:276} INFO - 21/05/17 14:10:15 INFO Executor: Finished task 72.0 in stage 4.0 (TID 227). 4544 bytes result sent to driver
[2021-05-17 11:10:15,132] {docker.py:276} INFO - 21/05/17 14:10:15 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 231) (d733d8da4350, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:15,133] {docker.py:276} INFO - 21/05/17 14:10:15 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 227) in 2324 ms on d733d8da4350 (executor driver) (73/200)
[2021-05-17 11:10:15,134] {docker.py:276} INFO - 21/05/17 14:10:15 INFO Executor: Running task 76.0 in stage 4.0 (TID 231)
[2021-05-17 11:10:15,142] {docker.py:276} INFO - 21/05/17 14:10:15 INFO ShuffleBlockFetcherIterator: Getting 4 (24.7 KiB) non-empty blocks including 4 (24.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:15,144] {docker.py:276} INFO - 21/05/17 14:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214308908674329337043_0004_m_000076_231, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214308908674329337043_0004_m_000076_231}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214308908674329337043_0004}; taskId=attempt_202105171409214308908674329337043_0004_m_000076_231, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@506eda80}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:15,144] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Starting: Task committer attempt_202105171409214308908674329337043_0004_m_000076_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214308908674329337043_0004_m_000076_231
[2021-05-17 11:10:15,147] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Task committer attempt_202105171409214308908674329337043_0004_m_000076_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214308908674329337043_0004_m_000076_231 : duration 0:00.004s
[2021-05-17 11:10:15,405] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Starting: Task committer attempt_202105171409212729386795277752893_0004_m_000073_228: needsTaskCommit() Task attempt_202105171409212729386795277752893_0004_m_000073_228
[2021-05-17 11:10:15,406] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Task committer attempt_202105171409212729386795277752893_0004_m_000073_228: needsTaskCommit() Task attempt_202105171409212729386795277752893_0004_m_000073_228: duration 0:00.000s
[2021-05-17 11:10:15,406] {docker.py:276} INFO - 21/05/17 14:10:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212729386795277752893_0004_m_000073_228
[2021-05-17 11:10:15,408] {docker.py:276} INFO - 21/05/17 14:10:15 INFO Executor: Finished task 73.0 in stage 4.0 (TID 228). 4587 bytes result sent to driver
[2021-05-17 11:10:15,410] {docker.py:276} INFO - 21/05/17 14:10:15 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 232) (d733d8da4350, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:15,411] {docker.py:276} INFO - 21/05/17 14:10:15 INFO Executor: Running task 77.0 in stage 4.0 (TID 232)
[2021-05-17 11:10:15,411] {docker.py:276} INFO - 21/05/17 14:10:15 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 228) in 2454 ms on d733d8da4350 (executor driver) (74/200)
[2021-05-17 11:10:15,420] {docker.py:276} INFO - 21/05/17 14:10:15 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:15,421] {docker.py:276} INFO - 21/05/17 14:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:15,422] {docker.py:276} INFO - 21/05/17 14:10:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213746738362425047987_0004_m_000077_232, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213746738362425047987_0004_m_000077_232}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213746738362425047987_0004}; taskId=attempt_202105171409213746738362425047987_0004_m_000077_232, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1364438a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:15,422] {docker.py:276} INFO - 21/05/17 14:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:15 INFO StagingCommitter: Starting: Task committer attempt_202105171409213746738362425047987_0004_m_000077_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213746738362425047987_0004_m_000077_232
[2021-05-17 11:10:15,425] {docker.py:276} INFO - 21/05/17 14:10:15 INFO StagingCommitter: Task committer attempt_202105171409213746738362425047987_0004_m_000077_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213746738362425047987_0004_m_000077_232 : duration 0:00.003s
[2021-05-17 11:10:16,977] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_20210517140921945421374311448239_0004_m_000074_229: needsTaskCommit() Task attempt_20210517140921945421374311448239_0004_m_000074_229
21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_20210517140921945421374311448239_0004_m_000074_229: needsTaskCommit() Task attempt_20210517140921945421374311448239_0004_m_000074_229: duration 0:00.000s
21/05/17 14:10:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921945421374311448239_0004_m_000074_229
[2021-05-17 11:10:16,978] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Finished task 74.0 in stage 4.0 (TID 229). 4587 bytes result sent to driver
[2021-05-17 11:10:16,979] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 233) (d733d8da4350, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:16,979] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 229) in 2359 ms on d733d8da4350 (executor driver) (75/200)
[2021-05-17 11:10:16,979] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Running task 78.0 in stage 4.0 (TID 233)
[2021-05-17 11:10:16,987] {docker.py:276} INFO - 21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:16,989] {docker.py:276} INFO - 21/05/17 14:10:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:16,989] {docker.py:276} INFO - 21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214475555076886734952_0004_m_000078_233, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214475555076886734952_0004_m_000078_233}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214475555076886734952_0004}; taskId=attempt_202105171409214475555076886734952_0004_m_000078_233, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b2afb6c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409214475555076886734952_0004_m_000078_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214475555076886734952_0004_m_000078_233
[2021-05-17 11:10:16,992] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_202105171409214475555076886734952_0004_m_000078_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214475555076886734952_0004_m_000078_233 : duration 0:00.004s
[2021-05-17 11:10:17,592] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_20210517140921661318172306907509_0004_m_000075_230: needsTaskCommit() Task attempt_20210517140921661318172306907509_0004_m_000075_230
[2021-05-17 11:10:17,593] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_20210517140921661318172306907509_0004_m_000075_230: needsTaskCommit() Task attempt_20210517140921661318172306907509_0004_m_000075_230: duration 0:00.000s
21/05/17 14:10:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921661318172306907509_0004_m_000075_230
[2021-05-17 11:10:17,595] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Finished task 75.0 in stage 4.0 (TID 230). 4587 bytes result sent to driver
[2021-05-17 11:10:17,597] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 234) (d733d8da4350, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:17,598] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Running task 79.0 in stage 4.0 (TID 234)
[2021-05-17 11:10:17,598] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 230) in 2513 ms on d733d8da4350 (executor driver) (76/200)
[2021-05-17 11:10:17,612] {docker.py:276} INFO - 21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:17,612] {docker.py:276} INFO - 21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:17,614] {docker.py:276} INFO - 21/05/17 14:10:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:17,614] {docker.py:276} INFO - 21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:17,614] {docker.py:276} INFO - 21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215552234529125351488_0004_m_000079_234, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215552234529125351488_0004_m_000079_234}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215552234529125351488_0004}; taskId=attempt_202105171409215552234529125351488_0004_m_000079_234, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19cdaee4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409215552234529125351488_0004_m_000079_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215552234529125351488_0004_m_000079_234
[2021-05-17 11:10:17,618] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_202105171409215552234529125351488_0004_m_000079_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215552234529125351488_0004_m_000079_234 : duration 0:00.004s
[2021-05-17 11:10:17,620] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409214308908674329337043_0004_m_000076_231: needsTaskCommit() Task attempt_202105171409214308908674329337043_0004_m_000076_231
[2021-05-17 11:10:17,621] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_202105171409214308908674329337043_0004_m_000076_231: needsTaskCommit() Task attempt_202105171409214308908674329337043_0004_m_000076_231: duration 0:00.000s
21/05/17 14:10:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214308908674329337043_0004_m_000076_231
[2021-05-17 11:10:17,622] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Finished task 76.0 in stage 4.0 (TID 231). 4587 bytes result sent to driver
[2021-05-17 11:10:17,623] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 235) (d733d8da4350, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:17,623] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 231) in 2496 ms on d733d8da4350 (executor driver) (77/200)
[2021-05-17 11:10:17,624] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Running task 80.0 in stage 4.0 (TID 235)
[2021-05-17 11:10:17,632] {docker.py:276} INFO - 21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Getting 4 (22.7 KiB) non-empty blocks including 4 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:17,632] {docker.py:276} INFO - 21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:17,633] {docker.py:276} INFO - 21/05/17 14:10:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:17,634] {docker.py:276} INFO - 21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212968148991279661805_0004_m_000080_235, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212968148991279661805_0004_m_000080_235}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212968148991279661805_0004}; taskId=attempt_202105171409212968148991279661805_0004_m_000080_235, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f0c507a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409212968148991279661805_0004_m_000080_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212968148991279661805_0004_m_000080_235
[2021-05-17 11:10:17,637] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_202105171409212968148991279661805_0004_m_000080_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212968148991279661805_0004_m_000080_235 : duration 0:00.003s
[2021-05-17 11:10:17,874] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409213746738362425047987_0004_m_000077_232: needsTaskCommit() Task attempt_202105171409213746738362425047987_0004_m_000077_232
[2021-05-17 11:10:17,875] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_202105171409213746738362425047987_0004_m_000077_232: needsTaskCommit() Task attempt_202105171409213746738362425047987_0004_m_000077_232: duration 0:00.001s
21/05/17 14:10:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213746738362425047987_0004_m_000077_232
[2021-05-17 11:10:17,876] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Finished task 77.0 in stage 4.0 (TID 232). 4544 bytes result sent to driver
[2021-05-17 11:10:17,877] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 236) (d733d8da4350, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:17,878] {docker.py:276} INFO - 21/05/17 14:10:17 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 232) in 2472 ms on d733d8da4350 (executor driver) (78/200)
[2021-05-17 11:10:17,880] {docker.py:276} INFO - 21/05/17 14:10:17 INFO Executor: Running task 81.0 in stage 4.0 (TID 236)
[2021-05-17 11:10:17,890] {docker.py:276} INFO - 21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:17,891] {docker.py:276} INFO - 21/05/17 14:10:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:17,892] {docker.py:276} INFO - 21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:17,892] {docker.py:276} INFO - 21/05/17 14:10:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051714092189835534577789494_0004_m_000081_236, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092189835534577789494_0004_m_000081_236}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051714092189835534577789494_0004}; taskId=attempt_2021051714092189835534577789494_0004_m_000081_236, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78d77163}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:17 INFO StagingCommitter: Starting: Task committer attempt_2021051714092189835534577789494_0004_m_000081_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092189835534577789494_0004_m_000081_236
[2021-05-17 11:10:17,895] {docker.py:276} INFO - 21/05/17 14:10:17 INFO StagingCommitter: Task committer attempt_2021051714092189835534577789494_0004_m_000081_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092189835534577789494_0004_m_000081_236 : duration 0:00.003s
[2021-05-17 11:10:19,248] {docker.py:276} INFO - 21/05/17 14:10:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409214475555076886734952_0004_m_000078_233: needsTaskCommit() Task attempt_202105171409214475555076886734952_0004_m_000078_233
[2021-05-17 11:10:19,248] {docker.py:276} INFO - 21/05/17 14:10:19 INFO StagingCommitter: Task committer attempt_202105171409214475555076886734952_0004_m_000078_233: needsTaskCommit() Task attempt_202105171409214475555076886734952_0004_m_000078_233: duration 0:00.001s
21/05/17 14:10:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214475555076886734952_0004_m_000078_233
[2021-05-17 11:10:19,250] {docker.py:276} INFO - 21/05/17 14:10:19 INFO Executor: Finished task 78.0 in stage 4.0 (TID 233). 4544 bytes result sent to driver
[2021-05-17 11:10:19,251] {docker.py:276} INFO - 21/05/17 14:10:19 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 237) (d733d8da4350, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:19,252] {docker.py:276} INFO - 21/05/17 14:10:19 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 233) in 2276 ms on d733d8da4350 (executor driver) (79/200)
21/05/17 14:10:19 INFO Executor: Running task 82.0 in stage 4.0 (TID 237)
[2021-05-17 11:10:19,261] {docker.py:276} INFO - 21/05/17 14:10:19 INFO ShuffleBlockFetcherIterator: Getting 4 (22.1 KiB) non-empty blocks including 4 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:19,261] {docker.py:276} INFO - 21/05/17 14:10:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:19,263] {docker.py:276} INFO - 21/05/17 14:10:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:19,264] {docker.py:276} INFO - 21/05/17 14:10:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214437187986052247891_0004_m_000082_237, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214437187986052247891_0004_m_000082_237}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214437187986052247891_0004}; taskId=attempt_202105171409214437187986052247891_0004_m_000082_237, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d47fb09}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:19,264] {docker.py:276} INFO - 21/05/17 14:10:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409214437187986052247891_0004_m_000082_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214437187986052247891_0004_m_000082_237
[2021-05-17 11:10:19,267] {docker.py:276} INFO - 21/05/17 14:10:19 INFO StagingCommitter: Task committer attempt_202105171409214437187986052247891_0004_m_000082_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214437187986052247891_0004_m_000082_237 : duration 0:00.003s
[2021-05-17 11:10:19,986] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Starting: Task committer attempt_202105171409212968148991279661805_0004_m_000080_235: needsTaskCommit() Task attempt_202105171409212968148991279661805_0004_m_000080_235
[2021-05-17 11:10:19,988] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Task committer attempt_202105171409212968148991279661805_0004_m_000080_235: needsTaskCommit() Task attempt_202105171409212968148991279661805_0004_m_000080_235: duration 0:00.001s
21/05/17 14:10:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212968148991279661805_0004_m_000080_235
[2021-05-17 11:10:19,990] {docker.py:276} INFO - 21/05/17 14:10:20 INFO Executor: Finished task 80.0 in stage 4.0 (TID 235). 4544 bytes result sent to driver
[2021-05-17 11:10:19,992] {docker.py:276} INFO - 21/05/17 14:10:20 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 238) (d733d8da4350, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:19,993] {docker.py:276} INFO - 21/05/17 14:10:20 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 235) in 2372 ms on d733d8da4350 (executor driver) (80/200)
21/05/17 14:10:20 INFO Executor: Running task 83.0 in stage 4.0 (TID 238)
[2021-05-17 11:10:20,003] {docker.py:276} INFO - 21/05/17 14:10:20 INFO ShuffleBlockFetcherIterator: Getting 4 (22.5 KiB) non-empty blocks including 4 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:20,005] {docker.py:276} INFO - 21/05/17 14:10:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:20,006] {docker.py:276} INFO - 21/05/17 14:10:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215181862789913067637_0004_m_000083_238, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215181862789913067637_0004_m_000083_238}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215181862789913067637_0004}; taskId=attempt_202105171409215181862789913067637_0004_m_000083_238, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e094bee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:20,007] {docker.py:276} INFO - 21/05/17 14:10:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:20 INFO StagingCommitter: Starting: Task committer attempt_202105171409215181862789913067637_0004_m_000083_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215181862789913067637_0004_m_000083_238
[2021-05-17 11:10:20,010] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Task committer attempt_202105171409215181862789913067637_0004_m_000083_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215181862789913067637_0004_m_000083_238 : duration 0:00.003s
[2021-05-17 11:10:20,127] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Starting: Task committer attempt_202105171409215552234529125351488_0004_m_000079_234: needsTaskCommit() Task attempt_202105171409215552234529125351488_0004_m_000079_234
[2021-05-17 11:10:20,128] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Task committer attempt_202105171409215552234529125351488_0004_m_000079_234: needsTaskCommit() Task attempt_202105171409215552234529125351488_0004_m_000079_234: duration 0:00.000s
21/05/17 14:10:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215552234529125351488_0004_m_000079_234
[2021-05-17 11:10:20,129] {docker.py:276} INFO - 21/05/17 14:10:20 INFO Executor: Finished task 79.0 in stage 4.0 (TID 234). 4544 bytes result sent to driver
[2021-05-17 11:10:20,130] {docker.py:276} INFO - 21/05/17 14:10:20 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 239) (d733d8da4350, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:20,131] {docker.py:276} INFO - 21/05/17 14:10:20 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 234) in 2538 ms on d733d8da4350 (executor driver) (81/200)
[2021-05-17 11:10:20,132] {docker.py:276} INFO - 21/05/17 14:10:20 INFO Executor: Running task 84.0 in stage 4.0 (TID 239)
[2021-05-17 11:10:20,140] {docker.py:276} INFO - 21/05/17 14:10:20 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:20,142] {docker.py:276} INFO - 21/05/17 14:10:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215223894143193593214_0004_m_000084_239, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215223894143193593214_0004_m_000084_239}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215223894143193593214_0004}; taskId=attempt_202105171409215223894143193593214_0004_m_000084_239, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73768c02}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:20 INFO StagingCommitter: Starting: Task committer attempt_202105171409215223894143193593214_0004_m_000084_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215223894143193593214_0004_m_000084_239
[2021-05-17 11:10:20,146] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Task committer attempt_202105171409215223894143193593214_0004_m_000084_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215223894143193593214_0004_m_000084_239 : duration 0:00.004s
[2021-05-17 11:10:20,157] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Starting: Task committer attempt_2021051714092189835534577789494_0004_m_000081_236: needsTaskCommit() Task attempt_2021051714092189835534577789494_0004_m_000081_236
[2021-05-17 11:10:20,158] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Task committer attempt_2021051714092189835534577789494_0004_m_000081_236: needsTaskCommit() Task attempt_2021051714092189835534577789494_0004_m_000081_236: duration 0:00.001s
21/05/17 14:10:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051714092189835534577789494_0004_m_000081_236
[2021-05-17 11:10:20,159] {docker.py:276} INFO - 21/05/17 14:10:20 INFO Executor: Finished task 81.0 in stage 4.0 (TID 236). 4544 bytes result sent to driver
[2021-05-17 11:10:20,161] {docker.py:276} INFO - 21/05/17 14:10:20 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 240) (d733d8da4350, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:20,162] {docker.py:276} INFO - 21/05/17 14:10:20 INFO Executor: Running task 85.0 in stage 4.0 (TID 240)
21/05/17 14:10:20 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 236) in 2288 ms on d733d8da4350 (executor driver) (82/200)
[2021-05-17 11:10:20,169] {docker.py:276} INFO - 21/05/17 14:10:20 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:20,170] {docker.py:276} INFO - 21/05/17 14:10:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:20,171] {docker.py:276} INFO - 21/05/17 14:10:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:20,171] {docker.py:276} INFO - 21/05/17 14:10:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:20,172] {docker.py:276} INFO - 21/05/17 14:10:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211461255202069938519_0004_m_000085_240, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211461255202069938519_0004_m_000085_240}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211461255202069938519_0004}; taskId=attempt_202105171409211461255202069938519_0004_m_000085_240, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19aa29a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:20,172] {docker.py:276} INFO - 21/05/17 14:10:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:20,173] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Starting: Task committer attempt_202105171409211461255202069938519_0004_m_000085_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211461255202069938519_0004_m_000085_240
[2021-05-17 11:10:20,176] {docker.py:276} INFO - 21/05/17 14:10:20 INFO StagingCommitter: Task committer attempt_202105171409211461255202069938519_0004_m_000085_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211461255202069938519_0004_m_000085_240 : duration 0:00.003s
[2021-05-17 11:10:21,501] {docker.py:276} INFO - 21/05/17 14:10:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409214437187986052247891_0004_m_000082_237: needsTaskCommit() Task attempt_202105171409214437187986052247891_0004_m_000082_237
[2021-05-17 11:10:21,502] {docker.py:276} INFO - 21/05/17 14:10:21 INFO StagingCommitter: Task committer attempt_202105171409214437187986052247891_0004_m_000082_237: needsTaskCommit() Task attempt_202105171409214437187986052247891_0004_m_000082_237: duration 0:00.001s
21/05/17 14:10:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214437187986052247891_0004_m_000082_237
[2021-05-17 11:10:21,503] {docker.py:276} INFO - 21/05/17 14:10:21 INFO Executor: Finished task 82.0 in stage 4.0 (TID 237). 4544 bytes result sent to driver
[2021-05-17 11:10:21,504] {docker.py:276} INFO - 21/05/17 14:10:21 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 241) (d733d8da4350, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:21,506] {docker.py:276} INFO - 21/05/17 14:10:21 INFO Executor: Running task 86.0 in stage 4.0 (TID 241)
[2021-05-17 11:10:21,507] {docker.py:276} INFO - 21/05/17 14:10:21 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 237) in 2258 ms on d733d8da4350 (executor driver) (83/200)
[2021-05-17 11:10:21,515] {docker.py:276} INFO - 21/05/17 14:10:21 INFO ShuffleBlockFetcherIterator: Getting 4 (21.1 KiB) non-empty blocks including 4 (21.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:21,516] {docker.py:276} INFO - 21/05/17 14:10:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:21,518] {docker.py:276} INFO - 21/05/17 14:10:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:21,518] {docker.py:276} INFO - 21/05/17 14:10:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409219019479066209828571_0004_m_000086_241, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219019479066209828571_0004_m_000086_241}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409219019479066209828571_0004}; taskId=attempt_202105171409219019479066209828571_0004_m_000086_241, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@780f0f71}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409219019479066209828571_0004_m_000086_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219019479066209828571_0004_m_000086_241
[2021-05-17 11:10:21,522] {docker.py:276} INFO - 21/05/17 14:10:21 INFO StagingCommitter: Task committer attempt_202105171409219019479066209828571_0004_m_000086_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219019479066209828571_0004_m_000086_241 : duration 0:00.003s
[2021-05-17 11:10:22,312] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409215181862789913067637_0004_m_000083_238: needsTaskCommit() Task attempt_202105171409215181862789913067637_0004_m_000083_238
[2021-05-17 11:10:22,313] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Task committer attempt_202105171409215181862789913067637_0004_m_000083_238: needsTaskCommit() Task attempt_202105171409215181862789913067637_0004_m_000083_238: duration 0:00.002s
21/05/17 14:10:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215181862789913067637_0004_m_000083_238
[2021-05-17 11:10:22,316] {docker.py:276} INFO - 21/05/17 14:10:22 INFO Executor: Finished task 83.0 in stage 4.0 (TID 238). 4544 bytes result sent to driver
[2021-05-17 11:10:22,317] {docker.py:276} INFO - 21/05/17 14:10:22 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 242) (d733d8da4350, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:22,319] {docker.py:276} INFO - 21/05/17 14:10:22 INFO Executor: Running task 87.0 in stage 4.0 (TID 242)
[2021-05-17 11:10:22,319] {docker.py:276} INFO - 21/05/17 14:10:22 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 238) in 2330 ms on d733d8da4350 (executor driver) (84/200)
[2021-05-17 11:10:22,329] {docker.py:276} INFO - 21/05/17 14:10:22 INFO ShuffleBlockFetcherIterator: Getting 4 (19.9 KiB) non-empty blocks including 4 (19.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:22,331] {docker.py:276} INFO - 21/05/17 14:10:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214810946507779277472_0004_m_000087_242, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214810946507779277472_0004_m_000087_242}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214810946507779277472_0004}; taskId=attempt_202105171409214810946507779277472_0004_m_000087_242, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57afdc7d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:22,331] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409214810946507779277472_0004_m_000087_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214810946507779277472_0004_m_000087_242
[2021-05-17 11:10:22,334] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Task committer attempt_202105171409214810946507779277472_0004_m_000087_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214810946507779277472_0004_m_000087_242 : duration 0:00.003s
[2021-05-17 11:10:22,448] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409215223894143193593214_0004_m_000084_239: needsTaskCommit() Task attempt_202105171409215223894143193593214_0004_m_000084_239
[2021-05-17 11:10:22,450] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Task committer attempt_202105171409215223894143193593214_0004_m_000084_239: needsTaskCommit() Task attempt_202105171409215223894143193593214_0004_m_000084_239: duration 0:00.000s
21/05/17 14:10:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215223894143193593214_0004_m_000084_239
[2021-05-17 11:10:22,450] {docker.py:276} INFO - 21/05/17 14:10:22 INFO Executor: Finished task 84.0 in stage 4.0 (TID 239). 4544 bytes result sent to driver
[2021-05-17 11:10:22,452] {docker.py:276} INFO - 21/05/17 14:10:22 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 243) (d733d8da4350, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:22,453] {docker.py:276} INFO - 21/05/17 14:10:22 INFO Executor: Running task 88.0 in stage 4.0 (TID 243)
21/05/17 14:10:22 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 239) in 2327 ms on d733d8da4350 (executor driver) (85/200)
[2021-05-17 11:10:22,464] {docker.py:276} INFO - 21/05/17 14:10:22 INFO ShuffleBlockFetcherIterator: Getting 4 (20.8 KiB) non-empty blocks including 4 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:22,465] {docker.py:276} INFO - 21/05/17 14:10:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:22,466] {docker.py:276} INFO - 21/05/17 14:10:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217240203317266438572_0004_m_000088_243, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217240203317266438572_0004_m_000088_243}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217240203317266438572_0004}; taskId=attempt_202105171409217240203317266438572_0004_m_000088_243, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4513ec84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409217240203317266438572_0004_m_000088_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217240203317266438572_0004_m_000088_243
[2021-05-17 11:10:22,469] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Task committer attempt_202105171409217240203317266438572_0004_m_000088_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217240203317266438572_0004_m_000088_243 : duration 0:00.004s
[2021-05-17 11:10:22,490] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409211461255202069938519_0004_m_000085_240: needsTaskCommit() Task attempt_202105171409211461255202069938519_0004_m_000085_240
[2021-05-17 11:10:22,490] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Task committer attempt_202105171409211461255202069938519_0004_m_000085_240: needsTaskCommit() Task attempt_202105171409211461255202069938519_0004_m_000085_240: duration 0:00.001s
[2021-05-17 11:10:22,491] {docker.py:276} INFO - 21/05/17 14:10:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211461255202069938519_0004_m_000085_240
[2021-05-17 11:10:22,492] {docker.py:276} INFO - 21/05/17 14:10:22 INFO Executor: Finished task 85.0 in stage 4.0 (TID 240). 4544 bytes result sent to driver
[2021-05-17 11:10:22,493] {docker.py:276} INFO - 21/05/17 14:10:22 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 244) (d733d8da4350, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:22,494] {docker.py:276} INFO - 21/05/17 14:10:22 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 240) in 2336 ms on d733d8da4350 (executor driver) (86/200)
[2021-05-17 11:10:22,494] {docker.py:276} INFO - 21/05/17 14:10:22 INFO Executor: Running task 89.0 in stage 4.0 (TID 244)
[2021-05-17 11:10:22,502] {docker.py:276} INFO - 21/05/17 14:10:22 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:22,503] {docker.py:276} INFO - 21/05/17 14:10:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:22,504] {docker.py:276} INFO - 21/05/17 14:10:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213580436378356507298_0004_m_000089_244, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213580436378356507298_0004_m_000089_244}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213580436378356507298_0004}; taskId=attempt_202105171409213580436378356507298_0004_m_000089_244, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63bcd6b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409213580436378356507298_0004_m_000089_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213580436378356507298_0004_m_000089_244
[2021-05-17 11:10:22,506] {docker.py:276} INFO - 21/05/17 14:10:22 INFO StagingCommitter: Task committer attempt_202105171409213580436378356507298_0004_m_000089_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213580436378356507298_0004_m_000089_244 : duration 0:00.003s
[2021-05-17 11:10:23,930] {docker.py:276} INFO - 21/05/17 14:10:23 INFO StagingCommitter: Starting: Task committer attempt_202105171409219019479066209828571_0004_m_000086_241: needsTaskCommit() Task attempt_202105171409219019479066209828571_0004_m_000086_241
[2021-05-17 11:10:23,930] {docker.py:276} INFO - 21/05/17 14:10:23 INFO StagingCommitter: Task committer attempt_202105171409219019479066209828571_0004_m_000086_241: needsTaskCommit() Task attempt_202105171409219019479066209828571_0004_m_000086_241: duration 0:00.000s
21/05/17 14:10:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409219019479066209828571_0004_m_000086_241
[2021-05-17 11:10:23,932] {docker.py:276} INFO - 21/05/17 14:10:23 INFO Executor: Finished task 86.0 in stage 4.0 (TID 241). 4544 bytes result sent to driver
[2021-05-17 11:10:23,933] {docker.py:276} INFO - 21/05/17 14:10:23 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 245) (d733d8da4350, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:23,934] {docker.py:276} INFO - 21/05/17 14:10:23 INFO Executor: Running task 90.0 in stage 4.0 (TID 245)
[2021-05-17 11:10:23,935] {docker.py:276} INFO - 21/05/17 14:10:23 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 241) in 2434 ms on d733d8da4350 (executor driver) (87/200)
[2021-05-17 11:10:23,943] {docker.py:276} INFO - 21/05/17 14:10:23 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:23,943] {docker.py:276} INFO - 21/05/17 14:10:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:23,945] {docker.py:276} INFO - 21/05/17 14:10:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214457133853197871848_0004_m_000090_245, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214457133853197871848_0004_m_000090_245}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214457133853197871848_0004}; taskId=attempt_202105171409214457133853197871848_0004_m_000090_245, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@360b7b8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:23 INFO StagingCommitter: Starting: Task committer attempt_202105171409214457133853197871848_0004_m_000090_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214457133853197871848_0004_m_000090_245
[2021-05-17 11:10:23,948] {docker.py:276} INFO - 21/05/17 14:10:23 INFO StagingCommitter: Task committer attempt_202105171409214457133853197871848_0004_m_000090_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214457133853197871848_0004_m_000090_245 : duration 0:00.003s
[2021-05-17 11:10:24,666] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409214810946507779277472_0004_m_000087_242: needsTaskCommit() Task attempt_202105171409214810946507779277472_0004_m_000087_242
[2021-05-17 11:10:24,667] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Task committer attempt_202105171409214810946507779277472_0004_m_000087_242: needsTaskCommit() Task attempt_202105171409214810946507779277472_0004_m_000087_242: duration 0:00.001s
[2021-05-17 11:10:24,668] {docker.py:276} INFO - 21/05/17 14:10:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214810946507779277472_0004_m_000087_242
[2021-05-17 11:10:24,670] {docker.py:276} INFO - 21/05/17 14:10:24 INFO Executor: Finished task 87.0 in stage 4.0 (TID 242). 4587 bytes result sent to driver
[2021-05-17 11:10:24,672] {docker.py:276} INFO - 21/05/17 14:10:24 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 246) (d733d8da4350, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:24,673] {docker.py:276} INFO - 21/05/17 14:10:24 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 242) in 2359 ms on d733d8da4350 (executor driver) (88/200)
[2021-05-17 11:10:24,675] {docker.py:276} INFO - 21/05/17 14:10:24 INFO Executor: Running task 91.0 in stage 4.0 (TID 246)
[2021-05-17 11:10:24,684] {docker.py:276} INFO - 21/05/17 14:10:24 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:24,686] {docker.py:276} INFO - 21/05/17 14:10:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:24,686] {docker.py:276} INFO - 21/05/17 14:10:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:24,687] {docker.py:276} INFO - 21/05/17 14:10:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:24,687] {docker.py:276} INFO - 21/05/17 14:10:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217088477386461720339_0004_m_000091_246, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217088477386461720339_0004_m_000091_246}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217088477386461720339_0004}; taskId=attempt_202105171409217088477386461720339_0004_m_000091_246, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58b1241f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:24,688] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409217088477386461720339_0004_m_000091_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217088477386461720339_0004_m_000091_246
[2021-05-17 11:10:24,690] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Task committer attempt_202105171409217088477386461720339_0004_m_000091_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217088477386461720339_0004_m_000091_246 : duration 0:00.003s
[2021-05-17 11:10:24,800] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409213580436378356507298_0004_m_000089_244: needsTaskCommit() Task attempt_202105171409213580436378356507298_0004_m_000089_244
[2021-05-17 11:10:24,801] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Task committer attempt_202105171409213580436378356507298_0004_m_000089_244: needsTaskCommit() Task attempt_202105171409213580436378356507298_0004_m_000089_244: duration 0:00.000s
[2021-05-17 11:10:24,801] {docker.py:276} INFO - 21/05/17 14:10:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213580436378356507298_0004_m_000089_244
[2021-05-17 11:10:24,803] {docker.py:276} INFO - 21/05/17 14:10:24 INFO Executor: Finished task 89.0 in stage 4.0 (TID 244). 4587 bytes result sent to driver
[2021-05-17 11:10:24,804] {docker.py:276} INFO - 21/05/17 14:10:24 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 247) (d733d8da4350, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:24,805] {docker.py:276} INFO - 21/05/17 14:10:24 INFO Executor: Running task 92.0 in stage 4.0 (TID 247)
[2021-05-17 11:10:24,806] {docker.py:276} INFO - 21/05/17 14:10:24 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 244) in 2315 ms on d733d8da4350 (executor driver) (89/200)
[2021-05-17 11:10:24,816] {docker.py:276} INFO - 21/05/17 14:10:24 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:24,817] {docker.py:276} INFO - 21/05/17 14:10:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214881569628579916991_0004_m_000092_247, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214881569628579916991_0004_m_000092_247}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214881569628579916991_0004}; taskId=attempt_202105171409214881569628579916991_0004_m_000092_247, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a9b53a0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409214881569628579916991_0004_m_000092_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214881569628579916991_0004_m_000092_247
[2021-05-17 11:10:24,821] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Task committer attempt_202105171409214881569628579916991_0004_m_000092_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214881569628579916991_0004_m_000092_247 : duration 0:00.004s
[2021-05-17 11:10:24,918] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409217240203317266438572_0004_m_000088_243: needsTaskCommit() Task attempt_202105171409217240203317266438572_0004_m_000088_243
21/05/17 14:10:24 INFO StagingCommitter: Task committer attempt_202105171409217240203317266438572_0004_m_000088_243: needsTaskCommit() Task attempt_202105171409217240203317266438572_0004_m_000088_243: duration 0:00.000s
21/05/17 14:10:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217240203317266438572_0004_m_000088_243
[2021-05-17 11:10:24,921] {docker.py:276} INFO - 21/05/17 14:10:24 INFO Executor: Finished task 88.0 in stage 4.0 (TID 243). 4587 bytes result sent to driver
[2021-05-17 11:10:24,922] {docker.py:276} INFO - 21/05/17 14:10:24 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 248) (d733d8da4350, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:24,923] {docker.py:276} INFO - 21/05/17 14:10:24 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 243) in 2474 ms on d733d8da4350 (executor driver) (90/200)
[2021-05-17 11:10:24,924] {docker.py:276} INFO - 21/05/17 14:10:24 INFO Executor: Running task 93.0 in stage 4.0 (TID 248)
[2021-05-17 11:10:24,933] {docker.py:276} INFO - 21/05/17 14:10:24 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:24,934] {docker.py:276} INFO - 21/05/17 14:10:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:24,937] {docker.py:276} INFO - 21/05/17 14:10:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:24,937] {docker.py:276} INFO - 21/05/17 14:10:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:24,938] {docker.py:276} INFO - 21/05/17 14:10:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218518596620341583273_0004_m_000093_248, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218518596620341583273_0004_m_000093_248}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218518596620341583273_0004}; taskId=attempt_202105171409218518596620341583273_0004_m_000093_248, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8208f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:24,938] {docker.py:276} INFO - 21/05/17 14:10:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:24,938] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409218518596620341583273_0004_m_000093_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218518596620341583273_0004_m_000093_248
[2021-05-17 11:10:24,941] {docker.py:276} INFO - 21/05/17 14:10:24 INFO StagingCommitter: Task committer attempt_202105171409218518596620341583273_0004_m_000093_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218518596620341583273_0004_m_000093_248 : duration 0:00.003s
[2021-05-17 11:10:26,459] {docker.py:276} INFO - 21/05/17 14:10:26 INFO StagingCommitter: Starting: Task committer attempt_202105171409214457133853197871848_0004_m_000090_245: needsTaskCommit() Task attempt_202105171409214457133853197871848_0004_m_000090_245
[2021-05-17 11:10:26,460] {docker.py:276} INFO - 21/05/17 14:10:26 INFO StagingCommitter: Task committer attempt_202105171409214457133853197871848_0004_m_000090_245: needsTaskCommit() Task attempt_202105171409214457133853197871848_0004_m_000090_245: duration 0:00.002s
[2021-05-17 11:10:26,460] {docker.py:276} INFO - 21/05/17 14:10:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214457133853197871848_0004_m_000090_245
[2021-05-17 11:10:26,462] {docker.py:276} INFO - 21/05/17 14:10:26 INFO Executor: Finished task 90.0 in stage 4.0 (TID 245). 4587 bytes result sent to driver
[2021-05-17 11:10:26,464] {docker.py:276} INFO - 21/05/17 14:10:26 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 249) (d733d8da4350, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:26,465] {docker.py:276} INFO - 21/05/17 14:10:26 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 245) in 2501 ms on d733d8da4350 (executor driver) (91/200)
21/05/17 14:10:26 INFO Executor: Running task 94.0 in stage 4.0 (TID 249)
[2021-05-17 11:10:26,475] {docker.py:276} INFO - 21/05/17 14:10:26 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:26,477] {docker.py:276} INFO - 21/05/17 14:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:26,478] {docker.py:276} INFO - 21/05/17 14:10:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216645001710897376360_0004_m_000094_249, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216645001710897376360_0004_m_000094_249}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216645001710897376360_0004}; taskId=attempt_202105171409216645001710897376360_0004_m_000094_249, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24801c01}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:26,478] {docker.py:276} INFO - 21/05/17 14:10:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:26 INFO StagingCommitter: Starting: Task committer attempt_202105171409216645001710897376360_0004_m_000094_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216645001710897376360_0004_m_000094_249
[2021-05-17 11:10:26,482] {docker.py:276} INFO - 21/05/17 14:10:26 INFO StagingCommitter: Task committer attempt_202105171409216645001710897376360_0004_m_000094_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216645001710897376360_0004_m_000094_249 : duration 0:00.005s
[2021-05-17 11:10:27,148] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409214881569628579916991_0004_m_000092_247: needsTaskCommit() Task attempt_202105171409214881569628579916991_0004_m_000092_247
[2021-05-17 11:10:27,149] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Task committer attempt_202105171409214881569628579916991_0004_m_000092_247: needsTaskCommit() Task attempt_202105171409214881569628579916991_0004_m_000092_247: duration 0:00.001s
[2021-05-17 11:10:27,149] {docker.py:276} INFO - 21/05/17 14:10:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214881569628579916991_0004_m_000092_247
[2021-05-17 11:10:27,150] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409217088477386461720339_0004_m_000091_246: needsTaskCommit() Task attempt_202105171409217088477386461720339_0004_m_000091_246
[2021-05-17 11:10:27,150] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Task committer attempt_202105171409217088477386461720339_0004_m_000091_246: needsTaskCommit() Task attempt_202105171409217088477386461720339_0004_m_000091_246: duration 0:00.000s
[2021-05-17 11:10:27,151] {docker.py:276} INFO - 21/05/17 14:10:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217088477386461720339_0004_m_000091_246
[2021-05-17 11:10:27,151] {docker.py:276} INFO - 21/05/17 14:10:27 INFO Executor: Finished task 91.0 in stage 4.0 (TID 246). 4544 bytes result sent to driver
[2021-05-17 11:10:27,152] {docker.py:276} INFO - 21/05/17 14:10:27 INFO Executor: Finished task 92.0 in stage 4.0 (TID 247). 4544 bytes result sent to driver
[2021-05-17 11:10:27,155] {docker.py:276} INFO - 21/05/17 14:10:27 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 250) (d733d8da4350, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:27,156] {docker.py:276} INFO - 21/05/17 14:10:27 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 246) in 2454 ms on d733d8da4350 (executor driver) (92/200)
[2021-05-17 11:10:27,157] {docker.py:276} INFO - 21/05/17 14:10:27 INFO Executor: Running task 95.0 in stage 4.0 (TID 250)
[2021-05-17 11:10:27,159] {docker.py:276} INFO - 21/05/17 14:10:27 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 251) (d733d8da4350, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:27,160] {docker.py:276} INFO - 21/05/17 14:10:27 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 247) in 2325 ms on d733d8da4350 (executor driver) (93/200)
21/05/17 14:10:27 INFO Executor: Running task 96.0 in stage 4.0 (TID 251)
[2021-05-17 11:10:27,169] {docker.py:276} INFO - 21/05/17 14:10:27 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:27,169] {docker.py:276} INFO - 21/05/17 14:10:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:27,170] {docker.py:276} INFO - 21/05/17 14:10:27 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:27,171] {docker.py:276} INFO - 21/05/17 14:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:27,171] {docker.py:276} INFO - 21/05/17 14:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:27,172] {docker.py:276} INFO - 21/05/17 14:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:27,172] {docker.py:276} INFO - 21/05/17 14:10:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921732349269376643707_0004_m_000095_250, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921732349269376643707_0004_m_000095_250}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921732349269376643707_0004}; taskId=attempt_20210517140921732349269376643707_0004_m_000095_250, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46fa728}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:27,173] {docker.py:276} INFO - 21/05/17 14:10:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213064463484301811251_0004_m_000096_251, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213064463484301811251_0004_m_000096_251}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213064463484301811251_0004}; taskId=attempt_202105171409213064463484301811251_0004_m_000096_251, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49e9b4df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:27,173] {docker.py:276} INFO - 21/05/17 14:10:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:27,173] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Starting: Task committer attempt_20210517140921732349269376643707_0004_m_000095_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921732349269376643707_0004_m_000095_250
[2021-05-17 11:10:27,173] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409213064463484301811251_0004_m_000096_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213064463484301811251_0004_m_000096_251
[2021-05-17 11:10:27,176] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Task committer attempt_20210517140921732349269376643707_0004_m_000095_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921732349269376643707_0004_m_000095_250 : duration 0:00.003s
[2021-05-17 11:10:27,177] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Task committer attempt_202105171409213064463484301811251_0004_m_000096_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213064463484301811251_0004_m_000096_251 : duration 0:00.005s
[2021-05-17 11:10:27,379] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409218518596620341583273_0004_m_000093_248: needsTaskCommit() Task attempt_202105171409218518596620341583273_0004_m_000093_248
[2021-05-17 11:10:27,379] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Task committer attempt_202105171409218518596620341583273_0004_m_000093_248: needsTaskCommit() Task attempt_202105171409218518596620341583273_0004_m_000093_248: duration 0:00.001s
21/05/17 14:10:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218518596620341583273_0004_m_000093_248
[2021-05-17 11:10:27,381] {docker.py:276} INFO - 21/05/17 14:10:27 INFO Executor: Finished task 93.0 in stage 4.0 (TID 248). 4544 bytes result sent to driver
[2021-05-17 11:10:27,382] {docker.py:276} INFO - 21/05/17 14:10:27 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 252) (d733d8da4350, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:27,383] {docker.py:276} INFO - 21/05/17 14:10:27 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 248) in 2430 ms on d733d8da4350 (executor driver) (94/200)
[2021-05-17 11:10:27,384] {docker.py:276} INFO - 21/05/17 14:10:27 INFO Executor: Running task 97.0 in stage 4.0 (TID 252)
[2021-05-17 11:10:27,394] {docker.py:276} INFO - 21/05/17 14:10:27 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:27,395] {docker.py:276} INFO - 21/05/17 14:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:27,396] {docker.py:276} INFO - 21/05/17 14:10:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921372323200682790889_0004_m_000097_252, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921372323200682790889_0004_m_000097_252}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921372323200682790889_0004}; taskId=attempt_20210517140921372323200682790889_0004_m_000097_252, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c04fac0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:27,396] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Starting: Task committer attempt_20210517140921372323200682790889_0004_m_000097_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921372323200682790889_0004_m_000097_252
[2021-05-17 11:10:27,400] {docker.py:276} INFO - 21/05/17 14:10:27 INFO StagingCommitter: Task committer attempt_20210517140921372323200682790889_0004_m_000097_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921372323200682790889_0004_m_000097_252 : duration 0:00.004s
[2021-05-17 11:10:28,734] {docker.py:276} INFO - 21/05/17 14:10:28 INFO StagingCommitter: Starting: Task committer attempt_202105171409216645001710897376360_0004_m_000094_249: needsTaskCommit() Task attempt_202105171409216645001710897376360_0004_m_000094_249
21/05/17 14:10:28 INFO StagingCommitter: Task committer attempt_202105171409216645001710897376360_0004_m_000094_249: needsTaskCommit() Task attempt_202105171409216645001710897376360_0004_m_000094_249: duration 0:00.001s
21/05/17 14:10:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216645001710897376360_0004_m_000094_249
[2021-05-17 11:10:28,735] {docker.py:276} INFO - 21/05/17 14:10:28 INFO Executor: Finished task 94.0 in stage 4.0 (TID 249). 4544 bytes result sent to driver
[2021-05-17 11:10:28,737] {docker.py:276} INFO - 21/05/17 14:10:28 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 253) (d733d8da4350, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:28,738] {docker.py:276} INFO - 21/05/17 14:10:28 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 249) in 2276 ms on d733d8da4350 (executor driver) (95/200)
21/05/17 14:10:28 INFO Executor: Running task 98.0 in stage 4.0 (TID 253)
[2021-05-17 11:10:28,749] {docker.py:276} INFO - 21/05/17 14:10:28 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:28,749] {docker.py:276} INFO - 21/05/17 14:10:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:28,751] {docker.py:276} INFO - 21/05/17 14:10:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214851552653149944617_0004_m_000098_253, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214851552653149944617_0004_m_000098_253}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214851552653149944617_0004}; taskId=attempt_202105171409214851552653149944617_0004_m_000098_253, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32ffe252}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:28,751] {docker.py:276} INFO - 21/05/17 14:10:28 INFO StagingCommitter: Starting: Task committer attempt_202105171409214851552653149944617_0004_m_000098_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214851552653149944617_0004_m_000098_253
[2021-05-17 11:10:28,754] {docker.py:276} INFO - 21/05/17 14:10:28 INFO StagingCommitter: Task committer attempt_202105171409214851552653149944617_0004_m_000098_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214851552653149944617_0004_m_000098_253 : duration 0:00.003s
[2021-05-17 11:10:29,445] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Starting: Task committer attempt_202105171409213064463484301811251_0004_m_000096_251: needsTaskCommit() Task attempt_202105171409213064463484301811251_0004_m_000096_251
[2021-05-17 11:10:29,445] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Task committer attempt_202105171409213064463484301811251_0004_m_000096_251: needsTaskCommit() Task attempt_202105171409213064463484301811251_0004_m_000096_251: duration 0:00.001s
[2021-05-17 11:10:29,446] {docker.py:276} INFO - 21/05/17 14:10:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213064463484301811251_0004_m_000096_251
[2021-05-17 11:10:29,447] {docker.py:276} INFO - 21/05/17 14:10:29 INFO Executor: Finished task 96.0 in stage 4.0 (TID 251). 4544 bytes result sent to driver
[2021-05-17 11:10:29,450] {docker.py:276} INFO - 21/05/17 14:10:29 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 254) (d733d8da4350, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:29,451] {docker.py:276} INFO - 21/05/17 14:10:29 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 251) in 2294 ms on d733d8da4350 (executor driver) (96/200)
[2021-05-17 11:10:29,452] {docker.py:276} INFO - 21/05/17 14:10:29 INFO Executor: Running task 99.0 in stage 4.0 (TID 254)
[2021-05-17 11:10:29,459] {docker.py:276} INFO - 21/05/17 14:10:29 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:29,461] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Starting: Task committer attempt_20210517140921732349269376643707_0004_m_000095_250: needsTaskCommit() Task attempt_20210517140921732349269376643707_0004_m_000095_250
[2021-05-17 11:10:29,462] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Task committer attempt_20210517140921732349269376643707_0004_m_000095_250: needsTaskCommit() Task attempt_20210517140921732349269376643707_0004_m_000095_250: duration 0:00.000s
[2021-05-17 11:10:29,462] {docker.py:276} INFO - 21/05/17 14:10:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921732349269376643707_0004_m_000095_250
[2021-05-17 11:10:29,463] {docker.py:276} INFO - 21/05/17 14:10:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:29,463] {docker.py:276} INFO - 21/05/17 14:10:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:29,463] {docker.py:276} INFO - 21/05/17 14:10:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409219118423822763397789_0004_m_000099_254, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219118423822763397789_0004_m_000099_254}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409219118423822763397789_0004}; taskId=attempt_202105171409219118423822763397789_0004_m_000099_254, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@739cb908}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:29,464] {docker.py:276} INFO - 21/05/17 14:10:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:29,464] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Starting: Task committer attempt_202105171409219118423822763397789_0004_m_000099_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219118423822763397789_0004_m_000099_254
[2021-05-17 11:10:29,464] {docker.py:276} INFO - 21/05/17 14:10:29 INFO Executor: Finished task 95.0 in stage 4.0 (TID 250). 4544 bytes result sent to driver
[2021-05-17 11:10:29,465] {docker.py:276} INFO - 21/05/17 14:10:29 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 255) (d733d8da4350, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:29,466] {docker.py:276} INFO - 21/05/17 14:10:29 INFO Executor: Running task 100.0 in stage 4.0 (TID 255)
[2021-05-17 11:10:29,467] {docker.py:276} INFO - 21/05/17 14:10:29 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 250) in 2316 ms on d733d8da4350 (executor driver) (97/200)
[2021-05-17 11:10:29,468] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Task committer attempt_202105171409219118423822763397789_0004_m_000099_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219118423822763397789_0004_m_000099_254 : duration 0:00.005s
[2021-05-17 11:10:29,475] {docker.py:276} INFO - 21/05/17 14:10:29 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:29,476] {docker.py:276} INFO - 21/05/17 14:10:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:29,477] {docker.py:276} INFO - 21/05/17 14:10:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:29,478] {docker.py:276} INFO - 21/05/17 14:10:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:29,478] {docker.py:276} INFO - 21/05/17 14:10:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:29,478] {docker.py:276} INFO - 21/05/17 14:10:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211299402254349055010_0004_m_000100_255, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211299402254349055010_0004_m_000100_255}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211299402254349055010_0004}; taskId=attempt_202105171409211299402254349055010_0004_m_000100_255, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17783b2f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:29,479] {docker.py:276} INFO - 21/05/17 14:10:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:29,479] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Starting: Task committer attempt_202105171409211299402254349055010_0004_m_000100_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211299402254349055010_0004_m_000100_255
[2021-05-17 11:10:29,482] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Task committer attempt_202105171409211299402254349055010_0004_m_000100_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211299402254349055010_0004_m_000100_255 : duration 0:00.003s
[2021-05-17 11:10:29,703] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Starting: Task committer attempt_20210517140921372323200682790889_0004_m_000097_252: needsTaskCommit() Task attempt_20210517140921372323200682790889_0004_m_000097_252
[2021-05-17 11:10:29,705] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Task committer attempt_20210517140921372323200682790889_0004_m_000097_252: needsTaskCommit() Task attempt_20210517140921372323200682790889_0004_m_000097_252: duration 0:00.001s
[2021-05-17 11:10:29,705] {docker.py:276} INFO - 21/05/17 14:10:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921372323200682790889_0004_m_000097_252
[2021-05-17 11:10:29,707] {docker.py:276} INFO - 21/05/17 14:10:29 INFO Executor: Finished task 97.0 in stage 4.0 (TID 252). 4544 bytes result sent to driver
[2021-05-17 11:10:29,709] {docker.py:276} INFO - 21/05/17 14:10:29 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 256) (d733d8da4350, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:29,710] {docker.py:276} INFO - 21/05/17 14:10:29 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 252) in 2331 ms on d733d8da4350 (executor driver) (98/200)
[2021-05-17 11:10:29,710] {docker.py:276} INFO - 21/05/17 14:10:29 INFO Executor: Running task 101.0 in stage 4.0 (TID 256)
[2021-05-17 11:10:29,720] {docker.py:276} INFO - 21/05/17 14:10:29 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:29,722] {docker.py:276} INFO - 21/05/17 14:10:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:29,722] {docker.py:276} INFO - 21/05/17 14:10:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:29,723] {docker.py:276} INFO - 21/05/17 14:10:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211311489236182995660_0004_m_000101_256, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211311489236182995660_0004_m_000101_256}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211311489236182995660_0004}; taskId=attempt_202105171409211311489236182995660_0004_m_000101_256, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b0bd2b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:29 INFO StagingCommitter: Starting: Task committer attempt_202105171409211311489236182995660_0004_m_000101_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211311489236182995660_0004_m_000101_256
[2021-05-17 11:10:29,725] {docker.py:276} INFO - 21/05/17 14:10:29 INFO StagingCommitter: Task committer attempt_202105171409211311489236182995660_0004_m_000101_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211311489236182995660_0004_m_000101_256 : duration 0:00.003s
[2021-05-17 11:10:30,581] {docker.py:276} INFO - 21/05/17 14:10:30 INFO StagingCommitter: Starting: Task committer attempt_202105171409214851552653149944617_0004_m_000098_253: needsTaskCommit() Task attempt_202105171409214851552653149944617_0004_m_000098_253
[2021-05-17 11:10:30,582] {docker.py:276} INFO - 21/05/17 14:10:30 INFO StagingCommitter: Task committer attempt_202105171409214851552653149944617_0004_m_000098_253: needsTaskCommit() Task attempt_202105171409214851552653149944617_0004_m_000098_253: duration 0:00.000s
21/05/17 14:10:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214851552653149944617_0004_m_000098_253
[2021-05-17 11:10:30,583] {docker.py:276} INFO - 21/05/17 14:10:30 INFO Executor: Finished task 98.0 in stage 4.0 (TID 253). 4544 bytes result sent to driver
[2021-05-17 11:10:30,584] {docker.py:276} INFO - 21/05/17 14:10:30 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 257) (d733d8da4350, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:30,585] {docker.py:276} INFO - 21/05/17 14:10:30 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 253) in 1851 ms on d733d8da4350 (executor driver) (99/200)
[2021-05-17 11:10:30,586] {docker.py:276} INFO - 21/05/17 14:10:30 INFO Executor: Running task 102.0 in stage 4.0 (TID 257)
[2021-05-17 11:10:30,594] {docker.py:276} INFO - 21/05/17 14:10:30 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:30,596] {docker.py:276} INFO - 21/05/17 14:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:30,596] {docker.py:276} INFO - 21/05/17 14:10:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211783255771672799017_0004_m_000102_257, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211783255771672799017_0004_m_000102_257}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211783255771672799017_0004}; taskId=attempt_202105171409211783255771672799017_0004_m_000102_257, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@722b7ab3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:30 INFO StagingCommitter: Starting: Task committer attempt_202105171409211783255771672799017_0004_m_000102_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211783255771672799017_0004_m_000102_257
[2021-05-17 11:10:30,599] {docker.py:276} INFO - 21/05/17 14:10:30 INFO StagingCommitter: Task committer attempt_202105171409211783255771672799017_0004_m_000102_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211783255771672799017_0004_m_000102_257 : duration 0:00.003s
[2021-05-17 11:10:31,562] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409211311489236182995660_0004_m_000101_256: needsTaskCommit() Task attempt_202105171409211311489236182995660_0004_m_000101_256
[2021-05-17 11:10:31,564] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Task committer attempt_202105171409211311489236182995660_0004_m_000101_256: needsTaskCommit() Task attempt_202105171409211311489236182995660_0004_m_000101_256: duration 0:00.002s
21/05/17 14:10:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211311489236182995660_0004_m_000101_256
[2021-05-17 11:10:31,565] {docker.py:276} INFO - 21/05/17 14:10:31 INFO Executor: Finished task 101.0 in stage 4.0 (TID 256). 4544 bytes result sent to driver
[2021-05-17 11:10:31,566] {docker.py:276} INFO - 21/05/17 14:10:31 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 258) (d733d8da4350, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:31,568] {docker.py:276} INFO - 21/05/17 14:10:31 INFO Executor: Running task 103.0 in stage 4.0 (TID 258)
21/05/17 14:10:31 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 256) in 1862 ms on d733d8da4350 (executor driver) (100/200)
[2021-05-17 11:10:31,578] {docker.py:276} INFO - 21/05/17 14:10:31 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:31,579] {docker.py:276} INFO - 21/05/17 14:10:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:31,580] {docker.py:276} INFO - 21/05/17 14:10:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216987422927158203312_0004_m_000103_258, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216987422927158203312_0004_m_000103_258}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216987422927158203312_0004}; taskId=attempt_202105171409216987422927158203312_0004_m_000103_258, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a5f560}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:31,580] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409216987422927158203312_0004_m_000103_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216987422927158203312_0004_m_000103_258
[2021-05-17 11:10:31,583] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Task committer attempt_202105171409216987422927158203312_0004_m_000103_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216987422927158203312_0004_m_000103_258 : duration 0:00.003s
[2021-05-17 11:10:31,793] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409211299402254349055010_0004_m_000100_255: needsTaskCommit() Task attempt_202105171409211299402254349055010_0004_m_000100_255
[2021-05-17 11:10:31,794] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Task committer attempt_202105171409211299402254349055010_0004_m_000100_255: needsTaskCommit() Task attempt_202105171409211299402254349055010_0004_m_000100_255: duration 0:00.001s
21/05/17 14:10:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211299402254349055010_0004_m_000100_255
[2021-05-17 11:10:31,796] {docker.py:276} INFO - 21/05/17 14:10:31 INFO Executor: Finished task 100.0 in stage 4.0 (TID 255). 4544 bytes result sent to driver
[2021-05-17 11:10:31,798] {docker.py:276} INFO - 21/05/17 14:10:31 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 259) (d733d8da4350, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:31,799] {docker.py:276} INFO - 21/05/17 14:10:31 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 255) in 2336 ms on d733d8da4350 (executor driver) (101/200)
[2021-05-17 11:10:31,800] {docker.py:276} INFO - 21/05/17 14:10:31 INFO Executor: Running task 104.0 in stage 4.0 (TID 259)
[2021-05-17 11:10:31,809] {docker.py:276} INFO - 21/05/17 14:10:31 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:31,811] {docker.py:276} INFO - 21/05/17 14:10:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:31,812] {docker.py:276} INFO - 21/05/17 14:10:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:31,812] {docker.py:276} INFO - 21/05/17 14:10:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409219079991164120569912_0004_m_000104_259, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219079991164120569912_0004_m_000104_259}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409219079991164120569912_0004}; taskId=attempt_202105171409219079991164120569912_0004_m_000104_259, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ec92fba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:31,813] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409219079991164120569912_0004_m_000104_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219079991164120569912_0004_m_000104_259
[2021-05-17 11:10:31,815] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Task committer attempt_202105171409219079991164120569912_0004_m_000104_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219079991164120569912_0004_m_000104_259 : duration 0:00.003s
[2021-05-17 11:10:31,913] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409219118423822763397789_0004_m_000099_254: needsTaskCommit() Task attempt_202105171409219118423822763397789_0004_m_000099_254
[2021-05-17 11:10:31,914] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Task committer attempt_202105171409219118423822763397789_0004_m_000099_254: needsTaskCommit() Task attempt_202105171409219118423822763397789_0004_m_000099_254: duration 0:00.001s
21/05/17 14:10:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409219118423822763397789_0004_m_000099_254
[2021-05-17 11:10:31,916] {docker.py:276} INFO - 21/05/17 14:10:31 INFO Executor: Finished task 99.0 in stage 4.0 (TID 254). 4544 bytes result sent to driver
[2021-05-17 11:10:31,917] {docker.py:276} INFO - 21/05/17 14:10:31 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 260) (d733d8da4350, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:31,919] {docker.py:276} INFO - 21/05/17 14:10:31 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 254) in 2474 ms on d733d8da4350 (executor driver) (102/200)
[2021-05-17 11:10:31,919] {docker.py:276} INFO - 21/05/17 14:10:31 INFO Executor: Running task 105.0 in stage 4.0 (TID 260)
[2021-05-17 11:10:31,930] {docker.py:276} INFO - 21/05/17 14:10:31 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:31,932] {docker.py:276} INFO - 21/05/17 14:10:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:31,932] {docker.py:276} INFO - 21/05/17 14:10:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214573810128302271132_0004_m_000105_260, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214573810128302271132_0004_m_000105_260}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214573810128302271132_0004}; taskId=attempt_202105171409214573810128302271132_0004_m_000105_260, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b89fc9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:31 INFO StagingCommitter: Starting: Task committer attempt_202105171409214573810128302271132_0004_m_000105_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214573810128302271132_0004_m_000105_260
[2021-05-17 11:10:31,936] {docker.py:276} INFO - 21/05/17 14:10:31 INFO StagingCommitter: Task committer attempt_202105171409214573810128302271132_0004_m_000105_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214573810128302271132_0004_m_000105_260 : duration 0:00.004s
[2021-05-17 11:10:32,599] {docker.py:276} INFO - 21/05/17 14:10:32 INFO StagingCommitter: Starting: Task committer attempt_202105171409211783255771672799017_0004_m_000102_257: needsTaskCommit() Task attempt_202105171409211783255771672799017_0004_m_000102_257
[2021-05-17 11:10:32,599] {docker.py:276} INFO - 21/05/17 14:10:32 INFO StagingCommitter: Task committer attempt_202105171409211783255771672799017_0004_m_000102_257: needsTaskCommit() Task attempt_202105171409211783255771672799017_0004_m_000102_257: duration 0:00.001s
[2021-05-17 11:10:32,600] {docker.py:276} INFO - 21/05/17 14:10:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211783255771672799017_0004_m_000102_257
[2021-05-17 11:10:32,602] {docker.py:276} INFO - 21/05/17 14:10:32 INFO Executor: Finished task 102.0 in stage 4.0 (TID 257). 4544 bytes result sent to driver
[2021-05-17 11:10:32,612] {docker.py:276} INFO - 21/05/17 14:10:32 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 261) (d733d8da4350, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:32,612] {docker.py:276} INFO - 21/05/17 14:10:32 INFO Executor: Running task 106.0 in stage 4.0 (TID 261)
21/05/17 14:10:32 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 257) in 2031 ms on d733d8da4350 (executor driver) (103/200)
[2021-05-17 11:10:32,620] {docker.py:276} INFO - 21/05/17 14:10:32 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:32,623] {docker.py:276} INFO - 21/05/17 14:10:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215226459701495268265_0004_m_000106_261, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215226459701495268265_0004_m_000106_261}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215226459701495268265_0004}; taskId=attempt_202105171409215226459701495268265_0004_m_000106_261, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d406fc8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:32 INFO StagingCommitter: Starting: Task committer attempt_202105171409215226459701495268265_0004_m_000106_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215226459701495268265_0004_m_000106_261
[2021-05-17 11:10:32,626] {docker.py:276} INFO - 21/05/17 14:10:32 INFO StagingCommitter: Task committer attempt_202105171409215226459701495268265_0004_m_000106_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215226459701495268265_0004_m_000106_261 : duration 0:00.003s
[2021-05-17 11:10:33,616] {docker.py:276} INFO - 21/05/17 14:10:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409216987422927158203312_0004_m_000103_258: needsTaskCommit() Task attempt_202105171409216987422927158203312_0004_m_000103_258
[2021-05-17 11:10:33,617] {docker.py:276} INFO - 21/05/17 14:10:33 INFO StagingCommitter: Task committer attempt_202105171409216987422927158203312_0004_m_000103_258: needsTaskCommit() Task attempt_202105171409216987422927158203312_0004_m_000103_258: duration 0:00.000s
21/05/17 14:10:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216987422927158203312_0004_m_000103_258
[2021-05-17 11:10:33,620] {docker.py:276} INFO - 21/05/17 14:10:33 INFO Executor: Finished task 103.0 in stage 4.0 (TID 258). 4587 bytes result sent to driver
[2021-05-17 11:10:33,621] {docker.py:276} INFO - 21/05/17 14:10:33 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 262) (d733d8da4350, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:33,622] {docker.py:276} INFO - 21/05/17 14:10:33 INFO Executor: Running task 107.0 in stage 4.0 (TID 262)
[2021-05-17 11:10:33,623] {docker.py:276} INFO - 21/05/17 14:10:33 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 258) in 2060 ms on d733d8da4350 (executor driver) (104/200)
[2021-05-17 11:10:33,632] {docker.py:276} INFO - 21/05/17 14:10:33 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:33,632] {docker.py:276} INFO - 21/05/17 14:10:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:33,634] {docker.py:276} INFO - 21/05/17 14:10:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:33,635] {docker.py:276} INFO - 21/05/17 14:10:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:33,635] {docker.py:276} INFO - 21/05/17 14:10:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:33,636] {docker.py:276} INFO - 21/05/17 14:10:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213047330818908586972_0004_m_000107_262, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213047330818908586972_0004_m_000107_262}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213047330818908586972_0004}; taskId=attempt_202105171409213047330818908586972_0004_m_000107_262, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60c0afe1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:33,636] {docker.py:276} INFO - 21/05/17 14:10:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:33,636] {docker.py:276} INFO - 21/05/17 14:10:33 INFO StagingCommitter: Starting: Task committer attempt_202105171409213047330818908586972_0004_m_000107_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213047330818908586972_0004_m_000107_262
[2021-05-17 11:10:33,638] {docker.py:276} INFO - 21/05/17 14:10:33 INFO StagingCommitter: Task committer attempt_202105171409213047330818908586972_0004_m_000107_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213047330818908586972_0004_m_000107_262 : duration 0:00.004s
[2021-05-17 11:10:34,049] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409219079991164120569912_0004_m_000104_259: needsTaskCommit() Task attempt_202105171409219079991164120569912_0004_m_000104_259
[2021-05-17 11:10:34,050] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Task committer attempt_202105171409219079991164120569912_0004_m_000104_259: needsTaskCommit() Task attempt_202105171409219079991164120569912_0004_m_000104_259: duration 0:00.001s
21/05/17 14:10:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409219079991164120569912_0004_m_000104_259
[2021-05-17 11:10:34,052] {docker.py:276} INFO - 21/05/17 14:10:34 INFO Executor: Finished task 104.0 in stage 4.0 (TID 259). 4587 bytes result sent to driver
[2021-05-17 11:10:34,053] {docker.py:276} INFO - 21/05/17 14:10:34 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 263) (d733d8da4350, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:34,055] {docker.py:276} INFO - 21/05/17 14:10:34 INFO Executor: Running task 108.0 in stage 4.0 (TID 263)
[2021-05-17 11:10:34,056] {docker.py:276} INFO - 21/05/17 14:10:34 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 259) in 2261 ms on d733d8da4350 (executor driver) (105/200)
[2021-05-17 11:10:34,066] {docker.py:276} INFO - 21/05/17 14:10:34 INFO ShuffleBlockFetcherIterator: Getting 4 (19.8 KiB) non-empty blocks including 4 (19.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:34,068] {docker.py:276} INFO - 21/05/17 14:10:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:34,069] {docker.py:276} INFO - 21/05/17 14:10:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217177140726752287486_0004_m_000108_263, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217177140726752287486_0004_m_000108_263}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217177140726752287486_0004}; taskId=attempt_202105171409217177140726752287486_0004_m_000108_263, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e96d191}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:34,069] {docker.py:276} INFO - 21/05/17 14:10:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:34,069] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409217177140726752287486_0004_m_000108_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217177140726752287486_0004_m_000108_263
[2021-05-17 11:10:34,073] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Task committer attempt_202105171409217177140726752287486_0004_m_000108_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217177140726752287486_0004_m_000108_263 : duration 0:00.004s
[2021-05-17 11:10:34,370] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409214573810128302271132_0004_m_000105_260: needsTaskCommit() Task attempt_202105171409214573810128302271132_0004_m_000105_260
[2021-05-17 11:10:34,382] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Task committer attempt_202105171409214573810128302271132_0004_m_000105_260: needsTaskCommit() Task attempt_202105171409214573810128302271132_0004_m_000105_260: duration 0:00.001s
21/05/17 14:10:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214573810128302271132_0004_m_000105_260
[2021-05-17 11:10:34,382] {docker.py:276} INFO - 21/05/17 14:10:34 INFO Executor: Finished task 105.0 in stage 4.0 (TID 260). 4587 bytes result sent to driver
[2021-05-17 11:10:34,383] {docker.py:276} INFO - 21/05/17 14:10:34 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 264) (d733d8da4350, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:34,383] {docker.py:276} INFO - 21/05/17 14:10:34 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 260) in 2461 ms on d733d8da4350 (executor driver) (106/200)
[2021-05-17 11:10:34,383] {docker.py:276} INFO - 21/05/17 14:10:34 INFO Executor: Running task 109.0 in stage 4.0 (TID 264)
[2021-05-17 11:10:34,385] {docker.py:276} INFO - 21/05/17 14:10:34 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:34,385] {docker.py:276} INFO - 21/05/17 14:10:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:34,387] {docker.py:276} INFO - 21/05/17 14:10:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:10:34,387] {docker.py:276} INFO - 21/05/17 14:10:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:34,388] {docker.py:276} INFO - 21/05/17 14:10:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:34,388] {docker.py:276} INFO - 21/05/17 14:10:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218256714281427230118_0004_m_000109_264, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218256714281427230118_0004_m_000109_264}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218256714281427230118_0004}; taskId=attempt_202105171409218256714281427230118_0004_m_000109_264, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e700b6c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:34,388] {docker.py:276} INFO - 21/05/17 14:10:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:34,389] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409218256714281427230118_0004_m_000109_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218256714281427230118_0004_m_000109_264
[2021-05-17 11:10:34,392] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Task committer attempt_202105171409218256714281427230118_0004_m_000109_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218256714281427230118_0004_m_000109_264 : duration 0:00.004s
[2021-05-17 11:10:34,881] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409215226459701495268265_0004_m_000106_261: needsTaskCommit() Task attempt_202105171409215226459701495268265_0004_m_000106_261
21/05/17 14:10:34 INFO StagingCommitter: Task committer attempt_202105171409215226459701495268265_0004_m_000106_261: needsTaskCommit() Task attempt_202105171409215226459701495268265_0004_m_000106_261: duration 0:00.000s
[2021-05-17 11:10:34,882] {docker.py:276} INFO - 21/05/17 14:10:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215226459701495268265_0004_m_000106_261
[2021-05-17 11:10:34,884] {docker.py:276} INFO - 21/05/17 14:10:34 INFO Executor: Finished task 106.0 in stage 4.0 (TID 261). 4544 bytes result sent to driver
[2021-05-17 11:10:34,885] {docker.py:276} INFO - 21/05/17 14:10:34 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 265) (d733d8da4350, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:34,886] {docker.py:276} INFO - 21/05/17 14:10:34 INFO Executor: Running task 110.0 in stage 4.0 (TID 265)
[2021-05-17 11:10:34,887] {docker.py:276} INFO - 21/05/17 14:10:34 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 261) in 2277 ms on d733d8da4350 (executor driver) (107/200)
[2021-05-17 11:10:34,897] {docker.py:276} INFO - 21/05/17 14:10:34 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:34,899] {docker.py:276} INFO - 21/05/17 14:10:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216463574151300374877_0004_m_000110_265, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216463574151300374877_0004_m_000110_265}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216463574151300374877_0004}; taskId=attempt_202105171409216463574151300374877_0004_m_000110_265, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e3095f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:34,899] {docker.py:276} INFO - 21/05/17 14:10:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:34 INFO StagingCommitter: Starting: Task committer attempt_202105171409216463574151300374877_0004_m_000110_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216463574151300374877_0004_m_000110_265
[2021-05-17 11:10:34,902] {docker.py:276} INFO - 21/05/17 14:10:34 INFO StagingCommitter: Task committer attempt_202105171409216463574151300374877_0004_m_000110_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216463574151300374877_0004_m_000110_265 : duration 0:00.003s
[2021-05-17 11:10:36,032] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409213047330818908586972_0004_m_000107_262: needsTaskCommit() Task attempt_202105171409213047330818908586972_0004_m_000107_262
[2021-05-17 11:10:36,033] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Task committer attempt_202105171409213047330818908586972_0004_m_000107_262: needsTaskCommit() Task attempt_202105171409213047330818908586972_0004_m_000107_262: duration 0:00.000s
[2021-05-17 11:10:36,033] {docker.py:276} INFO - 21/05/17 14:10:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213047330818908586972_0004_m_000107_262
[2021-05-17 11:10:36,035] {docker.py:276} INFO - 21/05/17 14:10:36 INFO Executor: Finished task 107.0 in stage 4.0 (TID 262). 4544 bytes result sent to driver
[2021-05-17 11:10:36,036] {docker.py:276} INFO - 21/05/17 14:10:36 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 266) (d733d8da4350, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:36,037] {docker.py:276} INFO - 21/05/17 14:10:36 INFO Executor: Running task 111.0 in stage 4.0 (TID 266)
[2021-05-17 11:10:36,038] {docker.py:276} INFO - 21/05/17 14:10:36 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 262) in 2421 ms on d733d8da4350 (executor driver) (108/200)
[2021-05-17 11:10:36,047] {docker.py:276} INFO - 21/05/17 14:10:36 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:36,048] {docker.py:276} INFO - 21/05/17 14:10:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:36,049] {docker.py:276} INFO - 21/05/17 14:10:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211373993872115113949_0004_m_000111_266, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211373993872115113949_0004_m_000111_266}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211373993872115113949_0004}; taskId=attempt_202105171409211373993872115113949_0004_m_000111_266, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ce2f8e6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409211373993872115113949_0004_m_000111_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211373993872115113949_0004_m_000111_266
[2021-05-17 11:10:36,051] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Task committer attempt_202105171409211373993872115113949_0004_m_000111_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211373993872115113949_0004_m_000111_266 : duration 0:00.002s
[2021-05-17 11:10:36,438] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409217177140726752287486_0004_m_000108_263: needsTaskCommit() Task attempt_202105171409217177140726752287486_0004_m_000108_263
[2021-05-17 11:10:36,439] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Task committer attempt_202105171409217177140726752287486_0004_m_000108_263: needsTaskCommit() Task attempt_202105171409217177140726752287486_0004_m_000108_263: duration 0:00.000s
21/05/17 14:10:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217177140726752287486_0004_m_000108_263
[2021-05-17 11:10:36,440] {docker.py:276} INFO - 21/05/17 14:10:36 INFO Executor: Finished task 108.0 in stage 4.0 (TID 263). 4544 bytes result sent to driver
[2021-05-17 11:10:36,441] {docker.py:276} INFO - 21/05/17 14:10:36 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 267) (d733d8da4350, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:36,441] {docker.py:276} INFO - 21/05/17 14:10:36 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 263) in 2392 ms on d733d8da4350 (executor driver) (109/200)
[2021-05-17 11:10:36,442] {docker.py:276} INFO - 21/05/17 14:10:36 INFO Executor: Running task 112.0 in stage 4.0 (TID 267)
[2021-05-17 11:10:36,450] {docker.py:276} INFO - 21/05/17 14:10:36 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:36,452] {docker.py:276} INFO - 21/05/17 14:10:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:36,452] {docker.py:276} INFO - 21/05/17 14:10:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216902635479830543406_0004_m_000112_267, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216902635479830543406_0004_m_000112_267}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216902635479830543406_0004}; taskId=attempt_202105171409216902635479830543406_0004_m_000112_267, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@578beac1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:36,453] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409216902635479830543406_0004_m_000112_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216902635479830543406_0004_m_000112_267
[2021-05-17 11:10:36,457] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Task committer attempt_202105171409216902635479830543406_0004_m_000112_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216902635479830543406_0004_m_000112_267 : duration 0:00.005s
[2021-05-17 11:10:36,553] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409218256714281427230118_0004_m_000109_264: needsTaskCommit() Task attempt_202105171409218256714281427230118_0004_m_000109_264
[2021-05-17 11:10:36,554] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Task committer attempt_202105171409218256714281427230118_0004_m_000109_264: needsTaskCommit() Task attempt_202105171409218256714281427230118_0004_m_000109_264: duration 0:00.000s
21/05/17 14:10:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218256714281427230118_0004_m_000109_264
[2021-05-17 11:10:36,554] {docker.py:276} INFO - 21/05/17 14:10:36 INFO Executor: Finished task 109.0 in stage 4.0 (TID 264). 4544 bytes result sent to driver
[2021-05-17 11:10:36,555] {docker.py:276} INFO - 21/05/17 14:10:36 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 264) in 2185 ms on d733d8da4350 (executor driver) (110/200)
[2021-05-17 11:10:36,556] {docker.py:276} INFO - 21/05/17 14:10:36 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 268) (d733d8da4350, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:36,557] {docker.py:276} INFO - 21/05/17 14:10:36 INFO Executor: Running task 113.0 in stage 4.0 (TID 268)
[2021-05-17 11:10:36,565] {docker.py:276} INFO - 21/05/17 14:10:36 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:36,567] {docker.py:276} INFO - 21/05/17 14:10:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:36,567] {docker.py:276} INFO - 21/05/17 14:10:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:36,568] {docker.py:276} INFO - 21/05/17 14:10:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216568556166954100502_0004_m_000113_268, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216568556166954100502_0004_m_000113_268}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216568556166954100502_0004}; taskId=attempt_202105171409216568556166954100502_0004_m_000113_268, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f4527d9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:36,568] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Starting: Task committer attempt_202105171409216568556166954100502_0004_m_000113_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216568556166954100502_0004_m_000113_268
[2021-05-17 11:10:36,572] {docker.py:276} INFO - 21/05/17 14:10:36 INFO StagingCommitter: Task committer attempt_202105171409216568556166954100502_0004_m_000113_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216568556166954100502_0004_m_000113_268 : duration 0:00.004s
[2021-05-17 11:10:37,146] {docker.py:276} INFO - 21/05/17 14:10:37 INFO StagingCommitter: Starting: Task committer attempt_202105171409216463574151300374877_0004_m_000110_265: needsTaskCommit() Task attempt_202105171409216463574151300374877_0004_m_000110_265
[2021-05-17 11:10:37,147] {docker.py:276} INFO - 21/05/17 14:10:37 INFO StagingCommitter: Task committer attempt_202105171409216463574151300374877_0004_m_000110_265: needsTaskCommit() Task attempt_202105171409216463574151300374877_0004_m_000110_265: duration 0:00.001s
[2021-05-17 11:10:37,147] {docker.py:276} INFO - 21/05/17 14:10:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216463574151300374877_0004_m_000110_265
[2021-05-17 11:10:37,149] {docker.py:276} INFO - 21/05/17 14:10:37 INFO Executor: Finished task 110.0 in stage 4.0 (TID 265). 4544 bytes result sent to driver
[2021-05-17 11:10:37,150] {docker.py:276} INFO - 21/05/17 14:10:37 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 269) (d733d8da4350, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:37,151] {docker.py:276} INFO - 21/05/17 14:10:37 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 265) in 2269 ms on d733d8da4350 (executor driver) (111/200)
[2021-05-17 11:10:37,152] {docker.py:276} INFO - 21/05/17 14:10:37 INFO Executor: Running task 114.0 in stage 4.0 (TID 269)
[2021-05-17 11:10:37,161] {docker.py:276} INFO - 21/05/17 14:10:37 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:37,163] {docker.py:276} INFO - 21/05/17 14:10:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921514965064028828440_0004_m_000114_269, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921514965064028828440_0004_m_000114_269}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921514965064028828440_0004}; taskId=attempt_20210517140921514965064028828440_0004_m_000114_269, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d07bf8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:37 INFO StagingCommitter: Starting: Task committer attempt_20210517140921514965064028828440_0004_m_000114_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921514965064028828440_0004_m_000114_269
[2021-05-17 11:10:37,165] {docker.py:276} INFO - 21/05/17 14:10:37 INFO StagingCommitter: Task committer attempt_20210517140921514965064028828440_0004_m_000114_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921514965064028828440_0004_m_000114_269 : duration 0:00.003s
[2021-05-17 11:10:38,371] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409211373993872115113949_0004_m_000111_266: needsTaskCommit() Task attempt_202105171409211373993872115113949_0004_m_000111_266
21/05/17 14:10:38 INFO StagingCommitter: Task committer attempt_202105171409211373993872115113949_0004_m_000111_266: needsTaskCommit() Task attempt_202105171409211373993872115113949_0004_m_000111_266: duration 0:00.001s
21/05/17 14:10:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211373993872115113949_0004_m_000111_266
[2021-05-17 11:10:38,372] {docker.py:276} INFO - 21/05/17 14:10:38 INFO Executor: Finished task 111.0 in stage 4.0 (TID 266). 4544 bytes result sent to driver
[2021-05-17 11:10:38,374] {docker.py:276} INFO - 21/05/17 14:10:38 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 270) (d733d8da4350, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:38,375] {docker.py:276} INFO - 21/05/17 14:10:38 INFO Executor: Running task 115.0 in stage 4.0 (TID 270)
[2021-05-17 11:10:38,376] {docker.py:276} INFO - 21/05/17 14:10:38 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 266) in 2342 ms on d733d8da4350 (executor driver) (112/200)
[2021-05-17 11:10:38,386] {docker.py:276} INFO - 21/05/17 14:10:38 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:38,387] {docker.py:276} INFO - 21/05/17 14:10:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218266403841822853589_0004_m_000115_270, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218266403841822853589_0004_m_000115_270}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218266403841822853589_0004}; taskId=attempt_202105171409218266403841822853589_0004_m_000115_270, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a046b07}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:38,387] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409218266403841822853589_0004_m_000115_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218266403841822853589_0004_m_000115_270
[2021-05-17 11:10:38,390] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Task committer attempt_202105171409218266403841822853589_0004_m_000115_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218266403841822853589_0004_m_000115_270 : duration 0:00.003s
[2021-05-17 11:10:38,831] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409216568556166954100502_0004_m_000113_268: needsTaskCommit() Task attempt_202105171409216568556166954100502_0004_m_000113_268
[2021-05-17 11:10:38,831] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409216902635479830543406_0004_m_000112_267: needsTaskCommit() Task attempt_202105171409216902635479830543406_0004_m_000112_267
[2021-05-17 11:10:38,832] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Task committer attempt_202105171409216902635479830543406_0004_m_000112_267: needsTaskCommit() Task attempt_202105171409216902635479830543406_0004_m_000112_267: duration 0:00.000s
21/05/17 14:10:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216902635479830543406_0004_m_000112_267
[2021-05-17 11:10:38,832] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Task committer attempt_202105171409216568556166954100502_0004_m_000113_268: needsTaskCommit() Task attempt_202105171409216568556166954100502_0004_m_000113_268: duration 0:00.001s
21/05/17 14:10:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216568556166954100502_0004_m_000113_268
[2021-05-17 11:10:38,833] {docker.py:276} INFO - 21/05/17 14:10:38 INFO Executor: Finished task 113.0 in stage 4.0 (TID 268). 4544 bytes result sent to driver
[2021-05-17 11:10:38,834] {docker.py:276} INFO - 21/05/17 14:10:38 INFO Executor: Finished task 112.0 in stage 4.0 (TID 267). 4544 bytes result sent to driver
[2021-05-17 11:10:38,834] {docker.py:276} INFO - 21/05/17 14:10:38 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 271) (d733d8da4350, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:38,836] {docker.py:276} INFO - 21/05/17 14:10:38 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 272) (d733d8da4350, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:38,836] {docker.py:276} INFO - 21/05/17 14:10:38 INFO Executor: Running task 116.0 in stage 4.0 (TID 271)
[2021-05-17 11:10:38,837] {docker.py:276} INFO - 21/05/17 14:10:38 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 268) in 2284 ms on d733d8da4350 (executor driver) (113/200)
[2021-05-17 11:10:38,838] {docker.py:276} INFO - 21/05/17 14:10:38 INFO Executor: Running task 117.0 in stage 4.0 (TID 272)
[2021-05-17 11:10:38,838] {docker.py:276} INFO - 21/05/17 14:10:38 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 267) in 2400 ms on d733d8da4350 (executor driver) (114/200)
[2021-05-17 11:10:38,846] {docker.py:276} INFO - 21/05/17 14:10:38 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:38,847] {docker.py:276} INFO - 21/05/17 14:10:38 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:38,847] {docker.py:276} INFO - 21/05/17 14:10:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:38,848] {docker.py:276} INFO - 21/05/17 14:10:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:38,848] {docker.py:276} INFO - 21/05/17 14:10:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215743207734893082763_0004_m_000116_271, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215743207734893082763_0004_m_000116_271}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215743207734893082763_0004}; taskId=attempt_202105171409215743207734893082763_0004_m_000116_271, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11c66a25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409215743207734893082763_0004_m_000116_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215743207734893082763_0004_m_000116_271
[2021-05-17 11:10:38,850] {docker.py:276} INFO - 21/05/17 14:10:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:38,851] {docker.py:276} INFO - 21/05/17 14:10:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409219018738074559306131_0004_m_000117_272, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219018738074559306131_0004_m_000117_272}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409219018738074559306131_0004}; taskId=attempt_202105171409219018738074559306131_0004_m_000117_272, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a50b3d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:38,852] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Starting: Task committer attempt_202105171409219018738074559306131_0004_m_000117_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219018738074559306131_0004_m_000117_272
[2021-05-17 11:10:38,852] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Task committer attempt_202105171409215743207734893082763_0004_m_000116_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215743207734893082763_0004_m_000116_271 : duration 0:00.004s
[2021-05-17 11:10:38,857] {docker.py:276} INFO - 21/05/17 14:10:38 INFO StagingCommitter: Task committer attempt_202105171409219018738074559306131_0004_m_000117_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219018738074559306131_0004_m_000117_272 : duration 0:00.007s
[2021-05-17 11:10:39,663] {docker.py:276} INFO - 21/05/17 14:10:39 INFO StagingCommitter: Starting: Task committer attempt_20210517140921514965064028828440_0004_m_000114_269: needsTaskCommit() Task attempt_20210517140921514965064028828440_0004_m_000114_269
[2021-05-17 11:10:39,664] {docker.py:276} INFO - 21/05/17 14:10:39 INFO StagingCommitter: Task committer attempt_20210517140921514965064028828440_0004_m_000114_269: needsTaskCommit() Task attempt_20210517140921514965064028828440_0004_m_000114_269: duration 0:00.002s
[2021-05-17 11:10:39,664] {docker.py:276} INFO - 21/05/17 14:10:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921514965064028828440_0004_m_000114_269
[2021-05-17 11:10:39,666] {docker.py:276} INFO - 21/05/17 14:10:39 INFO Executor: Finished task 114.0 in stage 4.0 (TID 269). 4544 bytes result sent to driver
[2021-05-17 11:10:39,667] {docker.py:276} INFO - 21/05/17 14:10:39 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 273) (d733d8da4350, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:39,668] {docker.py:276} INFO - 21/05/17 14:10:39 INFO Executor: Running task 118.0 in stage 4.0 (TID 273)
[2021-05-17 11:10:39,668] {docker.py:276} INFO - 21/05/17 14:10:39 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 269) in 2521 ms on d733d8da4350 (executor driver) (115/200)
[2021-05-17 11:10:39,677] {docker.py:276} INFO - 21/05/17 14:10:39 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:39,678] {docker.py:276} INFO - 21/05/17 14:10:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 11:10:39,680] {docker.py:276} INFO - 21/05/17 14:10:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:39,681] {docker.py:276} INFO - 21/05/17 14:10:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217993631194845659213_0004_m_000118_273, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217993631194845659213_0004_m_000118_273}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217993631194845659213_0004}; taskId=attempt_202105171409217993631194845659213_0004_m_000118_273, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12a92ec2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:39 INFO StagingCommitter: Starting: Task committer attempt_202105171409217993631194845659213_0004_m_000118_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217993631194845659213_0004_m_000118_273
[2021-05-17 11:10:39,683] {docker.py:276} INFO - 21/05/17 14:10:39 INFO StagingCommitter: Task committer attempt_202105171409217993631194845659213_0004_m_000118_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217993631194845659213_0004_m_000118_273 : duration 0:00.003s
[2021-05-17 11:10:40,694] {docker.py:276} INFO - 21/05/17 14:10:40 INFO StagingCommitter: Starting: Task committer attempt_202105171409218266403841822853589_0004_m_000115_270: needsTaskCommit() Task attempt_202105171409218266403841822853589_0004_m_000115_270
[2021-05-17 11:10:40,694] {docker.py:276} INFO - 21/05/17 14:10:40 INFO StagingCommitter: Task committer attempt_202105171409218266403841822853589_0004_m_000115_270: needsTaskCommit() Task attempt_202105171409218266403841822853589_0004_m_000115_270: duration 0:00.001s
21/05/17 14:10:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218266403841822853589_0004_m_000115_270
[2021-05-17 11:10:40,696] {docker.py:276} INFO - 21/05/17 14:10:40 INFO Executor: Finished task 115.0 in stage 4.0 (TID 270). 4544 bytes result sent to driver
[2021-05-17 11:10:40,697] {docker.py:276} INFO - 21/05/17 14:10:40 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 274) (d733d8da4350, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:40,698] {docker.py:276} INFO - 21/05/17 14:10:40 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 270) in 2327 ms on d733d8da4350 (executor driver) (116/200)
[2021-05-17 11:10:40,699] {docker.py:276} INFO - 21/05/17 14:10:40 INFO Executor: Running task 119.0 in stage 4.0 (TID 274)
[2021-05-17 11:10:40,709] {docker.py:276} INFO - 21/05/17 14:10:40 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:40,710] {docker.py:276} INFO - 21/05/17 14:10:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214780912613485447582_0004_m_000119_274, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214780912613485447582_0004_m_000119_274}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214780912613485447582_0004}; taskId=attempt_202105171409214780912613485447582_0004_m_000119_274, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e5e16e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:40 INFO StagingCommitter: Starting: Task committer attempt_202105171409214780912613485447582_0004_m_000119_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214780912613485447582_0004_m_000119_274
[2021-05-17 11:10:40,714] {docker.py:276} INFO - 21/05/17 14:10:40 INFO StagingCommitter: Task committer attempt_202105171409214780912613485447582_0004_m_000119_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214780912613485447582_0004_m_000119_274 : duration 0:00.003s
[2021-05-17 11:10:41,204] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409215743207734893082763_0004_m_000116_271: needsTaskCommit() Task attempt_202105171409215743207734893082763_0004_m_000116_271
[2021-05-17 11:10:41,205] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Task committer attempt_202105171409215743207734893082763_0004_m_000116_271: needsTaskCommit() Task attempt_202105171409215743207734893082763_0004_m_000116_271: duration 0:00.001s
21/05/17 14:10:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215743207734893082763_0004_m_000116_271
[2021-05-17 11:10:41,206] {docker.py:276} INFO - 21/05/17 14:10:41 INFO Executor: Finished task 116.0 in stage 4.0 (TID 271). 4544 bytes result sent to driver
[2021-05-17 11:10:41,207] {docker.py:276} INFO - 21/05/17 14:10:41 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 275) (d733d8da4350, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:41,208] {docker.py:276} INFO - 21/05/17 14:10:41 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 271) in 2377 ms on d733d8da4350 (executor driver) (117/200)
[2021-05-17 11:10:41,209] {docker.py:276} INFO - 21/05/17 14:10:41 INFO Executor: Running task 120.0 in stage 4.0 (TID 275)
[2021-05-17 11:10:41,216] {docker.py:276} INFO - 21/05/17 14:10:41 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:41,228] {docker.py:276} INFO - 21/05/17 14:10:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:41,229] {docker.py:276} INFO - 21/05/17 14:10:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212622885557430340067_0004_m_000120_275, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212622885557430340067_0004_m_000120_275}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212622885557430340067_0004}; taskId=attempt_202105171409212622885557430340067_0004_m_000120_275, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57f28316}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:41,229] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409212622885557430340067_0004_m_000120_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212622885557430340067_0004_m_000120_275
[2021-05-17 11:10:41,231] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Task committer attempt_202105171409212622885557430340067_0004_m_000120_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212622885557430340067_0004_m_000120_275 : duration 0:00.004s
[2021-05-17 11:10:41,324] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409219018738074559306131_0004_m_000117_272: needsTaskCommit() Task attempt_202105171409219018738074559306131_0004_m_000117_272
[2021-05-17 11:10:41,325] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Task committer attempt_202105171409219018738074559306131_0004_m_000117_272: needsTaskCommit() Task attempt_202105171409219018738074559306131_0004_m_000117_272: duration 0:00.001s
21/05/17 14:10:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409219018738074559306131_0004_m_000117_272
[2021-05-17 11:10:41,331] {docker.py:276} INFO - 21/05/17 14:10:41 INFO Executor: Finished task 117.0 in stage 4.0 (TID 272). 4587 bytes result sent to driver
[2021-05-17 11:10:41,332] {docker.py:276} INFO - 21/05/17 14:10:41 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 276) (d733d8da4350, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:41,332] {docker.py:276} INFO - 21/05/17 14:10:41 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 272) in 2495 ms on d733d8da4350 (executor driver) (118/200)
[2021-05-17 11:10:41,332] {docker.py:276} INFO - 21/05/17 14:10:41 INFO Executor: Running task 121.0 in stage 4.0 (TID 276)
[2021-05-17 11:10:41,341] {docker.py:276} INFO - 21/05/17 14:10:41 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:41,346] {docker.py:276} INFO - 21/05/17 14:10:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216301853617081042050_0004_m_000121_276, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216301853617081042050_0004_m_000121_276}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216301853617081042050_0004}; taskId=attempt_202105171409216301853617081042050_0004_m_000121_276, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60bd1861}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409216301853617081042050_0004_m_000121_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216301853617081042050_0004_m_000121_276
[2021-05-17 11:10:41,357] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Task committer attempt_202105171409216301853617081042050_0004_m_000121_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216301853617081042050_0004_m_000121_276 : duration 0:00.007s
[2021-05-17 11:10:41,530] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409217993631194845659213_0004_m_000118_273: needsTaskCommit() Task attempt_202105171409217993631194845659213_0004_m_000118_273
[2021-05-17 11:10:41,531] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Task committer attempt_202105171409217993631194845659213_0004_m_000118_273: needsTaskCommit() Task attempt_202105171409217993631194845659213_0004_m_000118_273: duration 0:00.001s
21/05/17 14:10:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217993631194845659213_0004_m_000118_273
[2021-05-17 11:10:41,532] {docker.py:276} INFO - 21/05/17 14:10:41 INFO Executor: Finished task 118.0 in stage 4.0 (TID 273). 4587 bytes result sent to driver
[2021-05-17 11:10:41,534] {docker.py:276} INFO - 21/05/17 14:10:41 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 277) (d733d8da4350, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:41,535] {docker.py:276} INFO - 21/05/17 14:10:41 INFO Executor: Running task 122.0 in stage 4.0 (TID 277)
21/05/17 14:10:41 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 273) in 1871 ms on d733d8da4350 (executor driver) (119/200)
[2021-05-17 11:10:41,547] {docker.py:276} INFO - 21/05/17 14:10:41 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:41,549] {docker.py:276} INFO - 21/05/17 14:10:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217272487962448370645_0004_m_000122_277, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217272487962448370645_0004_m_000122_277}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217272487962448370645_0004}; taskId=attempt_202105171409217272487962448370645_0004_m_000122_277, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e9ecf47}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:41,549] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Starting: Task committer attempt_202105171409217272487962448370645_0004_m_000122_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217272487962448370645_0004_m_000122_277
[2021-05-17 11:10:41,552] {docker.py:276} INFO - 21/05/17 14:10:41 INFO StagingCommitter: Task committer attempt_202105171409217272487962448370645_0004_m_000122_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217272487962448370645_0004_m_000122_277 : duration 0:00.004s
[2021-05-17 11:10:42,485] {docker.py:276} INFO - 21/05/17 14:10:42 INFO StagingCommitter: Starting: Task committer attempt_202105171409214780912613485447582_0004_m_000119_274: needsTaskCommit() Task attempt_202105171409214780912613485447582_0004_m_000119_274
[2021-05-17 11:10:42,485] {docker.py:276} INFO - 21/05/17 14:10:42 INFO StagingCommitter: Task committer attempt_202105171409214780912613485447582_0004_m_000119_274: needsTaskCommit() Task attempt_202105171409214780912613485447582_0004_m_000119_274: duration 0:00.000s
21/05/17 14:10:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214780912613485447582_0004_m_000119_274
[2021-05-17 11:10:42,486] {docker.py:276} INFO - 21/05/17 14:10:42 INFO Executor: Finished task 119.0 in stage 4.0 (TID 274). 4587 bytes result sent to driver
[2021-05-17 11:10:42,487] {docker.py:276} INFO - 21/05/17 14:10:42 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 278) (d733d8da4350, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:42,487] {docker.py:276} INFO - 21/05/17 14:10:42 INFO Executor: Running task 123.0 in stage 4.0 (TID 278)
21/05/17 14:10:42 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 274) in 1794 ms on d733d8da4350 (executor driver) (120/200)
[2021-05-17 11:10:42,496] {docker.py:276} INFO - 21/05/17 14:10:42 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:42,496] {docker.py:276} INFO - 21/05/17 14:10:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:42,498] {docker.py:276} INFO - 21/05/17 14:10:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:42,499] {docker.py:276} INFO - 21/05/17 14:10:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211267203764502601035_0004_m_000123_278, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211267203764502601035_0004_m_000123_278}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211267203764502601035_0004}; taskId=attempt_202105171409211267203764502601035_0004_m_000123_278, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@479208e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:42,499] {docker.py:276} INFO - 21/05/17 14:10:42 INFO StagingCommitter: Starting: Task committer attempt_202105171409211267203764502601035_0004_m_000123_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211267203764502601035_0004_m_000123_278
[2021-05-17 11:10:42,502] {docker.py:276} INFO - 21/05/17 14:10:42 INFO StagingCommitter: Task committer attempt_202105171409211267203764502601035_0004_m_000123_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211267203764502601035_0004_m_000123_278 : duration 0:00.003s
[2021-05-17 11:10:43,695] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409212622885557430340067_0004_m_000120_275: needsTaskCommit() Task attempt_202105171409212622885557430340067_0004_m_000120_275
[2021-05-17 11:10:43,696] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Task committer attempt_202105171409212622885557430340067_0004_m_000120_275: needsTaskCommit() Task attempt_202105171409212622885557430340067_0004_m_000120_275: duration 0:00.000s
[2021-05-17 11:10:43,697] {docker.py:276} INFO - 21/05/17 14:10:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212622885557430340067_0004_m_000120_275
[2021-05-17 11:10:43,699] {docker.py:276} INFO - 21/05/17 14:10:43 INFO Executor: Finished task 120.0 in stage 4.0 (TID 275). 4587 bytes result sent to driver
[2021-05-17 11:10:43,700] {docker.py:276} INFO - 21/05/17 14:10:43 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 279) (d733d8da4350, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:43,701] {docker.py:276} INFO - 21/05/17 14:10:43 INFO Executor: Running task 124.0 in stage 4.0 (TID 279)
[2021-05-17 11:10:43,702] {docker.py:276} INFO - 21/05/17 14:10:43 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 275) in 2496 ms on d733d8da4350 (executor driver) (121/200)
[2021-05-17 11:10:43,707] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409217272487962448370645_0004_m_000122_277: needsTaskCommit() Task attempt_202105171409217272487962448370645_0004_m_000122_277
[2021-05-17 11:10:43,708] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Task committer attempt_202105171409217272487962448370645_0004_m_000122_277: needsTaskCommit() Task attempt_202105171409217272487962448370645_0004_m_000122_277: duration 0:00.001s
[2021-05-17 11:10:43,708] {docker.py:276} INFO - 21/05/17 14:10:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217272487962448370645_0004_m_000122_277
[2021-05-17 11:10:43,711] {docker.py:276} INFO - 21/05/17 14:10:43 INFO Executor: Finished task 122.0 in stage 4.0 (TID 277). 4544 bytes result sent to driver
[2021-05-17 11:10:43,712] {docker.py:276} INFO - 21/05/17 14:10:43 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 280) (d733d8da4350, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:43,712] {docker.py:276} INFO - 21/05/17 14:10:43 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 277) in 2182 ms on d733d8da4350 (executor driver) (122/200)
[2021-05-17 11:10:43,714] {docker.py:276} INFO - 21/05/17 14:10:43 INFO ShuffleBlockFetcherIterator: Getting 4 (19.9 KiB) non-empty blocks including 4 (19.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:43,714] {docker.py:276} INFO - 21/05/17 14:10:43 INFO Executor: Running task 125.0 in stage 4.0 (TID 280)
[2021-05-17 11:10:43,716] {docker.py:276} INFO - 21/05/17 14:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:43,717] {docker.py:276} INFO - 21/05/17 14:10:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214327247123904571566_0004_m_000124_279, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214327247123904571566_0004_m_000124_279}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214327247123904571566_0004}; taskId=attempt_202105171409214327247123904571566_0004_m_000124_279, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64254740}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:43,717] {docker.py:276} INFO - 21/05/17 14:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:43,718] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409214327247123904571566_0004_m_000124_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214327247123904571566_0004_m_000124_279
[2021-05-17 11:10:43,722] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Task committer attempt_202105171409214327247123904571566_0004_m_000124_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214327247123904571566_0004_m_000124_279 : duration 0:00.005s
[2021-05-17 11:10:43,725] {docker.py:276} INFO - 21/05/17 14:10:43 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:43,727] {docker.py:276} INFO - 21/05/17 14:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:43,727] {docker.py:276} INFO - 21/05/17 14:10:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217849377067045304506_0004_m_000125_280, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217849377067045304506_0004_m_000125_280}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217849377067045304506_0004}; taskId=attempt_202105171409217849377067045304506_0004_m_000125_280, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39e28c30}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:43,727] {docker.py:276} INFO - 21/05/17 14:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:43,728] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Starting: Task committer attempt_202105171409217849377067045304506_0004_m_000125_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217849377067045304506_0004_m_000125_280
[2021-05-17 11:10:43,730] {docker.py:276} INFO - 21/05/17 14:10:43 INFO StagingCommitter: Task committer attempt_202105171409217849377067045304506_0004_m_000125_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217849377067045304506_0004_m_000125_280 : duration 0:00.003s
[2021-05-17 11:10:44,809] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Starting: Task committer attempt_202105171409216301853617081042050_0004_m_000121_276: needsTaskCommit() Task attempt_202105171409216301853617081042050_0004_m_000121_276
[2021-05-17 11:10:44,811] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Task committer attempt_202105171409216301853617081042050_0004_m_000121_276: needsTaskCommit() Task attempt_202105171409216301853617081042050_0004_m_000121_276: duration 0:00.002s
21/05/17 14:10:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216301853617081042050_0004_m_000121_276
[2021-05-17 11:10:44,814] {docker.py:276} INFO - 21/05/17 14:10:44 INFO Executor: Finished task 121.0 in stage 4.0 (TID 276). 4544 bytes result sent to driver
[2021-05-17 11:10:44,816] {docker.py:276} INFO - 21/05/17 14:10:44 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 281) (d733d8da4350, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:44,817] {docker.py:276} INFO - 21/05/17 14:10:44 INFO Executor: Running task 126.0 in stage 4.0 (TID 281)
[2021-05-17 11:10:44,817] {docker.py:276} INFO - 21/05/17 14:10:44 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 276) in 3493 ms on d733d8da4350 (executor driver) (123/200)
[2021-05-17 11:10:44,827] {docker.py:276} INFO - 21/05/17 14:10:44 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:44,829] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Starting: Task committer attempt_202105171409211267203764502601035_0004_m_000123_278: needsTaskCommit() Task attempt_202105171409211267203764502601035_0004_m_000123_278
[2021-05-17 11:10:44,829] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Task committer attempt_202105171409211267203764502601035_0004_m_000123_278: needsTaskCommit() Task attempt_202105171409211267203764502601035_0004_m_000123_278: duration 0:00.001s
[2021-05-17 11:10:44,830] {docker.py:276} INFO - 21/05/17 14:10:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211267203764502601035_0004_m_000123_278
[2021-05-17 11:10:44,831] {docker.py:276} INFO - 21/05/17 14:10:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213493642844239661537_0004_m_000126_281, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213493642844239661537_0004_m_000126_281}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213493642844239661537_0004}; taskId=attempt_202105171409213493642844239661537_0004_m_000126_281, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e160ef1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:44,831] {docker.py:276} INFO - 21/05/17 14:10:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:44,832] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Starting: Task committer attempt_202105171409213493642844239661537_0004_m_000126_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213493642844239661537_0004_m_000126_281
[2021-05-17 11:10:44,832] {docker.py:276} INFO - 21/05/17 14:10:44 INFO Executor: Finished task 123.0 in stage 4.0 (TID 278). 4544 bytes result sent to driver
[2021-05-17 11:10:44,834] {docker.py:276} INFO - 21/05/17 14:10:44 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 282) (d733d8da4350, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:44,834] {docker.py:276} INFO - 21/05/17 14:10:44 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 278) in 2351 ms on d733d8da4350 (executor driver) (124/200)
[2021-05-17 11:10:44,835] {docker.py:276} INFO - 21/05/17 14:10:44 INFO Executor: Running task 127.0 in stage 4.0 (TID 282)
[2021-05-17 11:10:44,839] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Task committer attempt_202105171409213493642844239661537_0004_m_000126_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213493642844239661537_0004_m_000126_281 : duration 0:00.007s
[2021-05-17 11:10:44,844] {docker.py:276} INFO - 21/05/17 14:10:44 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:44,846] {docker.py:276} INFO - 21/05/17 14:10:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:44,847] {docker.py:276} INFO - 21/05/17 14:10:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217596576238816250973_0004_m_000127_282, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217596576238816250973_0004_m_000127_282}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217596576238816250973_0004}; taskId=attempt_202105171409217596576238816250973_0004_m_000127_282, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@597e8bfd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:44,847] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Starting: Task committer attempt_202105171409217596576238816250973_0004_m_000127_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217596576238816250973_0004_m_000127_282
[2021-05-17 11:10:44,850] {docker.py:276} INFO - 21/05/17 14:10:44 INFO StagingCommitter: Task committer attempt_202105171409217596576238816250973_0004_m_000127_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217596576238816250973_0004_m_000127_282 : duration 0:00.003s
[2021-05-17 11:10:46,003] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Starting: Task committer attempt_202105171409217849377067045304506_0004_m_000125_280: needsTaskCommit() Task attempt_202105171409217849377067045304506_0004_m_000125_280
[2021-05-17 11:10:46,004] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Task committer attempt_202105171409217849377067045304506_0004_m_000125_280: needsTaskCommit() Task attempt_202105171409217849377067045304506_0004_m_000125_280: duration 0:00.001s
21/05/17 14:10:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217849377067045304506_0004_m_000125_280
[2021-05-17 11:10:46,010] {docker.py:276} INFO - 21/05/17 14:10:46 INFO Executor: Finished task 125.0 in stage 4.0 (TID 280). 4544 bytes result sent to driver
[2021-05-17 11:10:46,012] {docker.py:276} INFO - 21/05/17 14:10:46 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 283) (d733d8da4350, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:46,013] {docker.py:276} INFO - 21/05/17 14:10:46 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 280) in 2304 ms on d733d8da4350 (executor driver) (125/200)
[2021-05-17 11:10:46,013] {docker.py:276} INFO - 21/05/17 14:10:46 INFO Executor: Running task 128.0 in stage 4.0 (TID 283)
[2021-05-17 11:10:46,023] {docker.py:276} INFO - 21/05/17 14:10:46 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:46,025] {docker.py:276} INFO - 21/05/17 14:10:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921388165983417189965_0004_m_000128_283, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921388165983417189965_0004_m_000128_283}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921388165983417189965_0004}; taskId=attempt_20210517140921388165983417189965_0004_m_000128_283, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5365573d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:46 INFO StagingCommitter: Starting: Task committer attempt_20210517140921388165983417189965_0004_m_000128_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921388165983417189965_0004_m_000128_283
[2021-05-17 11:10:46,030] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Task committer attempt_20210517140921388165983417189965_0004_m_000128_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921388165983417189965_0004_m_000128_283 : duration 0:00.005s
[2021-05-17 11:10:46,051] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Starting: Task committer attempt_202105171409214327247123904571566_0004_m_000124_279: needsTaskCommit() Task attempt_202105171409214327247123904571566_0004_m_000124_279
[2021-05-17 11:10:46,051] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Task committer attempt_202105171409214327247123904571566_0004_m_000124_279: needsTaskCommit() Task attempt_202105171409214327247123904571566_0004_m_000124_279: duration 0:00.000s
21/05/17 14:10:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214327247123904571566_0004_m_000124_279
[2021-05-17 11:10:46,052] {docker.py:276} INFO - 21/05/17 14:10:46 INFO Executor: Finished task 124.0 in stage 4.0 (TID 279). 4544 bytes result sent to driver
[2021-05-17 11:10:46,053] {docker.py:276} INFO - 21/05/17 14:10:46 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 284) (d733d8da4350, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:46,054] {docker.py:276} INFO - 21/05/17 14:10:46 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 279) in 2358 ms on d733d8da4350 (executor driver) (126/200)
21/05/17 14:10:46 INFO Executor: Running task 129.0 in stage 4.0 (TID 284)
[2021-05-17 11:10:46,062] {docker.py:276} INFO - 21/05/17 14:10:46 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:46,062] {docker.py:276} INFO - 21/05/17 14:10:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:46,063] {docker.py:276} INFO - 21/05/17 14:10:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:46,064] {docker.py:276} INFO - 21/05/17 14:10:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212838404317155157333_0004_m_000129_284, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212838404317155157333_0004_m_000129_284}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212838404317155157333_0004}; taskId=attempt_202105171409212838404317155157333_0004_m_000129_284, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@608cda76}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:46,064] {docker.py:276} INFO - 21/05/17 14:10:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:46,064] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Starting: Task committer attempt_202105171409212838404317155157333_0004_m_000129_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212838404317155157333_0004_m_000129_284
[2021-05-17 11:10:46,067] {docker.py:276} INFO - 21/05/17 14:10:46 INFO StagingCommitter: Task committer attempt_202105171409212838404317155157333_0004_m_000129_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212838404317155157333_0004_m_000129_284 : duration 0:00.003s
[2021-05-17 11:10:47,159] {docker.py:276} INFO - 21/05/17 14:10:47 INFO StagingCommitter: Starting: Task committer attempt_202105171409217596576238816250973_0004_m_000127_282: needsTaskCommit() Task attempt_202105171409217596576238816250973_0004_m_000127_282
[2021-05-17 11:10:47,159] {docker.py:276} INFO - 21/05/17 14:10:47 INFO StagingCommitter: Task committer attempt_202105171409217596576238816250973_0004_m_000127_282: needsTaskCommit() Task attempt_202105171409217596576238816250973_0004_m_000127_282: duration 0:00.000s
21/05/17 14:10:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217596576238816250973_0004_m_000127_282
[2021-05-17 11:10:47,160] {docker.py:276} INFO - 21/05/17 14:10:47 INFO Executor: Finished task 127.0 in stage 4.0 (TID 282). 4544 bytes result sent to driver
[2021-05-17 11:10:47,161] {docker.py:276} INFO - 21/05/17 14:10:47 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 285) (d733d8da4350, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 14:10:47 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 282) in 2330 ms on d733d8da4350 (executor driver) (127/200)
[2021-05-17 11:10:47,162] {docker.py:276} INFO - 21/05/17 14:10:47 INFO Executor: Running task 130.0 in stage 4.0 (TID 285)
[2021-05-17 11:10:47,169] {docker.py:276} INFO - 21/05/17 14:10:47 INFO ShuffleBlockFetcherIterator: Getting 4 (20.0 KiB) non-empty blocks including 4 (20.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:47,171] {docker.py:276} INFO - 21/05/17 14:10:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921372722136860646067_0004_m_000130_285, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921372722136860646067_0004_m_000130_285}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921372722136860646067_0004}; taskId=attempt_20210517140921372722136860646067_0004_m_000130_285, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c17816e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:47 INFO StagingCommitter: Starting: Task committer attempt_20210517140921372722136860646067_0004_m_000130_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921372722136860646067_0004_m_000130_285
[2021-05-17 11:10:47,173] {docker.py:276} INFO - 21/05/17 14:10:47 INFO StagingCommitter: Task committer attempt_20210517140921372722136860646067_0004_m_000130_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921372722136860646067_0004_m_000130_285 : duration 0:00.002s
[2021-05-17 11:10:47,242] {docker.py:276} INFO - 21/05/17 14:10:47 INFO StagingCommitter: Starting: Task committer attempt_202105171409213493642844239661537_0004_m_000126_281: needsTaskCommit() Task attempt_202105171409213493642844239661537_0004_m_000126_281
[2021-05-17 11:10:47,242] {docker.py:276} INFO - 21/05/17 14:10:47 INFO StagingCommitter: Task committer attempt_202105171409213493642844239661537_0004_m_000126_281: needsTaskCommit() Task attempt_202105171409213493642844239661537_0004_m_000126_281: duration 0:00.000s
[2021-05-17 11:10:47,243] {docker.py:276} INFO - 21/05/17 14:10:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213493642844239661537_0004_m_000126_281
[2021-05-17 11:10:47,244] {docker.py:276} INFO - 21/05/17 14:10:47 INFO Executor: Finished task 126.0 in stage 4.0 (TID 281). 4544 bytes result sent to driver
[2021-05-17 11:10:47,244] {docker.py:276} INFO - 21/05/17 14:10:47 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 286) (d733d8da4350, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:47,245] {docker.py:276} INFO - 21/05/17 14:10:47 INFO Executor: Running task 131.0 in stage 4.0 (TID 286)
[2021-05-17 11:10:47,245] {docker.py:276} INFO - 21/05/17 14:10:47 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 281) in 2433 ms on d733d8da4350 (executor driver) (128/200)
[2021-05-17 11:10:47,254] {docker.py:276} INFO - 21/05/17 14:10:47 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:47,256] {docker.py:276} INFO - 21/05/17 14:10:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213236147006290485443_0004_m_000131_286, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213236147006290485443_0004_m_000131_286}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213236147006290485443_0004}; taskId=attempt_202105171409213236147006290485443_0004_m_000131_286, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79e7f5bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:47 INFO StagingCommitter: Starting: Task committer attempt_202105171409213236147006290485443_0004_m_000131_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213236147006290485443_0004_m_000131_286
[2021-05-17 11:10:47,259] {docker.py:276} INFO - 21/05/17 14:10:47 INFO StagingCommitter: Task committer attempt_202105171409213236147006290485443_0004_m_000131_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213236147006290485443_0004_m_000131_286 : duration 0:00.003s
[2021-05-17 11:10:48,386] {docker.py:276} INFO - 21/05/17 14:10:48 INFO StagingCommitter: Starting: Task committer attempt_20210517140921388165983417189965_0004_m_000128_283: needsTaskCommit() Task attempt_20210517140921388165983417189965_0004_m_000128_283
[2021-05-17 11:10:48,387] {docker.py:276} INFO - 21/05/17 14:10:48 INFO StagingCommitter: Task committer attempt_20210517140921388165983417189965_0004_m_000128_283: needsTaskCommit() Task attempt_20210517140921388165983417189965_0004_m_000128_283: duration 0:00.001s
[2021-05-17 11:10:48,388] {docker.py:276} INFO - 21/05/17 14:10:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921388165983417189965_0004_m_000128_283
[2021-05-17 11:10:48,389] {docker.py:276} INFO - 21/05/17 14:10:48 INFO Executor: Finished task 128.0 in stage 4.0 (TID 283). 4544 bytes result sent to driver
[2021-05-17 11:10:48,390] {docker.py:276} INFO - 21/05/17 14:10:48 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 287) (d733d8da4350, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:48,391] {docker.py:276} INFO - 21/05/17 14:10:48 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 283) in 2382 ms on d733d8da4350 (executor driver) (129/200)
[2021-05-17 11:10:48,392] {docker.py:276} INFO - 21/05/17 14:10:48 INFO Executor: Running task 132.0 in stage 4.0 (TID 287)
[2021-05-17 11:10:48,401] {docker.py:276} INFO - 21/05/17 14:10:48 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:48,403] {docker.py:276} INFO - 21/05/17 14:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921653613626192966705_0004_m_000132_287, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921653613626192966705_0004_m_000132_287}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921653613626192966705_0004}; taskId=attempt_20210517140921653613626192966705_0004_m_000132_287, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@678a05a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:48,404] {docker.py:276} INFO - 21/05/17 14:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:48 INFO StagingCommitter: Starting: Task committer attempt_20210517140921653613626192966705_0004_m_000132_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921653613626192966705_0004_m_000132_287
[2021-05-17 11:10:48,407] {docker.py:276} INFO - 21/05/17 14:10:48 INFO StagingCommitter: Task committer attempt_20210517140921653613626192966705_0004_m_000132_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921653613626192966705_0004_m_000132_287 : duration 0:00.003s
[2021-05-17 11:10:48,485] {docker.py:276} INFO - 21/05/17 14:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409212838404317155157333_0004_m_000129_284: needsTaskCommit() Task attempt_202105171409212838404317155157333_0004_m_000129_284
[2021-05-17 11:10:48,486] {docker.py:276} INFO - 21/05/17 14:10:48 INFO StagingCommitter: Task committer attempt_202105171409212838404317155157333_0004_m_000129_284: needsTaskCommit() Task attempt_202105171409212838404317155157333_0004_m_000129_284: duration 0:00.001s
21/05/17 14:10:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212838404317155157333_0004_m_000129_284
[2021-05-17 11:10:48,487] {docker.py:276} INFO - 21/05/17 14:10:48 INFO Executor: Finished task 129.0 in stage 4.0 (TID 284). 4544 bytes result sent to driver
[2021-05-17 11:10:48,488] {docker.py:276} INFO - 21/05/17 14:10:48 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 288) (d733d8da4350, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:48,489] {docker.py:276} INFO - 21/05/17 14:10:48 INFO Executor: Running task 133.0 in stage 4.0 (TID 288)
[2021-05-17 11:10:48,490] {docker.py:276} INFO - 21/05/17 14:10:48 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 284) in 2440 ms on d733d8da4350 (executor driver) (130/200)
[2021-05-17 11:10:48,499] {docker.py:276} INFO - 21/05/17 14:10:48 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:48,501] {docker.py:276} INFO - 21/05/17 14:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:48,501] {docker.py:276} INFO - 21/05/17 14:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409219076763985261908102_0004_m_000133_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219076763985261908102_0004_m_000133_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409219076763985261908102_0004}; taskId=attempt_202105171409219076763985261908102_0004_m_000133_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c7eeda1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105171409219076763985261908102_0004_m_000133_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219076763985261908102_0004_m_000133_288
[2021-05-17 11:10:48,504] {docker.py:276} INFO - 21/05/17 14:10:48 INFO StagingCommitter: Task committer attempt_202105171409219076763985261908102_0004_m_000133_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409219076763985261908102_0004_m_000133_288 : duration 0:00.003s
[2021-05-17 11:10:49,468] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Starting: Task committer attempt_20210517140921372722136860646067_0004_m_000130_285: needsTaskCommit() Task attempt_20210517140921372722136860646067_0004_m_000130_285
[2021-05-17 11:10:49,468] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Task committer attempt_20210517140921372722136860646067_0004_m_000130_285: needsTaskCommit() Task attempt_20210517140921372722136860646067_0004_m_000130_285: duration 0:00.000s
21/05/17 14:10:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921372722136860646067_0004_m_000130_285
[2021-05-17 11:10:49,470] {docker.py:276} INFO - 21/05/17 14:10:49 INFO Executor: Finished task 130.0 in stage 4.0 (TID 285). 4544 bytes result sent to driver
[2021-05-17 11:10:49,471] {docker.py:276} INFO - 21/05/17 14:10:49 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 289) (d733d8da4350, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:49,471] {docker.py:276} INFO - 21/05/17 14:10:49 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 285) in 2313 ms on d733d8da4350 (executor driver) (131/200)
[2021-05-17 11:10:49,473] {docker.py:276} INFO - 21/05/17 14:10:49 INFO Executor: Running task 134.0 in stage 4.0 (TID 289)
[2021-05-17 11:10:49,480] {docker.py:276} INFO - 21/05/17 14:10:49 INFO ShuffleBlockFetcherIterator: Getting 4 (20.8 KiB) non-empty blocks including 4 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:10:49,481] {docker.py:276} INFO - 21/05/17 14:10:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:49,482] {docker.py:276} INFO - 21/05/17 14:10:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:49,483] {docker.py:276} INFO - 21/05/17 14:10:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:49,483] {docker.py:276} INFO - 21/05/17 14:10:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213169427051242461527_0004_m_000134_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213169427051242461527_0004_m_000134_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213169427051242461527_0004}; taskId=attempt_202105171409213169427051242461527_0004_m_000134_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39b0f667}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:49,484] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Starting: Task committer attempt_202105171409213169427051242461527_0004_m_000134_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213169427051242461527_0004_m_000134_289
[2021-05-17 11:10:49,486] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Task committer attempt_202105171409213169427051242461527_0004_m_000134_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213169427051242461527_0004_m_000134_289 : duration 0:00.003s
[2021-05-17 11:10:49,636] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Starting: Task committer attempt_202105171409213236147006290485443_0004_m_000131_286: needsTaskCommit() Task attempt_202105171409213236147006290485443_0004_m_000131_286
[2021-05-17 11:10:49,637] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Task committer attempt_202105171409213236147006290485443_0004_m_000131_286: needsTaskCommit() Task attempt_202105171409213236147006290485443_0004_m_000131_286: duration 0:00.001s
21/05/17 14:10:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213236147006290485443_0004_m_000131_286
[2021-05-17 11:10:49,639] {docker.py:276} INFO - 21/05/17 14:10:49 INFO Executor: Finished task 131.0 in stage 4.0 (TID 286). 4544 bytes result sent to driver
[2021-05-17 11:10:49,640] {docker.py:276} INFO - 21/05/17 14:10:49 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 290) (d733d8da4350, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:49,642] {docker.py:276} INFO - 21/05/17 14:10:49 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 286) in 2400 ms on d733d8da4350 (executor driver) (132/200)
21/05/17 14:10:49 INFO Executor: Running task 135.0 in stage 4.0 (TID 290)
[2021-05-17 11:10:49,663] {docker.py:276} INFO - 21/05/17 14:10:49 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:49,665] {docker.py:276} INFO - 21/05/17 14:10:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:49,666] {docker.py:276} INFO - 21/05/17 14:10:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212965773488647824210_0004_m_000135_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212965773488647824210_0004_m_000135_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212965773488647824210_0004}; taskId=attempt_202105171409212965773488647824210_0004_m_000135_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4655ad66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:49,666] {docker.py:276} INFO - 21/05/17 14:10:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:49 INFO StagingCommitter: Starting: Task committer attempt_202105171409212965773488647824210_0004_m_000135_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212965773488647824210_0004_m_000135_290
[2021-05-17 11:10:49,671] {docker.py:276} INFO - 21/05/17 14:10:49 INFO StagingCommitter: Task committer attempt_202105171409212965773488647824210_0004_m_000135_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212965773488647824210_0004_m_000135_290 : duration 0:00.005s
[2021-05-17 11:10:50,830] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409219076763985261908102_0004_m_000133_288: needsTaskCommit() Task attempt_202105171409219076763985261908102_0004_m_000133_288
[2021-05-17 11:10:50,831] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Task committer attempt_202105171409219076763985261908102_0004_m_000133_288: needsTaskCommit() Task attempt_202105171409219076763985261908102_0004_m_000133_288: duration 0:00.002s
[2021-05-17 11:10:50,832] {docker.py:276} INFO - 21/05/17 14:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409219076763985261908102_0004_m_000133_288
[2021-05-17 11:10:50,834] {docker.py:276} INFO - 21/05/17 14:10:50 INFO Executor: Finished task 133.0 in stage 4.0 (TID 288). 4587 bytes result sent to driver
[2021-05-17 11:10:50,835] {docker.py:276} INFO - 21/05/17 14:10:50 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 291) (d733d8da4350, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:50,837] {docker.py:276} INFO - 21/05/17 14:10:50 INFO Executor: Running task 136.0 in stage 4.0 (TID 291)
[2021-05-17 11:10:50,838] {docker.py:276} INFO - 21/05/17 14:10:50 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 288) in 2351 ms on d733d8da4350 (executor driver) (133/200)
[2021-05-17 11:10:50,846] {docker.py:276} INFO - 21/05/17 14:10:50 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:50,848] {docker.py:276} INFO - 21/05/17 14:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:50,848] {docker.py:276} INFO - 21/05/17 14:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214666188506656067913_0004_m_000136_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214666188506656067913_0004_m_000136_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214666188506656067913_0004}; taskId=attempt_202105171409214666188506656067913_0004_m_000136_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12318c3c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:50,849] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105171409214666188506656067913_0004_m_000136_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214666188506656067913_0004_m_000136_291
[2021-05-17 11:10:50,852] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Task committer attempt_202105171409214666188506656067913_0004_m_000136_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214666188506656067913_0004_m_000136_291 : duration 0:00.004s
[2021-05-17 11:10:50,886] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Starting: Task committer attempt_20210517140921653613626192966705_0004_m_000132_287: needsTaskCommit() Task attempt_20210517140921653613626192966705_0004_m_000132_287
[2021-05-17 11:10:50,887] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Task committer attempt_20210517140921653613626192966705_0004_m_000132_287: needsTaskCommit() Task attempt_20210517140921653613626192966705_0004_m_000132_287: duration 0:00.001s
[2021-05-17 11:10:50,888] {docker.py:276} INFO - 21/05/17 14:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921653613626192966705_0004_m_000132_287
[2021-05-17 11:10:50,889] {docker.py:276} INFO - 21/05/17 14:10:50 INFO Executor: Finished task 132.0 in stage 4.0 (TID 287). 4587 bytes result sent to driver
[2021-05-17 11:10:50,890] {docker.py:276} INFO - 21/05/17 14:10:50 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 292) (d733d8da4350, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:50,891] {docker.py:276} INFO - 21/05/17 14:10:50 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 287) in 2504 ms on d733d8da4350 (executor driver) (134/200)
[2021-05-17 11:10:50,892] {docker.py:276} INFO - 21/05/17 14:10:50 INFO Executor: Running task 137.0 in stage 4.0 (TID 292)
[2021-05-17 11:10:50,901] {docker.py:276} INFO - 21/05/17 14:10:50 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:50,903] {docker.py:276} INFO - 21/05/17 14:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:50,903] {docker.py:276} INFO - 21/05/17 14:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051714092161569425714392682_0004_m_000137_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092161569425714392682_0004_m_000137_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051714092161569425714392682_0004}; taskId=attempt_2021051714092161569425714392682_0004_m_000137_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48a4ee3a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:50 INFO StagingCommitter: Starting: Task committer attempt_2021051714092161569425714392682_0004_m_000137_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092161569425714392682_0004_m_000137_292
[2021-05-17 11:10:50,907] {docker.py:276} INFO - 21/05/17 14:10:50 INFO StagingCommitter: Task committer attempt_2021051714092161569425714392682_0004_m_000137_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_2021051714092161569425714392682_0004_m_000137_292 : duration 0:00.004s
[2021-05-17 11:10:51,815] {docker.py:276} INFO - 21/05/17 14:10:51 INFO StagingCommitter: Starting: Task committer attempt_202105171409213169427051242461527_0004_m_000134_289: needsTaskCommit() Task attempt_202105171409213169427051242461527_0004_m_000134_289
[2021-05-17 11:10:51,816] {docker.py:276} INFO - 21/05/17 14:10:51 INFO StagingCommitter: Task committer attempt_202105171409213169427051242461527_0004_m_000134_289: needsTaskCommit() Task attempt_202105171409213169427051242461527_0004_m_000134_289: duration 0:00.001s
21/05/17 14:10:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213169427051242461527_0004_m_000134_289
[2021-05-17 11:10:51,817] {docker.py:276} INFO - 21/05/17 14:10:51 INFO Executor: Finished task 134.0 in stage 4.0 (TID 289). 4587 bytes result sent to driver
[2021-05-17 11:10:51,818] {docker.py:276} INFO - 21/05/17 14:10:51 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 293) (d733d8da4350, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:51,819] {docker.py:276} INFO - 21/05/17 14:10:51 INFO Executor: Running task 138.0 in stage 4.0 (TID 293)
[2021-05-17 11:10:51,820] {docker.py:276} INFO - 21/05/17 14:10:51 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 289) in 2351 ms on d733d8da4350 (executor driver) (135/200)
[2021-05-17 11:10:51,830] {docker.py:276} INFO - 21/05/17 14:10:51 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:51,831] {docker.py:276} INFO - 21/05/17 14:10:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216617863445437737881_0004_m_000138_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216617863445437737881_0004_m_000138_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216617863445437737881_0004}; taskId=attempt_202105171409216617863445437737881_0004_m_000138_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@766e2654}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:51 INFO StagingCommitter: Starting: Task committer attempt_202105171409216617863445437737881_0004_m_000138_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216617863445437737881_0004_m_000138_293
[2021-05-17 11:10:51,835] {docker.py:276} INFO - 21/05/17 14:10:51 INFO StagingCommitter: Task committer attempt_202105171409216617863445437737881_0004_m_000138_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216617863445437737881_0004_m_000138_293 : duration 0:00.003s
[2021-05-17 11:10:51,969] {docker.py:276} INFO - 21/05/17 14:10:51 INFO StagingCommitter: Starting: Task committer attempt_202105171409212965773488647824210_0004_m_000135_290: needsTaskCommit() Task attempt_202105171409212965773488647824210_0004_m_000135_290
[2021-05-17 11:10:51,971] {docker.py:276} INFO - 21/05/17 14:10:51 INFO StagingCommitter: Task committer attempt_202105171409212965773488647824210_0004_m_000135_290: needsTaskCommit() Task attempt_202105171409212965773488647824210_0004_m_000135_290: duration 0:00.001s
21/05/17 14:10:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212965773488647824210_0004_m_000135_290
[2021-05-17 11:10:51,972] {docker.py:276} INFO - 21/05/17 14:10:52 INFO Executor: Finished task 135.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-17 11:10:51,974] {docker.py:276} INFO - 21/05/17 14:10:52 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 294) (d733d8da4350, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:51,975] {docker.py:276} INFO - 21/05/17 14:10:52 INFO Executor: Running task 139.0 in stage 4.0 (TID 294)
21/05/17 14:10:52 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 290) in 2337 ms on d733d8da4350 (executor driver) (136/200)
[2021-05-17 11:10:51,985] {docker.py:276} INFO - 21/05/17 14:10:52 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:51,987] {docker.py:276} INFO - 21/05/17 14:10:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:51,987] {docker.py:276} INFO - 21/05/17 14:10:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217788888860753241153_0004_m_000139_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217788888860753241153_0004_m_000139_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217788888860753241153_0004}; taskId=attempt_202105171409217788888860753241153_0004_m_000139_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53f75aac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:52 INFO StagingCommitter: Starting: Task committer attempt_202105171409217788888860753241153_0004_m_000139_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217788888860753241153_0004_m_000139_294
[2021-05-17 11:10:51,990] {docker.py:276} INFO - 21/05/17 14:10:52 INFO StagingCommitter: Task committer attempt_202105171409217788888860753241153_0004_m_000139_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217788888860753241153_0004_m_000139_294 : duration 0:00.004s
[2021-05-17 11:10:53,108] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409214666188506656067913_0004_m_000136_291: needsTaskCommit() Task attempt_202105171409214666188506656067913_0004_m_000136_291
[2021-05-17 11:10:53,116] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Task committer attempt_202105171409214666188506656067913_0004_m_000136_291: needsTaskCommit() Task attempt_202105171409214666188506656067913_0004_m_000136_291: duration 0:00.001s
21/05/17 14:10:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214666188506656067913_0004_m_000136_291
21/05/17 14:10:53 INFO Executor: Finished task 136.0 in stage 4.0 (TID 291). 4544 bytes result sent to driver
21/05/17 14:10:53 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 295) (d733d8da4350, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 14:10:53 INFO Executor: Running task 140.0 in stage 4.0 (TID 295)
21/05/17 14:10:53 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 291) in 2281 ms on d733d8da4350 (executor driver) (137/200)
[2021-05-17 11:10:53,125] {docker.py:276} INFO - 21/05/17 14:10:53 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:53,126] {docker.py:276} INFO - 21/05/17 14:10:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218075434294418674025_0004_m_000140_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218075434294418674025_0004_m_000140_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218075434294418674025_0004}; taskId=attempt_202105171409218075434294418674025_0004_m_000140_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63ecdeb8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409218075434294418674025_0004_m_000140_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218075434294418674025_0004_m_000140_295
[2021-05-17 11:10:53,129] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Task committer attempt_202105171409218075434294418674025_0004_m_000140_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218075434294418674025_0004_m_000140_295 : duration 0:00.003s
[2021-05-17 11:10:53,203] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Starting: Task committer attempt_2021051714092161569425714392682_0004_m_000137_292: needsTaskCommit() Task attempt_2021051714092161569425714392682_0004_m_000137_292
[2021-05-17 11:10:53,204] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Task committer attempt_2021051714092161569425714392682_0004_m_000137_292: needsTaskCommit() Task attempt_2021051714092161569425714392682_0004_m_000137_292: duration 0:00.000s
21/05/17 14:10:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051714092161569425714392682_0004_m_000137_292
[2021-05-17 11:10:53,207] {docker.py:276} INFO - 21/05/17 14:10:53 INFO Executor: Finished task 137.0 in stage 4.0 (TID 292). 4544 bytes result sent to driver
21/05/17 14:10:53 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 296) (d733d8da4350, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:53,207] {docker.py:276} INFO - 21/05/17 14:10:53 INFO Executor: Running task 141.0 in stage 4.0 (TID 296)
21/05/17 14:10:53 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 292) in 2319 ms on d733d8da4350 (executor driver) (138/200)
[2021-05-17 11:10:53,214] {docker.py:276} INFO - 21/05/17 14:10:53 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:53,216] {docker.py:276} INFO - 21/05/17 14:10:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212098700204721258333_0004_m_000141_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212098700204721258333_0004_m_000141_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212098700204721258333_0004}; taskId=attempt_202105171409212098700204721258333_0004_m_000141_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b373966}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409212098700204721258333_0004_m_000141_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212098700204721258333_0004_m_000141_296
[2021-05-17 11:10:53,220] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Task committer attempt_202105171409212098700204721258333_0004_m_000141_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212098700204721258333_0004_m_000141_296 : duration 0:00.003s
[2021-05-17 11:10:53,385] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409216617863445437737881_0004_m_000138_293: needsTaskCommit() Task attempt_202105171409216617863445437737881_0004_m_000138_293
[2021-05-17 11:10:53,386] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Task committer attempt_202105171409216617863445437737881_0004_m_000138_293: needsTaskCommit() Task attempt_202105171409216617863445437737881_0004_m_000138_293: duration 0:00.001s
21/05/17 14:10:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216617863445437737881_0004_m_000138_293
[2021-05-17 11:10:53,387] {docker.py:276} INFO - 21/05/17 14:10:53 INFO Executor: Finished task 138.0 in stage 4.0 (TID 293). 4544 bytes result sent to driver
[2021-05-17 11:10:53,388] {docker.py:276} INFO - 21/05/17 14:10:53 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 297) (d733d8da4350, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:53,389] {docker.py:276} INFO - 21/05/17 14:10:53 INFO Executor: Running task 142.0 in stage 4.0 (TID 297)
[2021-05-17 11:10:53,390] {docker.py:276} INFO - 21/05/17 14:10:53 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 293) in 1573 ms on d733d8da4350 (executor driver) (139/200)
[2021-05-17 11:10:53,400] {docker.py:276} INFO - 21/05/17 14:10:53 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:53,401] {docker.py:276} INFO - 21/05/17 14:10:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215326741380209147114_0004_m_000142_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215326741380209147114_0004_m_000142_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215326741380209147114_0004}; taskId=attempt_202105171409215326741380209147114_0004_m_000142_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@dff02cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:53 INFO StagingCommitter: Starting: Task committer attempt_202105171409215326741380209147114_0004_m_000142_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215326741380209147114_0004_m_000142_297
[2021-05-17 11:10:53,404] {docker.py:276} INFO - 21/05/17 14:10:53 INFO StagingCommitter: Task committer attempt_202105171409215326741380209147114_0004_m_000142_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215326741380209147114_0004_m_000142_297 : duration 0:00.002s
[2021-05-17 11:10:54,244] {docker.py:276} INFO - 21/05/17 14:10:54 INFO StagingCommitter: Starting: Task committer attempt_202105171409217788888860753241153_0004_m_000139_294: needsTaskCommit() Task attempt_202105171409217788888860753241153_0004_m_000139_294
[2021-05-17 11:10:54,244] {docker.py:276} INFO - 21/05/17 14:10:54 INFO StagingCommitter: Task committer attempt_202105171409217788888860753241153_0004_m_000139_294: needsTaskCommit() Task attempt_202105171409217788888860753241153_0004_m_000139_294: duration 0:00.001s
21/05/17 14:10:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217788888860753241153_0004_m_000139_294
[2021-05-17 11:10:54,245] {docker.py:276} INFO - 21/05/17 14:10:54 INFO Executor: Finished task 139.0 in stage 4.0 (TID 294). 4544 bytes result sent to driver
[2021-05-17 11:10:54,247] {docker.py:276} INFO - 21/05/17 14:10:54 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 298) (d733d8da4350, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:54,248] {docker.py:276} INFO - 21/05/17 14:10:54 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 294) in 2277 ms on d733d8da4350 (executor driver) (140/200)
21/05/17 14:10:54 INFO Executor: Running task 143.0 in stage 4.0 (TID 298)
[2021-05-17 11:10:54,260] {docker.py:276} INFO - 21/05/17 14:10:54 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:54,262] {docker.py:276} INFO - 21/05/17 14:10:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211686740664790800372_0004_m_000143_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211686740664790800372_0004_m_000143_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211686740664790800372_0004}; taskId=attempt_202105171409211686740664790800372_0004_m_000143_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b102cb2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:54,262] {docker.py:276} INFO - 21/05/17 14:10:54 INFO StagingCommitter: Starting: Task committer attempt_202105171409211686740664790800372_0004_m_000143_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211686740664790800372_0004_m_000143_298
[2021-05-17 11:10:54,265] {docker.py:276} INFO - 21/05/17 14:10:54 INFO StagingCommitter: Task committer attempt_202105171409211686740664790800372_0004_m_000143_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211686740664790800372_0004_m_000143_298 : duration 0:00.003s
[2021-05-17 11:10:55,442] {docker.py:276} INFO - 21/05/17 14:10:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409218075434294418674025_0004_m_000140_295: needsTaskCommit() Task attempt_202105171409218075434294418674025_0004_m_000140_295
[2021-05-17 11:10:55,443] {docker.py:276} INFO - 21/05/17 14:10:55 INFO StagingCommitter: Task committer attempt_202105171409218075434294418674025_0004_m_000140_295: needsTaskCommit() Task attempt_202105171409218075434294418674025_0004_m_000140_295: duration 0:00.001s
21/05/17 14:10:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218075434294418674025_0004_m_000140_295
[2021-05-17 11:10:55,446] {docker.py:276} INFO - 21/05/17 14:10:55 INFO Executor: Finished task 140.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-17 11:10:55,447] {docker.py:276} INFO - 21/05/17 14:10:55 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 299) (d733d8da4350, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:55,448] {docker.py:276} INFO - 21/05/17 14:10:55 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 295) in 2339 ms on d733d8da4350 (executor driver) (141/200)
[2021-05-17 11:10:55,449] {docker.py:276} INFO - 21/05/17 14:10:55 INFO Executor: Running task 144.0 in stage 4.0 (TID 299)
[2021-05-17 11:10:55,458] {docker.py:276} INFO - 21/05/17 14:10:55 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:55,459] {docker.py:276} INFO - 21/05/17 14:10:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218535194568346365913_0004_m_000144_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218535194568346365913_0004_m_000144_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218535194568346365913_0004}; taskId=attempt_202105171409218535194568346365913_0004_m_000144_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24658931}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409218535194568346365913_0004_m_000144_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218535194568346365913_0004_m_000144_299
[2021-05-17 11:10:55,463] {docker.py:276} INFO - 21/05/17 14:10:55 INFO StagingCommitter: Task committer attempt_202105171409218535194568346365913_0004_m_000144_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218535194568346365913_0004_m_000144_299 : duration 0:00.004s
[2021-05-17 11:10:55,653] {docker.py:276} INFO - 21/05/17 14:10:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409215326741380209147114_0004_m_000142_297: needsTaskCommit() Task attempt_202105171409215326741380209147114_0004_m_000142_297
[2021-05-17 11:10:55,653] {docker.py:276} INFO - 21/05/17 14:10:55 INFO StagingCommitter: Task committer attempt_202105171409215326741380209147114_0004_m_000142_297: needsTaskCommit() Task attempt_202105171409215326741380209147114_0004_m_000142_297: duration 0:00.000s
21/05/17 14:10:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215326741380209147114_0004_m_000142_297
[2021-05-17 11:10:55,656] {docker.py:276} INFO - 21/05/17 14:10:55 INFO Executor: Finished task 142.0 in stage 4.0 (TID 297). 4544 bytes result sent to driver
[2021-05-17 11:10:55,657] {docker.py:276} INFO - 21/05/17 14:10:55 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 300) (d733d8da4350, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:55,659] {docker.py:276} INFO - 21/05/17 14:10:55 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 297) in 2273 ms on d733d8da4350 (executor driver) (142/200)
21/05/17 14:10:55 INFO Executor: Running task 145.0 in stage 4.0 (TID 300)
[2021-05-17 11:10:55,668] {docker.py:276} INFO - 21/05/17 14:10:55 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:55,669] {docker.py:276} INFO - 21/05/17 14:10:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214729485718170432076_0004_m_000145_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214729485718170432076_0004_m_000145_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214729485718170432076_0004}; taskId=attempt_202105171409214729485718170432076_0004_m_000145_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f3c1023}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:55 INFO StagingCommitter: Starting: Task committer attempt_202105171409214729485718170432076_0004_m_000145_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214729485718170432076_0004_m_000145_300
[2021-05-17 11:10:55,673] {docker.py:276} INFO - 21/05/17 14:10:55 INFO StagingCommitter: Task committer attempt_202105171409214729485718170432076_0004_m_000145_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214729485718170432076_0004_m_000145_300 : duration 0:00.004s
[2021-05-17 11:10:56,188] {docker.py:276} INFO - 21/05/17 14:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105171409212098700204721258333_0004_m_000141_296: needsTaskCommit() Task attempt_202105171409212098700204721258333_0004_m_000141_296
21/05/17 14:10:56 INFO StagingCommitter: Task committer attempt_202105171409212098700204721258333_0004_m_000141_296: needsTaskCommit() Task attempt_202105171409212098700204721258333_0004_m_000141_296: duration 0:00.000s
[2021-05-17 11:10:56,188] {docker.py:276} INFO - 21/05/17 14:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212098700204721258333_0004_m_000141_296
[2021-05-17 11:10:56,190] {docker.py:276} INFO - 21/05/17 14:10:56 INFO Executor: Finished task 141.0 in stage 4.0 (TID 296). 4544 bytes result sent to driver
[2021-05-17 11:10:56,191] {docker.py:276} INFO - 21/05/17 14:10:56 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 301) (d733d8da4350, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:56,193] {docker.py:276} INFO - 21/05/17 14:10:56 INFO Executor: Running task 146.0 in stage 4.0 (TID 301)
21/05/17 14:10:56 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 296) in 2956 ms on d733d8da4350 (executor driver) (143/200)
[2021-05-17 11:10:56,203] {docker.py:276} INFO - 21/05/17 14:10:56 INFO ShuffleBlockFetcherIterator: Getting 4 (21.5 KiB) non-empty blocks including 4 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:56,204] {docker.py:276} INFO - 21/05/17 14:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:56,204] {docker.py:276} INFO - 21/05/17 14:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212229070670942769712_0004_m_000146_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212229070670942769712_0004_m_000146_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212229070670942769712_0004}; taskId=attempt_202105171409212229070670942769712_0004_m_000146_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@307a1d84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105171409212229070670942769712_0004_m_000146_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212229070670942769712_0004_m_000146_301
[2021-05-17 11:10:56,207] {docker.py:276} INFO - 21/05/17 14:10:56 INFO StagingCommitter: Task committer attempt_202105171409212229070670942769712_0004_m_000146_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212229070670942769712_0004_m_000146_301 : duration 0:00.003s
[2021-05-17 11:10:56,698] {docker.py:276} INFO - 21/05/17 14:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105171409211686740664790800372_0004_m_000143_298: needsTaskCommit() Task attempt_202105171409211686740664790800372_0004_m_000143_298
[2021-05-17 11:10:56,699] {docker.py:276} INFO - 21/05/17 14:10:56 INFO StagingCommitter: Task committer attempt_202105171409211686740664790800372_0004_m_000143_298: needsTaskCommit() Task attempt_202105171409211686740664790800372_0004_m_000143_298: duration 0:00.001s
21/05/17 14:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211686740664790800372_0004_m_000143_298
[2021-05-17 11:10:56,701] {docker.py:276} INFO - 21/05/17 14:10:56 INFO Executor: Finished task 143.0 in stage 4.0 (TID 298). 4544 bytes result sent to driver
[2021-05-17 11:10:56,702] {docker.py:276} INFO - 21/05/17 14:10:56 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 302) (d733d8da4350, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:56,703] {docker.py:276} INFO - 21/05/17 14:10:56 INFO Executor: Running task 147.0 in stage 4.0 (TID 302)
[2021-05-17 11:10:56,704] {docker.py:276} INFO - 21/05/17 14:10:56 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 298) in 2425 ms on d733d8da4350 (executor driver) (144/200)
[2021-05-17 11:10:56,713] {docker.py:276} INFO - 21/05/17 14:10:56 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:56,714] {docker.py:276} INFO - 21/05/17 14:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216872508571623122650_0004_m_000147_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216872508571623122650_0004_m_000147_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216872508571623122650_0004}; taskId=attempt_202105171409216872508571623122650_0004_m_000147_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73dcf984}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105171409216872508571623122650_0004_m_000147_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216872508571623122650_0004_m_000147_302
[2021-05-17 11:10:56,717] {docker.py:276} INFO - 21/05/17 14:10:56 INFO StagingCommitter: Task committer attempt_202105171409216872508571623122650_0004_m_000147_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216872508571623122650_0004_m_000147_302 : duration 0:00.002s
[2021-05-17 11:10:57,802] {docker.py:276} INFO - 21/05/17 14:10:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409218535194568346365913_0004_m_000144_299: needsTaskCommit() Task attempt_202105171409218535194568346365913_0004_m_000144_299
[2021-05-17 11:10:57,804] {docker.py:276} INFO - 21/05/17 14:10:57 INFO StagingCommitter: Task committer attempt_202105171409218535194568346365913_0004_m_000144_299: needsTaskCommit() Task attempt_202105171409218535194568346365913_0004_m_000144_299: duration 0:00.002s
21/05/17 14:10:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218535194568346365913_0004_m_000144_299
[2021-05-17 11:10:57,808] {docker.py:276} INFO - 21/05/17 14:10:57 INFO Executor: Finished task 144.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-17 11:10:57,810] {docker.py:276} INFO - 21/05/17 14:10:57 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 303) (d733d8da4350, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:57,811] {docker.py:276} INFO - 21/05/17 14:10:57 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 299) in 2332 ms on d733d8da4350 (executor driver) (145/200)
[2021-05-17 11:10:57,812] {docker.py:276} INFO - 21/05/17 14:10:57 INFO Executor: Running task 148.0 in stage 4.0 (TID 303)
[2021-05-17 11:10:57,820] {docker.py:276} INFO - 21/05/17 14:10:57 INFO ShuffleBlockFetcherIterator: Getting 4 (21.1 KiB) non-empty blocks including 4 (21.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:57,823] {docker.py:276} INFO - 21/05/17 14:10:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:10:57,824] {docker.py:276} INFO - 21/05/17 14:10:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:57,824] {docker.py:276} INFO - 21/05/17 14:10:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218169942626143050224_0004_m_000148_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218169942626143050224_0004_m_000148_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218169942626143050224_0004}; taskId=attempt_202105171409218169942626143050224_0004_m_000148_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bd50db5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:57,824] {docker.py:276} INFO - 21/05/17 14:10:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:10:57,824] {docker.py:276} INFO - 21/05/17 14:10:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409218169942626143050224_0004_m_000148_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218169942626143050224_0004_m_000148_303
[2021-05-17 11:10:57,828] {docker.py:276} INFO - 21/05/17 14:10:57 INFO StagingCommitter: Task committer attempt_202105171409218169942626143050224_0004_m_000148_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218169942626143050224_0004_m_000148_303 : duration 0:00.003s
[2021-05-17 11:10:57,990] {docker.py:276} INFO - 21/05/17 14:10:57 INFO StagingCommitter: Starting: Task committer attempt_202105171409214729485718170432076_0004_m_000145_300: needsTaskCommit() Task attempt_202105171409214729485718170432076_0004_m_000145_300
[2021-05-17 11:10:57,991] {docker.py:276} INFO - 21/05/17 14:10:57 INFO StagingCommitter: Task committer attempt_202105171409214729485718170432076_0004_m_000145_300: needsTaskCommit() Task attempt_202105171409214729485718170432076_0004_m_000145_300: duration 0:00.001s
[2021-05-17 11:10:57,991] {docker.py:276} INFO - 21/05/17 14:10:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214729485718170432076_0004_m_000145_300
[2021-05-17 11:10:57,993] {docker.py:276} INFO - 21/05/17 14:10:57 INFO Executor: Finished task 145.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
[2021-05-17 11:10:57,994] {docker.py:276} INFO - 21/05/17 14:10:57 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 304) (d733d8da4350, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:57,994] {docker.py:276} INFO - 21/05/17 14:10:57 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 300) in 2306 ms on d733d8da4350 (executor driver) (146/200)
[2021-05-17 11:10:57,995] {docker.py:276} INFO - 21/05/17 14:10:57 INFO Executor: Running task 149.0 in stage 4.0 (TID 304)
[2021-05-17 11:10:58,005] {docker.py:276} INFO - 21/05/17 14:10:58 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:58,006] {docker.py:276} INFO - 21/05/17 14:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212974665008681330386_0004_m_000149_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212974665008681330386_0004_m_000149_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212974665008681330386_0004}; taskId=attempt_202105171409212974665008681330386_0004_m_000149_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@132b0064}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:58,006] {docker.py:276} INFO - 21/05/17 14:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105171409212974665008681330386_0004_m_000149_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212974665008681330386_0004_m_000149_304
[2021-05-17 11:10:58,009] {docker.py:276} INFO - 21/05/17 14:10:58 INFO StagingCommitter: Task committer attempt_202105171409212974665008681330386_0004_m_000149_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212974665008681330386_0004_m_000149_304 : duration 0:00.003s
[2021-05-17 11:10:58,557] {docker.py:276} INFO - 21/05/17 14:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105171409216872508571623122650_0004_m_000147_302: needsTaskCommit() Task attempt_202105171409216872508571623122650_0004_m_000147_302
[2021-05-17 11:10:58,558] {docker.py:276} INFO - 21/05/17 14:10:58 INFO StagingCommitter: Task committer attempt_202105171409216872508571623122650_0004_m_000147_302: needsTaskCommit() Task attempt_202105171409216872508571623122650_0004_m_000147_302: duration 0:00.001s
21/05/17 14:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216872508571623122650_0004_m_000147_302
[2021-05-17 11:10:58,559] {docker.py:276} INFO - 21/05/17 14:10:58 INFO Executor: Finished task 147.0 in stage 4.0 (TID 302). 4587 bytes result sent to driver
[2021-05-17 11:10:58,560] {docker.py:276} INFO - 21/05/17 14:10:58 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 305) (d733d8da4350, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:58,561] {docker.py:276} INFO - 21/05/17 14:10:58 INFO Executor: Running task 150.0 in stage 4.0 (TID 305)
[2021-05-17 11:10:58,562] {docker.py:276} INFO - 21/05/17 14:10:58 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 302) in 1862 ms on d733d8da4350 (executor driver) (147/200)
[2021-05-17 11:10:58,571] {docker.py:276} INFO - 21/05/17 14:10:58 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:58,573] {docker.py:276} INFO - 21/05/17 14:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:58,573] {docker.py:276} INFO - 21/05/17 14:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211176161063510590225_0004_m_000150_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211176161063510590225_0004_m_000150_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211176161063510590225_0004}; taskId=attempt_202105171409211176161063510590225_0004_m_000150_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ea6a272}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105171409211176161063510590225_0004_m_000150_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211176161063510590225_0004_m_000150_305
[2021-05-17 11:10:58,577] {docker.py:276} INFO - 21/05/17 14:10:58 INFO StagingCommitter: Task committer attempt_202105171409211176161063510590225_0004_m_000150_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211176161063510590225_0004_m_000150_305 : duration 0:00.003s
[2021-05-17 11:10:58,600] {docker.py:276} INFO - 21/05/17 14:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105171409212229070670942769712_0004_m_000146_301: needsTaskCommit() Task attempt_202105171409212229070670942769712_0004_m_000146_301
21/05/17 14:10:58 INFO StagingCommitter: Task committer attempt_202105171409212229070670942769712_0004_m_000146_301: needsTaskCommit() Task attempt_202105171409212229070670942769712_0004_m_000146_301: duration 0:00.000s
21/05/17 14:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212229070670942769712_0004_m_000146_301
[2021-05-17 11:10:58,601] {docker.py:276} INFO - 21/05/17 14:10:58 INFO Executor: Finished task 146.0 in stage 4.0 (TID 301). 4587 bytes result sent to driver
[2021-05-17 11:10:58,602] {docker.py:276} INFO - 21/05/17 14:10:58 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 306) (d733d8da4350, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:10:58,603] {docker.py:276} INFO - 21/05/17 14:10:58 INFO Executor: Running task 151.0 in stage 4.0 (TID 306)
21/05/17 14:10:58 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 301) in 2414 ms on d733d8da4350 (executor driver) (148/200)
[2021-05-17 11:10:58,610] {docker.py:276} INFO - 21/05/17 14:10:58 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:10:58,611] {docker.py:276} INFO - 21/05/17 14:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213530051094410996595_0004_m_000151_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213530051094410996595_0004_m_000151_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213530051094410996595_0004}; taskId=attempt_202105171409213530051094410996595_0004_m_000151_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4134421}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:10:58,611] {docker.py:276} INFO - 21/05/17 14:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105171409213530051094410996595_0004_m_000151_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213530051094410996595_0004_m_000151_306
[2021-05-17 11:10:58,615] {docker.py:276} INFO - 21/05/17 14:10:58 INFO StagingCommitter: Task committer attempt_202105171409213530051094410996595_0004_m_000151_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213530051094410996595_0004_m_000151_306 : duration 0:00.004s
[2021-05-17 11:11:00,293] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409212974665008681330386_0004_m_000149_304: needsTaskCommit() Task attempt_202105171409212974665008681330386_0004_m_000149_304
21/05/17 14:11:00 INFO StagingCommitter: Task committer attempt_202105171409212974665008681330386_0004_m_000149_304: needsTaskCommit() Task attempt_202105171409212974665008681330386_0004_m_000149_304: duration 0:00.000s
21/05/17 14:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212974665008681330386_0004_m_000149_304
[2021-05-17 11:11:00,295] {docker.py:276} INFO - 21/05/17 14:11:00 INFO Executor: Finished task 149.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-17 11:11:00,296] {docker.py:276} INFO - 21/05/17 14:11:00 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 307) (d733d8da4350, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:00,297] {docker.py:276} INFO - 21/05/17 14:11:00 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 304) in 2306 ms on d733d8da4350 (executor driver) (149/200)
[2021-05-17 11:11:00,298] {docker.py:276} INFO - 21/05/17 14:11:00 INFO Executor: Running task 152.0 in stage 4.0 (TID 307)
[2021-05-17 11:11:00,305] {docker.py:276} INFO - 21/05/17 14:11:00 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:00,307] {docker.py:276} INFO - 21/05/17 14:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:00,307] {docker.py:276} INFO - 21/05/17 14:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:00,308] {docker.py:276} INFO - 21/05/17 14:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211951655138764718331_0004_m_000152_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211951655138764718331_0004_m_000152_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211951655138764718331_0004}; taskId=attempt_202105171409211951655138764718331_0004_m_000152_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28b4c48c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409211951655138764718331_0004_m_000152_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211951655138764718331_0004_m_000152_307
[2021-05-17 11:11:00,311] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Task committer attempt_202105171409211951655138764718331_0004_m_000152_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211951655138764718331_0004_m_000152_307 : duration 0:00.004s
[2021-05-17 11:11:00,579] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409211176161063510590225_0004_m_000150_305: needsTaskCommit() Task attempt_202105171409211176161063510590225_0004_m_000150_305
[2021-05-17 11:11:00,579] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Task committer attempt_202105171409211176161063510590225_0004_m_000150_305: needsTaskCommit() Task attempt_202105171409211176161063510590225_0004_m_000150_305: duration 0:00.000s
21/05/17 14:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211176161063510590225_0004_m_000150_305
[2021-05-17 11:11:00,580] {docker.py:276} INFO - 21/05/17 14:11:00 INFO Executor: Finished task 150.0 in stage 4.0 (TID 305). 4544 bytes result sent to driver
[2021-05-17 11:11:00,581] {docker.py:276} INFO - 21/05/17 14:11:00 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 308) (d733d8da4350, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:00,582] {docker.py:276} INFO - 21/05/17 14:11:00 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 305) in 2026 ms on d733d8da4350 (executor driver) (150/200)
[2021-05-17 11:11:00,582] {docker.py:276} INFO - 21/05/17 14:11:00 INFO Executor: Running task 153.0 in stage 4.0 (TID 308)
[2021-05-17 11:11:00,591] {docker.py:276} INFO - 21/05/17 14:11:00 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:00,593] {docker.py:276} INFO - 21/05/17 14:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:00,594] {docker.py:276} INFO - 21/05/17 14:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218630564962698042464_0004_m_000153_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218630564962698042464_0004_m_000153_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218630564962698042464_0004}; taskId=attempt_202105171409218630564962698042464_0004_m_000153_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@87a0ad8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409218630564962698042464_0004_m_000153_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218630564962698042464_0004_m_000153_308
[2021-05-17 11:11:00,597] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Task committer attempt_202105171409218630564962698042464_0004_m_000153_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218630564962698042464_0004_m_000153_308 : duration 0:00.004s
[2021-05-17 11:11:00,729] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409218169942626143050224_0004_m_000148_303: needsTaskCommit() Task attempt_202105171409218169942626143050224_0004_m_000148_303
[2021-05-17 11:11:00,730] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Task committer attempt_202105171409218169942626143050224_0004_m_000148_303: needsTaskCommit() Task attempt_202105171409218169942626143050224_0004_m_000148_303: duration 0:00.001s
21/05/17 14:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218169942626143050224_0004_m_000148_303
[2021-05-17 11:11:00,732] {docker.py:276} INFO - 21/05/17 14:11:00 INFO Executor: Finished task 148.0 in stage 4.0 (TID 303). 4587 bytes result sent to driver
[2021-05-17 11:11:00,733] {docker.py:276} INFO - 21/05/17 14:11:00 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 309) (d733d8da4350, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:00,735] {docker.py:276} INFO - 21/05/17 14:11:00 INFO Executor: Running task 154.0 in stage 4.0 (TID 309)
21/05/17 14:11:00 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 303) in 2928 ms on d733d8da4350 (executor driver) (151/200)
[2021-05-17 11:11:00,744] {docker.py:276} INFO - 21/05/17 14:11:00 INFO ShuffleBlockFetcherIterator: Getting 4 (19.5 KiB) non-empty blocks including 4 (19.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:00,747] {docker.py:276} INFO - 21/05/17 14:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218024954815079888419_0004_m_000154_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218024954815079888419_0004_m_000154_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218024954815079888419_0004}; taskId=attempt_202105171409218024954815079888419_0004_m_000154_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@338e0e24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:00,747] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105171409218024954815079888419_0004_m_000154_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218024954815079888419_0004_m_000154_309
[2021-05-17 11:11:00,750] {docker.py:276} INFO - 21/05/17 14:11:00 INFO StagingCommitter: Task committer attempt_202105171409218024954815079888419_0004_m_000154_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218024954815079888419_0004_m_000154_309 : duration 0:00.003s
[2021-05-17 11:11:01,161] {docker.py:276} INFO - 21/05/17 14:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105171409213530051094410996595_0004_m_000151_306: needsTaskCommit() Task attempt_202105171409213530051094410996595_0004_m_000151_306
[2021-05-17 11:11:01,162] {docker.py:276} INFO - 21/05/17 14:11:01 INFO StagingCommitter: Task committer attempt_202105171409213530051094410996595_0004_m_000151_306: needsTaskCommit() Task attempt_202105171409213530051094410996595_0004_m_000151_306: duration 0:00.000s
21/05/17 14:11:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213530051094410996595_0004_m_000151_306
[2021-05-17 11:11:01,163] {docker.py:276} INFO - 21/05/17 14:11:01 INFO Executor: Finished task 151.0 in stage 4.0 (TID 306). 4544 bytes result sent to driver
[2021-05-17 11:11:01,164] {docker.py:276} INFO - 21/05/17 14:11:01 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 310) (d733d8da4350, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:01,166] {docker.py:276} INFO - 21/05/17 14:11:01 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 306) in 2566 ms on d733d8da4350 (executor driver) (152/200)
21/05/17 14:11:01 INFO Executor: Running task 155.0 in stage 4.0 (TID 310)
[2021-05-17 11:11:01,174] {docker.py:276} INFO - 21/05/17 14:11:01 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:01,175] {docker.py:276} INFO - 21/05/17 14:11:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:01,176] {docker.py:276} INFO - 21/05/17 14:11:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217256820205777241031_0004_m_000155_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217256820205777241031_0004_m_000155_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217256820205777241031_0004}; taskId=attempt_202105171409217256820205777241031_0004_m_000155_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f93b152}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105171409217256820205777241031_0004_m_000155_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217256820205777241031_0004_m_000155_310
[2021-05-17 11:11:01,179] {docker.py:276} INFO - 21/05/17 14:11:01 INFO StagingCommitter: Task committer attempt_202105171409217256820205777241031_0004_m_000155_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217256820205777241031_0004_m_000155_310 : duration 0:00.003s
[2021-05-17 11:11:02,586] {docker.py:276} INFO - 21/05/17 14:11:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409211951655138764718331_0004_m_000152_307: needsTaskCommit() Task attempt_202105171409211951655138764718331_0004_m_000152_307
[2021-05-17 11:11:02,587] {docker.py:276} INFO - 21/05/17 14:11:02 INFO StagingCommitter: Task committer attempt_202105171409211951655138764718331_0004_m_000152_307: needsTaskCommit() Task attempt_202105171409211951655138764718331_0004_m_000152_307: duration 0:00.001s
[2021-05-17 11:11:02,587] {docker.py:276} INFO - 21/05/17 14:11:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211951655138764718331_0004_m_000152_307
[2021-05-17 11:11:02,589] {docker.py:276} INFO - 21/05/17 14:11:02 INFO Executor: Finished task 152.0 in stage 4.0 (TID 307). 4544 bytes result sent to driver
[2021-05-17 11:11:02,590] {docker.py:276} INFO - 21/05/17 14:11:02 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 311) (d733d8da4350, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:02,591] {docker.py:276} INFO - 21/05/17 14:11:02 INFO Executor: Running task 156.0 in stage 4.0 (TID 311)
[2021-05-17 11:11:02,592] {docker.py:276} INFO - 21/05/17 14:11:02 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 307) in 2298 ms on d733d8da4350 (executor driver) (153/200)
[2021-05-17 11:11:02,601] {docker.py:276} INFO - 21/05/17 14:11:02 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:02,603] {docker.py:276} INFO - 21/05/17 14:11:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212067206372449819901_0004_m_000156_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212067206372449819901_0004_m_000156_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212067206372449819901_0004}; taskId=attempt_202105171409212067206372449819901_0004_m_000156_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7cf7af2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409212067206372449819901_0004_m_000156_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212067206372449819901_0004_m_000156_311
[2021-05-17 11:11:02,606] {docker.py:276} INFO - 21/05/17 14:11:02 INFO StagingCommitter: Task committer attempt_202105171409212067206372449819901_0004_m_000156_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212067206372449819901_0004_m_000156_311 : duration 0:00.003s
[2021-05-17 11:11:02,939] {docker.py:276} INFO - 21/05/17 14:11:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409218630564962698042464_0004_m_000153_308: needsTaskCommit() Task attempt_202105171409218630564962698042464_0004_m_000153_308
[2021-05-17 11:11:02,940] {docker.py:276} INFO - 21/05/17 14:11:02 INFO StagingCommitter: Task committer attempt_202105171409218630564962698042464_0004_m_000153_308: needsTaskCommit() Task attempt_202105171409218630564962698042464_0004_m_000153_308: duration 0:00.001s
[2021-05-17 11:11:02,941] {docker.py:276} INFO - 21/05/17 14:11:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218630564962698042464_0004_m_000153_308
[2021-05-17 11:11:02,942] {docker.py:276} INFO - 21/05/17 14:11:02 INFO Executor: Finished task 153.0 in stage 4.0 (TID 308). 4544 bytes result sent to driver
[2021-05-17 11:11:02,943] {docker.py:276} INFO - 21/05/17 14:11:02 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 312) (d733d8da4350, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:02,945] {docker.py:276} INFO - 21/05/17 14:11:02 INFO Executor: Running task 157.0 in stage 4.0 (TID 312)
[2021-05-17 11:11:02,946] {docker.py:276} INFO - 21/05/17 14:11:02 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 308) in 2368 ms on d733d8da4350 (executor driver) (154/200)
[2021-05-17 11:11:02,955] {docker.py:276} INFO - 21/05/17 14:11:02 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:02,956] {docker.py:276} INFO - 21/05/17 14:11:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:02,957] {docker.py:276} INFO - 21/05/17 14:11:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217906585006210867705_0004_m_000157_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217906585006210867705_0004_m_000157_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217906585006210867705_0004}; taskId=attempt_202105171409217906585006210867705_0004_m_000157_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72ecbc94}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:02 INFO StagingCommitter: Starting: Task committer attempt_202105171409217906585006210867705_0004_m_000157_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217906585006210867705_0004_m_000157_312
[2021-05-17 11:11:02,959] {docker.py:276} INFO - 21/05/17 14:11:02 INFO StagingCommitter: Task committer attempt_202105171409217906585006210867705_0004_m_000157_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217906585006210867705_0004_m_000157_312 : duration 0:00.003s
[2021-05-17 11:11:03,092] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409218024954815079888419_0004_m_000154_309: needsTaskCommit() Task attempt_202105171409218024954815079888419_0004_m_000154_309
[2021-05-17 11:11:03,093] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Task committer attempt_202105171409218024954815079888419_0004_m_000154_309: needsTaskCommit() Task attempt_202105171409218024954815079888419_0004_m_000154_309: duration 0:00.002s
21/05/17 14:11:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218024954815079888419_0004_m_000154_309
[2021-05-17 11:11:03,095] {docker.py:276} INFO - 21/05/17 14:11:03 INFO Executor: Finished task 154.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
[2021-05-17 11:11:03,096] {docker.py:276} INFO - 21/05/17 14:11:03 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 313) (d733d8da4350, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:03,098] {docker.py:276} INFO - 21/05/17 14:11:03 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 309) in 2367 ms on d733d8da4350 (executor driver) (155/200)
21/05/17 14:11:03 INFO Executor: Running task 158.0 in stage 4.0 (TID 313)
[2021-05-17 11:11:03,107] {docker.py:276} INFO - 21/05/17 14:11:03 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:03,109] {docker.py:276} INFO - 21/05/17 14:11:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:03,110] {docker.py:276} INFO - 21/05/17 14:11:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218672258684634166650_0004_m_000158_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218672258684634166650_0004_m_000158_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218672258684634166650_0004}; taskId=attempt_202105171409218672258684634166650_0004_m_000158_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21a7ebdc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:03,110] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409218672258684634166650_0004_m_000158_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218672258684634166650_0004_m_000158_313
[2021-05-17 11:11:03,112] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Task committer attempt_202105171409218672258684634166650_0004_m_000158_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218672258684634166650_0004_m_000158_313 : duration 0:00.003s
[2021-05-17 11:11:03,627] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409217256820205777241031_0004_m_000155_310: needsTaskCommit() Task attempt_202105171409217256820205777241031_0004_m_000155_310
[2021-05-17 11:11:03,627] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Task committer attempt_202105171409217256820205777241031_0004_m_000155_310: needsTaskCommit() Task attempt_202105171409217256820205777241031_0004_m_000155_310: duration 0:00.001s
[2021-05-17 11:11:03,628] {docker.py:276} INFO - 21/05/17 14:11:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217256820205777241031_0004_m_000155_310
[2021-05-17 11:11:03,629] {docker.py:276} INFO - 21/05/17 14:11:03 INFO Executor: Finished task 155.0 in stage 4.0 (TID 310). 4544 bytes result sent to driver
[2021-05-17 11:11:03,630] {docker.py:276} INFO - 21/05/17 14:11:03 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 314) (d733d8da4350, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:03,631] {docker.py:276} INFO - 21/05/17 14:11:03 INFO Executor: Running task 159.0 in stage 4.0 (TID 314)
[2021-05-17 11:11:03,632] {docker.py:276} INFO - 21/05/17 14:11:03 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 310) in 2470 ms on d733d8da4350 (executor driver) (156/200)
[2021-05-17 11:11:03,641] {docker.py:276} INFO - 21/05/17 14:11:03 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:03,642] {docker.py:276} INFO - 21/05/17 14:11:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:03,643] {docker.py:276} INFO - 21/05/17 14:11:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214057282086702956185_0004_m_000159_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214057282086702956185_0004_m_000159_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214057282086702956185_0004}; taskId=attempt_202105171409214057282086702956185_0004_m_000159_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29799798}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105171409214057282086702956185_0004_m_000159_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214057282086702956185_0004_m_000159_314
[2021-05-17 11:11:03,646] {docker.py:276} INFO - 21/05/17 14:11:03 INFO StagingCommitter: Task committer attempt_202105171409214057282086702956185_0004_m_000159_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214057282086702956185_0004_m_000159_314 : duration 0:00.003s
[2021-05-17 11:11:05,106] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409212067206372449819901_0004_m_000156_311: needsTaskCommit() Task attempt_202105171409212067206372449819901_0004_m_000156_311
[2021-05-17 11:11:05,107] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Task committer attempt_202105171409212067206372449819901_0004_m_000156_311: needsTaskCommit() Task attempt_202105171409212067206372449819901_0004_m_000156_311: duration 0:00.001s
21/05/17 14:11:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212067206372449819901_0004_m_000156_311
[2021-05-17 11:11:05,109] {docker.py:276} INFO - 21/05/17 14:11:05 INFO Executor: Finished task 156.0 in stage 4.0 (TID 311). 4544 bytes result sent to driver
[2021-05-17 11:11:05,111] {docker.py:276} INFO - 21/05/17 14:11:05 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 315) (d733d8da4350, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:05,112] {docker.py:276} INFO - 21/05/17 14:11:05 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 311) in 2524 ms on d733d8da4350 (executor driver) (157/200)
[2021-05-17 11:11:05,112] {docker.py:276} INFO - 21/05/17 14:11:05 INFO Executor: Running task 160.0 in stage 4.0 (TID 315)
[2021-05-17 11:11:05,121] {docker.py:276} INFO - 21/05/17 14:11:05 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:05,124] {docker.py:276} INFO - 21/05/17 14:11:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:11:05,125] {docker.py:276} INFO - 21/05/17 14:11:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:05,125] {docker.py:276} INFO - 21/05/17 14:11:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:05,125] {docker.py:276} INFO - 21/05/17 14:11:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218715920804278579224_0004_m_000160_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218715920804278579224_0004_m_000160_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218715920804278579224_0004}; taskId=attempt_202105171409218715920804278579224_0004_m_000160_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c8acde2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:05,125] {docker.py:276} INFO - 21/05/17 14:11:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:05,126] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409218715920804278579224_0004_m_000160_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218715920804278579224_0004_m_000160_315
[2021-05-17 11:11:05,128] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Task committer attempt_202105171409218715920804278579224_0004_m_000160_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218715920804278579224_0004_m_000160_315 : duration 0:00.003s
[2021-05-17 11:11:05,236] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409217906585006210867705_0004_m_000157_312: needsTaskCommit() Task attempt_202105171409217906585006210867705_0004_m_000157_312
[2021-05-17 11:11:05,237] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Task committer attempt_202105171409217906585006210867705_0004_m_000157_312: needsTaskCommit() Task attempt_202105171409217906585006210867705_0004_m_000157_312: duration 0:00.002s
[2021-05-17 11:11:05,238] {docker.py:276} INFO - 21/05/17 14:11:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217906585006210867705_0004_m_000157_312
[2021-05-17 11:11:05,240] {docker.py:276} INFO - 21/05/17 14:11:05 INFO Executor: Finished task 157.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-17 11:11:05,242] {docker.py:276} INFO - 21/05/17 14:11:05 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 316) (d733d8da4350, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:05,243] {docker.py:276} INFO - 21/05/17 14:11:05 INFO Executor: Running task 161.0 in stage 4.0 (TID 316)
[2021-05-17 11:11:05,243] {docker.py:276} INFO - 21/05/17 14:11:05 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 312) in 2302 ms on d733d8da4350 (executor driver) (158/200)
[2021-05-17 11:11:05,252] {docker.py:276} INFO - 21/05/17 14:11:05 INFO ShuffleBlockFetcherIterator: Getting 4 (20.5 KiB) non-empty blocks including 4 (20.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:05,254] {docker.py:276} INFO - 21/05/17 14:11:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215138082646882356876_0004_m_000161_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215138082646882356876_0004_m_000161_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215138082646882356876_0004}; taskId=attempt_202105171409215138082646882356876_0004_m_000161_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c84e560}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409215138082646882356876_0004_m_000161_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215138082646882356876_0004_m_000161_316
[2021-05-17 11:11:05,257] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Task committer attempt_202105171409215138082646882356876_0004_m_000161_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215138082646882356876_0004_m_000161_316 : duration 0:00.004s
[2021-05-17 11:11:05,370] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409218672258684634166650_0004_m_000158_313: needsTaskCommit() Task attempt_202105171409218672258684634166650_0004_m_000158_313
[2021-05-17 11:11:05,371] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Task committer attempt_202105171409218672258684634166650_0004_m_000158_313: needsTaskCommit() Task attempt_202105171409218672258684634166650_0004_m_000158_313: duration 0:00.001s
21/05/17 14:11:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218672258684634166650_0004_m_000158_313
[2021-05-17 11:11:05,373] {docker.py:276} INFO - 21/05/17 14:11:05 INFO Executor: Finished task 158.0 in stage 4.0 (TID 313). 4544 bytes result sent to driver
[2021-05-17 11:11:05,375] {docker.py:276} INFO - 21/05/17 14:11:05 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 317) (d733d8da4350, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:05,376] {docker.py:276} INFO - 21/05/17 14:11:05 INFO Executor: Running task 162.0 in stage 4.0 (TID 317)
21/05/17 14:11:05 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 313) in 2283 ms on d733d8da4350 (executor driver) (159/200)
[2021-05-17 11:11:05,385] {docker.py:276} INFO - 21/05/17 14:11:05 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:05,387] {docker.py:276} INFO - 21/05/17 14:11:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216059251974870873544_0004_m_000162_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216059251974870873544_0004_m_000162_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216059251974870873544_0004}; taskId=attempt_202105171409216059251974870873544_0004_m_000162_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ab6850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:05,387] {docker.py:276} INFO - 21/05/17 14:11:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:05 INFO StagingCommitter: Starting: Task committer attempt_202105171409216059251974870873544_0004_m_000162_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216059251974870873544_0004_m_000162_317
[2021-05-17 11:11:05,390] {docker.py:276} INFO - 21/05/17 14:11:05 INFO StagingCommitter: Task committer attempt_202105171409216059251974870873544_0004_m_000162_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216059251974870873544_0004_m_000162_317 : duration 0:00.003s
[2021-05-17 11:11:06,077] {docker.py:276} INFO - 21/05/17 14:11:06 INFO StagingCommitter: Starting: Task committer attempt_202105171409214057282086702956185_0004_m_000159_314: needsTaskCommit() Task attempt_202105171409214057282086702956185_0004_m_000159_314
21/05/17 14:11:06 INFO StagingCommitter: Task committer attempt_202105171409214057282086702956185_0004_m_000159_314: needsTaskCommit() Task attempt_202105171409214057282086702956185_0004_m_000159_314: duration 0:00.000s
21/05/17 14:11:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214057282086702956185_0004_m_000159_314
[2021-05-17 11:11:06,078] {docker.py:276} INFO - 21/05/17 14:11:06 INFO Executor: Finished task 159.0 in stage 4.0 (TID 314). 4544 bytes result sent to driver
[2021-05-17 11:11:06,079] {docker.py:276} INFO - 21/05/17 14:11:06 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 318) (d733d8da4350, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:06,080] {docker.py:276} INFO - 21/05/17 14:11:06 INFO Executor: Running task 163.0 in stage 4.0 (TID 318)
[2021-05-17 11:11:06,081] {docker.py:276} INFO - 21/05/17 14:11:06 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 314) in 2453 ms on d733d8da4350 (executor driver) (160/200)
[2021-05-17 11:11:06,089] {docker.py:276} INFO - 21/05/17 14:11:06 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:06,091] {docker.py:276} INFO - 21/05/17 14:11:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212076788049608395274_0004_m_000163_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212076788049608395274_0004_m_000163_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212076788049608395274_0004}; taskId=attempt_202105171409212076788049608395274_0004_m_000163_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@344f411a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:06 INFO StagingCommitter: Starting: Task committer attempt_202105171409212076788049608395274_0004_m_000163_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212076788049608395274_0004_m_000163_318
[2021-05-17 11:11:06,095] {docker.py:276} INFO - 21/05/17 14:11:06 INFO StagingCommitter: Task committer attempt_202105171409212076788049608395274_0004_m_000163_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212076788049608395274_0004_m_000163_318 : duration 0:00.004s
[2021-05-17 11:11:07,454] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409218715920804278579224_0004_m_000160_315: needsTaskCommit() Task attempt_202105171409218715920804278579224_0004_m_000160_315
[2021-05-17 11:11:07,456] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Task committer attempt_202105171409218715920804278579224_0004_m_000160_315: needsTaskCommit() Task attempt_202105171409218715920804278579224_0004_m_000160_315: duration 0:00.002s
21/05/17 14:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218715920804278579224_0004_m_000160_315
21/05/17 14:11:07 INFO Executor: Finished task 160.0 in stage 4.0 (TID 315). 4544 bytes result sent to driver
[2021-05-17 11:11:07,458] {docker.py:276} INFO - 21/05/17 14:11:07 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 319) (d733d8da4350, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:07,460] {docker.py:276} INFO - 21/05/17 14:11:07 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 315) in 2351 ms on d733d8da4350 (executor driver) (161/200)
[2021-05-17 11:11:07,460] {docker.py:276} INFO - 21/05/17 14:11:07 INFO Executor: Running task 164.0 in stage 4.0 (TID 319)
[2021-05-17 11:11:07,481] {docker.py:276} INFO - 21/05/17 14:11:07 INFO ShuffleBlockFetcherIterator: Getting 4 (22.7 KiB) non-empty blocks including 4 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:07,482] {docker.py:276} INFO - 21/05/17 14:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218861084717276038344_0004_m_000164_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218861084717276038344_0004_m_000164_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218861084717276038344_0004}; taskId=attempt_202105171409218861084717276038344_0004_m_000164_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63ad782b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409218861084717276038344_0004_m_000164_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218861084717276038344_0004_m_000164_319
[2021-05-17 11:11:07,486] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Task committer attempt_202105171409218861084717276038344_0004_m_000164_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218861084717276038344_0004_m_000164_319 : duration 0:00.004s
[2021-05-17 11:11:07,560] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409215138082646882356876_0004_m_000161_316: needsTaskCommit() Task attempt_202105171409215138082646882356876_0004_m_000161_316
[2021-05-17 11:11:07,561] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Task committer attempt_202105171409215138082646882356876_0004_m_000161_316: needsTaskCommit() Task attempt_202105171409215138082646882356876_0004_m_000161_316: duration 0:00.000s
21/05/17 14:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215138082646882356876_0004_m_000161_316
[2021-05-17 11:11:07,563] {docker.py:276} INFO - 21/05/17 14:11:07 INFO Executor: Finished task 161.0 in stage 4.0 (TID 316). 4587 bytes result sent to driver
[2021-05-17 11:11:07,564] {docker.py:276} INFO - 21/05/17 14:11:07 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 320) (d733d8da4350, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:07,566] {docker.py:276} INFO - 21/05/17 14:11:07 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 316) in 2328 ms on d733d8da4350 (executor driver) (162/200)
[2021-05-17 11:11:07,567] {docker.py:276} INFO - 21/05/17 14:11:07 INFO Executor: Running task 165.0 in stage 4.0 (TID 320)
[2021-05-17 11:11:07,577] {docker.py:276} INFO - 21/05/17 14:11:07 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:07,578] {docker.py:276} INFO - 21/05/17 14:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213858371785206315283_0004_m_000165_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213858371785206315283_0004_m_000165_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213858371785206315283_0004}; taskId=attempt_202105171409213858371785206315283_0004_m_000165_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57d9a053}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:07,579] {docker.py:276} INFO - 21/05/17 14:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409213858371785206315283_0004_m_000165_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213858371785206315283_0004_m_000165_320
[2021-05-17 11:11:07,581] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Task committer attempt_202105171409213858371785206315283_0004_m_000165_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213858371785206315283_0004_m_000165_320 : duration 0:00.002s
[2021-05-17 11:11:07,751] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409216059251974870873544_0004_m_000162_317: needsTaskCommit() Task attempt_202105171409216059251974870873544_0004_m_000162_317
[2021-05-17 11:11:07,752] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Task committer attempt_202105171409216059251974870873544_0004_m_000162_317: needsTaskCommit() Task attempt_202105171409216059251974870873544_0004_m_000162_317: duration 0:00.000s
21/05/17 14:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216059251974870873544_0004_m_000162_317
[2021-05-17 11:11:07,753] {docker.py:276} INFO - 21/05/17 14:11:07 INFO Executor: Finished task 162.0 in stage 4.0 (TID 317). 4587 bytes result sent to driver
[2021-05-17 11:11:07,754] {docker.py:276} INFO - 21/05/17 14:11:07 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 321) (d733d8da4350, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:07,755] {docker.py:276} INFO - 21/05/17 14:11:07 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 317) in 2384 ms on d733d8da4350 (executor driver) (163/200)
[2021-05-17 11:11:07,756] {docker.py:276} INFO - 21/05/17 14:11:07 INFO Executor: Running task 166.0 in stage 4.0 (TID 321)
[2021-05-17 11:11:07,764] {docker.py:276} INFO - 21/05/17 14:11:07 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:07,766] {docker.py:276} INFO - 21/05/17 14:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217749671931183927873_0004_m_000166_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217749671931183927873_0004_m_000166_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217749671931183927873_0004}; taskId=attempt_202105171409217749671931183927873_0004_m_000166_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14645cb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:07,766] {docker.py:276} INFO - 21/05/17 14:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105171409217749671931183927873_0004_m_000166_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217749671931183927873_0004_m_000166_321
[2021-05-17 11:11:07,769] {docker.py:276} INFO - 21/05/17 14:11:07 INFO StagingCommitter: Task committer attempt_202105171409217749671931183927873_0004_m_000166_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217749671931183927873_0004_m_000166_321 : duration 0:00.003s
[2021-05-17 11:11:08,403] {docker.py:276} INFO - 21/05/17 14:11:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409212076788049608395274_0004_m_000163_318: needsTaskCommit() Task attempt_202105171409212076788049608395274_0004_m_000163_318
21/05/17 14:11:08 INFO StagingCommitter: Task committer attempt_202105171409212076788049608395274_0004_m_000163_318: needsTaskCommit() Task attempt_202105171409212076788049608395274_0004_m_000163_318: duration 0:00.001s
21/05/17 14:11:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212076788049608395274_0004_m_000163_318
[2021-05-17 11:11:08,405] {docker.py:276} INFO - 21/05/17 14:11:08 INFO Executor: Finished task 163.0 in stage 4.0 (TID 318). 4587 bytes result sent to driver
[2021-05-17 11:11:08,406] {docker.py:276} INFO - 21/05/17 14:11:08 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 322) (d733d8da4350, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:08,407] {docker.py:276} INFO - 21/05/17 14:11:08 INFO Executor: Running task 167.0 in stage 4.0 (TID 322)
[2021-05-17 11:11:08,408] {docker.py:276} INFO - 21/05/17 14:11:08 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 318) in 2331 ms on d733d8da4350 (executor driver) (164/200)
[2021-05-17 11:11:08,416] {docker.py:276} INFO - 21/05/17 14:11:08 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:08,418] {docker.py:276} INFO - 21/05/17 14:11:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214096206743951669258_0004_m_000167_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214096206743951669258_0004_m_000167_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214096206743951669258_0004}; taskId=attempt_202105171409214096206743951669258_0004_m_000167_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f50c32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:08,418] {docker.py:276} INFO - 21/05/17 14:11:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:08 INFO StagingCommitter: Starting: Task committer attempt_202105171409214096206743951669258_0004_m_000167_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214096206743951669258_0004_m_000167_322
[2021-05-17 11:11:08,421] {docker.py:276} INFO - 21/05/17 14:11:08 INFO StagingCommitter: Task committer attempt_202105171409214096206743951669258_0004_m_000167_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214096206743951669258_0004_m_000167_322 : duration 0:00.004s
[2021-05-17 11:11:09,744] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Starting: Task committer attempt_202105171409218861084717276038344_0004_m_000164_319: needsTaskCommit() Task attempt_202105171409218861084717276038344_0004_m_000164_319
[2021-05-17 11:11:09,745] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Task committer attempt_202105171409218861084717276038344_0004_m_000164_319: needsTaskCommit() Task attempt_202105171409218861084717276038344_0004_m_000164_319: duration 0:00.001s
21/05/17 14:11:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218861084717276038344_0004_m_000164_319
[2021-05-17 11:11:09,747] {docker.py:276} INFO - 21/05/17 14:11:09 INFO Executor: Finished task 164.0 in stage 4.0 (TID 319). 4587 bytes result sent to driver
[2021-05-17 11:11:09,748] {docker.py:276} INFO - 21/05/17 14:11:09 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 323) (d733d8da4350, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:09,749] {docker.py:276} INFO - 21/05/17 14:11:09 INFO Executor: Running task 168.0 in stage 4.0 (TID 323)
21/05/17 14:11:09 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 319) in 2294 ms on d733d8da4350 (executor driver) (165/200)
[2021-05-17 11:11:09,760] {docker.py:276} INFO - 21/05/17 14:11:09 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:09,762] {docker.py:276} INFO - 21/05/17 14:11:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212632833709489612646_0004_m_000168_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212632833709489612646_0004_m_000168_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212632833709489612646_0004}; taskId=attempt_202105171409212632833709489612646_0004_m_000168_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ed6f8b2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:09 INFO StagingCommitter: Starting: Task committer attempt_202105171409212632833709489612646_0004_m_000168_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212632833709489612646_0004_m_000168_323
[2021-05-17 11:11:09,764] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Task committer attempt_202105171409212632833709489612646_0004_m_000168_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212632833709489612646_0004_m_000168_323 : duration 0:00.003s
[2021-05-17 11:11:09,864] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Starting: Task committer attempt_202105171409213858371785206315283_0004_m_000165_320: needsTaskCommit() Task attempt_202105171409213858371785206315283_0004_m_000165_320
[2021-05-17 11:11:09,865] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Task committer attempt_202105171409213858371785206315283_0004_m_000165_320: needsTaskCommit() Task attempt_202105171409213858371785206315283_0004_m_000165_320: duration 0:00.001s
21/05/17 14:11:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213858371785206315283_0004_m_000165_320
[2021-05-17 11:11:09,866] {docker.py:276} INFO - 21/05/17 14:11:09 INFO Executor: Finished task 165.0 in stage 4.0 (TID 320). 4544 bytes result sent to driver
[2021-05-17 11:11:09,868] {docker.py:276} INFO - 21/05/17 14:11:09 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 324) (d733d8da4350, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:09,869] {docker.py:276} INFO - 21/05/17 14:11:09 INFO Executor: Running task 169.0 in stage 4.0 (TID 324)
[2021-05-17 11:11:09,870] {docker.py:276} INFO - 21/05/17 14:11:09 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 320) in 2308 ms on d733d8da4350 (executor driver) (166/200)
[2021-05-17 11:11:09,878] {docker.py:276} INFO - 21/05/17 14:11:09 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:09,881] {docker.py:276} INFO - 21/05/17 14:11:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:09,881] {docker.py:276} INFO - 21/05/17 14:11:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:09,881] {docker.py:276} INFO - 21/05/17 14:11:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213851246895935263593_0004_m_000169_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213851246895935263593_0004_m_000169_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213851246895935263593_0004}; taskId=attempt_202105171409213851246895935263593_0004_m_000169_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55b3df52}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:09,882] {docker.py:276} INFO - 21/05/17 14:11:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:09,882] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Starting: Task committer attempt_202105171409213851246895935263593_0004_m_000169_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213851246895935263593_0004_m_000169_324
[2021-05-17 11:11:09,885] {docker.py:276} INFO - 21/05/17 14:11:09 INFO StagingCommitter: Task committer attempt_202105171409213851246895935263593_0004_m_000169_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213851246895935263593_0004_m_000169_324 : duration 0:00.004s
[2021-05-17 11:11:10,117] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409217749671931183927873_0004_m_000166_321: needsTaskCommit() Task attempt_202105171409217749671931183927873_0004_m_000166_321
[2021-05-17 11:11:10,118] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Task committer attempt_202105171409217749671931183927873_0004_m_000166_321: needsTaskCommit() Task attempt_202105171409217749671931183927873_0004_m_000166_321: duration 0:00.000s
21/05/17 14:11:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217749671931183927873_0004_m_000166_321
[2021-05-17 11:11:10,119] {docker.py:276} INFO - 21/05/17 14:11:10 INFO Executor: Finished task 166.0 in stage 4.0 (TID 321). 4544 bytes result sent to driver
[2021-05-17 11:11:10,120] {docker.py:276} INFO - 21/05/17 14:11:10 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 325) (d733d8da4350, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:10,121] {docker.py:276} INFO - 21/05/17 14:11:10 INFO Executor: Running task 170.0 in stage 4.0 (TID 325)
[2021-05-17 11:11:10,122] {docker.py:276} INFO - 21/05/17 14:11:10 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 321) in 2371 ms on d733d8da4350 (executor driver) (167/200)
[2021-05-17 11:11:10,130] {docker.py:276} INFO - 21/05/17 14:11:10 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:10,130] {docker.py:276} INFO - 21/05/17 14:11:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:10,131] {docker.py:276} INFO - 21/05/17 14:11:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:10,132] {docker.py:276} INFO - 21/05/17 14:11:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:10,132] {docker.py:276} INFO - 21/05/17 14:11:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213464789324710375209_0004_m_000170_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213464789324710375209_0004_m_000170_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213464789324710375209_0004}; taskId=attempt_202105171409213464789324710375209_0004_m_000170_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31b19b6e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:10,132] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409213464789324710375209_0004_m_000170_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213464789324710375209_0004_m_000170_325
[2021-05-17 11:11:10,135] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Task committer attempt_202105171409213464789324710375209_0004_m_000170_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213464789324710375209_0004_m_000170_325 : duration 0:00.003s
[2021-05-17 11:11:10,854] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409214096206743951669258_0004_m_000167_322: needsTaskCommit() Task attempt_202105171409214096206743951669258_0004_m_000167_322
[2021-05-17 11:11:10,855] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Task committer attempt_202105171409214096206743951669258_0004_m_000167_322: needsTaskCommit() Task attempt_202105171409214096206743951669258_0004_m_000167_322: duration 0:00.001s
21/05/17 14:11:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214096206743951669258_0004_m_000167_322
[2021-05-17 11:11:10,857] {docker.py:276} INFO - 21/05/17 14:11:10 INFO Executor: Finished task 167.0 in stage 4.0 (TID 322). 4544 bytes result sent to driver
[2021-05-17 11:11:10,860] {docker.py:276} INFO - 21/05/17 14:11:10 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 326) (d733d8da4350, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:10,860] {docker.py:276} INFO - 21/05/17 14:11:10 INFO Executor: Running task 171.0 in stage 4.0 (TID 326)
[2021-05-17 11:11:10,861] {docker.py:276} INFO - 21/05/17 14:11:10 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 322) in 2458 ms on d733d8da4350 (executor driver) (168/200)
[2021-05-17 11:11:10,870] {docker.py:276} INFO - 21/05/17 14:11:10 INFO ShuffleBlockFetcherIterator: Getting 4 (23.5 KiB) non-empty blocks including 4 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:10,872] {docker.py:276} INFO - 21/05/17 14:11:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211021916470242555164_0004_m_000171_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211021916470242555164_0004_m_000171_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211021916470242555164_0004}; taskId=attempt_202105171409211021916470242555164_0004_m_000171_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@596000af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:10,872] {docker.py:276} INFO - 21/05/17 14:11:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:10 INFO StagingCommitter: Starting: Task committer attempt_202105171409211021916470242555164_0004_m_000171_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211021916470242555164_0004_m_000171_326
[2021-05-17 11:11:10,875] {docker.py:276} INFO - 21/05/17 14:11:10 INFO StagingCommitter: Task committer attempt_202105171409211021916470242555164_0004_m_000171_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211021916470242555164_0004_m_000171_326 : duration 0:00.002s
[2021-05-17 11:11:12,052] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409212632833709489612646_0004_m_000168_323: needsTaskCommit() Task attempt_202105171409212632833709489612646_0004_m_000168_323
[2021-05-17 11:11:12,054] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Task committer attempt_202105171409212632833709489612646_0004_m_000168_323: needsTaskCommit() Task attempt_202105171409212632833709489612646_0004_m_000168_323: duration 0:00.002s
[2021-05-17 11:11:12,055] {docker.py:276} INFO - 21/05/17 14:11:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212632833709489612646_0004_m_000168_323
[2021-05-17 11:11:12,056] {docker.py:276} INFO - 21/05/17 14:11:12 INFO Executor: Finished task 168.0 in stage 4.0 (TID 323). 4544 bytes result sent to driver
[2021-05-17 11:11:12,057] {docker.py:276} INFO - 21/05/17 14:11:12 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 327) (d733d8da4350, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:12,058] {docker.py:276} INFO - 21/05/17 14:11:12 INFO Executor: Running task 172.0 in stage 4.0 (TID 327)
[2021-05-17 11:11:12,059] {docker.py:276} INFO - 21/05/17 14:11:12 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 323) in 2314 ms on d733d8da4350 (executor driver) (169/200)
[2021-05-17 11:11:12,068] {docker.py:276} INFO - 21/05/17 14:11:12 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:12,070] {docker.py:276} INFO - 21/05/17 14:11:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409218485246650958725063_0004_m_000172_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218485246650958725063_0004_m_000172_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409218485246650958725063_0004}; taskId=attempt_202105171409218485246650958725063_0004_m_000172_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c02f50c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:12,070] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409218485246650958725063_0004_m_000172_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218485246650958725063_0004_m_000172_327
[2021-05-17 11:11:12,073] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Task committer attempt_202105171409218485246650958725063_0004_m_000172_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409218485246650958725063_0004_m_000172_327 : duration 0:00.003s
[2021-05-17 11:11:12,146] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409213851246895935263593_0004_m_000169_324: needsTaskCommit() Task attempt_202105171409213851246895935263593_0004_m_000169_324
[2021-05-17 11:11:12,147] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Task committer attempt_202105171409213851246895935263593_0004_m_000169_324: needsTaskCommit() Task attempt_202105171409213851246895935263593_0004_m_000169_324: duration 0:00.000s
[2021-05-17 11:11:12,148] {docker.py:276} INFO - 21/05/17 14:11:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213851246895935263593_0004_m_000169_324
[2021-05-17 11:11:12,148] {docker.py:276} INFO - 21/05/17 14:11:12 INFO Executor: Finished task 169.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-17 11:11:12,150] {docker.py:276} INFO - 21/05/17 14:11:12 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 328) (d733d8da4350, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:12,151] {docker.py:276} INFO - 21/05/17 14:11:12 INFO Executor: Running task 173.0 in stage 4.0 (TID 328)
[2021-05-17 11:11:12,152] {docker.py:276} INFO - 21/05/17 14:11:12 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 324) in 2286 ms on d733d8da4350 (executor driver) (170/200)
[2021-05-17 11:11:12,161] {docker.py:276} INFO - 21/05/17 14:11:12 INFO ShuffleBlockFetcherIterator: Getting 4 (20.8 KiB) non-empty blocks including 4 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:12,163] {docker.py:276} INFO - 21/05/17 14:11:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215832540333428710705_0004_m_000173_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215832540333428710705_0004_m_000173_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215832540333428710705_0004}; taskId=attempt_202105171409215832540333428710705_0004_m_000173_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e373095}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:12,163] {docker.py:276} INFO - 21/05/17 14:11:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409215832540333428710705_0004_m_000173_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215832540333428710705_0004_m_000173_328
[2021-05-17 11:11:12,167] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Task committer attempt_202105171409215832540333428710705_0004_m_000173_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215832540333428710705_0004_m_000173_328 : duration 0:00.004s
[2021-05-17 11:11:12,623] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409213464789324710375209_0004_m_000170_325: needsTaskCommit() Task attempt_202105171409213464789324710375209_0004_m_000170_325
21/05/17 14:11:12 INFO StagingCommitter: Task committer attempt_202105171409213464789324710375209_0004_m_000170_325: needsTaskCommit() Task attempt_202105171409213464789324710375209_0004_m_000170_325: duration 0:00.001s
21/05/17 14:11:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213464789324710375209_0004_m_000170_325
[2021-05-17 11:11:12,624] {docker.py:276} INFO - 21/05/17 14:11:12 INFO Executor: Finished task 170.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
[2021-05-17 11:11:12,626] {docker.py:276} INFO - 21/05/17 14:11:12 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 329) (d733d8da4350, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:12,626] {docker.py:276} INFO - 21/05/17 14:11:12 INFO Executor: Running task 174.0 in stage 4.0 (TID 329)
21/05/17 14:11:12 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 325) in 2509 ms on d733d8da4350 (executor driver) (171/200)
[2021-05-17 11:11:12,636] {docker.py:276} INFO - 21/05/17 14:11:12 INFO ShuffleBlockFetcherIterator: Getting 4 (22.9 KiB) non-empty blocks including 4 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:12,638] {docker.py:276} INFO - 21/05/17 14:11:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214930013768729774461_0004_m_000174_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214930013768729774461_0004_m_000174_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214930013768729774461_0004}; taskId=attempt_202105171409214930013768729774461_0004_m_000174_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c188b25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:12 INFO StagingCommitter: Starting: Task committer attempt_202105171409214930013768729774461_0004_m_000174_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214930013768729774461_0004_m_000174_329
[2021-05-17 11:11:12,643] {docker.py:276} INFO - 21/05/17 14:11:12 INFO StagingCommitter: Task committer attempt_202105171409214930013768729774461_0004_m_000174_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214930013768729774461_0004_m_000174_329 : duration 0:00.004s
[2021-05-17 11:11:13,140] {docker.py:276} INFO - 21/05/17 14:11:13 INFO StagingCommitter: Starting: Task committer attempt_202105171409211021916470242555164_0004_m_000171_326: needsTaskCommit() Task attempt_202105171409211021916470242555164_0004_m_000171_326
[2021-05-17 11:11:13,141] {docker.py:276} INFO - 21/05/17 14:11:13 INFO StagingCommitter: Task committer attempt_202105171409211021916470242555164_0004_m_000171_326: needsTaskCommit() Task attempt_202105171409211021916470242555164_0004_m_000171_326: duration 0:00.001s
21/05/17 14:11:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211021916470242555164_0004_m_000171_326
[2021-05-17 11:11:13,143] {docker.py:276} INFO - 21/05/17 14:11:13 INFO Executor: Finished task 171.0 in stage 4.0 (TID 326). 4544 bytes result sent to driver
[2021-05-17 11:11:13,145] {docker.py:276} INFO - 21/05/17 14:11:13 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 330) (d733d8da4350, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:13,145] {docker.py:276} INFO - 21/05/17 14:11:13 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 326) in 2290 ms on d733d8da4350 (executor driver) (172/200)
[2021-05-17 11:11:13,146] {docker.py:276} INFO - 21/05/17 14:11:13 INFO Executor: Running task 175.0 in stage 4.0 (TID 330)
[2021-05-17 11:11:13,155] {docker.py:276} INFO - 21/05/17 14:11:13 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:13,156] {docker.py:276} INFO - 21/05/17 14:11:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921295024567361886276_0004_m_000175_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921295024567361886276_0004_m_000175_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921295024567361886276_0004}; taskId=attempt_20210517140921295024567361886276_0004_m_000175_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4224e956}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:13,157] {docker.py:276} INFO - 21/05/17 14:11:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:13,157] {docker.py:276} INFO - 21/05/17 14:11:13 INFO StagingCommitter: Starting: Task committer attempt_20210517140921295024567361886276_0004_m_000175_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921295024567361886276_0004_m_000175_330
[2021-05-17 11:11:13,160] {docker.py:276} INFO - 21/05/17 14:11:13 INFO StagingCommitter: Task committer attempt_20210517140921295024567361886276_0004_m_000175_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921295024567361886276_0004_m_000175_330 : duration 0:00.003s
[2021-05-17 11:11:14,391] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409218485246650958725063_0004_m_000172_327: needsTaskCommit() Task attempt_202105171409218485246650958725063_0004_m_000172_327
21/05/17 14:11:14 INFO StagingCommitter: Task committer attempt_202105171409218485246650958725063_0004_m_000172_327: needsTaskCommit() Task attempt_202105171409218485246650958725063_0004_m_000172_327: duration 0:00.000s
[2021-05-17 11:11:14,392] {docker.py:276} INFO - 21/05/17 14:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409218485246650958725063_0004_m_000172_327
[2021-05-17 11:11:14,393] {docker.py:276} INFO - 21/05/17 14:11:14 INFO Executor: Finished task 172.0 in stage 4.0 (TID 327). 4544 bytes result sent to driver
[2021-05-17 11:11:14,394] {docker.py:276} INFO - 21/05/17 14:11:14 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 331) (d733d8da4350, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:14,395] {docker.py:276} INFO - 21/05/17 14:11:14 INFO Executor: Running task 176.0 in stage 4.0 (TID 331)
21/05/17 14:11:14 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 327) in 2341 ms on d733d8da4350 (executor driver) (173/200)
[2021-05-17 11:11:14,408] {docker.py:276} INFO - 21/05/17 14:11:14 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:14,410] {docker.py:276} INFO - 21/05/17 14:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216831121721909788076_0004_m_000176_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216831121721909788076_0004_m_000176_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216831121721909788076_0004}; taskId=attempt_202105171409216831121721909788076_0004_m_000176_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1eab57aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:14,411] {docker.py:276} INFO - 21/05/17 14:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409216831121721909788076_0004_m_000176_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216831121721909788076_0004_m_000176_331
[2021-05-17 11:11:14,415] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Task committer attempt_202105171409216831121721909788076_0004_m_000176_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216831121721909788076_0004_m_000176_331 : duration 0:00.005s
[2021-05-17 11:11:14,577] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409214930013768729774461_0004_m_000174_329: needsTaskCommit() Task attempt_202105171409214930013768729774461_0004_m_000174_329
[2021-05-17 11:11:14,579] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Task committer attempt_202105171409214930013768729774461_0004_m_000174_329: needsTaskCommit() Task attempt_202105171409214930013768729774461_0004_m_000174_329: duration 0:00.001s
[2021-05-17 11:11:14,580] {docker.py:276} INFO - 21/05/17 14:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214930013768729774461_0004_m_000174_329
21/05/17 14:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409215832540333428710705_0004_m_000173_328: needsTaskCommit() Task attempt_202105171409215832540333428710705_0004_m_000173_328
[2021-05-17 11:11:14,580] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Task committer attempt_202105171409215832540333428710705_0004_m_000173_328: needsTaskCommit() Task attempt_202105171409215832540333428710705_0004_m_000173_328: duration 0:00.001s
21/05/17 14:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215832540333428710705_0004_m_000173_328
[2021-05-17 11:11:14,582] {docker.py:276} INFO - 21/05/17 14:11:14 INFO Executor: Finished task 173.0 in stage 4.0 (TID 328). 4544 bytes result sent to driver
[2021-05-17 11:11:14,584] {docker.py:276} INFO - 21/05/17 14:11:14 INFO Executor: Finished task 174.0 in stage 4.0 (TID 329). 4587 bytes result sent to driver
[2021-05-17 11:11:14,585] {docker.py:276} INFO - 21/05/17 14:11:14 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 332) (d733d8da4350, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:14,587] {docker.py:276} INFO - 21/05/17 14:11:14 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 333) (d733d8da4350, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:14,588] {docker.py:276} INFO - 21/05/17 14:11:14 INFO Executor: Running task 177.0 in stage 4.0 (TID 332)
[2021-05-17 11:11:14,588] {docker.py:276} INFO - 21/05/17 14:11:14 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 328) in 2442 ms on d733d8da4350 (executor driver) (174/200)
[2021-05-17 11:11:14,589] {docker.py:276} INFO - 21/05/17 14:11:14 INFO Executor: Running task 178.0 in stage 4.0 (TID 333)
[2021-05-17 11:11:14,590] {docker.py:276} INFO - 21/05/17 14:11:14 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 329) in 1967 ms on d733d8da4350 (executor driver) (175/200)
[2021-05-17 11:11:14,599] {docker.py:276} INFO - 21/05/17 14:11:14 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:14,600] {docker.py:276} INFO - 21/05/17 14:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:14,600] {docker.py:276} INFO - 21/05/17 14:11:14 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:14,601] {docker.py:276} INFO - 21/05/17 14:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:14,602] {docker.py:276} INFO - 21/05/17 14:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:14,602] {docker.py:276} INFO - 21/05/17 14:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212538369056638728088_0004_m_000178_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212538369056638728088_0004_m_000178_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212538369056638728088_0004}; taskId=attempt_202105171409212538369056638728088_0004_m_000178_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45765c8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409212538369056638728088_0004_m_000178_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212538369056638728088_0004_m_000178_333
[2021-05-17 11:11:14,603] {docker.py:276} INFO - 21/05/17 14:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:14,604] {docker.py:276} INFO - 21/05/17 14:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:14,604] {docker.py:276} INFO - 21/05/17 14:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212210775168032727630_0004_m_000177_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212210775168032727630_0004_m_000177_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212210775168032727630_0004}; taskId=attempt_202105171409212210775168032727630_0004_m_000177_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35407617}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:14,604] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105171409212210775168032727630_0004_m_000177_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212210775168032727630_0004_m_000177_332
[2021-05-17 11:11:14,606] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Task committer attempt_202105171409212538369056638728088_0004_m_000178_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212538369056638728088_0004_m_000178_333 : duration 0:00.004s
[2021-05-17 11:11:14,612] {docker.py:276} INFO - 21/05/17 14:11:14 INFO StagingCommitter: Task committer attempt_202105171409212210775168032727630_0004_m_000177_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212210775168032727630_0004_m_000177_332 : duration 0:00.008s
[2021-05-17 11:11:15,472] {docker.py:276} INFO - 21/05/17 14:11:15 INFO StagingCommitter: Starting: Task committer attempt_20210517140921295024567361886276_0004_m_000175_330: needsTaskCommit() Task attempt_20210517140921295024567361886276_0004_m_000175_330
21/05/17 14:11:15 INFO StagingCommitter: Task committer attempt_20210517140921295024567361886276_0004_m_000175_330: needsTaskCommit() Task attempt_20210517140921295024567361886276_0004_m_000175_330: duration 0:00.001s
[2021-05-17 11:11:15,473] {docker.py:276} INFO - 21/05/17 14:11:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921295024567361886276_0004_m_000175_330
[2021-05-17 11:11:15,475] {docker.py:276} INFO - 21/05/17 14:11:15 INFO Executor: Finished task 175.0 in stage 4.0 (TID 330). 4544 bytes result sent to driver
[2021-05-17 11:11:15,476] {docker.py:276} INFO - 21/05/17 14:11:15 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 334) (d733d8da4350, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:15,477] {docker.py:276} INFO - 21/05/17 14:11:15 INFO Executor: Running task 179.0 in stage 4.0 (TID 334)
[2021-05-17 11:11:15,478] {docker.py:276} INFO - 21/05/17 14:11:15 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 330) in 2336 ms on d733d8da4350 (executor driver) (176/200)
[2021-05-17 11:11:15,498] {docker.py:276} INFO - 21/05/17 14:11:15 INFO ShuffleBlockFetcherIterator: Getting 4 (21.9 KiB) non-empty blocks including 4 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:15,498] {docker.py:276} INFO - 21/05/17 14:11:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:15,499] {docker.py:276} INFO - 21/05/17 14:11:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:11:15,500] {docker.py:276} INFO - 21/05/17 14:11:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:15,500] {docker.py:276} INFO - 21/05/17 14:11:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:15,500] {docker.py:276} INFO - 21/05/17 14:11:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212958177084391221719_0004_m_000179_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212958177084391221719_0004_m_000179_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212958177084391221719_0004}; taskId=attempt_202105171409212958177084391221719_0004_m_000179_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5138e48}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:15,501] {docker.py:276} INFO - 21/05/17 14:11:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:15 INFO StagingCommitter: Starting: Task committer attempt_202105171409212958177084391221719_0004_m_000179_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212958177084391221719_0004_m_000179_334
[2021-05-17 11:11:15,503] {docker.py:276} INFO - 21/05/17 14:11:15 INFO StagingCommitter: Task committer attempt_202105171409212958177084391221719_0004_m_000179_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212958177084391221719_0004_m_000179_334 : duration 0:00.003s
[2021-05-17 11:11:16,761] {docker.py:276} INFO - 21/05/17 14:11:16 INFO StagingCommitter: Starting: Task committer attempt_202105171409212210775168032727630_0004_m_000177_332: needsTaskCommit() Task attempt_202105171409212210775168032727630_0004_m_000177_332
[2021-05-17 11:11:16,762] {docker.py:276} INFO - 21/05/17 14:11:16 INFO StagingCommitter: Task committer attempt_202105171409212210775168032727630_0004_m_000177_332: needsTaskCommit() Task attempt_202105171409212210775168032727630_0004_m_000177_332: duration 0:00.002s
21/05/17 14:11:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212210775168032727630_0004_m_000177_332
[2021-05-17 11:11:16,763] {docker.py:276} INFO - 21/05/17 14:11:16 INFO Executor: Finished task 177.0 in stage 4.0 (TID 332). 4587 bytes result sent to driver
[2021-05-17 11:11:16,765] {docker.py:276} INFO - 21/05/17 14:11:16 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 335) (d733d8da4350, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:16,765] {docker.py:276} INFO - 21/05/17 14:11:16 INFO Executor: Running task 180.0 in stage 4.0 (TID 335)
[2021-05-17 11:11:16,766] {docker.py:276} INFO - 21/05/17 14:11:16 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 332) in 2183 ms on d733d8da4350 (executor driver) (177/200)
[2021-05-17 11:11:16,774] {docker.py:276} INFO - 21/05/17 14:11:16 INFO ShuffleBlockFetcherIterator: Getting 4 (22.0 KiB) non-empty blocks including 4 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:16,776] {docker.py:276} INFO - 21/05/17 14:11:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213977365804559741970_0004_m_000180_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213977365804559741970_0004_m_000180_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213977365804559741970_0004}; taskId=attempt_202105171409213977365804559741970_0004_m_000180_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15c39e51}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:16 INFO StagingCommitter: Starting: Task committer attempt_202105171409213977365804559741970_0004_m_000180_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213977365804559741970_0004_m_000180_335
[2021-05-17 11:11:16,779] {docker.py:276} INFO - 21/05/17 14:11:16 INFO StagingCommitter: Task committer attempt_202105171409213977365804559741970_0004_m_000180_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213977365804559741970_0004_m_000180_335 : duration 0:00.003s
[2021-05-17 11:11:16,962] {docker.py:276} INFO - 21/05/17 14:11:16 INFO StagingCommitter: Starting: Task committer attempt_202105171409212538369056638728088_0004_m_000178_333: needsTaskCommit() Task attempt_202105171409212538369056638728088_0004_m_000178_333
[2021-05-17 11:11:16,963] {docker.py:276} INFO - 21/05/17 14:11:16 INFO StagingCommitter: Task committer attempt_202105171409212538369056638728088_0004_m_000178_333: needsTaskCommit() Task attempt_202105171409212538369056638728088_0004_m_000178_333: duration 0:00.001s
[2021-05-17 11:11:16,964] {docker.py:276} INFO - 21/05/17 14:11:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212538369056638728088_0004_m_000178_333
[2021-05-17 11:11:16,965] {docker.py:276} INFO - 21/05/17 14:11:16 INFO Executor: Finished task 178.0 in stage 4.0 (TID 333). 4587 bytes result sent to driver
[2021-05-17 11:11:16,967] {docker.py:276} INFO - 21/05/17 14:11:16 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 336) (d733d8da4350, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:16,968] {docker.py:276} INFO - 21/05/17 14:11:16 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 333) in 2383 ms on d733d8da4350 (executor driver) (178/200)
[2021-05-17 11:11:16,969] {docker.py:276} INFO - 21/05/17 14:11:16 INFO Executor: Running task 181.0 in stage 4.0 (TID 336)
[2021-05-17 11:11:16,978] {docker.py:276} INFO - 21/05/17 14:11:17 INFO ShuffleBlockFetcherIterator: Getting 4 (20.4 KiB) non-empty blocks including 4 (20.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:16,980] {docker.py:276} INFO - 21/05/17 14:11:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:16,980] {docker.py:276} INFO - 21/05/17 14:11:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212081674244748515772_0004_m_000181_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212081674244748515772_0004_m_000181_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212081674244748515772_0004}; taskId=attempt_202105171409212081674244748515772_0004_m_000181_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5476a46d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409212081674244748515772_0004_m_000181_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212081674244748515772_0004_m_000181_336
[2021-05-17 11:11:16,983] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Task committer attempt_202105171409212081674244748515772_0004_m_000181_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212081674244748515772_0004_m_000181_336 : duration 0:00.003s
[2021-05-17 11:11:17,351] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409212958177084391221719_0004_m_000179_334: needsTaskCommit() Task attempt_202105171409212958177084391221719_0004_m_000179_334
[2021-05-17 11:11:17,352] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Task committer attempt_202105171409212958177084391221719_0004_m_000179_334: needsTaskCommit() Task attempt_202105171409212958177084391221719_0004_m_000179_334: duration 0:00.000s
21/05/17 14:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212958177084391221719_0004_m_000179_334
[2021-05-17 11:11:17,353] {docker.py:276} INFO - 21/05/17 14:11:17 INFO Executor: Finished task 179.0 in stage 4.0 (TID 334). 4587 bytes result sent to driver
[2021-05-17 11:11:17,354] {docker.py:276} INFO - 21/05/17 14:11:17 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 337) (d733d8da4350, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:17,354] {docker.py:276} INFO - 21/05/17 14:11:17 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 334) in 1881 ms on d733d8da4350 (executor driver) (179/200)
[2021-05-17 11:11:17,354] {docker.py:276} INFO - 21/05/17 14:11:17 INFO Executor: Running task 182.0 in stage 4.0 (TID 337)
[2021-05-17 11:11:17,361] {docker.py:276} INFO - 21/05/17 14:11:17 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:17,363] {docker.py:276} INFO - 21/05/17 14:11:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:17,363] {docker.py:276} INFO - 21/05/17 14:11:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215101126954005847985_0004_m_000182_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215101126954005847985_0004_m_000182_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215101126954005847985_0004}; taskId=attempt_202105171409215101126954005847985_0004_m_000182_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7357e95d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:17,363] {docker.py:276} INFO - 21/05/17 14:11:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409215101126954005847985_0004_m_000182_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215101126954005847985_0004_m_000182_337
[2021-05-17 11:11:17,367] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Task committer attempt_202105171409215101126954005847985_0004_m_000182_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215101126954005847985_0004_m_000182_337 : duration 0:00.004s
[2021-05-17 11:11:17,494] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105171409216831121721909788076_0004_m_000176_331: needsTaskCommit() Task attempt_202105171409216831121721909788076_0004_m_000176_331
[2021-05-17 11:11:17,496] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Task committer attempt_202105171409216831121721909788076_0004_m_000176_331: needsTaskCommit() Task attempt_202105171409216831121721909788076_0004_m_000176_331: duration 0:00.000s
21/05/17 14:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216831121721909788076_0004_m_000176_331
[2021-05-17 11:11:17,497] {docker.py:276} INFO - 21/05/17 14:11:17 INFO Executor: Finished task 176.0 in stage 4.0 (TID 331). 4587 bytes result sent to driver
[2021-05-17 11:11:17,500] {docker.py:276} INFO - 21/05/17 14:11:17 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 338) (d733d8da4350, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:17,501] {docker.py:276} INFO - 21/05/17 14:11:17 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 331) in 3111 ms on d733d8da4350 (executor driver) (180/200)
[2021-05-17 11:11:17,502] {docker.py:276} INFO - 21/05/17 14:11:17 INFO Executor: Running task 183.0 in stage 4.0 (TID 338)
[2021-05-17 11:11:17,510] {docker.py:276} INFO - 21/05/17 14:11:17 INFO ShuffleBlockFetcherIterator: Getting 4 (20.8 KiB) non-empty blocks including 4 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:17,513] {docker.py:276} INFO - 21/05/17 14:11:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921297153046839624478_0004_m_000183_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921297153046839624478_0004_m_000183_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921297153046839624478_0004}; taskId=attempt_20210517140921297153046839624478_0004_m_000183_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bf3128b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:17 INFO StagingCommitter: Starting: Task committer attempt_20210517140921297153046839624478_0004_m_000183_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921297153046839624478_0004_m_000183_338
[2021-05-17 11:11:17,516] {docker.py:276} INFO - 21/05/17 14:11:17 INFO StagingCommitter: Task committer attempt_20210517140921297153046839624478_0004_m_000183_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921297153046839624478_0004_m_000183_338 : duration 0:00.003s
[2021-05-17 11:11:19,018] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409213977365804559741970_0004_m_000180_335: needsTaskCommit() Task attempt_202105171409213977365804559741970_0004_m_000180_335
21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409213977365804559741970_0004_m_000180_335: needsTaskCommit() Task attempt_202105171409213977365804559741970_0004_m_000180_335: duration 0:00.001s
[2021-05-17 11:11:19,019] {docker.py:276} INFO - 21/05/17 14:11:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213977365804559741970_0004_m_000180_335
[2021-05-17 11:11:19,020] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Finished task 180.0 in stage 4.0 (TID 335). 4544 bytes result sent to driver
[2021-05-17 11:11:19,021] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 339) (d733d8da4350, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:19,022] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Running task 184.0 in stage 4.0 (TID 339)
[2021-05-17 11:11:19,023] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 335) in 2261 ms on d733d8da4350 (executor driver) (181/200)
[2021-05-17 11:11:19,033] {docker.py:276} INFO - 21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Getting 4 (22.5 KiB) non-empty blocks including 4 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:19,034] {docker.py:276} INFO - 21/05/17 14:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:19,034] {docker.py:276} INFO - 21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215311131798217505615_0004_m_000184_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215311131798217505615_0004_m_000184_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215311131798217505615_0004}; taskId=attempt_202105171409215311131798217505615_0004_m_000184_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ea29706}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409215311131798217505615_0004_m_000184_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215311131798217505615_0004_m_000184_339
[2021-05-17 11:11:19,040] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409215311131798217505615_0004_m_000184_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215311131798217505615_0004_m_000184_339 : duration 0:00.005s
[2021-05-17 11:11:19,386] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409215101126954005847985_0004_m_000182_337: needsTaskCommit() Task attempt_202105171409215101126954005847985_0004_m_000182_337
21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409215101126954005847985_0004_m_000182_337: needsTaskCommit() Task attempt_202105171409215101126954005847985_0004_m_000182_337: duration 0:00.000s
21/05/17 14:11:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215101126954005847985_0004_m_000182_337
[2021-05-17 11:11:19,387] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Finished task 182.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
[2021-05-17 11:11:19,389] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 340) (d733d8da4350, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:19,390] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Running task 185.0 in stage 4.0 (TID 340)
[2021-05-17 11:11:19,390] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 337) in 2039 ms on d733d8da4350 (executor driver) (182/200)
[2021-05-17 11:11:19,399] {docker.py:276} INFO - 21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Getting 4 (23.5 KiB) non-empty blocks including 4 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:19,400] {docker.py:276} INFO - 21/05/17 14:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214129443281835030215_0004_m_000185_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214129443281835030215_0004_m_000185_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214129443281835030215_0004}; taskId=attempt_202105171409214129443281835030215_0004_m_000185_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12e72e38}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:19,400] {docker.py:276} INFO - 21/05/17 14:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409214129443281835030215_0004_m_000185_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214129443281835030215_0004_m_000185_340
[2021-05-17 11:11:19,402] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409214129443281835030215_0004_m_000185_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214129443281835030215_0004_m_000185_340 : duration 0:00.002s
[2021-05-17 11:11:19,413] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409212081674244748515772_0004_m_000181_336: needsTaskCommit() Task attempt_202105171409212081674244748515772_0004_m_000181_336
[2021-05-17 11:11:19,413] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409212081674244748515772_0004_m_000181_336: needsTaskCommit() Task attempt_202105171409212081674244748515772_0004_m_000181_336: duration 0:00.001s
[2021-05-17 11:11:19,414] {docker.py:276} INFO - 21/05/17 14:11:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212081674244748515772_0004_m_000181_336
[2021-05-17 11:11:19,415] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Finished task 181.0 in stage 4.0 (TID 336). 4544 bytes result sent to driver
[2021-05-17 11:11:19,416] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 341) (d733d8da4350, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:19,417] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 336) in 2453 ms on d733d8da4350 (executor driver) (183/200)
[2021-05-17 11:11:19,418] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Running task 186.0 in stage 4.0 (TID 341)
[2021-05-17 11:11:19,424] {docker.py:276} INFO - 21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Getting 4 (23.5 KiB) non-empty blocks including 4 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:19,426] {docker.py:276} INFO - 21/05/17 14:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:19,427] {docker.py:276} INFO - 21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217127772036350580416_0004_m_000186_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217127772036350580416_0004_m_000186_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217127772036350580416_0004}; taskId=attempt_202105171409217127772036350580416_0004_m_000186_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3102f178}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:19,427] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409217127772036350580416_0004_m_000186_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217127772036350580416_0004_m_000186_341
[2021-05-17 11:11:19,430] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409217127772036350580416_0004_m_000186_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217127772036350580416_0004_m_000186_341 : duration 0:00.003s
[2021-05-17 11:11:19,802] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_20210517140921297153046839624478_0004_m_000183_338: needsTaskCommit() Task attempt_20210517140921297153046839624478_0004_m_000183_338
[2021-05-17 11:11:19,803] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_20210517140921297153046839624478_0004_m_000183_338: needsTaskCommit() Task attempt_20210517140921297153046839624478_0004_m_000183_338: duration 0:00.001s
21/05/17 14:11:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921297153046839624478_0004_m_000183_338
[2021-05-17 11:11:19,805] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Finished task 183.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-17 11:11:19,805] {docker.py:276} INFO - 21/05/17 14:11:19 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 342) (d733d8da4350, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:19,807] {docker.py:276} INFO - 21/05/17 14:11:19 INFO Executor: Running task 187.0 in stage 4.0 (TID 342)
21/05/17 14:11:19 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 338) in 2309 ms on d733d8da4350 (executor driver) (184/200)
[2021-05-17 11:11:19,816] {docker.py:276} INFO - 21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Getting 4 (21.4 KiB) non-empty blocks including 4 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:19,818] {docker.py:276} INFO - 21/05/17 14:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409217742877299782200481_0004_m_000187_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217742877299782200481_0004_m_000187_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409217742877299782200481_0004}; taskId=attempt_202105171409217742877299782200481_0004_m_000187_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35941f1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:19,818] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Starting: Task committer attempt_202105171409217742877299782200481_0004_m_000187_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217742877299782200481_0004_m_000187_342
[2021-05-17 11:11:19,821] {docker.py:276} INFO - 21/05/17 14:11:19 INFO StagingCommitter: Task committer attempt_202105171409217742877299782200481_0004_m_000187_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409217742877299782200481_0004_m_000187_342 : duration 0:00.003s
[2021-05-17 11:11:21,321] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409215311131798217505615_0004_m_000184_339: needsTaskCommit() Task attempt_202105171409215311131798217505615_0004_m_000184_339
[2021-05-17 11:11:21,323] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Task committer attempt_202105171409215311131798217505615_0004_m_000184_339: needsTaskCommit() Task attempt_202105171409215311131798217505615_0004_m_000184_339: duration 0:00.001s
21/05/17 14:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215311131798217505615_0004_m_000184_339
[2021-05-17 11:11:21,324] {docker.py:276} INFO - 21/05/17 14:11:21 INFO Executor: Finished task 184.0 in stage 4.0 (TID 339). 4544 bytes result sent to driver
[2021-05-17 11:11:21,325] {docker.py:276} INFO - 21/05/17 14:11:21 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 343) (d733d8da4350, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:21,326] {docker.py:276} INFO - 21/05/17 14:11:21 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 339) in 2308 ms on d733d8da4350 (executor driver) (185/200)
21/05/17 14:11:21 INFO Executor: Running task 188.0 in stage 4.0 (TID 343)
[2021-05-17 11:11:21,336] {docker.py:276} INFO - 21/05/17 14:11:21 INFO ShuffleBlockFetcherIterator: Getting 4 (22.3 KiB) non-empty blocks including 4 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:21,338] {docker.py:276} INFO - 21/05/17 14:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:21,338] {docker.py:276} INFO - 21/05/17 14:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:21,339] {docker.py:276} INFO - 21/05/17 14:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212019419335294221183_0004_m_000188_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212019419335294221183_0004_m_000188_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212019419335294221183_0004}; taskId=attempt_202105171409212019419335294221183_0004_m_000188_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@304d2778}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:21,339] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409212019419335294221183_0004_m_000188_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212019419335294221183_0004_m_000188_343
[2021-05-17 11:11:21,342] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Task committer attempt_202105171409212019419335294221183_0004_m_000188_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212019419335294221183_0004_m_000188_343 : duration 0:00.004s
[2021-05-17 11:11:21,714] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409214129443281835030215_0004_m_000185_340: needsTaskCommit() Task attempt_202105171409214129443281835030215_0004_m_000185_340
[2021-05-17 11:11:21,715] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Task committer attempt_202105171409214129443281835030215_0004_m_000185_340: needsTaskCommit() Task attempt_202105171409214129443281835030215_0004_m_000185_340: duration 0:00.001s
21/05/17 14:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214129443281835030215_0004_m_000185_340
[2021-05-17 11:11:21,716] {docker.py:276} INFO - 21/05/17 14:11:21 INFO Executor: Finished task 185.0 in stage 4.0 (TID 340). 4544 bytes result sent to driver
[2021-05-17 11:11:21,718] {docker.py:276} INFO - 21/05/17 14:11:21 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 344) (d733d8da4350, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:21,719] {docker.py:276} INFO - 21/05/17 14:11:21 INFO Executor: Running task 189.0 in stage 4.0 (TID 344)
[2021-05-17 11:11:21,720] {docker.py:276} INFO - 21/05/17 14:11:21 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 340) in 2333 ms on d733d8da4350 (executor driver) (186/200)
[2021-05-17 11:11:21,729] {docker.py:276} INFO - 21/05/17 14:11:21 INFO ShuffleBlockFetcherIterator: Getting 4 (22.4 KiB) non-empty blocks including 4 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:21,730] {docker.py:276} INFO - 21/05/17 14:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:21,731] {docker.py:276} INFO - 21/05/17 14:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:21,732] {docker.py:276} INFO - 21/05/17 14:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:21,732] {docker.py:276} INFO - 21/05/17 14:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214297373945771646637_0004_m_000189_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214297373945771646637_0004_m_000189_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214297373945771646637_0004}; taskId=attempt_202105171409214297373945771646637_0004_m_000189_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ce2fb61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:21,733] {docker.py:276} INFO - 21/05/17 14:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:21,733] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409214297373945771646637_0004_m_000189_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214297373945771646637_0004_m_000189_344
[2021-05-17 11:11:21,736] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Task committer attempt_202105171409214297373945771646637_0004_m_000189_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214297373945771646637_0004_m_000189_344 : duration 0:00.004s
[2021-05-17 11:11:21,777] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409217127772036350580416_0004_m_000186_341: needsTaskCommit() Task attempt_202105171409217127772036350580416_0004_m_000186_341
[2021-05-17 11:11:21,778] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Task committer attempt_202105171409217127772036350580416_0004_m_000186_341: needsTaskCommit() Task attempt_202105171409217127772036350580416_0004_m_000186_341: duration 0:00.000s
[2021-05-17 11:11:21,778] {docker.py:276} INFO - 21/05/17 14:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217127772036350580416_0004_m_000186_341
[2021-05-17 11:11:21,779] {docker.py:276} INFO - 21/05/17 14:11:21 INFO Executor: Finished task 186.0 in stage 4.0 (TID 341). 4544 bytes result sent to driver
[2021-05-17 11:11:21,780] {docker.py:276} INFO - 21/05/17 14:11:21 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 345) (d733d8da4350, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:21,781] {docker.py:276} INFO - 21/05/17 14:11:21 INFO Executor: Running task 190.0 in stage 4.0 (TID 345)
[2021-05-17 11:11:21,782] {docker.py:276} INFO - 21/05/17 14:11:21 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 341) in 2369 ms on d733d8da4350 (executor driver) (187/200)
[2021-05-17 11:11:21,790] {docker.py:276} INFO - 21/05/17 14:11:21 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:21,793] {docker.py:276} INFO - 21/05/17 14:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:21,793] {docker.py:276} INFO - 21/05/17 14:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409211554370960090682519_0004_m_000190_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211554370960090682519_0004_m_000190_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409211554370960090682519_0004}; taskId=attempt_202105171409211554370960090682519_0004_m_000190_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ddb8a84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:21,793] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105171409211554370960090682519_0004_m_000190_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211554370960090682519_0004_m_000190_345
[2021-05-17 11:11:21,795] {docker.py:276} INFO - 21/05/17 14:11:21 INFO StagingCommitter: Task committer attempt_202105171409211554370960090682519_0004_m_000190_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409211554370960090682519_0004_m_000190_345 : duration 0:00.003s
[2021-05-17 11:11:22,290] {docker.py:276} INFO - 21/05/17 14:11:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409217742877299782200481_0004_m_000187_342: needsTaskCommit() Task attempt_202105171409217742877299782200481_0004_m_000187_342
[2021-05-17 11:11:22,290] {docker.py:276} INFO - 21/05/17 14:11:22 INFO StagingCommitter: Task committer attempt_202105171409217742877299782200481_0004_m_000187_342: needsTaskCommit() Task attempt_202105171409217742877299782200481_0004_m_000187_342: duration 0:00.001s
[2021-05-17 11:11:22,291] {docker.py:276} INFO - 21/05/17 14:11:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409217742877299782200481_0004_m_000187_342
[2021-05-17 11:11:22,291] {docker.py:276} INFO - 21/05/17 14:11:22 INFO Executor: Finished task 187.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-17 11:11:22,292] {docker.py:276} INFO - 21/05/17 14:11:22 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 346) (d733d8da4350, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:22,294] {docker.py:276} INFO - 21/05/17 14:11:22 INFO Executor: Running task 191.0 in stage 4.0 (TID 346)
[2021-05-17 11:11:22,295] {docker.py:276} INFO - 21/05/17 14:11:22 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 342) in 2491 ms on d733d8da4350 (executor driver) (188/200)
[2021-05-17 11:11:22,302] {docker.py:276} INFO - 21/05/17 14:11:22 INFO ShuffleBlockFetcherIterator: Getting 4 (20.9 KiB) non-empty blocks including 4 (20.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:22,302] {docker.py:276} INFO - 21/05/17 14:11:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:22,304] {docker.py:276} INFO - 21/05/17 14:11:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:22,304] {docker.py:276} INFO - 21/05/17 14:11:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409216775453093017540846_0004_m_000191_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216775453093017540846_0004_m_000191_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409216775453093017540846_0004}; taskId=attempt_202105171409216775453093017540846_0004_m_000191_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44a8352d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:22,305] {docker.py:276} INFO - 21/05/17 14:11:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:22,305] {docker.py:276} INFO - 21/05/17 14:11:22 INFO StagingCommitter: Starting: Task committer attempt_202105171409216775453093017540846_0004_m_000191_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216775453093017540846_0004_m_000191_346
[2021-05-17 11:11:22,308] {docker.py:276} INFO - 21/05/17 14:11:22 INFO StagingCommitter: Task committer attempt_202105171409216775453093017540846_0004_m_000191_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409216775453093017540846_0004_m_000191_346 : duration 0:00.003s
[2021-05-17 11:11:23,617] {docker.py:276} INFO - 21/05/17 14:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105171409214297373945771646637_0004_m_000189_344: needsTaskCommit() Task attempt_202105171409214297373945771646637_0004_m_000189_344
[2021-05-17 11:11:23,617] {docker.py:276} INFO - 21/05/17 14:11:23 INFO StagingCommitter: Task committer attempt_202105171409214297373945771646637_0004_m_000189_344: needsTaskCommit() Task attempt_202105171409214297373945771646637_0004_m_000189_344: duration 0:00.001s
[2021-05-17 11:11:23,618] {docker.py:276} INFO - 21/05/17 14:11:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214297373945771646637_0004_m_000189_344
[2021-05-17 11:11:23,619] {docker.py:276} INFO - 21/05/17 14:11:23 INFO Executor: Finished task 189.0 in stage 4.0 (TID 344). 4544 bytes result sent to driver
[2021-05-17 11:11:23,620] {docker.py:276} INFO - 21/05/17 14:11:23 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 347) (d733d8da4350, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:23,621] {docker.py:276} INFO - 21/05/17 14:11:23 INFO Executor: Running task 192.0 in stage 4.0 (TID 347)
[2021-05-17 11:11:23,621] {docker.py:276} INFO - 21/05/17 14:11:23 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 344) in 1908 ms on d733d8da4350 (executor driver) (189/200)
[2021-05-17 11:11:23,630] {docker.py:276} INFO - 21/05/17 14:11:23 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:23,632] {docker.py:276} INFO - 21/05/17 14:11:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517140921414305636950465069_0004_m_000192_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921414305636950465069_0004_m_000192_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517140921414305636950465069_0004}; taskId=attempt_20210517140921414305636950465069_0004_m_000192_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12f934cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:23 INFO StagingCommitter: Starting: Task committer attempt_20210517140921414305636950465069_0004_m_000192_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921414305636950465069_0004_m_000192_347
[2021-05-17 11:11:23,636] {docker.py:276} INFO - 21/05/17 14:11:23 INFO StagingCommitter: Task committer attempt_20210517140921414305636950465069_0004_m_000192_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_20210517140921414305636950465069_0004_m_000192_347 : duration 0:00.004s
[2021-05-17 11:11:23,809] {docker.py:276} INFO - 21/05/17 14:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105171409212019419335294221183_0004_m_000188_343: needsTaskCommit() Task attempt_202105171409212019419335294221183_0004_m_000188_343
21/05/17 14:11:23 INFO StagingCommitter: Task committer attempt_202105171409212019419335294221183_0004_m_000188_343: needsTaskCommit() Task attempt_202105171409212019419335294221183_0004_m_000188_343: duration 0:00.001s
21/05/17 14:11:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212019419335294221183_0004_m_000188_343
[2021-05-17 11:11:23,811] {docker.py:276} INFO - 21/05/17 14:11:23 INFO Executor: Finished task 188.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-17 11:11:23,812] {docker.py:276} INFO - 21/05/17 14:11:23 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 348) (d733d8da4350, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:23,812] {docker.py:276} INFO - 21/05/17 14:11:23 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 343) in 2491 ms on d733d8da4350 (executor driver) (190/200)
[2021-05-17 11:11:23,813] {docker.py:276} INFO - 21/05/17 14:11:23 INFO Executor: Running task 193.0 in stage 4.0 (TID 348)
[2021-05-17 11:11:23,821] {docker.py:276} INFO - 21/05/17 14:11:23 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:23,823] {docker.py:276} INFO - 21/05/17 14:11:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215448041637229363251_0004_m_000193_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215448041637229363251_0004_m_000193_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215448041637229363251_0004}; taskId=attempt_202105171409215448041637229363251_0004_m_000193_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14c9a01f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:23,824] {docker.py:276} INFO - 21/05/17 14:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105171409215448041637229363251_0004_m_000193_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215448041637229363251_0004_m_000193_348
[2021-05-17 11:11:23,826] {docker.py:276} INFO - 21/05/17 14:11:23 INFO StagingCommitter: Task committer attempt_202105171409215448041637229363251_0004_m_000193_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215448041637229363251_0004_m_000193_348 : duration 0:00.003s
[2021-05-17 11:11:24,105] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409211554370960090682519_0004_m_000190_345: needsTaskCommit() Task attempt_202105171409211554370960090682519_0004_m_000190_345
21/05/17 14:11:24 INFO StagingCommitter: Task committer attempt_202105171409211554370960090682519_0004_m_000190_345: needsTaskCommit() Task attempt_202105171409211554370960090682519_0004_m_000190_345: duration 0:00.001s
21/05/17 14:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409211554370960090682519_0004_m_000190_345
[2021-05-17 11:11:24,108] {docker.py:276} INFO - 21/05/17 14:11:24 INFO Executor: Finished task 190.0 in stage 4.0 (TID 345). 4544 bytes result sent to driver
[2021-05-17 11:11:24,109] {docker.py:276} INFO - 21/05/17 14:11:24 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 349) (d733d8da4350, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:24,110] {docker.py:276} INFO - 21/05/17 14:11:24 INFO Executor: Running task 194.0 in stage 4.0 (TID 349)
[2021-05-17 11:11:24,110] {docker.py:276} INFO - 21/05/17 14:11:24 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 345) in 2333 ms on d733d8da4350 (executor driver) (191/200)
[2021-05-17 11:11:24,119] {docker.py:276} INFO - 21/05/17 14:11:24 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:24,121] {docker.py:276} INFO - 21/05/17 14:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:11:24,121] {docker.py:276} INFO - 21/05/17 14:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:24,122] {docker.py:276} INFO - 21/05/17 14:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:24,122] {docker.py:276} INFO - 21/05/17 14:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409213874216675690209752_0004_m_000194_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213874216675690209752_0004_m_000194_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409213874216675690209752_0004}; taskId=attempt_202105171409213874216675690209752_0004_m_000194_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49d200d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:24,122] {docker.py:276} INFO - 21/05/17 14:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:24,123] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409213874216675690209752_0004_m_000194_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213874216675690209752_0004_m_000194_349
[2021-05-17 11:11:24,126] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Task committer attempt_202105171409213874216675690209752_0004_m_000194_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409213874216675690209752_0004_m_000194_349 : duration 0:00.003s
[2021-05-17 11:11:24,564] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409216775453093017540846_0004_m_000191_346: needsTaskCommit() Task attempt_202105171409216775453093017540846_0004_m_000191_346
[2021-05-17 11:11:24,565] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Task committer attempt_202105171409216775453093017540846_0004_m_000191_346: needsTaskCommit() Task attempt_202105171409216775453093017540846_0004_m_000191_346: duration 0:00.001s
21/05/17 14:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409216775453093017540846_0004_m_000191_346
[2021-05-17 11:11:24,568] {docker.py:276} INFO - 21/05/17 14:11:24 INFO Executor: Finished task 191.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-17 11:11:24,569] {docker.py:276} INFO - 21/05/17 14:11:24 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 350) (d733d8da4350, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:24,570] {docker.py:276} INFO - 21/05/17 14:11:24 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 346) in 2280 ms on d733d8da4350 (executor driver) (192/200)
[2021-05-17 11:11:24,571] {docker.py:276} INFO - 21/05/17 14:11:24 INFO Executor: Running task 195.0 in stage 4.0 (TID 350)
[2021-05-17 11:11:24,580] {docker.py:276} INFO - 21/05/17 14:11:24 INFO ShuffleBlockFetcherIterator: Getting 4 (22.5 KiB) non-empty blocks including 4 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 11:11:24,581] {docker.py:276} INFO - 21/05/17 14:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:24,582] {docker.py:276} INFO - 21/05/17 14:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 11:11:24,583] {docker.py:276} INFO - 21/05/17 14:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 11:11:24,583] {docker.py:276} INFO - 21/05/17 14:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:24,584] {docker.py:276} INFO - 21/05/17 14:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215281977180201552710_0004_m_000195_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215281977180201552710_0004_m_000195_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215281977180201552710_0004}; taskId=attempt_202105171409215281977180201552710_0004_m_000195_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@331b092b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:24,584] {docker.py:276} INFO - 21/05/17 14:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:24,584] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105171409215281977180201552710_0004_m_000195_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215281977180201552710_0004_m_000195_350
[2021-05-17 11:11:24,587] {docker.py:276} INFO - 21/05/17 14:11:24 INFO StagingCommitter: Task committer attempt_202105171409215281977180201552710_0004_m_000195_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215281977180201552710_0004_m_000195_350 : duration 0:00.003s
[2021-05-17 11:11:25,543] {docker.py:276} INFO - 21/05/17 14:11:25 INFO StagingCommitter: Starting: Task committer attempt_20210517140921414305636950465069_0004_m_000192_347: needsTaskCommit() Task attempt_20210517140921414305636950465069_0004_m_000192_347
21/05/17 14:11:25 INFO StagingCommitter: Task committer attempt_20210517140921414305636950465069_0004_m_000192_347: needsTaskCommit() Task attempt_20210517140921414305636950465069_0004_m_000192_347: duration 0:00.001s
21/05/17 14:11:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517140921414305636950465069_0004_m_000192_347
[2021-05-17 11:11:25,544] {docker.py:276} INFO - 21/05/17 14:11:25 INFO Executor: Finished task 192.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-17 11:11:25,546] {docker.py:276} INFO - 21/05/17 14:11:25 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 351) (d733d8da4350, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:25,547] {docker.py:276} INFO - 21/05/17 14:11:25 INFO Executor: Running task 196.0 in stage 4.0 (TID 351)
[2021-05-17 11:11:25,547] {docker.py:276} INFO - 21/05/17 14:11:25 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 347) in 1929 ms on d733d8da4350 (executor driver) (193/200)
[2021-05-17 11:11:25,556] {docker.py:276} INFO - 21/05/17 14:11:25 INFO ShuffleBlockFetcherIterator: Getting 4 (21.8 KiB) non-empty blocks including 4 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:25,558] {docker.py:276} INFO - 21/05/17 14:11:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212473577212733074522_0004_m_000196_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212473577212733074522_0004_m_000196_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212473577212733074522_0004}; taskId=attempt_202105171409212473577212733074522_0004_m_000196_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5882dad4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:25 INFO StagingCommitter: Starting: Task committer attempt_202105171409212473577212733074522_0004_m_000196_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212473577212733074522_0004_m_000196_351
[2021-05-17 11:11:25,561] {docker.py:276} INFO - 21/05/17 14:11:25 INFO StagingCommitter: Task committer attempt_202105171409212473577212733074522_0004_m_000196_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212473577212733074522_0004_m_000196_351 : duration 0:00.003s
[2021-05-17 11:11:25,845] {docker.py:276} INFO - 21/05/17 14:11:25 INFO StagingCommitter: Starting: Task committer attempt_202105171409215448041637229363251_0004_m_000193_348: needsTaskCommit() Task attempt_202105171409215448041637229363251_0004_m_000193_348
[2021-05-17 11:11:25,846] {docker.py:276} INFO - 21/05/17 14:11:25 INFO StagingCommitter: Task committer attempt_202105171409215448041637229363251_0004_m_000193_348: needsTaskCommit() Task attempt_202105171409215448041637229363251_0004_m_000193_348: duration 0:00.001s
[2021-05-17 11:11:25,847] {docker.py:276} INFO - 21/05/17 14:11:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215448041637229363251_0004_m_000193_348
[2021-05-17 11:11:25,848] {docker.py:276} INFO - 21/05/17 14:11:25 INFO Executor: Finished task 193.0 in stage 4.0 (TID 348). 4587 bytes result sent to driver
[2021-05-17 11:11:25,850] {docker.py:276} INFO - 21/05/17 14:11:25 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 352) (d733d8da4350, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:25,851] {docker.py:276} INFO - 21/05/17 14:11:25 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 348) in 2007 ms on d733d8da4350 (executor driver) (194/200)
[2021-05-17 11:11:25,851] {docker.py:276} INFO - 21/05/17 14:11:25 INFO Executor: Running task 197.0 in stage 4.0 (TID 352)
[2021-05-17 11:11:25,862] {docker.py:276} INFO - 21/05/17 14:11:25 INFO ShuffleBlockFetcherIterator: Getting 4 (21.3 KiB) non-empty blocks including 4 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:25,864] {docker.py:276} INFO - 21/05/17 14:11:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409215131005600997013326_0004_m_000197_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215131005600997013326_0004_m_000197_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409215131005600997013326_0004}; taskId=attempt_202105171409215131005600997013326_0004_m_000197_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20f951c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 11:11:25,865] {docker.py:276} INFO - 21/05/17 14:11:25 INFO StagingCommitter: Starting: Task committer attempt_202105171409215131005600997013326_0004_m_000197_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215131005600997013326_0004_m_000197_352
[2021-05-17 11:11:25,867] {docker.py:276} INFO - 21/05/17 14:11:25 INFO StagingCommitter: Task committer attempt_202105171409215131005600997013326_0004_m_000197_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409215131005600997013326_0004_m_000197_352 : duration 0:00.003s
[2021-05-17 11:11:26,387] {docker.py:276} INFO - 21/05/17 14:11:26 INFO StagingCommitter: Starting: Task committer attempt_202105171409213874216675690209752_0004_m_000194_349: needsTaskCommit() Task attempt_202105171409213874216675690209752_0004_m_000194_349
[2021-05-17 11:11:26,388] {docker.py:276} INFO - 21/05/17 14:11:26 INFO StagingCommitter: Task committer attempt_202105171409213874216675690209752_0004_m_000194_349: needsTaskCommit() Task attempt_202105171409213874216675690209752_0004_m_000194_349: duration 0:00.000s
21/05/17 14:11:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409213874216675690209752_0004_m_000194_349
[2021-05-17 11:11:26,390] {docker.py:276} INFO - 21/05/17 14:11:26 INFO Executor: Finished task 194.0 in stage 4.0 (TID 349). 4587 bytes result sent to driver
[2021-05-17 11:11:26,391] {docker.py:276} INFO - 21/05/17 14:11:26 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 353) (d733d8da4350, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:26,392] {docker.py:276} INFO - 21/05/17 14:11:26 INFO Executor: Running task 198.0 in stage 4.0 (TID 353)
21/05/17 14:11:26 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 349) in 2251 ms on d733d8da4350 (executor driver) (195/200)
[2021-05-17 11:11:26,400] {docker.py:276} INFO - 21/05/17 14:11:26 INFO ShuffleBlockFetcherIterator: Getting 4 (23.0 KiB) non-empty blocks including 4 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:26,401] {docker.py:276} INFO - 21/05/17 14:11:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:26,402] {docker.py:276} INFO - 21/05/17 14:11:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409214318685645766195160_0004_m_000198_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214318685645766195160_0004_m_000198_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409214318685645766195160_0004}; taskId=attempt_202105171409214318685645766195160_0004_m_000198_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17af9ecc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 14:11:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:26 INFO StagingCommitter: Starting: Task committer attempt_202105171409214318685645766195160_0004_m_000198_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214318685645766195160_0004_m_000198_353
[2021-05-17 11:11:26,405] {docker.py:276} INFO - 21/05/17 14:11:26 INFO StagingCommitter: Task committer attempt_202105171409214318685645766195160_0004_m_000198_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409214318685645766195160_0004_m_000198_353 : duration 0:00.004s
[2021-05-17 11:11:27,043] {docker.py:276} INFO - 21/05/17 14:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409215281977180201552710_0004_m_000195_350: needsTaskCommit() Task attempt_202105171409215281977180201552710_0004_m_000195_350
[2021-05-17 11:11:27,044] {docker.py:276} INFO - 21/05/17 14:11:27 INFO StagingCommitter: Task committer attempt_202105171409215281977180201552710_0004_m_000195_350: needsTaskCommit() Task attempt_202105171409215281977180201552710_0004_m_000195_350: duration 0:00.001s
[2021-05-17 11:11:27,045] {docker.py:276} INFO - 21/05/17 14:11:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215281977180201552710_0004_m_000195_350
[2021-05-17 11:11:27,046] {docker.py:276} INFO - 21/05/17 14:11:27 INFO Executor: Finished task 195.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-17 11:11:27,047] {docker.py:276} INFO - 21/05/17 14:11:27 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 354) (d733d8da4350, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 11:11:27,048] {docker.py:276} INFO - 21/05/17 14:11:27 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 350) in 2447 ms on d733d8da4350 (executor driver) (196/200)
[2021-05-17 11:11:27,048] {docker.py:276} INFO - 21/05/17 14:11:27 INFO Executor: Running task 199.0 in stage 4.0 (TID 354)
[2021-05-17 11:11:27,057] {docker.py:276} INFO - 21/05/17 14:11:27 INFO ShuffleBlockFetcherIterator: Getting 4 (22.5 KiB) non-empty blocks including 4 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 14:11:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 11:11:27,058] {docker.py:276} INFO - 21/05/17 14:11:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 14:11:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 14:11:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 14:11:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105171409212700723488700126536_0004_m_000199_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212700723488700126536_0004_m_000199_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105171409212700723488700126536_0004}; taskId=attempt_202105171409212700723488700126536_0004_m_000199_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@568367ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/d4472e48-8838-4161-9330-2d11794f212f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:27,059] {docker.py:276} INFO - 21/05/17 14:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 14:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409212700723488700126536_0004_m_000199_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212700723488700126536_0004_m_000199_354
[2021-05-17 11:11:27,061] {docker.py:276} INFO - 21/05/17 14:11:27 INFO StagingCommitter: Task committer attempt_202105171409212700723488700126536_0004_m_000199_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/d4472e48-8838-4161-9330-2d11794f212f/_temporary/0/_temporary/attempt_202105171409212700723488700126536_0004_m_000199_354 : duration 0:00.003s
[2021-05-17 11:11:27,826] {docker.py:276} INFO - 21/05/17 14:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105171409212473577212733074522_0004_m_000196_351: needsTaskCommit() Task attempt_202105171409212473577212733074522_0004_m_000196_351
[2021-05-17 11:11:27,827] {docker.py:276} INFO - 21/05/17 14:11:27 INFO StagingCommitter: Task committer attempt_202105171409212473577212733074522_0004_m_000196_351: needsTaskCommit() Task attempt_202105171409212473577212733074522_0004_m_000196_351: duration 0:00.001s
21/05/17 14:11:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212473577212733074522_0004_m_000196_351
[2021-05-17 11:11:27,828] {docker.py:276} INFO - 21/05/17 14:11:27 INFO Executor: Finished task 196.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-17 11:11:27,830] {docker.py:276} INFO - 21/05/17 14:11:27 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 351) in 2252 ms on d733d8da4350 (executor driver) (197/200)
[2021-05-17 11:11:28,258] {docker.py:276} INFO - 21/05/17 14:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105171409214318685645766195160_0004_m_000198_353: needsTaskCommit() Task attempt_202105171409214318685645766195160_0004_m_000198_353
[2021-05-17 11:11:28,259] {docker.py:276} INFO - 21/05/17 14:11:28 INFO StagingCommitter: Task committer attempt_202105171409214318685645766195160_0004_m_000198_353: needsTaskCommit() Task attempt_202105171409214318685645766195160_0004_m_000198_353: duration 0:00.001s
21/05/17 14:11:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409214318685645766195160_0004_m_000198_353
[2021-05-17 11:11:28,261] {docker.py:276} INFO - 21/05/17 14:11:28 INFO Executor: Finished task 198.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-17 11:11:28,262] {docker.py:276} INFO - 21/05/17 14:11:28 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 353) in 1873 ms on d733d8da4350 (executor driver) (198/200)
[2021-05-17 11:11:28,283] {docker.py:276} INFO - 21/05/17 14:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105171409215131005600997013326_0004_m_000197_352: needsTaskCommit() Task attempt_202105171409215131005600997013326_0004_m_000197_352
[2021-05-17 11:11:28,284] {docker.py:276} INFO - 21/05/17 14:11:28 INFO StagingCommitter: Task committer attempt_202105171409215131005600997013326_0004_m_000197_352: needsTaskCommit() Task attempt_202105171409215131005600997013326_0004_m_000197_352: duration 0:00.001s
21/05/17 14:11:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409215131005600997013326_0004_m_000197_352
[2021-05-17 11:11:28,285] {docker.py:276} INFO - 21/05/17 14:11:28 INFO Executor: Finished task 197.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-17 11:11:28,286] {docker.py:276} INFO - 21/05/17 14:11:28 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 352) in 2440 ms on d733d8da4350 (executor driver) (199/200)
[2021-05-17 11:11:28,884] {docker.py:276} INFO - 21/05/17 14:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105171409212700723488700126536_0004_m_000199_354: needsTaskCommit() Task attempt_202105171409212700723488700126536_0004_m_000199_354
[2021-05-17 11:11:28,885] {docker.py:276} INFO - 21/05/17 14:11:28 INFO StagingCommitter: Task committer attempt_202105171409212700723488700126536_0004_m_000199_354: needsTaskCommit() Task attempt_202105171409212700723488700126536_0004_m_000199_354: duration 0:00.001s
21/05/17 14:11:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105171409212700723488700126536_0004_m_000199_354
[2021-05-17 11:11:28,886] {docker.py:276} INFO - 21/05/17 14:11:28 INFO Executor: Finished task 199.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-17 11:11:28,887] {docker.py:276} INFO - 21/05/17 14:11:28 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 354) in 1842 ms on d733d8da4350 (executor driver) (200/200)
21/05/17 14:11:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-17 11:11:28,888] {docker.py:276} INFO - 21/05/17 14:11:28 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 117.195 s
[2021-05-17 11:11:28,889] {docker.py:276} INFO - 21/05/17 14:11:28 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 14:11:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-17 11:11:28,889] {docker.py:276} INFO - 21/05/17 14:11:28 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 127.162980 s
[2021-05-17 11:11:28,891] {docker.py:276} INFO - 21/05/17 14:11:28 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105171409217135854710666598768_0000_m_000000_0: commitJob((no job ID))
[2021-05-17 11:11:28,909] {docker.py:276} INFO - 21/05/17 14:11:28 WARN AbstractS3ACommitter: Task committer attempt_202105171409217135854710666598768_0000_m_000000_0: No pending uploads to commit
[2021-05-17 11:11:29,404] {docker.py:276} INFO - 21/05/17 14:11:29 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/17 14:11:29 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-17 11:11:29,576] {docker.py:276} INFO - 21/05/17 14:11:29 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.175s
21/05/17 14:11:29 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.175s
[2021-05-17 11:11:29,577] {docker.py:276} INFO - 21/05/17 14:11:29 INFO AbstractS3ACommitter: Task committer attempt_202105171409217135854710666598768_0000_m_000000_0: commitJob((no job ID)): duration 0:00.687s
[2021-05-17 11:11:30,091] {docker.py:276} INFO - 21/05/17 14:11:30 INFO FileFormatWriter: Write Job d4472e48-8838-4161-9330-2d11794f212f committed.
[2021-05-17 11:11:30,103] {docker.py:276} INFO - 21/05/17 14:11:30 INFO FileFormatWriter: Finished processing stats for write job d4472e48-8838-4161-9330-2d11794f212f.
[2021-05-17 11:11:30,219] {docker.py:276} INFO - 21/05/17 14:11:30 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-17 11:11:30,234] {docker.py:276} INFO - 21/05/17 14:11:30 INFO SparkUI: Stopped Spark web UI at http://d733d8da4350:4040
[2021-05-17 11:11:30,260] {docker.py:276} INFO - 21/05/17 14:11:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-17 11:11:30,279] {docker.py:276} INFO - 21/05/17 14:11:30 INFO MemoryStore: MemoryStore cleared
[2021-05-17 11:11:30,279] {docker.py:276} INFO - 21/05/17 14:11:30 INFO BlockManager: BlockManager stopped
[2021-05-17 11:11:30,282] {docker.py:276} INFO - 21/05/17 14:11:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-17 11:11:30,287] {docker.py:276} INFO - 21/05/17 14:11:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-17 11:11:30,293] {docker.py:276} INFO - 21/05/17 14:11:30 INFO SparkContext: Successfully stopped SparkContext
[2021-05-17 11:11:30,294] {docker.py:276} INFO - 21/05/17 14:11:30 INFO ShutdownHookManager: Shutdown hook called
[2021-05-17 11:11:30,295] {docker.py:276} INFO - 21/05/17 14:11:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-e7395da7-99e4-4210-989f-66de22c6285b
[2021-05-17 11:11:30,298] {docker.py:276} INFO - 21/05/17 14:11:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2a67798-d3ce-48ad-ba72-cfb192065d8f
[2021-05-17 11:11:30,300] {docker.py:276} INFO - 21/05/17 14:11:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2a67798-d3ce-48ad-ba72-cfb192065d8f/pyspark-3f54c12f-0944-4023-ad53-34b5ddf28de9
[2021-05-17 11:11:30,307] {docker.py:276} INFO - 21/05/17 14:11:30 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-17 11:11:30,308] {docker.py:276} INFO - 21/05/17 14:11:30 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-17 11:11:30,309] {docker.py:276} INFO - 21/05/17 14:11:30 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-17 11:11:30,542] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210517T090000, start_date=20210517T140848, end_date=20210517T141130
[2021-05-17 11:11:30,592] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-17 11:11:30,627] {local_task_job.py:146} INFO - Task exited with return code 0
