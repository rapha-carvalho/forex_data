[2021-05-15 13:21:46,250] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T16:20:43.082053+00:00 [queued]>
[2021-05-15 13:21:46,260] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T16:20:43.082053+00:00 [queued]>
[2021-05-15 13:21:46,260] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 13:21:46,260] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-15 13:21:46,260] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 13:21:46,268] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-15T16:20:43.082053+00:00
[2021-05-15 13:21:46,272] {standard_task_runner.py:52} INFO - Started process 29076 to run task
[2021-05-15 13:21:46,280] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-15T16:20:43.082053+00:00', '--job-id', '731', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpluwhkhsn', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpz65snd2b']
[2021-05-15 13:21:46,283] {standard_task_runner.py:77} INFO - Job 731: Subtask run_spark_job
[2021-05-15 13:21:46,314] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-15T16:20:43.082053+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-15 13:21:46,339] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-15T16:20:43.082053+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-15T16:20:43.082053+00:00
[2021-05-15 13:21:46,341] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-15 13:21:49,192] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-15 13:21:49,194] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-15 13:21:49,195] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-15 13:21:49,199] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-15 13:21:51,373] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-15 13:21:52,046] {docker.py:276} INFO - 21/05/15 16:21:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-15 13:21:54,530] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-15 13:21:54,553] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SparkContext: Running Spark version 3.1.1
[2021-05-15 13:21:54,656] {docker.py:276} INFO - 21/05/15 16:21:54 INFO ResourceUtils: ==============================================================
[2021-05-15 13:21:54,657] {docker.py:276} INFO - 21/05/15 16:21:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-15 13:21:54,658] {docker.py:276} INFO - 21/05/15 16:21:54 INFO ResourceUtils: ==============================================================
[2021-05-15 13:21:54,659] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SparkContext: Submitted application: spark.py
[2021-05-15 13:21:54,696] {docker.py:276} INFO - 21/05/15 16:21:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-15 13:21:54,713] {docker.py:276} INFO - 21/05/15 16:21:54 INFO ResourceProfile: Limiting resource is cpu
[2021-05-15 13:21:54,714] {docker.py:276} INFO - 21/05/15 16:21:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-15 13:21:54,788] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-15 13:21:54,788] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-15 13:21:54,789] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SecurityManager: Changing view acls groups to:
[2021-05-15 13:21:54,789] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SecurityManager: Changing modify acls groups to:
[2021-05-15 13:21:54,790] {docker.py:276} INFO - 21/05/15 16:21:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-15 13:21:55,243] {docker.py:276} INFO - 21/05/15 16:21:55 INFO Utils: Successfully started service 'sparkDriver' on port 42185.
[2021-05-15 13:21:55,285] {docker.py:276} INFO - 21/05/15 16:21:55 INFO SparkEnv: Registering MapOutputTracker
[2021-05-15 13:21:55,335] {docker.py:276} INFO - 21/05/15 16:21:55 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-15 13:21:55,398] {docker.py:276} INFO - 21/05/15 16:21:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-15 13:21:55,399] {docker.py:276} INFO - 21/05/15 16:21:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-15 13:21:55,407] {docker.py:276} INFO - 21/05/15 16:21:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-15 13:21:55,425] {docker.py:276} INFO - 21/05/15 16:21:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5d77311-457e-48a2-b88b-c1adc53dd5a0
[2021-05-15 13:21:55,455] {docker.py:276} INFO - 21/05/15 16:21:55 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-15 13:21:55,483] {docker.py:276} INFO - 21/05/15 16:21:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-15 13:21:55,881] {docker.py:276} INFO - 21/05/15 16:21:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-15 13:21:56,001] {docker.py:276} INFO - 21/05/15 16:21:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c776c6fed6bc:4040
[2021-05-15 13:21:56,280] {docker.py:276} INFO - 21/05/15 16:21:56 INFO Executor: Starting executor ID driver on host c776c6fed6bc
[2021-05-15 13:21:56,329] {docker.py:276} INFO - 21/05/15 16:21:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34717.
[2021-05-15 13:21:56,330] {docker.py:276} INFO - 21/05/15 16:21:56 INFO NettyBlockTransferService: Server created on c776c6fed6bc:34717
[2021-05-15 13:21:56,333] {docker.py:276} INFO - 21/05/15 16:21:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-15 13:21:56,347] {docker.py:276} INFO - 21/05/15 16:21:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c776c6fed6bc, 34717, None)
[2021-05-15 13:21:56,356] {docker.py:276} INFO - 21/05/15 16:21:56 INFO BlockManagerMasterEndpoint: Registering block manager c776c6fed6bc:34717 with 934.4 MiB RAM, BlockManagerId(driver, c776c6fed6bc, 34717, None)
[2021-05-15 13:21:56,361] {docker.py:276} INFO - 21/05/15 16:21:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c776c6fed6bc, 34717, None)
[2021-05-15 13:21:56,363] {docker.py:276} INFO - 21/05/15 16:21:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c776c6fed6bc, 34717, None)
[2021-05-15 13:21:57,070] {docker.py:276} INFO - 21/05/15 16:21:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-15 13:21:57,070] {docker.py:276} INFO - 21/05/15 16:21:57 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-15 13:21:58,376] {docker.py:276} INFO - 21/05/15 16:21:58 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-15 13:21:58,432] {docker.py:276} INFO - 21/05/15 16:21:58 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-15 13:21:58,433] {docker.py:276} INFO - 21/05/15 16:21:58 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-15 13:22:04,591] {docker.py:276} INFO - 21/05/15 16:22:04 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621009258_to_1621011058.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621011058_to_1621012858.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621012858_to_1621014658.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621014658_to_1621016458.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621016458_to_1621018258.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621018258_to_1621020058.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621020058_to_1621021858.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621021858_to_1621023658.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621023658_to_1621025458.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621025458_to_1621027258.csv.
[2021-05-15 13:22:05,185] {docker.py:276} INFO - 21/05/15 16:22:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:05,217] {docker.py:276} INFO - 21/05/15 16:22:05 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-15 13:22:05,218] {docker.py:276} INFO - 21/05/15 16:22:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 13:22:05,225] {docker.py:276} INFO - 21/05/15 16:22:05 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 13:22:05,230] {docker.py:276} INFO - 21/05/15 16:22:05 INFO DAGScheduler: Missing parents: List()
[2021-05-15 13:22:05,240] {docker.py:276} INFO - 21/05/15 16:22:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 13:22:05,397] {docker.py:276} INFO - 21/05/15 16:22:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-15 13:22:05,476] {docker.py:276} INFO - 21/05/15 16:22:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-15 13:22:05,481] {docker.py:276} INFO - 21/05/15 16:22:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c776c6fed6bc:34717 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 13:22:05,489] {docker.py:276} INFO - 21/05/15 16:22:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-15 13:22:05,523] {docker.py:276} INFO - 21/05/15 16:22:05 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 13:22:05,525] {docker.py:276} INFO - 21/05/15 16:22:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-15 13:22:05,627] {docker.py:276} INFO - 21/05/15 16:22:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c776c6fed6bc, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:05,631] {docker.py:276} INFO - 21/05/15 16:22:05 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (c776c6fed6bc, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:05,632] {docker.py:276} INFO - 21/05/15 16:22:05 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (c776c6fed6bc, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:05,633] {docker.py:276} INFO - 21/05/15 16:22:05 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (c776c6fed6bc, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:05,653] {docker.py:276} INFO - 21/05/15 16:22:05 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
21/05/15 16:22:05 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-15 13:22:05,656] {docker.py:276} INFO - 21/05/15 16:22:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-15 13:22:05,657] {docker.py:276} INFO - 21/05/15 16:22:05 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-15 13:22:06,071] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-15 13:22:06,078] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (c776c6fed6bc, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,079] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-15 13:22:06,086] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 454 ms on c776c6fed6bc (executor driver) (1/141)
[2021-05-15 13:22:06,262] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-15 13:22:06,265] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (c776c6fed6bc, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,267] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-15 13:22:06,267] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 190 ms on c776c6fed6bc (executor driver) (2/141)
[2021-05-15 13:22:06,447] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-15 13:22:06,451] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (c776c6fed6bc, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,453] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 189 ms on c776c6fed6bc (executor driver) (3/141)
[2021-05-15 13:22:06,455] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-15 13:22:06,538] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-15 13:22:06,542] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (c776c6fed6bc, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,546] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-15 13:22:06,546] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 952 ms on c776c6fed6bc (executor driver) (4/141)
[2021-05-15 13:22:06,556] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-15 13:22:06,556] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (c776c6fed6bc, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,557] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 924 ms on c776c6fed6bc (executor driver) (5/141)
[2021-05-15 13:22:06,561] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
21/05/15 16:22:06 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-15 13:22:06,566] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (c776c6fed6bc, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,567] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-15 13:22:06,568] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 930 ms on c776c6fed6bc (executor driver) (6/141)
[2021-05-15 13:22:06,658] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1886 bytes result sent to driver
[2021-05-15 13:22:06,659] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (c776c6fed6bc, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,661] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
21/05/15 16:22:06 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 210 ms on c776c6fed6bc (executor driver) (7/141)
[2021-05-15 13:22:06,741] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-15 13:22:06,744] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (c776c6fed6bc, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,746] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 206 ms on c776c6fed6bc (executor driver) (8/141)
[2021-05-15 13:22:06,747] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-15 13:22:06,750] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-15 13:22:06,751] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-15 13:22:06,752] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (c776c6fed6bc, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,753] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 195 ms on c776c6fed6bc (executor driver) (9/141)
[2021-05-15 13:22:06,754] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-15 13:22:06,755] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (c776c6fed6bc, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,756] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-15 13:22:06,756] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 202 ms on c776c6fed6bc (executor driver) (10/141)
[2021-05-15 13:22:06,835] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-15 13:22:06,837] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (c776c6fed6bc, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,838] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 180 ms on c776c6fed6bc (executor driver) (11/141)
[2021-05-15 13:22:06,841] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-15 13:22:06,940] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-15 13:22:06,943] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-15 13:22:06,943] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (c776c6fed6bc, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,945] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-15 13:22:06,946] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (c776c6fed6bc, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:06,947] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 193 ms on c776c6fed6bc (executor driver) (12/141)
[2021-05-15 13:22:06,948] {docker.py:276} INFO - 21/05/15 16:22:06 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 197 ms on c776c6fed6bc (executor driver) (13/141)
[2021-05-15 13:22:06,949] {docker.py:276} INFO - 21/05/15 16:22:06 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-15 13:22:07,017] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-15 13:22:07,020] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (c776c6fed6bc, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,022] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 185 ms on c776c6fed6bc (executor driver) (14/141)
[2021-05-15 13:22:07,023] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-15 13:22:07,078] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-15 13:22:07,081] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (c776c6fed6bc, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,083] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 339 ms on c776c6fed6bc (executor driver) (15/141)
[2021-05-15 13:22:07,084] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-15 13:22:07,128] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
[2021-05-15 13:22:07,131] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1886 bytes result sent to driver
[2021-05-15 13:22:07,132] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (c776c6fed6bc, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,133] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-15 13:22:07,134] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (c776c6fed6bc, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,135] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 190 ms on c776c6fed6bc (executor driver) (16/141)
[2021-05-15 13:22:07,137] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 195 ms on c776c6fed6bc (executor driver) (17/141)
[2021-05-15 13:22:07,138] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-15 13:22:07,199] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-15 13:22:07,202] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (c776c6fed6bc, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,203] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
21/05/15 16:22:07 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 184 ms on c776c6fed6bc (executor driver) (18/141)
[2021-05-15 13:22:07,258] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-15 13:22:07,260] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (c776c6fed6bc, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,262] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
21/05/15 16:22:07 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 182 ms on c776c6fed6bc (executor driver) (19/141)
[2021-05-15 13:22:07,313] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
[2021-05-15 13:22:07,318] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-15 13:22:07,319] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (c776c6fed6bc, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,321] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-15 13:22:07,330] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (c776c6fed6bc, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,331] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 198 ms on c776c6fed6bc (executor driver) (20/141)
[2021-05-15 13:22:07,332] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
21/05/15 16:22:07 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 202 ms on c776c6fed6bc (executor driver) (21/141)
[2021-05-15 13:22:07,383] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1886 bytes result sent to driver
[2021-05-15 13:22:07,385] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (c776c6fed6bc, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,387] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 187 ms on c776c6fed6bc (executor driver) (22/141)
[2021-05-15 13:22:07,388] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-15 13:22:07,440] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1886 bytes result sent to driver
[2021-05-15 13:22:07,442] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (c776c6fed6bc, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,444] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
21/05/15 16:22:07 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 185 ms on c776c6fed6bc (executor driver) (23/141)
[2021-05-15 13:22:07,506] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-15 13:22:07,510] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (c776c6fed6bc, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,511] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 189 ms on c776c6fed6bc (executor driver) (24/141)
[2021-05-15 13:22:07,512] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1886 bytes result sent to driver
[2021-05-15 13:22:07,513] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-15 13:22:07,514] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (c776c6fed6bc, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,515] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-15 13:22:07,522] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 200 ms on c776c6fed6bc (executor driver) (25/141)
[2021-05-15 13:22:07,561] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-15 13:22:07,563] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (c776c6fed6bc, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,564] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 179 ms on c776c6fed6bc (executor driver) (26/141)
[2021-05-15 13:22:07,567] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-15 13:22:07,617] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
[2021-05-15 13:22:07,618] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (c776c6fed6bc, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,620] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 178 ms on c776c6fed6bc (executor driver) (27/141)
21/05/15 16:22:07 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-15 13:22:07,693] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-15 13:22:07,694] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (c776c6fed6bc, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,696] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 184 ms on c776c6fed6bc (executor driver) (28/141)
21/05/15 16:22:07 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-15 13:22:07,700] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
[2021-05-15 13:22:07,708] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (c776c6fed6bc, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,709] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 200 ms on c776c6fed6bc (executor driver) (29/141)
[2021-05-15 13:22:07,710] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-15 13:22:07,737] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-15 13:22:07,739] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (c776c6fed6bc, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,742] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 179 ms on c776c6fed6bc (executor driver) (30/141)
[2021-05-15 13:22:07,742] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-15 13:22:07,791] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-15 13:22:07,793] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (c776c6fed6bc, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,793] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 176 ms on c776c6fed6bc (executor driver) (31/141)
[2021-05-15 13:22:07,794] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-15 13:22:07,878] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-15 13:22:07,882] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-15 13:22:07,883] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (c776c6fed6bc, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,885] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 191 ms on c776c6fed6bc (executor driver) (32/141)
[2021-05-15 13:22:07,887] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-15 13:22:07,890] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (c776c6fed6bc, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,897] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-15 13:22:07,898] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 188 ms on c776c6fed6bc (executor driver) (33/141)
[2021-05-15 13:22:07,915] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-15 13:22:07,918] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (c776c6fed6bc, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,920] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 180 ms on c776c6fed6bc (executor driver) (34/141)
[2021-05-15 13:22:07,921] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-15 13:22:07,965] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-15 13:22:07,969] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (c776c6fed6bc, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:07,971] {docker.py:276} INFO - 21/05/15 16:22:07 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 178 ms on c776c6fed6bc (executor driver) (35/141)
[2021-05-15 13:22:07,972] {docker.py:276} INFO - 21/05/15 16:22:07 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-15 13:22:08,091] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-15 13:22:08,093] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (c776c6fed6bc, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,095] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 212 ms on c776c6fed6bc (executor driver) (36/141)
[2021-05-15 13:22:08,098] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-15 13:22:08,116] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1886 bytes result sent to driver
[2021-05-15 13:22:08,117] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (c776c6fed6bc, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,119] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 230 ms on c776c6fed6bc (executor driver) (37/141)
[2021-05-15 13:22:08,119] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-15 13:22:08,133] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1886 bytes result sent to driver
[2021-05-15 13:22:08,134] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (c776c6fed6bc, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,135] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 219 ms on c776c6fed6bc (executor driver) (38/141)
[2021-05-15 13:22:08,137] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-15 13:22:08,166] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1886 bytes result sent to driver
[2021-05-15 13:22:08,167] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (c776c6fed6bc, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,168] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-15 13:22:08,170] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 202 ms on c776c6fed6bc (executor driver) (39/141)
[2021-05-15 13:22:08,286] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1886 bytes result sent to driver
[2021-05-15 13:22:08,289] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (c776c6fed6bc, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,290] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 198 ms on c776c6fed6bc (executor driver) (40/141)
[2021-05-15 13:22:08,291] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-15 13:22:08,293] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-15 13:22:08,295] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (c776c6fed6bc, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,297] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 180 ms on c776c6fed6bc (executor driver) (41/141)
21/05/15 16:22:08 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-15 13:22:08,313] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
[2021-05-15 13:22:08,315] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (c776c6fed6bc, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,316] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-15 13:22:08,317] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 182 ms on c776c6fed6bc (executor driver) (42/141)
[2021-05-15 13:22:08,339] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-15 13:22:08,342] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (c776c6fed6bc, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,344] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 177 ms on c776c6fed6bc (executor driver) (43/141)
[2021-05-15 13:22:08,345] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-15 13:22:08,468] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-15 13:22:08,470] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (c776c6fed6bc, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,472] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-15 13:22:08,473] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 185 ms on c776c6fed6bc (executor driver) (44/141)
[2021-05-15 13:22:08,474] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-15 13:22:08,476] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (c776c6fed6bc, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,477] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 183 ms on c776c6fed6bc (executor driver) (45/141)
21/05/15 16:22:08 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-15 13:22:08,494] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-15 13:22:08,496] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (c776c6fed6bc, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,497] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 182 ms on c776c6fed6bc (executor driver) (46/141)
[2021-05-15 13:22:08,498] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-15 13:22:08,533] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-15 13:22:08,535] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (c776c6fed6bc, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,536] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 194 ms on c776c6fed6bc (executor driver) (47/141)
[2021-05-15 13:22:08,538] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-15 13:22:08,651] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-15 13:22:08,653] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (c776c6fed6bc, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,655] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 180 ms on c776c6fed6bc (executor driver) (48/141)
[2021-05-15 13:22:08,656] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-15 13:22:08,658] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1886 bytes result sent to driver
[2021-05-15 13:22:08,659] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (c776c6fed6bc, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,660] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 191 ms on c776c6fed6bc (executor driver) (49/141)
[2021-05-15 13:22:08,663] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-15 13:22:08,675] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-15 13:22:08,676] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (c776c6fed6bc, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,677] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 182 ms on c776c6fed6bc (executor driver) (50/141)
[2021-05-15 13:22:08,678] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-15 13:22:08,711] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-15 13:22:08,712] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (c776c6fed6bc, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,714] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 179 ms on c776c6fed6bc (executor driver) (51/141)
[2021-05-15 13:22:08,715] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-15 13:22:08,833] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-15 13:22:08,835] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (c776c6fed6bc, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,837] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
21/05/15 16:22:08 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 177 ms on c776c6fed6bc (executor driver) (52/141)
[2021-05-15 13:22:08,838] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-15 13:22:08,840] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (c776c6fed6bc, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,841] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 189 ms on c776c6fed6bc (executor driver) (53/141)
[2021-05-15 13:22:08,842] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-15 13:22:08,859] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1886 bytes result sent to driver
[2021-05-15 13:22:08,868] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (c776c6fed6bc, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,870] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 192 ms on c776c6fed6bc (executor driver) (54/141)
[2021-05-15 13:22:08,871] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-15 13:22:08,891] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1886 bytes result sent to driver
[2021-05-15 13:22:08,893] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (c776c6fed6bc, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:08,895] {docker.py:276} INFO - 21/05/15 16:22:08 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 182 ms on c776c6fed6bc (executor driver) (55/141)
[2021-05-15 13:22:08,898] {docker.py:276} INFO - 21/05/15 16:22:08 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-15 13:22:09,043] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-15 13:22:09,045] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (c776c6fed6bc, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,046] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1886 bytes result sent to driver
21/05/15 16:22:09 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 183 ms on c776c6fed6bc (executor driver) (56/141)
[2021-05-15 13:22:09,047] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-15 13:22:09,048] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (c776c6fed6bc, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,050] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 215 ms on c776c6fed6bc (executor driver) (57/141)
[2021-05-15 13:22:09,051] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-15 13:22:09,062] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1886 bytes result sent to driver
[2021-05-15 13:22:09,064] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (c776c6fed6bc, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,065] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 227 ms on c776c6fed6bc (executor driver) (58/141)
[2021-05-15 13:22:09,066] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-15 13:22:09,070] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-15 13:22:09,071] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (c776c6fed6bc, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,072] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 181 ms on c776c6fed6bc (executor driver) (59/141)
[2021-05-15 13:22:09,073] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-15 13:22:09,225] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-15 13:22:09,227] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-15 13:22:09,228] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (c776c6fed6bc, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,230] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 185 ms on c776c6fed6bc (executor driver) (60/141)
21/05/15 16:22:09 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-15 13:22:09,231] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (c776c6fed6bc, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,233] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 185 ms on c776c6fed6bc (executor driver) (61/141)
[2021-05-15 13:22:09,234] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-15 13:22:09,250] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-15 13:22:09,252] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (c776c6fed6bc, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,253] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 189 ms on c776c6fed6bc (executor driver) (62/141)
[2021-05-15 13:22:09,254] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-15 13:22:09,256] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-15 13:22:09,258] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (c776c6fed6bc, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,259] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 188 ms on c776c6fed6bc (executor driver) (63/141)
[2021-05-15 13:22:09,260] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-15 13:22:09,406] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-15 13:22:09,408] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (c776c6fed6bc, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,411] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 184 ms on c776c6fed6bc (executor driver) (64/141)
[2021-05-15 13:22:09,412] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1886 bytes result sent to driver
[2021-05-15 13:22:09,413] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-15 13:22:09,414] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (c776c6fed6bc, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,416] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 185 ms on c776c6fed6bc (executor driver) (65/141)
[2021-05-15 13:22:09,417] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-15 13:22:09,429] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1843 bytes result sent to driver
[2021-05-15 13:22:09,430] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (c776c6fed6bc, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,432] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 174 ms on c776c6fed6bc (executor driver) (66/141)
21/05/15 16:22:09 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-15 13:22:09,436] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
[2021-05-15 13:22:09,437] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (c776c6fed6bc, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,438] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-15 13:22:09,439] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 187 ms on c776c6fed6bc (executor driver) (67/141)
[2021-05-15 13:22:09,592] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-15 13:22:09,595] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-15 13:22:09,596] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (c776c6fed6bc, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,598] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-15 13:22:09,599] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (c776c6fed6bc, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,600] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 187 ms on c776c6fed6bc (executor driver) (68/141)
[2021-05-15 13:22:09,601] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 195 ms on c776c6fed6bc (executor driver) (69/141)
[2021-05-15 13:22:09,602] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-15 13:22:09,608] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-15 13:22:09,609] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (c776c6fed6bc, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,610] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 180 ms on c776c6fed6bc (executor driver) (70/141)
[2021-05-15 13:22:09,611] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-15 13:22:09,624] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1886 bytes result sent to driver
[2021-05-15 13:22:09,626] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (c776c6fed6bc, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,627] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-15 13:22:09,627] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 191 ms on c776c6fed6bc (executor driver) (71/141)
[2021-05-15 13:22:09,780] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1886 bytes result sent to driver
[2021-05-15 13:22:09,782] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (c776c6fed6bc, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,784] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 184 ms on c776c6fed6bc (executor driver) (72/141)
21/05/15 16:22:09 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-15 13:22:09,786] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1886 bytes result sent to driver
[2021-05-15 13:22:09,788] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (c776c6fed6bc, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,789] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 195 ms on c776c6fed6bc (executor driver) (73/141)
[2021-05-15 13:22:09,790] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-15 13:22:09,794] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1886 bytes result sent to driver
[2021-05-15 13:22:09,795] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (c776c6fed6bc, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,797] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-15 13:22:09,797] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 188 ms on c776c6fed6bc (executor driver) (74/141)
[2021-05-15 13:22:09,798] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1843 bytes result sent to driver
[2021-05-15 13:22:09,800] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (c776c6fed6bc, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,801] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-15 13:22:09,802] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 176 ms on c776c6fed6bc (executor driver) (75/141)
[2021-05-15 13:22:09,963] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1843 bytes result sent to driver
[2021-05-15 13:22:09,968] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (c776c6fed6bc, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,969] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 185 ms on c776c6fed6bc (executor driver) (76/141)
[2021-05-15 13:22:09,969] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-15 13:22:09,972] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-15 13:22:09,974] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (c776c6fed6bc, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,975] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-15 13:22:09,976] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-15 13:22:09,977] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 190 ms on c776c6fed6bc (executor driver) (77/141)
21/05/15 16:22:09 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-15 13:22:09,980] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (c776c6fed6bc, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,984] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 184 ms on c776c6fed6bc (executor driver) (78/141)
[2021-05-15 13:22:09,987] {docker.py:276} INFO - 21/05/15 16:22:09 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (c776c6fed6bc, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:09,988] {docker.py:276} INFO - 21/05/15 16:22:09 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-15 13:22:09,991] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 195 ms on c776c6fed6bc (executor driver) (79/141)
[2021-05-15 13:22:09,994] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-15 13:22:10,154] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
21/05/15 16:22:10 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (c776c6fed6bc, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 16:22:10 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 178 ms on c776c6fed6bc (executor driver) (80/141)
21/05/15 16:22:10 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-15 13:22:10,155] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-15 13:22:10,156] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 193 ms on c776c6fed6bc (executor driver) (81/141)
[2021-05-15 13:22:10,157] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (c776c6fed6bc, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,160] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-15 13:22:10,175] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
21/05/15 16:22:10 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-15 13:22:10,176] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (c776c6fed6bc, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,177] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
21/05/15 16:22:10 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 198 ms on c776c6fed6bc (executor driver) (82/141)
[2021-05-15 13:22:10,179] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (c776c6fed6bc, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,181] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-15 13:22:10,181] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 194 ms on c776c6fed6bc (executor driver) (83/141)
[2021-05-15 13:22:10,333] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-15 13:22:10,334] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (c776c6fed6bc, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,335] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 185 ms on c776c6fed6bc (executor driver) (84/141)
[2021-05-15 13:22:10,336] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-15 13:22:10,337] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
21/05/15 16:22:10 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (c776c6fed6bc, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,338] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 181 ms on c776c6fed6bc (executor driver) (85/141)
21/05/15 16:22:10 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-15 13:22:10,353] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-15 13:22:10,353] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
[2021-05-15 13:22:10,355] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (c776c6fed6bc, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,356] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 180 ms on c776c6fed6bc (executor driver) (86/141)
[2021-05-15 13:22:10,357] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-15 13:22:10,358] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (c776c6fed6bc, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,359] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 181 ms on c776c6fed6bc (executor driver) (87/141)
[2021-05-15 13:22:10,360] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-15 13:22:10,516] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1843 bytes result sent to driver
[2021-05-15 13:22:10,518] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (c776c6fed6bc, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,520] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 186 ms on c776c6fed6bc (executor driver) (88/141)
[2021-05-15 13:22:10,521] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
21/05/15 16:22:10 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1843 bytes result sent to driver
[2021-05-15 13:22:10,523] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (c776c6fed6bc, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,524] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 187 ms on c776c6fed6bc (executor driver) (89/141)
21/05/15 16:22:10 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-15 13:22:10,538] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1886 bytes result sent to driver
21/05/15 16:22:10 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1886 bytes result sent to driver
[2021-05-15 13:22:10,541] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (c776c6fed6bc, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,543] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-15 13:22:10,544] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (c776c6fed6bc, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,545] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 191 ms on c776c6fed6bc (executor driver) (90/141)
21/05/15 16:22:10 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 188 ms on c776c6fed6bc (executor driver) (91/141)
[2021-05-15 13:22:10,547] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-15 13:22:10,711] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1886 bytes result sent to driver
[2021-05-15 13:22:10,713] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (c776c6fed6bc, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,714] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1843 bytes result sent to driver
[2021-05-15 13:22:10,714] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1886 bytes result sent to driver
[2021-05-15 13:22:10,714] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 198 ms on c776c6fed6bc (executor driver) (92/141)
[2021-05-15 13:22:10,716] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-15 13:22:10,717] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (c776c6fed6bc, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,719] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 176 ms on c776c6fed6bc (executor driver) (93/141)
21/05/15 16:22:10 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-15 13:22:10,720] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 197 ms on c776c6fed6bc (executor driver) (94/141)
[2021-05-15 13:22:10,722] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (c776c6fed6bc, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,723] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1843 bytes result sent to driver
[2021-05-15 13:22:10,724] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-15 13:22:10,725] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (c776c6fed6bc, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,726] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 188 ms on c776c6fed6bc (executor driver) (95/141)
[2021-05-15 13:22:10,728] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-15 13:22:10,893] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1843 bytes result sent to driver
[2021-05-15 13:22:10,894] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1843 bytes result sent to driver
[2021-05-15 13:22:10,896] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (c776c6fed6bc, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,900] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
21/05/15 16:22:10 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (c776c6fed6bc, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 16:22:10 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
21/05/15 16:22:10 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 187 ms on c776c6fed6bc (executor driver) (96/141)
21/05/15 16:22:10 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 183 ms on c776c6fed6bc (executor driver) (97/141)
[2021-05-15 13:22:10,903] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1843 bytes result sent to driver
[2021-05-15 13:22:10,904] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (c776c6fed6bc, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,906] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 181 ms on c776c6fed6bc (executor driver) (98/141)
[2021-05-15 13:22:10,907] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-15 13:22:10,908] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1843 bytes result sent to driver
[2021-05-15 13:22:10,909] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 189 ms on c776c6fed6bc (executor driver) (99/141)
[2021-05-15 13:22:10,911] {docker.py:276} INFO - 21/05/15 16:22:10 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (c776c6fed6bc, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:10,913] {docker.py:276} INFO - 21/05/15 16:22:10 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-15 13:22:11,076] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1843 bytes result sent to driver
[2021-05-15 13:22:11,078] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (c776c6fed6bc, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,080] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 185 ms on c776c6fed6bc (executor driver) (100/141)
[2021-05-15 13:22:11,081] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-15 13:22:11,085] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1843 bytes result sent to driver
21/05/15 16:22:11 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1843 bytes result sent to driver
[2021-05-15 13:22:11,086] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 176 ms on c776c6fed6bc (executor driver) (101/141)
[2021-05-15 13:22:11,088] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (c776c6fed6bc, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,089] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-15 13:22:11,090] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (c776c6fed6bc, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,091] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1843 bytes result sent to driver
[2021-05-15 13:22:11,092] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-15 13:22:11,092] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 187 ms on c776c6fed6bc (executor driver) (102/141)
[2021-05-15 13:22:11,093] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (c776c6fed6bc, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,094] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 197 ms on c776c6fed6bc (executor driver) (103/141)
[2021-05-15 13:22:11,095] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-15 13:22:11,265] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1843 bytes result sent to driver
[2021-05-15 13:22:11,268] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (c776c6fed6bc, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,270] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
21/05/15 16:22:11 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 176 ms on c776c6fed6bc (executor driver) (104/141)
[2021-05-15 13:22:11,273] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1843 bytes result sent to driver
[2021-05-15 13:22:11,275] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1843 bytes result sent to driver
[2021-05-15 13:22:11,275] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (c776c6fed6bc, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,277] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1843 bytes result sent to driver
[2021-05-15 13:22:11,277] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 200 ms on c776c6fed6bc (executor driver) (105/141)
21/05/15 16:22:11 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-15 13:22:11,278] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (c776c6fed6bc, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,280] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-15 13:22:11,281] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (c776c6fed6bc, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,282] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 193 ms on c776c6fed6bc (executor driver) (106/141)
21/05/15 16:22:11 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-15 13:22:11,283] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 196 ms on c776c6fed6bc (executor driver) (107/141)
[2021-05-15 13:22:11,458] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1843 bytes result sent to driver
[2021-05-15 13:22:11,461] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (c776c6fed6bc, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,462] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 195 ms on c776c6fed6bc (executor driver) (108/141)
[2021-05-15 13:22:11,464] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-15 13:22:11,466] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1843 bytes result sent to driver
[2021-05-15 13:22:11,476] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (c776c6fed6bc, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,478] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 203 ms on c776c6fed6bc (executor driver) (109/141)
[2021-05-15 13:22:11,478] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-15 13:22:11,479] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1886 bytes result sent to driver
[2021-05-15 13:22:11,481] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 200 ms on c776c6fed6bc (executor driver) (110/141)
[2021-05-15 13:22:11,483] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (c776c6fed6bc, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,485] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-15 13:22:11,487] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1886 bytes result sent to driver
[2021-05-15 13:22:11,489] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (c776c6fed6bc, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,491] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 213 ms on c776c6fed6bc (executor driver) (111/141)
[2021-05-15 13:22:11,494] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-15 13:22:11,658] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1843 bytes result sent to driver
[2021-05-15 13:22:11,660] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (c776c6fed6bc, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,662] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1886 bytes result sent to driver
[2021-05-15 13:22:11,663] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1843 bytes result sent to driver
[2021-05-15 13:22:11,664] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 188 ms on c776c6fed6bc (executor driver) (112/141)
[2021-05-15 13:22:11,665] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-15 13:22:11,667] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (c776c6fed6bc, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,668] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-15 13:22:11,670] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (c776c6fed6bc, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,671] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 213 ms on c776c6fed6bc (executor driver) (113/141)
[2021-05-15 13:22:11,672] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-15 13:22:11,672] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 189 ms on c776c6fed6bc (executor driver) (114/141)
[2021-05-15 13:22:11,676] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1843 bytes result sent to driver
[2021-05-15 13:22:11,677] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (c776c6fed6bc, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,679] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 190 ms on c776c6fed6bc (executor driver) (115/141)
[2021-05-15 13:22:11,680] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-15 13:22:11,841] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-15 13:22:11,843] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (c776c6fed6bc, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,844] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 184 ms on c776c6fed6bc (executor driver) (116/141)
[2021-05-15 13:22:11,845] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-15 13:22:11,848] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1843 bytes result sent to driver
[2021-05-15 13:22:11,849] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1843 bytes result sent to driver
[2021-05-15 13:22:11,850] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (c776c6fed6bc, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,851] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 183 ms on c776c6fed6bc (executor driver) (117/141)
21/05/15 16:22:11 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-15 13:22:11,853] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (c776c6fed6bc, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,856] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 189 ms on c776c6fed6bc (executor driver) (118/141)
[2021-05-15 13:22:11,856] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1843 bytes result sent to driver
[2021-05-15 13:22:11,857] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-15 13:22:11,858] {docker.py:276} INFO - 21/05/15 16:22:11 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (c776c6fed6bc, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:11,860] {docker.py:276} INFO - 21/05/15 16:22:11 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
21/05/15 16:22:11 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 183 ms on c776c6fed6bc (executor driver) (119/141)
[2021-05-15 13:22:12,033] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1843 bytes result sent to driver
[2021-05-15 13:22:12,034] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1843 bytes result sent to driver
[2021-05-15 13:22:12,035] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1843 bytes result sent to driver
[2021-05-15 13:22:12,036] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1843 bytes result sent to driver
[2021-05-15 13:22:12,037] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 177 ms on c776c6fed6bc (executor driver) (120/141)
[2021-05-15 13:22:12,038] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (c776c6fed6bc, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,039] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-15 13:22:12,042] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (c776c6fed6bc, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,043] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-15 13:22:12,045] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (c776c6fed6bc, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,046] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 197 ms on c776c6fed6bc (executor driver) (121/141)
[2021-05-15 13:22:12,048] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-15 13:22:12,050] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (c776c6fed6bc, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,051] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 210 ms on c776c6fed6bc (executor driver) (122/141)
[2021-05-15 13:22:12,051] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-15 13:22:12,052] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 199 ms on c776c6fed6bc (executor driver) (123/141)
[2021-05-15 13:22:12,216] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1843 bytes result sent to driver
[2021-05-15 13:22:12,218] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (c776c6fed6bc, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,220] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 182 ms on c776c6fed6bc (executor driver) (124/141)
[2021-05-15 13:22:12,221] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-15 13:22:12,225] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1843 bytes result sent to driver
[2021-05-15 13:22:12,227] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (c776c6fed6bc, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,229] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 185 ms on c776c6fed6bc (executor driver) (125/141)
[2021-05-15 13:22:12,229] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
21/05/15 16:22:12 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1843 bytes result sent to driver
[2021-05-15 13:22:12,231] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1843 bytes result sent to driver
[2021-05-15 13:22:12,233] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (c776c6fed6bc, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,234] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 185 ms on c776c6fed6bc (executor driver) (126/141)
[2021-05-15 13:22:12,235] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-15 13:22:12,236] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (c776c6fed6bc, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,237] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 197 ms on c776c6fed6bc (executor driver) (127/141)
[2021-05-15 13:22:12,237] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-15 13:22:12,417] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1886 bytes result sent to driver
[2021-05-15 13:22:12,418] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1886 bytes result sent to driver
[2021-05-15 13:22:12,420] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1886 bytes result sent to driver
21/05/15 16:22:12 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (c776c6fed6bc, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,422] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-15 13:22:12,423] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (c776c6fed6bc, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,425] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 190 ms on c776c6fed6bc (executor driver) (128/141)
21/05/15 16:22:12 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-15 13:22:12,426] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 193 ms on c776c6fed6bc (executor driver) (129/141)
[2021-05-15 13:22:12,428] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1886 bytes result sent to driver
[2021-05-15 13:22:12,430] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (c776c6fed6bc, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,431] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 214 ms on c776c6fed6bc (executor driver) (130/141)
21/05/15 16:22:12 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-15 13:22:12,436] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (c776c6fed6bc, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,438] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 212 ms on c776c6fed6bc (executor driver) (131/141)
[2021-05-15 13:22:12,439] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-15 13:22:12,607] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1843 bytes result sent to driver
[2021-05-15 13:22:12,608] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1843 bytes result sent to driver
[2021-05-15 13:22:12,609] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (c776c6fed6bc, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,611] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-15 13:22:12,612] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1843 bytes result sent to driver
21/05/15 16:22:12 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (c776c6fed6bc, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,613] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 183 ms on c776c6fed6bc (executor driver) (132/141)
[2021-05-15 13:22:12,614] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 194 ms on c776c6fed6bc (executor driver) (133/141)
[2021-05-15 13:22:12,615] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (c776c6fed6bc, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,616] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 193 ms on c776c6fed6bc (executor driver) (134/141)
[2021-05-15 13:22:12,617] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-15 13:22:12,619] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1843 bytes result sent to driver
[2021-05-15 13:22:12,621] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (c776c6fed6bc, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,621] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-15 13:22:12,622] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-15 13:22:12,627] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 186 ms on c776c6fed6bc (executor driver) (135/141)
[2021-05-15 13:22:12,790] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1843 bytes result sent to driver
21/05/15 16:22:12 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1843 bytes result sent to driver
[2021-05-15 13:22:12,792] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (c776c6fed6bc, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,793] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 184 ms on c776c6fed6bc (executor driver) (136/141)
21/05/15 16:22:12 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1843 bytes result sent to driver
[2021-05-15 13:22:12,794] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-15 13:22:12,796] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (c776c6fed6bc, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:12,797] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 177 ms on c776c6fed6bc (executor driver) (137/141)
[2021-05-15 13:22:12,798] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-15 13:22:12,799] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1843 bytes result sent to driver
[2021-05-15 13:22:12,808] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 193 ms on c776c6fed6bc (executor driver) (138/141)
[2021-05-15 13:22:12,809] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 199 ms on c776c6fed6bc (executor driver) (139/141)
[2021-05-15 13:22:12,969] {docker.py:276} INFO - 21/05/15 16:22:12 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1843 bytes result sent to driver
[2021-05-15 13:22:12,971] {docker.py:276} INFO - 21/05/15 16:22:12 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 175 ms on c776c6fed6bc (executor driver) (140/141)
[2021-05-15 13:22:13,008] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1843 bytes result sent to driver
[2021-05-15 13:22:13,010] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 219 ms on c776c6fed6bc (executor driver) (141/141)
[2021-05-15 13:22:13,014] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-15 13:22:13,015] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 7.730 s
[2021-05-15 13:22:13,023] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 13:22:13,023] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-15 13:22:13,028] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 7.850144 s
[2021-05-15 13:22:13,069] {docker.py:276} INFO - 21/05/15 16:22:13 INFO InMemoryFileIndex: It took 8504 ms to list leaf files for 141 paths.
[2021-05-15 13:22:13,192] {docker.py:276} INFO - 21/05/15 16:22:13 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621009258_to_1621011058.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621011058_to_1621012858.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621012858_to_1621014658.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621014658_to_1621016458.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621016458_to_1621018258.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621018258_to_1621020058.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621020058_to_1621021858.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621021858_to_1621023658.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621023658_to_1621025458.csv, s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621025458_to_1621027258.csv.
[2021-05-15 13:22:13,241] {docker.py:276} INFO - 21/05/15 16:22:13 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:13,243] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/15 16:22:13 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 13:22:13,244] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 13:22:13,244] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Missing parents: List()
[2021-05-15 13:22:13,245] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 13:22:13,260] {docker.py:276} INFO - 21/05/15 16:22:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-15 13:22:13,276] {docker.py:276} INFO - 21/05/15 16:22:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-15 13:22:13,276] {docker.py:276} INFO - 21/05/15 16:22:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c776c6fed6bc:34717 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-15 13:22:13,278] {docker.py:276} INFO - 21/05/15 16:22:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-15 13:22:13,288] {docker.py:276} INFO - 21/05/15 16:22:13 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 13:22:13,289] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-15 13:22:13,291] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (c776c6fed6bc, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,292] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (c776c6fed6bc, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,293] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (c776c6fed6bc, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,293] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (c776c6fed6bc, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,294] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-15 13:22:13,295] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-15 13:22:13,295] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-15 13:22:13,295] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-15 13:22:13,466] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-15 13:22:13,468] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (c776c6fed6bc, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,469] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-15 13:22:13,469] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-15 13:22:13,471] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (c776c6fed6bc, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,472] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 182 ms on c776c6fed6bc (executor driver) (1/141)
[2021-05-15 13:22:13,473] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
21/05/15 16:22:13 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 182 ms on c776c6fed6bc (executor driver) (2/141)
[2021-05-15 13:22:13,476] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-15 13:22:13,478] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (c776c6fed6bc, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,479] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 187 ms on c776c6fed6bc (executor driver) (3/141)
[2021-05-15 13:22:13,480] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-15 13:22:13,625] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-15 13:22:13,626] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (c776c6fed6bc, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,627] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 335 ms on c776c6fed6bc (executor driver) (4/141)
[2021-05-15 13:22:13,628] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-15 13:22:13,641] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
[2021-05-15 13:22:13,642] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (c776c6fed6bc, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,643] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-15 13:22:13,643] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 176 ms on c776c6fed6bc (executor driver) (5/141)
[2021-05-15 13:22:13,644] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-15 13:22:13,645] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (c776c6fed6bc, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,646] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 175 ms on c776c6fed6bc (executor driver) (6/141)
[2021-05-15 13:22:13,646] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-15 13:22:13,648] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-15 13:22:13,649] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (c776c6fed6bc, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,650] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 173 ms on c776c6fed6bc (executor driver) (7/141)
[2021-05-15 13:22:13,651] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-15 13:22:13,801] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-15 13:22:13,803] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (c776c6fed6bc, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,804] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 178 ms on c776c6fed6bc (executor driver) (8/141)
[2021-05-15 13:22:13,805] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-15 13:22:13,813] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
21/05/15 16:22:13 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
[2021-05-15 13:22:13,815] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (c776c6fed6bc, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,815] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 171 ms on c776c6fed6bc (executor driver) (9/141)
[2021-05-15 13:22:13,816] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-15 13:22:13,817] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (c776c6fed6bc, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,818] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 177 ms on c776c6fed6bc (executor driver) (10/141)
[2021-05-15 13:22:13,819] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-15 13:22:13,821] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-15 13:22:13,822] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (c776c6fed6bc, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,823] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 175 ms on c776c6fed6bc (executor driver) (11/141)
[2021-05-15 13:22:13,825] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
[2021-05-15 13:22:13,978] {docker.py:276} INFO - 21/05/15 16:22:13 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-15 13:22:13,981] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (c776c6fed6bc, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,983] {docker.py:276} INFO - 21/05/15 16:22:13 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 180 ms on c776c6fed6bc (executor driver) (12/141)
[2021-05-15 13:22:13,986] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1843 bytes result sent to driver
[2021-05-15 13:22:13,988] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-15 13:22:13,989] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (c776c6fed6bc, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,990] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
[2021-05-15 13:22:13,991] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 176 ms on c776c6fed6bc (executor driver) (13/141)
[2021-05-15 13:22:13,993] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1843 bytes result sent to driver
[2021-05-15 13:22:13,995] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (c776c6fed6bc, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:13,996] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 180 ms on c776c6fed6bc (executor driver) (14/141)
21/05/15 16:22:14 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-15 13:22:14,002] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1843 bytes result sent to driver
[2021-05-15 13:22:14,003] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 181 ms on c776c6fed6bc (executor driver) (15/141)
[2021-05-15 13:22:14,005] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (c776c6fed6bc, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,006] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-15 13:22:14,158] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1843 bytes result sent to driver
[2021-05-15 13:22:14,160] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (c776c6fed6bc, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,161] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
21/05/15 16:22:14 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 173 ms on c776c6fed6bc (executor driver) (16/141)
[2021-05-15 13:22:14,182] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1929 bytes result sent to driver
21/05/15 16:22:14 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1886 bytes result sent to driver
[2021-05-15 13:22:14,184] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (c776c6fed6bc, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,185] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-15 13:22:14,186] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (c776c6fed6bc, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,187] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 183 ms on c776c6fed6bc (executor driver) (17/141)
[2021-05-15 13:22:14,187] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-15 13:22:14,188] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 209 ms on c776c6fed6bc (executor driver) (18/141)
[2021-05-15 13:22:14,198] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1886 bytes result sent to driver
[2021-05-15 13:22:14,200] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (c776c6fed6bc, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,201] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-15 13:22:14,202] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 207 ms on c776c6fed6bc (executor driver) (19/141)
[2021-05-15 13:22:14,351] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1886 bytes result sent to driver
[2021-05-15 13:22:14,352] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (c776c6fed6bc, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,353] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-15 13:22:14,353] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 193 ms on c776c6fed6bc (executor driver) (20/141)
[2021-05-15 13:22:14,356] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-15 13:22:14,357] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (c776c6fed6bc, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,358] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 172 ms on c776c6fed6bc (executor driver) (21/141)
[2021-05-15 13:22:14,358] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
[2021-05-15 13:22:14,368] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-15 13:22:14,370] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (c776c6fed6bc, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,371] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 188 ms on c776c6fed6bc (executor driver) (22/141)
[2021-05-15 13:22:14,372] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-15 13:22:14,377] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-15 13:22:14,378] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 178 ms on c776c6fed6bc (executor driver) (23/141)
[2021-05-15 13:22:14,379] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (c776c6fed6bc, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,380] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-15 13:22:14,525] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-15 13:22:14,527] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (c776c6fed6bc, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,529] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 177 ms on c776c6fed6bc (executor driver) (24/141)
[2021-05-15 13:22:14,530] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-15 13:22:14,532] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1843 bytes result sent to driver
[2021-05-15 13:22:14,534] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (c776c6fed6bc, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,535] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 179 ms on c776c6fed6bc (executor driver) (25/141)
21/05/15 16:22:14 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-15 13:22:14,546] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1843 bytes result sent to driver
[2021-05-15 13:22:14,547] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (c776c6fed6bc, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,548] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 179 ms on c776c6fed6bc (executor driver) (26/141)
[2021-05-15 13:22:14,549] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-15 13:22:14,556] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1843 bytes result sent to driver
[2021-05-15 13:22:14,558] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (c776c6fed6bc, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,558] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 180 ms on c776c6fed6bc (executor driver) (27/141)
[2021-05-15 13:22:14,558] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-15 13:22:14,704] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
21/05/15 16:22:14 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1843 bytes result sent to driver
[2021-05-15 13:22:14,706] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (c776c6fed6bc, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,707] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 173 ms on c776c6fed6bc (executor driver) (28/141)
[2021-05-15 13:22:14,708] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (c776c6fed6bc, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,710] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-15 13:22:14,711] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 183 ms on c776c6fed6bc (executor driver) (29/141)
[2021-05-15 13:22:14,712] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-15 13:22:14,720] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1843 bytes result sent to driver
[2021-05-15 13:22:14,721] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (c776c6fed6bc, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,723] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 175 ms on c776c6fed6bc (executor driver) (30/141)
21/05/15 16:22:14 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-15 13:22:14,724] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-15 13:22:14,726] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (c776c6fed6bc, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,727] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 170 ms on c776c6fed6bc (executor driver) (31/141)
[2021-05-15 13:22:14,728] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-15 13:22:14,882] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
[2021-05-15 13:22:14,884] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (c776c6fed6bc, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,885] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 180 ms on c776c6fed6bc (executor driver) (32/141)
[2021-05-15 13:22:14,886] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-15 13:22:14,888] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-15 13:22:14,889] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (c776c6fed6bc, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,890] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 183 ms on c776c6fed6bc (executor driver) (33/141)
21/05/15 16:22:14 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-15 13:22:14,893] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1843 bytes result sent to driver
[2021-05-15 13:22:14,894] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (c776c6fed6bc, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,896] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-15 13:22:14,897] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 170 ms on c776c6fed6bc (executor driver) (34/141)
[2021-05-15 13:22:14,899] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1843 bytes result sent to driver
[2021-05-15 13:22:14,902] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (c776c6fed6bc, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:14,904] {docker.py:276} INFO - 21/05/15 16:22:14 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 183 ms on c776c6fed6bc (executor driver) (35/141)
[2021-05-15 13:22:14,905] {docker.py:276} INFO - 21/05/15 16:22:14 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-15 13:22:15,065] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1886 bytes result sent to driver
[2021-05-15 13:22:15,066] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (c776c6fed6bc, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,068] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1886 bytes result sent to driver
[2021-05-15 13:22:15,069] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (c776c6fed6bc, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,071] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 186 ms on c776c6fed6bc (executor driver) (36/141)
21/05/15 16:22:15 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-15 13:22:15,071] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 181 ms on c776c6fed6bc (executor driver) (37/141)
[2021-05-15 13:22:15,072] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-15 13:22:15,075] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1886 bytes result sent to driver
[2021-05-15 13:22:15,076] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (c776c6fed6bc, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,077] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 183 ms on c776c6fed6bc (executor driver) (38/141)
[2021-05-15 13:22:15,080] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1886 bytes result sent to driver
[2021-05-15 13:22:15,080] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-15 13:22:15,081] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (c776c6fed6bc, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,082] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 181 ms on c776c6fed6bc (executor driver) (39/141)
[2021-05-15 13:22:15,083] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-15 13:22:15,242] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1843 bytes result sent to driver
[2021-05-15 13:22:15,243] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1843 bytes result sent to driver
[2021-05-15 13:22:15,245] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (c776c6fed6bc, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,246] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 182 ms on c776c6fed6bc (executor driver) (40/141)
[2021-05-15 13:22:15,247] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (c776c6fed6bc, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,248] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-15 13:22:15,249] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 182 ms on c776c6fed6bc (executor driver) (41/141)
[2021-05-15 13:22:15,251] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-15 13:22:15,253] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1843 bytes result sent to driver
[2021-05-15 13:22:15,254] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (c776c6fed6bc, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,256] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 175 ms on c776c6fed6bc (executor driver) (42/141)
[2021-05-15 13:22:15,256] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
[2021-05-15 13:22:15,257] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1843 bytes result sent to driver
[2021-05-15 13:22:15,259] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (c776c6fed6bc, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,260] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 184 ms on c776c6fed6bc (executor driver) (43/141)
[2021-05-15 13:22:15,261] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-15 13:22:15,429] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1843 bytes result sent to driver
[2021-05-15 13:22:15,431] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (c776c6fed6bc, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,433] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 185 ms on c776c6fed6bc (executor driver) (44/141)
[2021-05-15 13:22:15,434] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1843 bytes result sent to driver
[2021-05-15 13:22:15,436] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-15 13:22:15,437] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (c776c6fed6bc, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,438] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
21/05/15 16:22:15 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 193 ms on c776c6fed6bc (executor driver) (45/141)
[2021-05-15 13:22:15,441] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1843 bytes result sent to driver
[2021-05-15 13:22:15,442] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (c776c6fed6bc, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,443] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 184 ms on c776c6fed6bc (executor driver) (46/141)
[2021-05-15 13:22:15,443] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-15 13:22:15,448] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1843 bytes result sent to driver
[2021-05-15 13:22:15,449] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (c776c6fed6bc, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,450] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 197 ms on c776c6fed6bc (executor driver) (47/141)
[2021-05-15 13:22:15,451] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-15 13:22:15,612] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
[2021-05-15 13:22:15,613] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (c776c6fed6bc, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,614] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 178 ms on c776c6fed6bc (executor driver) (48/141)
21/05/15 16:22:15 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-15 13:22:15,622] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
[2021-05-15 13:22:15,623] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
[2021-05-15 13:22:15,624] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (c776c6fed6bc, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,626] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 177 ms on c776c6fed6bc (executor driver) (49/141)
21/05/15 16:22:15 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
[2021-05-15 13:22:15,627] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (c776c6fed6bc, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,628] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 187 ms on c776c6fed6bc (executor driver) (50/141)
[2021-05-15 13:22:15,629] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-15 13:22:15,718] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1843 bytes result sent to driver
[2021-05-15 13:22:15,719] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (c776c6fed6bc, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,720] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-15 13:22:15,721] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 291 ms on c776c6fed6bc (executor driver) (51/141)
[2021-05-15 13:22:15,790] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1843 bytes result sent to driver
[2021-05-15 13:22:15,794] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (c776c6fed6bc, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 16:22:15 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
21/05/15 16:22:15 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1843 bytes result sent to driver
[2021-05-15 13:22:15,795] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (c776c6fed6bc, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,797] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 185 ms on c776c6fed6bc (executor driver) (52/141)
[2021-05-15 13:22:15,798] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-15 13:22:15,798] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 173 ms on c776c6fed6bc (executor driver) (53/141)
[2021-05-15 13:22:15,801] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1843 bytes result sent to driver
[2021-05-15 13:22:15,802] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 175 ms on c776c6fed6bc (executor driver) (54/141)
[2021-05-15 13:22:15,803] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (c776c6fed6bc, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,805] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-15 13:22:15,890] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1886 bytes result sent to driver
[2021-05-15 13:22:15,892] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (c776c6fed6bc, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,894] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 175 ms on c776c6fed6bc (executor driver) (55/141)
[2021-05-15 13:22:15,896] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-15 13:22:15,963] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1886 bytes result sent to driver
[2021-05-15 13:22:15,965] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (c776c6fed6bc, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,966] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 175 ms on c776c6fed6bc (executor driver) (56/141)
[2021-05-15 13:22:15,967] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-15 13:22:15,973] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1886 bytes result sent to driver
[2021-05-15 13:22:15,974] {docker.py:276} INFO - 21/05/15 16:22:15 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (c776c6fed6bc, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,975] {docker.py:276} INFO - 21/05/15 16:22:15 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
21/05/15 16:22:15 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 182 ms on c776c6fed6bc (executor driver) (57/141)
[2021-05-15 13:22:15,982] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1886 bytes result sent to driver
[2021-05-15 13:22:15,984] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (c776c6fed6bc, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:15,985] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 182 ms on c776c6fed6bc (executor driver) (58/141)
[2021-05-15 13:22:15,985] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
[2021-05-15 13:22:16,063] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1843 bytes result sent to driver
[2021-05-15 13:22:16,066] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (c776c6fed6bc, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,067] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 175 ms on c776c6fed6bc (executor driver) (59/141)
[2021-05-15 13:22:16,068] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-15 13:22:16,137] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1843 bytes result sent to driver
[2021-05-15 13:22:16,139] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (c776c6fed6bc, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,140] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 176 ms on c776c6fed6bc (executor driver) (60/141)
[2021-05-15 13:22:16,141] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-15 13:22:16,146] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1843 bytes result sent to driver
[2021-05-15 13:22:16,148] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (c776c6fed6bc, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,148] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 175 ms on c776c6fed6bc (executor driver) (61/141)
[2021-05-15 13:22:16,149] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-15 13:22:16,152] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1843 bytes result sent to driver
[2021-05-15 13:22:16,153] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (c776c6fed6bc, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,154] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 171 ms on c776c6fed6bc (executor driver) (62/141)
[2021-05-15 13:22:16,155] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-15 13:22:16,237] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1843 bytes result sent to driver
[2021-05-15 13:22:16,240] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (c776c6fed6bc, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,241] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 175 ms on c776c6fed6bc (executor driver) (63/141)
[2021-05-15 13:22:16,242] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-15 13:22:16,310] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1843 bytes result sent to driver
[2021-05-15 13:22:16,320] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (c776c6fed6bc, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,321] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 176 ms on c776c6fed6bc (executor driver) (64/141)
[2021-05-15 13:22:16,321] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-15 13:22:16,322] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1843 bytes result sent to driver
21/05/15 16:22:16 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1843 bytes result sent to driver
[2021-05-15 13:22:16,323] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (c776c6fed6bc, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,324] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-15 13:22:16,325] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (c776c6fed6bc, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,325] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 171 ms on c776c6fed6bc (executor driver) (65/141)
[2021-05-15 13:22:16,326] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-15 13:22:16,326] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 178 ms on c776c6fed6bc (executor driver) (66/141)
[2021-05-15 13:22:16,411] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1843 bytes result sent to driver
[2021-05-15 13:22:16,413] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (c776c6fed6bc, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,433] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-15 13:22:16,434] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 176 ms on c776c6fed6bc (executor driver) (67/141)
[2021-05-15 13:22:16,491] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
[2021-05-15 13:22:16,493] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1843 bytes result sent to driver
[2021-05-15 13:22:16,494] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (c776c6fed6bc, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,496] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-15 13:22:16,497] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (c776c6fed6bc, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,498] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 186 ms on c776c6fed6bc (executor driver) (68/141)
[2021-05-15 13:22:16,499] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 176 ms on c776c6fed6bc (executor driver) (69/141)
[2021-05-15 13:22:16,500] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
[2021-05-15 13:22:16,500] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-15 13:22:16,502] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (c776c6fed6bc, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,503] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 180 ms on c776c6fed6bc (executor driver) (70/141)
[2021-05-15 13:22:16,504] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-15 13:22:16,597] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
[2021-05-15 13:22:16,598] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (c776c6fed6bc, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,599] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
[2021-05-15 13:22:16,600] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 187 ms on c776c6fed6bc (executor driver) (71/141)
[2021-05-15 13:22:16,670] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1843 bytes result sent to driver
21/05/15 16:22:16 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1843 bytes result sent to driver
[2021-05-15 13:22:16,672] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (c776c6fed6bc, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 16:22:16 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1843 bytes result sent to driver
[2021-05-15 13:22:16,673] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-15 13:22:16,674] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (c776c6fed6bc, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,675] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 180 ms on c776c6fed6bc (executor driver) (72/141)
[2021-05-15 13:22:16,676] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 183 ms on c776c6fed6bc (executor driver) (73/141)
[2021-05-15 13:22:16,677] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-15 13:22:16,678] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (c776c6fed6bc, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,679] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 178 ms on c776c6fed6bc (executor driver) (74/141)
[2021-05-15 13:22:16,680] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-15 13:22:16,773] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1886 bytes result sent to driver
[2021-05-15 13:22:16,776] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (c776c6fed6bc, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,777] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 180 ms on c776c6fed6bc (executor driver) (75/141)
[2021-05-15 13:22:16,778] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-15 13:22:16,854] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1886 bytes result sent to driver
[2021-05-15 13:22:16,855] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (c776c6fed6bc, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,856] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 185 ms on c776c6fed6bc (executor driver) (76/141)
21/05/15 16:22:16 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
[2021-05-15 13:22:16,859] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1886 bytes result sent to driver
[2021-05-15 13:22:16,861] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (c776c6fed6bc, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,862] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 184 ms on c776c6fed6bc (executor driver) (77/141)
[2021-05-15 13:22:16,863] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
[2021-05-15 13:22:16,865] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1886 bytes result sent to driver
[2021-05-15 13:22:16,866] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (c776c6fed6bc, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,868] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-15 13:22:16,869] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 197 ms on c776c6fed6bc (executor driver) (78/141)
[2021-05-15 13:22:16,955] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1843 bytes result sent to driver
[2021-05-15 13:22:16,957] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (c776c6fed6bc, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:16,958] {docker.py:276} INFO - 21/05/15 16:22:16 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 183 ms on c776c6fed6bc (executor driver) (79/141)
[2021-05-15 13:22:16,960] {docker.py:276} INFO - 21/05/15 16:22:16 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
[2021-05-15 13:22:17,037] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1843 bytes result sent to driver
[2021-05-15 13:22:17,039] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1843 bytes result sent to driver
[2021-05-15 13:22:17,040] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1843 bytes result sent to driver
[2021-05-15 13:22:17,041] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (c776c6fed6bc, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,042] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-15 13:22:17,043] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 188 ms on c776c6fed6bc (executor driver) (80/141)
[2021-05-15 13:22:17,045] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (c776c6fed6bc, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,046] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 180 ms on c776c6fed6bc (executor driver) (81/141)
[2021-05-15 13:22:17,047] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (c776c6fed6bc, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,048] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 188 ms on c776c6fed6bc (executor driver) (82/141)
21/05/15 16:22:17 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
21/05/15 16:22:17 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-15 13:22:17,132] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1843 bytes result sent to driver
[2021-05-15 13:22:17,134] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (c776c6fed6bc, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,135] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 179 ms on c776c6fed6bc (executor driver) (83/141)
[2021-05-15 13:22:17,136] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-15 13:22:17,216] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1843 bytes result sent to driver
21/05/15 16:22:17 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1843 bytes result sent to driver
[2021-05-15 13:22:17,218] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (c776c6fed6bc, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,220] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (c776c6fed6bc, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,221] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 173 ms on c776c6fed6bc (executor driver) (84/141)
[2021-05-15 13:22:17,221] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 177 ms on c776c6fed6bc (executor driver) (85/141)
[2021-05-15 13:22:17,222] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-15 13:22:17,226] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-15 13:22:17,227] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1843 bytes result sent to driver
[2021-05-15 13:22:17,228] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (c776c6fed6bc, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,228] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 188 ms on c776c6fed6bc (executor driver) (86/141)
[2021-05-15 13:22:17,229] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
[2021-05-15 13:22:17,305] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-15 13:22:17,306] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (c776c6fed6bc, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,307] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 174 ms on c776c6fed6bc (executor driver) (87/141)
[2021-05-15 13:22:17,308] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-15 13:22:17,392] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1843 bytes result sent to driver
[2021-05-15 13:22:17,394] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (c776c6fed6bc, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,395] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-15 13:22:17,396] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 178 ms on c776c6fed6bc (executor driver) (88/141)
[2021-05-15 13:22:17,397] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1843 bytes result sent to driver
[2021-05-15 13:22:17,398] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (c776c6fed6bc, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,399] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-15 13:22:17,400] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 172 ms on c776c6fed6bc (executor driver) (89/141)
[2021-05-15 13:22:17,401] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1843 bytes result sent to driver
[2021-05-15 13:22:17,402] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (c776c6fed6bc, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,403] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-15 13:22:17,404] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 186 ms on c776c6fed6bc (executor driver) (90/141)
[2021-05-15 13:22:17,481] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1843 bytes result sent to driver
[2021-05-15 13:22:17,482] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (c776c6fed6bc, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,483] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-15 13:22:17,483] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 177 ms on c776c6fed6bc (executor driver) (91/141)
[2021-05-15 13:22:17,570] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1843 bytes result sent to driver
[2021-05-15 13:22:17,571] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1843 bytes result sent to driver
[2021-05-15 13:22:17,572] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (c776c6fed6bc, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,573] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-15 13:22:17,573] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (c776c6fed6bc, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,574] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 172 ms on c776c6fed6bc (executor driver) (92/141)
[2021-05-15 13:22:17,575] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1843 bytes result sent to driver
[2021-05-15 13:22:17,575] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 183 ms on c776c6fed6bc (executor driver) (93/141)
21/05/15 16:22:17 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-15 13:22:17,576] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (c776c6fed6bc, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,577] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 178 ms on c776c6fed6bc (executor driver) (94/141)
[2021-05-15 13:22:17,577] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-15 13:22:17,660] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1886 bytes result sent to driver
[2021-05-15 13:22:17,662] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (c776c6fed6bc, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,664] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 182 ms on c776c6fed6bc (executor driver) (95/141)
21/05/15 16:22:17 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-15 13:22:17,754] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1886 bytes result sent to driver
21/05/15 16:22:17 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1886 bytes result sent to driver
[2021-05-15 13:22:17,755] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (c776c6fed6bc, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,758] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (c776c6fed6bc, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,758] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-15 13:22:17,760] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 189 ms on c776c6fed6bc (executor driver) (96/141)
[2021-05-15 13:22:17,761] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 185 ms on c776c6fed6bc (executor driver) (97/141)
[2021-05-15 13:22:17,762] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-15 13:22:17,763] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1886 bytes result sent to driver
[2021-05-15 13:22:17,764] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (c776c6fed6bc, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,765] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 192 ms on c776c6fed6bc (executor driver) (98/141)
21/05/15 16:22:17 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
[2021-05-15 13:22:17,838] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1843 bytes result sent to driver
[2021-05-15 13:22:17,841] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (c776c6fed6bc, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,842] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 181 ms on c776c6fed6bc (executor driver) (99/141)
[2021-05-15 13:22:17,843] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-15 13:22:17,939] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1843 bytes result sent to driver
[2021-05-15 13:22:17,941] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (c776c6fed6bc, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,943] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1843 bytes result sent to driver
[2021-05-15 13:22:17,944] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 181 ms on c776c6fed6bc (executor driver) (100/141)
[2021-05-15 13:22:17,945] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-15 13:22:17,947] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (c776c6fed6bc, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,948] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1843 bytes result sent to driver
[2021-05-15 13:22:17,949] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
21/05/15 16:22:17 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (c776c6fed6bc, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:17,950] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 193 ms on c776c6fed6bc (executor driver) (101/141)
[2021-05-15 13:22:17,950] {docker.py:276} INFO - 21/05/15 16:22:17 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 196 ms on c776c6fed6bc (executor driver) (102/141)
[2021-05-15 13:22:17,953] {docker.py:276} INFO - 21/05/15 16:22:17 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-15 13:22:18,017] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
[2021-05-15 13:22:18,019] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (c776c6fed6bc, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,021] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 181 ms on c776c6fed6bc (executor driver) (103/141)
[2021-05-15 13:22:18,022] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-15 13:22:18,117] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1843 bytes result sent to driver
21/05/15 16:22:18 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1843 bytes result sent to driver
[2021-05-15 13:22:18,118] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (c776c6fed6bc, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,118] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 170 ms on c776c6fed6bc (executor driver) (104/141)
[2021-05-15 13:22:18,119] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-15 13:22:18,120] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (c776c6fed6bc, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,121] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 175 ms on c776c6fed6bc (executor driver) (105/141)
[2021-05-15 13:22:18,121] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-15 13:22:18,123] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1843 bytes result sent to driver
[2021-05-15 13:22:18,124] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (c776c6fed6bc, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,125] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-15 13:22:18,125] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 186 ms on c776c6fed6bc (executor driver) (106/141)
[2021-05-15 13:22:18,192] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1843 bytes result sent to driver
[2021-05-15 13:22:18,193] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (c776c6fed6bc, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,194] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 176 ms on c776c6fed6bc (executor driver) (107/141)
21/05/15 16:22:18 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
[2021-05-15 13:22:18,293] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1843 bytes result sent to driver
[2021-05-15 13:22:18,294] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1843 bytes result sent to driver
[2021-05-15 13:22:18,295] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (c776c6fed6bc, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,296] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 172 ms on c776c6fed6bc (executor driver) (108/141)
[2021-05-15 13:22:18,297] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-15 13:22:18,299] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1843 bytes result sent to driver
[2021-05-15 13:22:18,299] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (c776c6fed6bc, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,301] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-15 13:22:18,301] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (c776c6fed6bc, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,302] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
21/05/15 16:22:18 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 182 ms on c776c6fed6bc (executor driver) (109/141)
[2021-05-15 13:22:18,303] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 186 ms on c776c6fed6bc (executor driver) (110/141)
[2021-05-15 13:22:18,366] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1843 bytes result sent to driver
[2021-05-15 13:22:18,368] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (c776c6fed6bc, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,369] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 177 ms on c776c6fed6bc (executor driver) (111/141)
[2021-05-15 13:22:18,370] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-15 13:22:18,467] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1886 bytes result sent to driver
[2021-05-15 13:22:18,468] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (c776c6fed6bc, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,469] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 171 ms on c776c6fed6bc (executor driver) (112/141)
21/05/15 16:22:18 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-15 13:22:18,479] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1886 bytes result sent to driver
[2021-05-15 13:22:18,480] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1886 bytes result sent to driver
[2021-05-15 13:22:18,481] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (c776c6fed6bc, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,482] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-15 13:22:18,483] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (c776c6fed6bc, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,485] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 191 ms on c776c6fed6bc (executor driver) (113/141)
[2021-05-15 13:22:18,486] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 185 ms on c776c6fed6bc (executor driver) (114/141)
[2021-05-15 13:22:18,486] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-15 13:22:18,549] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1886 bytes result sent to driver
[2021-05-15 13:22:18,551] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (c776c6fed6bc, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,552] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
21/05/15 16:22:18 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 185 ms on c776c6fed6bc (executor driver) (115/141)
[2021-05-15 13:22:18,639] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1843 bytes result sent to driver
[2021-05-15 13:22:18,641] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (c776c6fed6bc, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,644] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-15 13:22:18,645] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 176 ms on c776c6fed6bc (executor driver) (116/141)
[2021-05-15 13:22:18,651] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1843 bytes result sent to driver
[2021-05-15 13:22:18,653] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (c776c6fed6bc, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,653] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 174 ms on c776c6fed6bc (executor driver) (117/141)
21/05/15 16:22:18 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-15 13:22:18,658] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-15 13:22:18,659] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (c776c6fed6bc, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,660] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-15 13:22:18,660] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 179 ms on c776c6fed6bc (executor driver) (118/141)
[2021-05-15 13:22:18,723] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1843 bytes result sent to driver
[2021-05-15 13:22:18,725] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (c776c6fed6bc, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,726] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 176 ms on c776c6fed6bc (executor driver) (119/141)
[2021-05-15 13:22:18,728] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-15 13:22:18,825] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1843 bytes result sent to driver
[2021-05-15 13:22:18,827] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (c776c6fed6bc, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,828] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 189 ms on c776c6fed6bc (executor driver) (120/141)
[2021-05-15 13:22:18,829] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
[2021-05-15 13:22:18,841] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1843 bytes result sent to driver
[2021-05-15 13:22:18,842] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (c776c6fed6bc, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,843] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 190 ms on c776c6fed6bc (executor driver) (121/141)
[2021-05-15 13:22:18,843] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
[2021-05-15 13:22:18,872] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1843 bytes result sent to driver
[2021-05-15 13:22:18,874] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (c776c6fed6bc, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,875] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 216 ms on c776c6fed6bc (executor driver) (122/141)
[2021-05-15 13:22:18,876] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-15 13:22:18,903] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1843 bytes result sent to driver
[2021-05-15 13:22:18,904] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (c776c6fed6bc, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:18,905] {docker.py:276} INFO - 21/05/15 16:22:18 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 182 ms on c776c6fed6bc (executor driver) (123/141)
[2021-05-15 13:22:18,908] {docker.py:276} INFO - 21/05/15 16:22:18 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-15 13:22:19,007] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1843 bytes result sent to driver
[2021-05-15 13:22:19,009] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (c776c6fed6bc, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,010] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 184 ms on c776c6fed6bc (executor driver) (124/141)
[2021-05-15 13:22:19,010] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-15 13:22:19,016] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1843 bytes result sent to driver
[2021-05-15 13:22:19,018] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (c776c6fed6bc, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,018] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 177 ms on c776c6fed6bc (executor driver) (125/141)
[2021-05-15 13:22:19,019] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
[2021-05-15 13:22:19,044] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1843 bytes result sent to driver
[2021-05-15 13:22:19,045] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (c776c6fed6bc, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,047] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-15 13:22:19,048] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 175 ms on c776c6fed6bc (executor driver) (126/141)
[2021-05-15 13:22:19,079] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1843 bytes result sent to driver
[2021-05-15 13:22:19,081] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (c776c6fed6bc, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,083] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-15 13:22:19,083] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 179 ms on c776c6fed6bc (executor driver) (127/141)
[2021-05-15 13:22:19,189] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1843 bytes result sent to driver
21/05/15 16:22:19 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1843 bytes result sent to driver
[2021-05-15 13:22:19,190] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (c776c6fed6bc, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,192] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-15 13:22:19,194] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (c776c6fed6bc, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,195] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 177 ms on c776c6fed6bc (executor driver) (128/141)
[2021-05-15 13:22:19,196] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-15 13:22:19,196] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 187 ms on c776c6fed6bc (executor driver) (129/141)
[2021-05-15 13:22:19,214] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1843 bytes result sent to driver
[2021-05-15 13:22:19,215] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (c776c6fed6bc, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,216] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-15 13:22:19,217] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 172 ms on c776c6fed6bc (executor driver) (130/141)
[2021-05-15 13:22:19,253] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1886 bytes result sent to driver
[2021-05-15 13:22:19,256] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (c776c6fed6bc, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,257] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
21/05/15 16:22:19 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 177 ms on c776c6fed6bc (executor driver) (131/141)
[2021-05-15 13:22:19,364] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1886 bytes result sent to driver
[2021-05-15 13:22:19,366] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (c776c6fed6bc, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,367] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 175 ms on c776c6fed6bc (executor driver) (132/141)
21/05/15 16:22:19 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-15 13:22:19,369] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1886 bytes result sent to driver
[2021-05-15 13:22:19,369] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (c776c6fed6bc, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,371] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 181 ms on c776c6fed6bc (executor driver) (133/141)
[2021-05-15 13:22:19,371] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-15 13:22:19,392] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1886 bytes result sent to driver
[2021-05-15 13:22:19,394] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (c776c6fed6bc, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,395] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-15 13:22:19,396] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 180 ms on c776c6fed6bc (executor driver) (134/141)
[2021-05-15 13:22:19,424] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1843 bytes result sent to driver
[2021-05-15 13:22:19,425] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (c776c6fed6bc, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,426] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-15 13:22:19,427] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 172 ms on c776c6fed6bc (executor driver) (135/141)
[2021-05-15 13:22:19,540] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1843 bytes result sent to driver
[2021-05-15 13:22:19,541] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (c776c6fed6bc, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,543] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 173 ms on c776c6fed6bc (executor driver) (136/141)
[2021-05-15 13:22:19,544] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1843 bytes result sent to driver
[2021-05-15 13:22:19,545] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-15 13:22:19,546] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (c776c6fed6bc, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:19,547] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-15 13:22:19,548] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 183 ms on c776c6fed6bc (executor driver) (137/141)
[2021-05-15 13:22:19,560] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1843 bytes result sent to driver
[2021-05-15 13:22:19,561] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 168 ms on c776c6fed6bc (executor driver) (138/141)
[2021-05-15 13:22:19,595] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1843 bytes result sent to driver
[2021-05-15 13:22:19,597] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 171 ms on c776c6fed6bc (executor driver) (139/141)
[2021-05-15 13:22:19,715] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-15 13:22:19,716] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 170 ms on c776c6fed6bc (executor driver) (140/141)
[2021-05-15 13:22:19,720] {docker.py:276} INFO - 21/05/15 16:22:19 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1843 bytes result sent to driver
[2021-05-15 13:22:19,722] {docker.py:276} INFO - 21/05/15 16:22:19 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 181 ms on c776c6fed6bc (executor driver) (141/141)
21/05/15 16:22:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-15 13:22:19,723] {docker.py:276} INFO - 21/05/15 16:22:19 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.478 s
[2021-05-15 13:22:19,724] {docker.py:276} INFO - 21/05/15 16:22:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 16:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-15 13:22:19,724] {docker.py:276} INFO - 21/05/15 16:22:19 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.490725 s
[2021-05-15 13:22:19,739] {docker.py:276} INFO - 21/05/15 16:22:19 INFO InMemoryFileIndex: It took 6556 ms to list leaf files for 141 paths.
[2021-05-15 13:22:20,261] {docker.py:276} INFO - 21/05/15 16:22:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c776c6fed6bc:34717 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 13:22:22,986] {docker.py:276} INFO - 21/05/15 16:22:23 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 13:22:22,992] {docker.py:276} INFO - 21/05/15 16:22:23 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-15 13:22:23,002] {docker.py:276} INFO - 21/05/15 16:22:23 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 13:22:23,737] {docker.py:276} INFO - 21/05/15 16:22:23 INFO CodeGenerator: Code generated in 426.3999 ms
[2021-05-15 13:22:23,760] {docker.py:276} INFO - 21/05/15 16:22:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.1 MiB)
[2021-05-15 13:22:23,796] {docker.py:276} INFO - 21/05/15 16:22:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.1 MiB)
[2021-05-15 13:22:23,797] {docker.py:276} INFO - 21/05/15 16:22:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c776c6fed6bc:34717 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 13:22:23,799] {docker.py:276} INFO - 21/05/15 16:22:23 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:23,834] {docker.py:276} INFO - 21/05/15 16:22:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 13:22:23,886] {docker.py:276} INFO - 21/05/15 16:22:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c776c6fed6bc:34717 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 13:22:24,008] {docker.py:276} INFO - 21/05/15 16:22:24 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:24,010] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/15 16:22:24 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/15 16:22:24 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 13:22:24,011] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: Missing parents: List()
[2021-05-15 13:22:24,012] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 13:22:24,060] {docker.py:276} INFO - 21/05/15 16:22:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-15 13:22:24,083] {docker.py:276} INFO - 21/05/15 16:22:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-15 13:22:24,086] {docker.py:276} INFO - 21/05/15 16:22:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c776c6fed6bc:34717 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 13:22:24,087] {docker.py:276} INFO - 21/05/15 16:22:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-15 13:22:24,090] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/15 16:22:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-15 13:22:24,096] {docker.py:276} INFO - 21/05/15 16:22:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (c776c6fed6bc, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:24,097] {docker.py:276} INFO - 21/05/15 16:22:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-15 13:22:24,229] {docker.py:276} INFO - 21/05/15 16:22:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621027258_to_1621029058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:24,275] {docker.py:276} INFO - 21/05/15 16:22:24 INFO CodeGenerator: Code generated in 28.5698 ms
[2021-05-15 13:22:24,669] {docker.py:276} INFO - 21/05/15 16:22:24 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-15 13:22:24,671] {docker.py:276} INFO - 21/05/15 16:22:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 579 ms on c776c6fed6bc (executor driver) (1/1)
[2021-05-15 13:22:24,672] {docker.py:276} INFO - 21/05/15 16:22:24 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-15 13:22:24,672] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.654 s
[2021-05-15 13:22:24,673] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 16:22:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-15 13:22:24,673] {docker.py:276} INFO - 21/05/15 16:22:24 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.666487 s
[2021-05-15 13:22:24,724] {docker.py:276} INFO - 21/05/15 16:22:24 INFO CodeGenerator: Code generated in 29.7662 ms
[2021-05-15 13:22:24,811] {docker.py:276} INFO - 21/05/15 16:22:24 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 13:22:24,812] {docker.py:276} INFO - 21/05/15 16:22:24 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 13:22:24,813] {docker.py:276} INFO - 21/05/15 16:22:24 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 13:22:24,821] {docker.py:276} INFO - 21/05/15 16:22:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-15 13:22:24,843] {docker.py:276} INFO - 21/05/15 16:22:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on c776c6fed6bc:34717 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 13:22:24,846] {docker.py:276} INFO - 21/05/15 16:22:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-15 13:22:24,847] {docker.py:276} INFO - 21/05/15 16:22:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c776c6fed6bc:34717 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 13:22:24,848] {docker.py:276} INFO - 21/05/15 16:22:24 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:24,855] {docker.py:276} INFO - 21/05/15 16:22:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 13:22:25,496] {docker.py:276} INFO - 21/05/15 16:22:25 INFO FileSourceStrategy: Pushed Filters: 
21/05/15 16:22:25 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 13:22:25,497] {docker.py:276} INFO - 21/05/15 16:22:25 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-15 13:22:26,296] {docker.py:276} INFO - 21/05/15 16:22:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:26,301] {docker.py:276} INFO - 21/05/15 16:22:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:26,302] {docker.py:276} INFO - 21/05/15 16:22:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263298027914728332583_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263298027914728332583_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263298027914728332583_0000}; taskId=attempt_202105151622263298027914728332583_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ec69441}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:26,304] {docker.py:276} INFO - 21/05/15 16:22:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:26,347] {docker.py:276} INFO - 21/05/15 16:22:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 13:22:26,460] {docker.py:276} INFO - 21/05/15 16:22:26 INFO CodeGenerator: Code generated in 63.7308 ms
[2021-05-15 13:22:26,463] {docker.py:276} INFO - 21/05/15 16:22:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 13:22:26,513] {docker.py:276} INFO - 21/05/15 16:22:26 INFO CodeGenerator: Code generated in 42.2707 ms
[2021-05-15 13:22:26,518] {docker.py:276} INFO - 21/05/15 16:22:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-15 13:22:26,538] {docker.py:276} INFO - 21/05/15 16:22:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-15 13:22:26,543] {docker.py:276} INFO - 21/05/15 16:22:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c776c6fed6bc:34717 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 13:22:26,544] {docker.py:276} INFO - 21/05/15 16:22:26 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:26,554] {docker.py:276} INFO - 21/05/15 16:22:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 13:22:26,615] {docker.py:276} INFO - 21/05/15 16:22:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on c776c6fed6bc:34717 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 13:22:26,688] {docker.py:276} INFO - 21/05/15 16:22:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 13:22:26,692] {docker.py:276} INFO - 21/05/15 16:22:26 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-15 13:22:26,694] {docker.py:276} INFO - 21/05/15 16:22:26 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/15 16:22:26 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
21/05/15 16:22:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-15 13:22:26,697] {docker.py:276} INFO - 21/05/15 16:22:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-15 13:22:26,698] {docker.py:276} INFO - 21/05/15 16:22:26 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 13:22:26,712] {docker.py:276} INFO - 21/05/15 16:22:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-15 13:22:26,727] {docker.py:276} INFO - 21/05/15 16:22:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-15 13:22:26,728] {docker.py:276} INFO - 21/05/15 16:22:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c776c6fed6bc:34717 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-15 13:22:26,729] {docker.py:276} INFO - 21/05/15 16:22:26 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-15 13:22:26,731] {docker.py:276} INFO - 21/05/15 16:22:26 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21/05/15 16:22:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-15 13:22:26,733] {docker.py:276} INFO - 21/05/15 16:22:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (c776c6fed6bc, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:26,734] {docker.py:276} INFO - 21/05/15 16:22:26 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (c776c6fed6bc, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:26,735] {docker.py:276} INFO - 21/05/15 16:22:26 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (c776c6fed6bc, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:26,736] {docker.py:276} INFO - 21/05/15 16:22:26 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (c776c6fed6bc, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:26,737] {docker.py:276} INFO - 21/05/15 16:22:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
21/05/15 16:22:26 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-15 13:22:26,737] {docker.py:276} INFO - 21/05/15 16:22:26 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-15 13:22:26,738] {docker.py:276} INFO - 21/05/15 16:22:26 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-15 13:22:26,853] {docker.py:276} INFO - 21/05/15 16:22:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on c776c6fed6bc:34717 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 13:22:26,867] {docker.py:276} INFO - 21/05/15 16:22:26 INFO CodeGenerator: Code generated in 52.666 ms
[2021-05-15 13:22:26,903] {docker.py:276} INFO - 21/05/15 16:22:26 INFO CodeGenerator: Code generated in 12.6717 ms
[2021-05-15 13:22:26,932] {docker.py:276} INFO - 21/05/15 16:22:26 INFO CodeGenerator: Code generated in 21.4059 ms
[2021-05-15 13:22:26,950] {docker.py:276} INFO - 21/05/15 16:22:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621084858_to_1621086658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:26,957] {docker.py:276} INFO - 21/05/15 16:22:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621070458_to_1621072258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:26,958] {docker.py:276} INFO - 21/05/15 16:22:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621027258_to_1621029058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:26,958] {docker.py:276} INFO - 21/05/15 16:22:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621061458_to_1621063258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:27,779] {docker.py:276} INFO - 21/05/15 16:22:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621063258_to_1621065058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:28,258] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621029058_to_1621030858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:28,277] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621072258_to_1621074058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:28,303] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621086658_to_1621088458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:28,321] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621065058_to_1621066858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:28,674] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621066858_to_1621068658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:28,744] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621030858_to_1621032658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:28,751] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621074058_to_1621075858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:28,840] {docker.py:276} INFO - 21/05/15 16:22:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621088458_to_1621090258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:29,051] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621068658_to_1621070458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:29,117] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621032658_to_1621034458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:29,125] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621075858_to_1621077658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:29,239] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621090258_to_1621092058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:29,423] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621070458_to_1621072258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:29,464] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621034458_to_1621036258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:29,494] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621077658_to_1621079458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:29,612] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621092058_to_1621093858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:29,799] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621072258_to_1621074058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:29,864] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621079458_to_1621081258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:29,866] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621036258_to_1621038058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:29,978] {docker.py:276} INFO - 21/05/15 16:22:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621025458_to_1621027258.csv, range: 0-111695, partition values: [empty row]
[2021-05-15 13:22:30,185] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621074058_to_1621075858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:30,204] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621038058_to_1621039858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:30,226] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621081258_to_1621083058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:30,328] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621023658_to_1621025458.csv, range: 0-111646, partition values: [empty row]
[2021-05-15 13:22:30,546] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621075858_to_1621077658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:30,557] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621039858_to_1621041658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:30,575] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621083058_to_1621084858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:30,683] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621021858_to_1621023658.csv, range: 0-109422, partition values: [empty row]
[2021-05-15 13:22:30,903] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621077658_to_1621079458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:30,904] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621041658_to_1621043458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:30,933] {docker.py:276} INFO - 21/05/15 16:22:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621084858_to_1621086658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,027] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621027258_to_1621029058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,258] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621043458_to_1621045258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:31,261] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621079458_to_1621081258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,277] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621086658_to_1621088458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,376] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621029058_to_1621030858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,606] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621045258_to_1621047058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:31,620] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621088458_to_1621090258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,622] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621081258_to_1621083058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,721] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621030858_to_1621032658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,954] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621047058_to_1621048858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:31,968] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621083058_to_1621084858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:31,983] {docker.py:276} INFO - 21/05/15 16:22:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621090258_to_1621092058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:32,064] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621032658_to_1621034458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:32,303] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621048858_to_1621050658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:32,338] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621084858_to_1621086658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:32,343] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621092058_to_1621093858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:32,638] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621050658_to_1621052458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:32,696] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621086658_to_1621088458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:32,700] {docker.py:276} INFO - 21/05/15 16:22:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621027258_to_1621029058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:32,997] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621052458_to_1621054258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:33,055] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621029058_to_1621030858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,061] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621088458_to_1621090258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,365] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621054258_to_1621056058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:33,379] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621034458_to_1621036258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,398] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621090258_to_1621092058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,403] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621030858_to_1621032658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,705] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621056058_to_1621057858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:33,729] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621036258_to_1621038058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,748] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621032658_to_1621034458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:33,752] {docker.py:276} INFO - 21/05/15 16:22:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621092058_to_1621093858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:34,044] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621057858_to_1621059658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:34,103] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621034458_to_1621036258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:34,123] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621025458_to_1621027258.csv, range: 0-104455, partition values: [empty row]
[2021-05-15 13:22:34,213] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621038058_to_1621039858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:34,423] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621059658_to_1621061458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:34,456] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621036258_to_1621038058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:34,469] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621023658_to_1621025458.csv, range: 0-104369, partition values: [empty row]
[2021-05-15 13:22:34,559] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621039858_to_1621041658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:34,770] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621061458_to_1621063258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:34,801] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621038058_to_1621039858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:34,816] {docker.py:276} INFO - 21/05/15 16:22:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621014658_to_1621016458.csv, range: 0-104336, partition values: [empty row]
[2021-05-15 13:22:35,053] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621041658_to_1621043458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:35,109] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621063258_to_1621065058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:35,135] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621039858_to_1621041658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:35,172] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621016458_to_1621018258.csv, range: 0-104335, partition values: [empty row]
[2021-05-15 13:22:35,402] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621043458_to_1621045258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:35,459] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621065058_to_1621066858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:35,499] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621041658_to_1621043458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:35,519] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621016458_to_1621018258.csv, range: 0-104317, partition values: [empty row]
[2021-05-15 13:22:35,747] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621045258_to_1621047058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:35,809] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621066858_to_1621068658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:35,834] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621043458_to_1621045258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:35,872] {docker.py:276} INFO - 21/05/15 16:22:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621020058_to_1621021858.csv, range: 0-104256, partition values: [empty row]
[2021-05-15 13:22:36,098] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621047058_to_1621048858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:36,151] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621068658_to_1621070458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:36,163] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621045258_to_1621047058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:36,224] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621011058_to_1621012858.csv, range: 0-104256, partition values: [empty row]
[2021-05-15 13:22:36,454] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621048858_to_1621050658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:36,511] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621070458_to_1621072258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:36,519] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621047058_to_1621048858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:36,568] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621018258_to_1621020058.csv, range: 0-104252, partition values: [empty row]
[2021-05-15 13:22:36,814] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621050658_to_1621052458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:36,858] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621048858_to_1621050658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:36,881] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621072258_to_1621074058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:36,913] {docker.py:276} INFO - 21/05/15 16:22:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621009258_to_1621011058.csv, range: 0-104248, partition values: [empty row]
[2021-05-15 13:22:37,161] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621052458_to_1621054258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:37,199] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621050658_to_1621052458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:37,213] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621074058_to_1621075858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:37,264] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621025458_to_1621027258.csv, range: 0-104237, partition values: [empty row]
[2021-05-15 13:22:37,512] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621054258_to_1621056058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:37,546] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621052458_to_1621054258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:37,564] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621075858_to_1621077658.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:37,622] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621012858_to_1621014658.csv, range: 0-104210, partition values: [empty row]
[2021-05-15 13:22:37,865] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621056058_to_1621057858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:37,901] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621054258_to_1621056058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:37,924] {docker.py:276} INFO - 21/05/15 16:22:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621077658_to_1621079458.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:38,009] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621014658_to_1621016458.csv, range: 0-104202, partition values: [empty row]
[2021-05-15 13:22:38,215] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621057858_to_1621059658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:38,250] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621056058_to_1621057858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:38,266] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621079458_to_1621081258.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:38,367] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621012858_to_1621014658.csv, range: 0-104199, partition values: [empty row]
[2021-05-15 13:22:38,576] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621059658_to_1621061458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:38,593] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621057858_to_1621059658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:38,604] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621081258_to_1621083058.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:38,717] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621011058_to_1621012858.csv, range: 0-104165, partition values: [empty row]
[2021-05-15 13:22:38,934] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621061458_to_1621063258.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:38,940] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621059658_to_1621061458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:38,943] {docker.py:276} INFO - 21/05/15 16:22:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621083058_to_1621084858.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 13:22:39,293] {docker.py:276} INFO - 21/05/15 16:22:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621063258_to_1621065058.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:39,519] {docker.py:276} INFO - 21/05/15 16:22:39 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2722 bytes result sent to driver
[2021-05-15 13:22:39,520] {docker.py:276} INFO - 21/05/15 16:22:39 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (c776c6fed6bc, executor driver, partition 4, PROCESS_LOCAL, 6214 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:39,521] {docker.py:276} INFO - 21/05/15 16:22:39 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-15 13:22:39,522] {docker.py:276} INFO - 21/05/15 16:22:39 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 12766 ms on c776c6fed6bc (executor driver) (1/5)
[2021-05-15 13:22:39,538] {docker.py:276} INFO - 21/05/15 16:22:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621009258_to_1621011058.csv, range: 0-104131, partition values: [empty row]
[2021-05-15 13:22:39,646] {docker.py:276} INFO - 21/05/15 16:22:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621065058_to_1621066858.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:39,699] {docker.py:276} INFO - 21/05/15 16:22:39 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-15 13:22:39,702] {docker.py:276} INFO - 21/05/15 16:22:39 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 12947 ms on c776c6fed6bc (executor driver) (2/5)
[2021-05-15 13:22:39,720] {docker.py:276} INFO - 21/05/15 16:22:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2679 bytes result sent to driver
[2021-05-15 13:22:39,721] {docker.py:276} INFO - 21/05/15 16:22:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 12969 ms on c776c6fed6bc (executor driver) (3/5)
[2021-05-15 13:22:39,888] {docker.py:276} INFO - 21/05/15 16:22:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621021858_to_1621023658.csv, range: 0-104128, partition values: [empty row]
[2021-05-15 13:22:40,008] {docker.py:276} INFO - 21/05/15 16:22:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621066858_to_1621068658.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:40,244] {docker.py:276} INFO - 21/05/15 16:22:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621018258_to_1621020058.csv, range: 0-104105, partition values: [empty row]
[2021-05-15 13:22:40,350] {docker.py:276} INFO - 21/05/15 16:22:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621068658_to_1621070458.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 13:22:40,590] {docker.py:276} INFO - 21/05/15 16:22:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_13_20_58/from_1621020058_to_1621021858.csv, range: 0-104041, partition values: [empty row]
[2021-05-15 13:22:40,883] {docker.py:276} INFO - 21/05/15 16:22:40 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2679 bytes result sent to driver
[2021-05-15 13:22:40,884] {docker.py:276} INFO - 21/05/15 16:22:40 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 14132 ms on c776c6fed6bc (executor driver) (4/5)
[2021-05-15 13:22:40,929] {docker.py:276} INFO - 21/05/15 16:22:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621009258_to_1621011058.csv, range: 0-103969, partition values: [empty row]
[2021-05-15 13:22:41,274] {docker.py:276} INFO - 21/05/15 16:22:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621011058_to_1621012858.csv, range: 0-103925, partition values: [empty row]
[2021-05-15 13:22:41,611] {docker.py:276} INFO - 21/05/15 16:22:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621012858_to_1621014658.csv, range: 0-103923, partition values: [empty row]
[2021-05-15 13:22:41,948] {docker.py:276} INFO - 21/05/15 16:22:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621014658_to_1621016458.csv, range: 0-103732, partition values: [empty row]
[2021-05-15 13:22:42,290] {docker.py:276} INFO - 21/05/15 16:22:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621020058_to_1621021858.csv, range: 0-103713, partition values: [empty row]
[2021-05-15 13:22:42,649] {docker.py:276} INFO - 21/05/15 16:22:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621016458_to_1621018258.csv, range: 0-103711, partition values: [empty row]
[2021-05-15 13:22:42,987] {docker.py:276} INFO - 21/05/15 16:22:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_13_20_58/from_1621018258_to_1621020058.csv, range: 0-103698, partition values: [empty row]
[2021-05-15 13:22:43,340] {docker.py:276} INFO - 21/05/15 16:22:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621023658_to_1621025458.csv, range: 0-103675, partition values: [empty row]
[2021-05-15 13:22:43,680] {docker.py:276} INFO - 21/05/15 16:22:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_13_20_58/from_1621021858_to_1621023658.csv, range: 0-103653, partition values: [empty row]
[2021-05-15 13:22:44,213] {docker.py:276} INFO - 21/05/15 16:22:44 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-15 13:22:44,214] {docker.py:276} INFO - 21/05/15 16:22:44 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 4699 ms on c776c6fed6bc (executor driver) (5/5)
[2021-05-15 13:22:44,215] {docker.py:276} INFO - 21/05/15 16:22:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-15 13:22:44,215] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 17.495 s
[2021-05-15 13:22:44,216] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: looking for newly runnable stages
[2021-05-15 13:22:44,217] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: running: Set()
[2021-05-15 13:22:44,218] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-15 13:22:44,219] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: failed: Set()
[2021-05-15 13:22:44,224] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 13:22:44,313] {docker.py:276} INFO - 21/05/15 16:22:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-15 13:22:44,323] {docker.py:276} INFO - 21/05/15 16:22:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-15 13:22:44,324] {docker.py:276} INFO - 21/05/15 16:22:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c776c6fed6bc:34717 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-15 13:22:44,324] {docker.py:276} INFO - 21/05/15 16:22:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-15 13:22:44,326] {docker.py:276} INFO - 21/05/15 16:22:44 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/15 16:22:44 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-15 13:22:44,340] {docker.py:276} INFO - 21/05/15 16:22:44 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (c776c6fed6bc, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:44,341] {docker.py:276} INFO - 21/05/15 16:22:44 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (c776c6fed6bc, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:44,342] {docker.py:276} INFO - 21/05/15 16:22:44 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (c776c6fed6bc, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:44,343] {docker.py:276} INFO - 21/05/15 16:22:44 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (c776c6fed6bc, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:44,344] {docker.py:276} INFO - 21/05/15 16:22:44 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
[2021-05-15 13:22:44,345] {docker.py:276} INFO - 21/05/15 16:22:44 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-15 13:22:44,352] {docker.py:276} INFO - 21/05/15 16:22:44 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-15 13:22:44,353] {docker.py:276} INFO - 21/05/15 16:22:44 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-15 13:22:44,456] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Getting 5 (21.7 KiB) non-empty blocks including 5 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:44,459] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:44,460] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:44,461] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2021-05-15 13:22:44,461] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:44,462] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2021-05-15 13:22:44,463] {docker.py:276} INFO - 21/05/15 16:22:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2021-05-15 13:22:44,486] {docker.py:276} INFO - 21/05/15 16:22:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,488] {docker.py:276} INFO - 21/05/15 16:22:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:44,489] {docker.py:276} INFO - 21/05/15 16:22:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:22:44,490] {docker.py:276} INFO - 21/05/15 16:22:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:44,492] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269156494059654024830_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269156494059654024830_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269156494059654024830_0004}; taskId=attempt_202105151622269156494059654024830_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@296bc96f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:44,493] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,495] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263075729265448939185_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263075729265448939185_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263075729265448939185_0004}; taskId=attempt_202105151622263075729265448939185_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cf74feb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,496] {docker.py:276} INFO - 21/05/15 16:22:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:22:44,497] {docker.py:276} INFO - 21/05/15 16:22:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:44,501] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,502] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268245323719781539824_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268245323719781539824_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268245323719781539824_0004}; taskId=attempt_202105151622268245323719781539824_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a19f540}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,503] {docker.py:276} INFO - 21/05/15 16:22:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:44,504] {docker.py:276} INFO - 21/05/15 16:22:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:44,504] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,505] {docker.py:276} INFO - 21/05/15 16:22:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268441259552971637176_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268441259552971637176_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268441259552971637176_0004}; taskId=attempt_202105151622268441259552971637176_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ee43f0f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:44,505] {docker.py:276} INFO - 21/05/15 16:22:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:44,506] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622268441259552971637176_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268441259552971637176_0004_m_000002_290
[2021-05-15 13:22:44,508] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622263075729265448939185_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263075729265448939185_0004_m_000003_291
[2021-05-15 13:22:44,509] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622268245323719781539824_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268245323719781539824_0004_m_000000_288
[2021-05-15 13:22:44,511] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622269156494059654024830_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269156494059654024830_0004_m_000001_289
[2021-05-15 13:22:44,548] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Task committer attempt_202105151622268441259552971637176_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268441259552971637176_0004_m_000002_290 : duration 0:00.042s
[2021-05-15 13:22:44,549] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Task committer attempt_202105151622269156494059654024830_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269156494059654024830_0004_m_000001_289 : duration 0:00.043s
[2021-05-15 13:22:44,550] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Task committer attempt_202105151622268245323719781539824_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268245323719781539824_0004_m_000000_288 : duration 0:00.044s
[2021-05-15 13:22:44,551] {docker.py:276} INFO - 21/05/15 16:22:44 INFO StagingCommitter: Task committer attempt_202105151622263075729265448939185_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263075729265448939185_0004_m_000003_291 : duration 0:00.045s
[2021-05-15 13:22:46,633] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622269156494059654024830_0004_m_000001_289: needsTaskCommit() Task attempt_202105151622269156494059654024830_0004_m_000001_289
[2021-05-15 13:22:46,634] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622269156494059654024830_0004_m_000001_289: needsTaskCommit() Task attempt_202105151622269156494059654024830_0004_m_000001_289: duration 0:00.001s
[2021-05-15 13:22:46,636] {docker.py:276} INFO - 21/05/15 16:22:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269156494059654024830_0004_m_000001_289
[2021-05-15 13:22:46,642] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 4630 bytes result sent to driver
[2021-05-15 13:22:46,643] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622268441259552971637176_0004_m_000002_290: needsTaskCommit() Task attempt_202105151622268441259552971637176_0004_m_000002_290
[2021-05-15 13:22:46,644] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622268441259552971637176_0004_m_000002_290: needsTaskCommit() Task attempt_202105151622268441259552971637176_0004_m_000002_290: duration 0:00.001s
[2021-05-15 13:22:46,644] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (c776c6fed6bc, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:46,647] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622263075729265448939185_0004_m_000003_291: needsTaskCommit() Task attempt_202105151622263075729265448939185_0004_m_000003_291
[2021-05-15 13:22:46,647] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
21/05/15 16:22:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268441259552971637176_0004_m_000002_290
[2021-05-15 13:22:46,648] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622263075729265448939185_0004_m_000003_291: needsTaskCommit() Task attempt_202105151622263075729265448939185_0004_m_000003_291: duration 0:00.001s
21/05/15 16:22:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263075729265448939185_0004_m_000003_291
[2021-05-15 13:22:46,649] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 2311 ms on c776c6fed6bc (executor driver) (1/200)
[2021-05-15 13:22:46,650] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
[2021-05-15 13:22:46,652] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (c776c6fed6bc, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:46,653] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-15 13:22:46,653] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 2313 ms on c776c6fed6bc (executor driver) (2/200)
[2021-05-15 13:22:46,655] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (c776c6fed6bc, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:46,655] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
[2021-05-15 13:22:46,656] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 2317 ms on c776c6fed6bc (executor driver) (3/200)
[2021-05-15 13:22:46,659] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-15 13:22:46,675] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622268245323719781539824_0004_m_000000_288: needsTaskCommit() Task attempt_202105151622268245323719781539824_0004_m_000000_288
[2021-05-15 13:22:46,675] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622268245323719781539824_0004_m_000000_288: needsTaskCommit() Task attempt_202105151622268245323719781539824_0004_m_000000_288: duration 0:00.000s
[2021-05-15 13:22:46,676] {docker.py:276} INFO - 21/05/15 16:22:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268245323719781539824_0004_m_000000_288
[2021-05-15 13:22:46,677] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 4587 bytes result sent to driver
[2021-05-15 13:22:46,679] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (c776c6fed6bc, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:46,680] {docker.py:276} INFO - 21/05/15 16:22:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 2347 ms on c776c6fed6bc (executor driver) (4/200)
[2021-05-15 13:22:46,682] {docker.py:276} INFO - 21/05/15 16:22:46 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-15 13:22:46,686] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:46,686] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:46,690] {docker.py:276} INFO - 21/05/15 16:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:22:46,691] {docker.py:276} INFO - 21/05/15 16:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:46,691] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:46,692] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268451511196547815092_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268451511196547815092_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268451511196547815092_0004}; taskId=attempt_202105151622268451511196547815092_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20befdf3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:46,692] {docker.py:276} INFO - 21/05/15 16:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:46,693] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622268451511196547815092_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268451511196547815092_0004_m_000006_294
[2021-05-15 13:22:46,705] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:46,706] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:46,708] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622268451511196547815092_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268451511196547815092_0004_m_000006_294 : duration 0:00.016s
[2021-05-15 13:22:46,709] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-15 13:22:46,712] {docker.py:276} INFO - 21/05/15 16:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:46,712] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267930237777830599221_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267930237777830599221_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267930237777830599221_0004}; taskId=attempt_202105151622267930237777830599221_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3063f403}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:46,717] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622267930237777830599221_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267930237777830599221_0004_m_000005_293
[2021-05-15 13:22:46,719] {docker.py:276} INFO - 21/05/15 16:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:46,723] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622267930237777830599221_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267930237777830599221_0004_m_000005_293 : duration 0:00.010s
[2021-05-15 13:22:46,734] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:46,734] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264621403462920101907_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264621403462920101907_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264621403462920101907_0004}; taskId=attempt_202105151622264621403462920101907_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1282177e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:46,735] {docker.py:276} INFO - 21/05/15 16:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:46,735] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622264621403462920101907_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264621403462920101907_0004_m_000004_292
[2021-05-15 13:22:46,746] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622264621403462920101907_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264621403462920101907_0004_m_000004_292 : duration 0:00.011s
[2021-05-15 13:22:46,750] {docker.py:276} INFO - 21/05/15 16:22:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-15 13:22:46,755] {docker.py:276} INFO - 21/05/15 16:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:46,755] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:46,756] {docker.py:276} INFO - 21/05/15 16:22:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269110312673127632580_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269110312673127632580_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269110312673127632580_0004}; taskId=attempt_202105151622269110312673127632580_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74c7ca82}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:46,756] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622269110312673127632580_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269110312673127632580_0004_m_000007_295
[2021-05-15 13:22:46,768] {docker.py:276} INFO - 21/05/15 16:22:46 INFO StagingCommitter: Task committer attempt_202105151622269110312673127632580_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269110312673127632580_0004_m_000007_295 : duration 0:00.012s
[2021-05-15 13:22:49,318] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622268451511196547815092_0004_m_000006_294: needsTaskCommit() Task attempt_202105151622268451511196547815092_0004_m_000006_294
[2021-05-15 13:22:49,320] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622268451511196547815092_0004_m_000006_294: needsTaskCommit() Task attempt_202105151622268451511196547815092_0004_m_000006_294: duration 0:00.001s
21/05/15 16:22:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268451511196547815092_0004_m_000006_294
[2021-05-15 13:22:49,322] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 4587 bytes result sent to driver
[2021-05-15 13:22:49,323] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (c776c6fed6bc, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:49,324] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 2673 ms on c776c6fed6bc (executor driver) (5/200)
[2021-05-15 13:22:49,325] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-15 13:22:49,336] {docker.py:276} INFO - 21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:22:49,340] {docker.py:276} INFO - 21/05/15 16:22:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:49,340] {docker.py:276} INFO - 21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226367451189451394657_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226367451189451394657_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226367451189451394657_0004}; taskId=attempt_20210515162226367451189451394657_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27325ff1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:49,341] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_20210515162226367451189451394657_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226367451189451394657_0004_m_000008_296
[2021-05-15 13:22:49,346] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_20210515162226367451189451394657_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226367451189451394657_0004_m_000008_296 : duration 0:00.007s
[2021-05-15 13:22:49,403] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622269110312673127632580_0004_m_000007_295: needsTaskCommit() Task attempt_202105151622269110312673127632580_0004_m_000007_295
[2021-05-15 13:22:49,404] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622269110312673127632580_0004_m_000007_295: needsTaskCommit() Task attempt_202105151622269110312673127632580_0004_m_000007_295: duration 0:00.002s
21/05/15 16:22:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269110312673127632580_0004_m_000007_295
[2021-05-15 13:22:49,406] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 4587 bytes result sent to driver
[2021-05-15 13:22:49,408] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (c776c6fed6bc, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:49,408] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-15 13:22:49,409] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 2735 ms on c776c6fed6bc (executor driver) (6/200)
[2021-05-15 13:22:49,421] {docker.py:276} INFO - 21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:49,422] {docker.py:276} INFO - 21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:49,426] {docker.py:276} INFO - 21/05/15 16:22:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:49,427] {docker.py:276} INFO - 21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262513069707778720941_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262513069707778720941_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262513069707778720941_0004}; taskId=attempt_202105151622262513069707778720941_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d8c5487}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:49,429] {docker.py:276} INFO - 21/05/15 16:22:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:49,433] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622262513069707778720941_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262513069707778720941_0004_m_000009_297
[2021-05-15 13:22:49,437] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622262513069707778720941_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262513069707778720941_0004_m_000009_297 : duration 0:00.010s
[2021-05-15 13:22:49,456] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622267930237777830599221_0004_m_000005_293: needsTaskCommit() Task attempt_202105151622267930237777830599221_0004_m_000005_293
[2021-05-15 13:22:49,457] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622267930237777830599221_0004_m_000005_293: needsTaskCommit() Task attempt_202105151622267930237777830599221_0004_m_000005_293: duration 0:00.001s
[2021-05-15 13:22:49,458] {docker.py:276} INFO - 21/05/15 16:22:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267930237777830599221_0004_m_000005_293
[2021-05-15 13:22:49,463] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 4587 bytes result sent to driver
[2021-05-15 13:22:49,466] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (c776c6fed6bc, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:49,466] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 2816 ms on c776c6fed6bc (executor driver) (7/200)
[2021-05-15 13:22:49,479] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
[2021-05-15 13:22:49,497] {docker.py:276} INFO - 21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:49,501] {docker.py:276} INFO - 21/05/15 16:22:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267059665002269139271_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267059665002269139271_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267059665002269139271_0004}; taskId=attempt_202105151622267059665002269139271_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ba1422f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622267059665002269139271_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267059665002269139271_0004_m_000010_298
[2021-05-15 13:22:49,510] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622267059665002269139271_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267059665002269139271_0004_m_000010_298 : duration 0:00.008s
[2021-05-15 13:22:49,533] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622264621403462920101907_0004_m_000004_292: needsTaskCommit() Task attempt_202105151622264621403462920101907_0004_m_000004_292
[2021-05-15 13:22:49,534] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622264621403462920101907_0004_m_000004_292: needsTaskCommit() Task attempt_202105151622264621403462920101907_0004_m_000004_292: duration 0:00.002s
[2021-05-15 13:22:49,535] {docker.py:276} INFO - 21/05/15 16:22:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264621403462920101907_0004_m_000004_292
[2021-05-15 13:22:49,535] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 4587 bytes result sent to driver
[2021-05-15 13:22:49,536] {docker.py:276} INFO - 21/05/15 16:22:49 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (c776c6fed6bc, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:22:49 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 2893 ms on c776c6fed6bc (executor driver) (8/200)
[2021-05-15 13:22:49,537] {docker.py:276} INFO - 21/05/15 16:22:49 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-15 13:22:49,572] {docker.py:276} INFO - 21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:49,573] {docker.py:276} INFO - 21/05/15 16:22:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:49,577] {docker.py:276} INFO - 21/05/15 16:22:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:22:49,578] {docker.py:276} INFO - 21/05/15 16:22:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:49,580] {docker.py:276} INFO - 21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:49,582] {docker.py:276} INFO - 21/05/15 16:22:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264481936373563413959_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264481936373563413959_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264481936373563413959_0004}; taskId=attempt_202105151622264481936373563413959_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@741f9ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:49,582] {docker.py:276} INFO - 21/05/15 16:22:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:49,585] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622264481936373563413959_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264481936373563413959_0004_m_000011_299
[2021-05-15 13:22:49,591] {docker.py:276} INFO - 21/05/15 16:22:49 INFO StagingCommitter: Task committer attempt_202105151622264481936373563413959_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264481936373563413959_0004_m_000011_299 : duration 0:00.008s
[2021-05-15 13:22:51,977] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_20210515162226367451189451394657_0004_m_000008_296: needsTaskCommit() Task attempt_20210515162226367451189451394657_0004_m_000008_296
[2021-05-15 13:22:51,986] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_20210515162226367451189451394657_0004_m_000008_296: needsTaskCommit() Task attempt_20210515162226367451189451394657_0004_m_000008_296: duration 0:00.001s
21/05/15 16:22:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226367451189451394657_0004_m_000008_296
[2021-05-15 13:22:51,998] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 4544 bytes result sent to driver
[2021-05-15 13:22:51,998] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (c776c6fed6bc, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:51,999] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 2662 ms on c776c6fed6bc (executor driver) (9/200)
[2021-05-15 13:22:51,999] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-15 13:22:52,008] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-15 13:22:52,017] {docker.py:276} INFO - 21/05/15 16:22:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:52,018] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:52,018] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261095442604689321034_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261095442604689321034_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261095442604689321034_0004}; taskId=attempt_202105151622261095442604689321034_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7528ae23}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:52,019] {docker.py:276} INFO - 21/05/15 16:22:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:52,019] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622261095442604689321034_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261095442604689321034_0004_m_000012_300
[2021-05-15 13:22:52,024] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622261095442604689321034_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261095442604689321034_0004_m_000012_300 : duration 0:00.007s
[2021-05-15 13:22:52,198] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622264481936373563413959_0004_m_000011_299: needsTaskCommit() Task attempt_202105151622264481936373563413959_0004_m_000011_299
[2021-05-15 13:22:52,199] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622264481936373563413959_0004_m_000011_299: needsTaskCommit() Task attempt_202105151622264481936373563413959_0004_m_000011_299: duration 0:00.001s
21/05/15 16:22:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264481936373563413959_0004_m_000011_299
[2021-05-15 13:22:52,200] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-15 13:22:52,201] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (c776c6fed6bc, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:52,202] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
[2021-05-15 13:22:52,205] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 2676 ms on c776c6fed6bc (executor driver) (10/200)
[2021-05-15 13:22:52,221] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:52,296] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:22:52,298] {docker.py:276} INFO - 21/05/15 16:22:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:52,298] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263100157392754110115_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263100157392754110115_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263100157392754110115_0004}; taskId=attempt_202105151622263100157392754110115_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79b5932b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:52,300] {docker.py:276} INFO - 21/05/15 16:22:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:52,301] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622263100157392754110115_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263100157392754110115_0004_m_000013_301
[2021-05-15 13:22:52,301] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622263100157392754110115_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263100157392754110115_0004_m_000013_301 : duration 0:00.006s
[2021-05-15 13:22:52,302] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622267059665002269139271_0004_m_000010_298: needsTaskCommit() Task attempt_202105151622267059665002269139271_0004_m_000010_298
[2021-05-15 13:22:52,302] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622267059665002269139271_0004_m_000010_298: needsTaskCommit() Task attempt_202105151622267059665002269139271_0004_m_000010_298: duration 0:00.001s
21/05/15 16:22:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267059665002269139271_0004_m_000010_298
[2021-05-15 13:22:52,303] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 4544 bytes result sent to driver
[2021-05-15 13:22:52,303] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (c776c6fed6bc, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:52,304] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
[2021-05-15 13:22:52,304] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 2827 ms on c776c6fed6bc (executor driver) (11/200)
[2021-05-15 13:22:52,305] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622262513069707778720941_0004_m_000009_297: needsTaskCommit() Task attempt_202105151622262513069707778720941_0004_m_000009_297
[2021-05-15 13:22:52,305] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622262513069707778720941_0004_m_000009_297: needsTaskCommit() Task attempt_202105151622262513069707778720941_0004_m_000009_297: duration 0:00.001s
[2021-05-15 13:22:52,306] {docker.py:276} INFO - 21/05/15 16:22:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262513069707778720941_0004_m_000009_297
[2021-05-15 13:22:52,306] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 4544 bytes result sent to driver
[2021-05-15 13:22:52,306] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (c776c6fed6bc, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:52,306] {docker.py:276} INFO - 21/05/15 16:22:52 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 2890 ms on c776c6fed6bc (executor driver) (12/200)
[2021-05-15 13:22:52,307] {docker.py:276} INFO - 21/05/15 16:22:52 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-15 13:22:52,307] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:52,307] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:22:52,308] {docker.py:276} INFO - 21/05/15 16:22:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:52,308] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:52,309] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266589130592168077035_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266589130592168077035_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266589130592168077035_0004}; taskId=attempt_202105151622266589130592168077035_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ac81ab1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622266589130592168077035_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266589130592168077035_0004_m_000014_302
[2021-05-15 13:22:52,311] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622266589130592168077035_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266589130592168077035_0004_m_000014_302 : duration 0:00.006s
[2021-05-15 13:22:52,313] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:52,314] {docker.py:276} INFO - 21/05/15 16:22:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:52,319] {docker.py:276} INFO - 21/05/15 16:22:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:52,319] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:52,320] {docker.py:276} INFO - 21/05/15 16:22:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263721395951685026863_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263721395951685026863_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263721395951685026863_0004}; taskId=attempt_202105151622263721395951685026863_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a8b56f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:52,321] {docker.py:276} INFO - 21/05/15 16:22:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:52,322] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622263721395951685026863_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263721395951685026863_0004_m_000015_303
[2021-05-15 13:22:52,328] {docker.py:276} INFO - 21/05/15 16:22:52 INFO StagingCommitter: Task committer attempt_202105151622263721395951685026863_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263721395951685026863_0004_m_000015_303 : duration 0:00.008s
[2021-05-15 13:22:54,702] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622261095442604689321034_0004_m_000012_300: needsTaskCommit() Task attempt_202105151622261095442604689321034_0004_m_000012_300
[2021-05-15 13:22:54,703] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Task committer attempt_202105151622261095442604689321034_0004_m_000012_300: needsTaskCommit() Task attempt_202105151622261095442604689321034_0004_m_000012_300: duration 0:00.004s
21/05/15 16:22:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261095442604689321034_0004_m_000012_300
[2021-05-15 13:22:54,705] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622263721395951685026863_0004_m_000015_303: needsTaskCommit() Task attempt_202105151622263721395951685026863_0004_m_000015_303
[2021-05-15 13:22:54,706] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Task committer attempt_202105151622263721395951685026863_0004_m_000015_303: needsTaskCommit() Task attempt_202105151622263721395951685026863_0004_m_000015_303: duration 0:00.002s
21/05/15 16:22:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263721395951685026863_0004_m_000015_303
[2021-05-15 13:22:54,707] {docker.py:276} INFO - 21/05/15 16:22:54 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 4544 bytes result sent to driver
[2021-05-15 13:22:54,709] {docker.py:276} INFO - 21/05/15 16:22:54 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (c776c6fed6bc, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:22:54 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-15 13:22:54,711] {docker.py:276} INFO - 21/05/15 16:22:54 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 2421 ms on c776c6fed6bc (executor driver) (13/200)
[2021-05-15 13:22:54,714] {docker.py:276} INFO - 21/05/15 16:22:54 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
21/05/15 16:22:54 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (c776c6fed6bc, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:54,715] {docker.py:276} INFO - 21/05/15 16:22:54 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 2738 ms on c776c6fed6bc (executor driver) (14/200)
[2021-05-15 13:22:54,716] {docker.py:276} INFO - 21/05/15 16:22:54 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
[2021-05-15 13:22:54,728] {docker.py:276} INFO - 21/05/15 16:22:54 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:22:54,733] {docker.py:276} INFO - 21/05/15 16:22:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:54 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:22:54,734] {docker.py:276} INFO - 21/05/15 16:22:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262965671414127378699_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262965671414127378699_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262965671414127378699_0004}; taskId=attempt_202105151622262965671414127378699_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1dad6c64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:54,734] {docker.py:276} INFO - 21/05/15 16:22:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:54,735] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622262965671414127378699_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262965671414127378699_0004_m_000016_304
[2021-05-15 13:22:54,735] {docker.py:276} INFO - 21/05/15 16:22:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:54,736] {docker.py:276} INFO - 21/05/15 16:22:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:54,737] {docker.py:276} INFO - 21/05/15 16:22:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267095461604439788618_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267095461604439788618_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267095461604439788618_0004}; taskId=attempt_202105151622267095461604439788618_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6241930c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:54,737] {docker.py:276} INFO - 21/05/15 16:22:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:54,737] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622267095461604439788618_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267095461604439788618_0004_m_000017_305
[2021-05-15 13:22:54,741] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Task committer attempt_202105151622262965671414127378699_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262965671414127378699_0004_m_000016_304 : duration 0:00.008s
21/05/15 16:22:54 INFO StagingCommitter: Task committer attempt_202105151622267095461604439788618_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267095461604439788618_0004_m_000017_305 : duration 0:00.006s
[2021-05-15 13:22:54,946] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622263100157392754110115_0004_m_000013_301: needsTaskCommit() Task attempt_202105151622263100157392754110115_0004_m_000013_301
[2021-05-15 13:22:54,946] {docker.py:276} INFO - 21/05/15 16:22:54 INFO StagingCommitter: Task committer attempt_202105151622263100157392754110115_0004_m_000013_301: needsTaskCommit() Task attempt_202105151622263100157392754110115_0004_m_000013_301: duration 0:00.000s
[2021-05-15 13:22:54,947] {docker.py:276} INFO - 21/05/15 16:22:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263100157392754110115_0004_m_000013_301
[2021-05-15 13:22:54,948] {docker.py:276} INFO - 21/05/15 16:22:54 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-15 13:22:54,949] {docker.py:276} INFO - 21/05/15 16:22:54 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (c776c6fed6bc, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:54,952] {docker.py:276} INFO - 21/05/15 16:22:54 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 2753 ms on c776c6fed6bc (executor driver) (15/200)
[2021-05-15 13:22:54,953] {docker.py:276} INFO - 21/05/15 16:22:54 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
[2021-05-15 13:22:54,980] {docker.py:276} INFO - 21/05/15 16:22:55 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:54,980] {docker.py:276} INFO - 21/05/15 16:22:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261385415694415801567_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261385415694415801567_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261385415694415801567_0004}; taskId=attempt_202105151622261385415694415801567_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75fd7592}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:54,990] {docker.py:276} INFO - 21/05/15 16:22:55 INFO StagingCommitter: Starting: Task committer attempt_202105151622261385415694415801567_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261385415694415801567_0004_m_000018_306
[2021-05-15 13:22:55,006] {docker.py:276} INFO - 21/05/15 16:22:55 INFO StagingCommitter: Task committer attempt_202105151622261385415694415801567_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261385415694415801567_0004_m_000018_306 : duration 0:00.018s
[2021-05-15 13:22:55,069] {docker.py:276} INFO - 21/05/15 16:22:55 INFO StagingCommitter: Starting: Task committer attempt_202105151622266589130592168077035_0004_m_000014_302: needsTaskCommit() Task attempt_202105151622266589130592168077035_0004_m_000014_302
21/05/15 16:22:55 INFO StagingCommitter: Task committer attempt_202105151622266589130592168077035_0004_m_000014_302: needsTaskCommit() Task attempt_202105151622266589130592168077035_0004_m_000014_302: duration 0:00.006s
21/05/15 16:22:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266589130592168077035_0004_m_000014_302
[2021-05-15 13:22:55,070] {docker.py:276} INFO - 21/05/15 16:22:55 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 4544 bytes result sent to driver
[2021-05-15 13:22:55,071] {docker.py:276} INFO - 21/05/15 16:22:55 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (c776c6fed6bc, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:55,072] {docker.py:276} INFO - 21/05/15 16:22:55 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-15 13:22:55,074] {docker.py:276} INFO - 21/05/15 16:22:55 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 2787 ms on c776c6fed6bc (executor driver) (16/200)
[2021-05-15 13:22:55,120] {docker.py:276} INFO - 21/05/15 16:22:55 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:55,120] {docker.py:276} INFO - 21/05/15 16:22:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:55,126] {docker.py:276} INFO - 21/05/15 16:22:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:55,127] {docker.py:276} INFO - 21/05/15 16:22:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:55,128] {docker.py:276} INFO - 21/05/15 16:22:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263781285072320477879_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263781285072320477879_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263781285072320477879_0004}; taskId=attempt_202105151622263781285072320477879_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55155641}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:55,129] {docker.py:276} INFO - 21/05/15 16:22:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:55,130] {docker.py:276} INFO - 21/05/15 16:22:55 INFO StagingCommitter: Starting: Task committer attempt_202105151622263781285072320477879_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263781285072320477879_0004_m_000019_307
[2021-05-15 13:22:55,137] {docker.py:276} INFO - 21/05/15 16:22:55 INFO StagingCommitter: Task committer attempt_202105151622263781285072320477879_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263781285072320477879_0004_m_000019_307 : duration 0:00.007s
[2021-05-15 13:22:57,491] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622267095461604439788618_0004_m_000017_305: needsTaskCommit() Task attempt_202105151622267095461604439788618_0004_m_000017_305
[2021-05-15 13:22:57,492] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622267095461604439788618_0004_m_000017_305: needsTaskCommit() Task attempt_202105151622267095461604439788618_0004_m_000017_305: duration 0:00.002s
21/05/15 16:22:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267095461604439788618_0004_m_000017_305
[2021-05-15 13:22:57,494] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
[2021-05-15 13:22:57,495] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (c776c6fed6bc, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:57,496] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-15 13:22:57,497] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 2786 ms on c776c6fed6bc (executor driver) (17/200)
[2021-05-15 13:22:57,511] {docker.py:276} INFO - 21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:22:57,512] {docker.py:276} INFO - 21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:57,517] {docker.py:276} INFO - 21/05/15 16:22:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:22:57,518] {docker.py:276} INFO - 21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263181719283610137707_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263181719283610137707_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263181719283610137707_0004}; taskId=attempt_202105151622263181719283610137707_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@116ff778}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:57,518] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622263181719283610137707_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263181719283610137707_0004_m_000020_308
[2021-05-15 13:22:57,527] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622263181719283610137707_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263181719283610137707_0004_m_000020_308 : duration 0:00.009s
[2021-05-15 13:22:57,537] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622262965671414127378699_0004_m_000016_304: needsTaskCommit() Task attempt_202105151622262965671414127378699_0004_m_000016_304
[2021-05-15 13:22:57,537] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622262965671414127378699_0004_m_000016_304: needsTaskCommit() Task attempt_202105151622262965671414127378699_0004_m_000016_304: duration 0:00.001s
[2021-05-15 13:22:57,538] {docker.py:276} INFO - 21/05/15 16:22:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262965671414127378699_0004_m_000016_304
[2021-05-15 13:22:57,539] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-15 13:22:57,541] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 2836 ms on c776c6fed6bc (executor driver) (18/200)
[2021-05-15 13:22:57,542] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (c776c6fed6bc, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:57,544] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-15 13:22:57,568] {docker.py:276} INFO - 21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Getting 5 (21.3 KiB) non-empty blocks including 5 (21.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:57,572] {docker.py:276} INFO - 21/05/15 16:22:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:22:57,572] {docker.py:276} INFO - 21/05/15 16:22:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:57,573] {docker.py:276} INFO - 21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265968395611486644406_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265968395611486644406_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265968395611486644406_0004}; taskId=attempt_202105151622265968395611486644406_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e472e61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:57,574] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622265968395611486644406_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265968395611486644406_0004_m_000021_309
[2021-05-15 13:22:57,580] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622265968395611486644406_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265968395611486644406_0004_m_000021_309 : duration 0:00.006s
[2021-05-15 13:22:57,678] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622261385415694415801567_0004_m_000018_306: needsTaskCommit() Task attempt_202105151622261385415694415801567_0004_m_000018_306
[2021-05-15 13:22:57,679] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622261385415694415801567_0004_m_000018_306: needsTaskCommit() Task attempt_202105151622261385415694415801567_0004_m_000018_306: duration 0:00.001s
21/05/15 16:22:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261385415694415801567_0004_m_000018_306
[2021-05-15 13:22:57,682] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
[2021-05-15 13:22:57,684] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (c776c6fed6bc, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:57,687] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 2740 ms on c776c6fed6bc (executor driver) (19/200)
[2021-05-15 13:22:57,688] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-15 13:22:57,706] {docker.py:276} INFO - 21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:22:57,712] {docker.py:276} INFO - 21/05/15 16:22:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:57,713] {docker.py:276} INFO - 21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263819993849566671981_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263819993849566671981_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263819993849566671981_0004}; taskId=attempt_202105151622263819993849566671981_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f296af9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:57,713] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622263819993849566671981_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263819993849566671981_0004_m_000022_310
[2021-05-15 13:22:57,721] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622263819993849566671981_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263819993849566671981_0004_m_000022_310 : duration 0:00.007s
[2021-05-15 13:22:57,736] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622263781285072320477879_0004_m_000019_307: needsTaskCommit() Task attempt_202105151622263781285072320477879_0004_m_000019_307
[2021-05-15 13:22:57,737] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622263781285072320477879_0004_m_000019_307: needsTaskCommit() Task attempt_202105151622263781285072320477879_0004_m_000019_307: duration 0:00.004s
[2021-05-15 13:22:57,737] {docker.py:276} INFO - 21/05/15 16:22:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263781285072320477879_0004_m_000019_307
[2021-05-15 13:22:57,738] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-15 13:22:57,738] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (c776c6fed6bc, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:22:57,739] {docker.py:276} INFO - 21/05/15 16:22:57 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-15 13:22:57,740] {docker.py:276} INFO - 21/05/15 16:22:57 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 2681 ms on c776c6fed6bc (executor driver) (20/200)
[2021-05-15 13:22:57,755] {docker.py:276} INFO - 21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:22:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:22:57,757] {docker.py:276} INFO - 21/05/15 16:22:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:22:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:22:57,758] {docker.py:276} INFO - 21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261710633951875388105_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261710633951875388105_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261710633951875388105_0004}; taskId=attempt_202105151622261710633951875388105_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78848179}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:22:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:22:57,758] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622261710633951875388105_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261710633951875388105_0004_m_000023_311
[2021-05-15 13:22:57,765] {docker.py:276} INFO - 21/05/15 16:22:57 INFO StagingCommitter: Task committer attempt_202105151622261710633951875388105_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261710633951875388105_0004_m_000023_311 : duration 0:00.007s
[2021-05-15 13:23:00,157] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622263181719283610137707_0004_m_000020_308: needsTaskCommit() Task attempt_202105151622263181719283610137707_0004_m_000020_308
[2021-05-15 13:23:00,158] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622263181719283610137707_0004_m_000020_308: needsTaskCommit() Task attempt_202105151622263181719283610137707_0004_m_000020_308: duration 0:00.002s
21/05/15 16:23:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263181719283610137707_0004_m_000020_308
[2021-05-15 13:23:00,161] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 4544 bytes result sent to driver
[2021-05-15 13:23:00,163] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (c776c6fed6bc, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:00,164] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-15 13:23:00,165] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 2638 ms on c776c6fed6bc (executor driver) (21/200)
[2021-05-15 13:23:00,175] {docker.py:276} INFO - 21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:00,179] {docker.py:276} INFO - 21/05/15 16:23:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:00,179] {docker.py:276} INFO - 21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262316483650199783097_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262316483650199783097_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262316483650199783097_0004}; taskId=attempt_202105151622262316483650199783097_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21c4c99b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:00,180] {docker.py:276} INFO - 21/05/15 16:23:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622262316483650199783097_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262316483650199783097_0004_m_000024_312
[2021-05-15 13:23:00,186] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622262316483650199783097_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262316483650199783097_0004_m_000024_312 : duration 0:00.005s
[2021-05-15 13:23:00,225] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622265968395611486644406_0004_m_000021_309: needsTaskCommit() Task attempt_202105151622265968395611486644406_0004_m_000021_309
[2021-05-15 13:23:00,226] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622265968395611486644406_0004_m_000021_309: needsTaskCommit() Task attempt_202105151622265968395611486644406_0004_m_000021_309: duration 0:00.001s
[2021-05-15 13:23:00,226] {docker.py:276} INFO - 21/05/15 16:23:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265968395611486644406_0004_m_000021_309
[2021-05-15 13:23:00,228] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
[2021-05-15 13:23:00,230] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (c776c6fed6bc, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:00,231] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
[2021-05-15 13:23:00,232] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 2657 ms on c776c6fed6bc (executor driver) (22/200)
[2021-05-15 13:23:00,243] {docker.py:276} INFO - 21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Getting 5 (21.4 KiB) non-empty blocks including 5 (21.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:00,247] {docker.py:276} INFO - 21/05/15 16:23:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:00,248] {docker.py:276} INFO - 21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263536319244202720513_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263536319244202720513_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263536319244202720513_0004}; taskId=attempt_202105151622263536319244202720513_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7302cae8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:00,248] {docker.py:276} INFO - 21/05/15 16:23:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:00,249] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622263536319244202720513_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263536319244202720513_0004_m_000025_313
[2021-05-15 13:23:00,254] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622263536319244202720513_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263536319244202720513_0004_m_000025_313 : duration 0:00.006s
[2021-05-15 13:23:00,364] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622261710633951875388105_0004_m_000023_311: needsTaskCommit() Task attempt_202105151622261710633951875388105_0004_m_000023_311
[2021-05-15 13:23:00,365] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622261710633951875388105_0004_m_000023_311: needsTaskCommit() Task attempt_202105151622261710633951875388105_0004_m_000023_311: duration 0:00.001s
21/05/15 16:23:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261710633951875388105_0004_m_000023_311
[2021-05-15 13:23:00,367] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 4544 bytes result sent to driver
[2021-05-15 13:23:00,369] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (c776c6fed6bc, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:00,369] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-15 13:23:00,370] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 2601 ms on c776c6fed6bc (executor driver) (23/200)
[2021-05-15 13:23:00,382] {docker.py:276} INFO - 21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:00,385] {docker.py:276} INFO - 21/05/15 16:23:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268022864531315544338_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268022864531315544338_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268022864531315544338_0004}; taskId=attempt_202105151622268022864531315544338_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f72e05b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:00,386] {docker.py:276} INFO - 21/05/15 16:23:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:00,386] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622268022864531315544338_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268022864531315544338_0004_m_000026_314
[2021-05-15 13:23:00,391] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622268022864531315544338_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268022864531315544338_0004_m_000026_314 : duration 0:00.005s
[2021-05-15 13:23:00,436] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622263819993849566671981_0004_m_000022_310: needsTaskCommit() Task attempt_202105151622263819993849566671981_0004_m_000022_310
[2021-05-15 13:23:00,437] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622263819993849566671981_0004_m_000022_310: needsTaskCommit() Task attempt_202105151622263819993849566671981_0004_m_000022_310: duration 0:00.001s
[2021-05-15 13:23:00,437] {docker.py:276} INFO - 21/05/15 16:23:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263819993849566671981_0004_m_000022_310
[2021-05-15 13:23:00,439] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 4544 bytes result sent to driver
[2021-05-15 13:23:00,441] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (c776c6fed6bc, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:00,443] {docker.py:276} INFO - 21/05/15 16:23:00 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 2729 ms on c776c6fed6bc (executor driver) (24/200)
[2021-05-15 13:23:00,445] {docker.py:276} INFO - 21/05/15 16:23:00 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
[2021-05-15 13:23:00,456] {docker.py:276} INFO - 21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:00,460] {docker.py:276} INFO - 21/05/15 16:23:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:00,461] {docker.py:276} INFO - 21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:00,463] {docker.py:276} INFO - 21/05/15 16:23:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262361321584504814072_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262361321584504814072_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262361321584504814072_0004}; taskId=attempt_202105151622262361321584504814072_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50f8a01e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:00,463] {docker.py:276} INFO - 21/05/15 16:23:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:00 INFO StagingCommitter: Starting: Task committer attempt_202105151622262361321584504814072_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262361321584504814072_0004_m_000027_315
[2021-05-15 13:23:00,470] {docker.py:276} INFO - 21/05/15 16:23:00 INFO StagingCommitter: Task committer attempt_202105151622262361321584504814072_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262361321584504814072_0004_m_000027_315 : duration 0:00.009s
[2021-05-15 13:23:02,836] {docker.py:276} INFO - 21/05/15 16:23:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622263536319244202720513_0004_m_000025_313: needsTaskCommit() Task attempt_202105151622263536319244202720513_0004_m_000025_313
[2021-05-15 13:23:02,837] {docker.py:276} INFO - 21/05/15 16:23:02 INFO StagingCommitter: Task committer attempt_202105151622263536319244202720513_0004_m_000025_313: needsTaskCommit() Task attempt_202105151622263536319244202720513_0004_m_000025_313: duration 0:00.001s
21/05/15 16:23:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263536319244202720513_0004_m_000025_313
[2021-05-15 13:23:02,838] {docker.py:276} INFO - 21/05/15 16:23:02 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 4544 bytes result sent to driver
[2021-05-15 13:23:02,839] {docker.py:276} INFO - 21/05/15 16:23:02 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (c776c6fed6bc, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:02,840] {docker.py:276} INFO - 21/05/15 16:23:02 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
21/05/15 16:23:02 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 2614 ms on c776c6fed6bc (executor driver) (25/200)
[2021-05-15 13:23:02,848] {docker.py:276} INFO - 21/05/15 16:23:02 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:02,850] {docker.py:276} INFO - 21/05/15 16:23:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622262316483650199783097_0004_m_000024_312: needsTaskCommit() Task attempt_202105151622262316483650199783097_0004_m_000024_312
[2021-05-15 13:23:02,850] {docker.py:276} INFO - 21/05/15 16:23:02 INFO StagingCommitter: Task committer attempt_202105151622262316483650199783097_0004_m_000024_312: needsTaskCommit() Task attempt_202105151622262316483650199783097_0004_m_000024_312: duration 0:00.000s
21/05/15 16:23:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262316483650199783097_0004_m_000024_312
[2021-05-15 13:23:02,851] {docker.py:276} INFO - 21/05/15 16:23:02 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-15 13:23:02,852] {docker.py:276} INFO - 21/05/15 16:23:02 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (c776c6fed6bc, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:02,853] {docker.py:276} INFO - 21/05/15 16:23:02 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 2695 ms on c776c6fed6bc (executor driver) (26/200)
[2021-05-15 13:23:02,854] {docker.py:276} INFO - 21/05/15 16:23:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:02,854] {docker.py:276} INFO - 21/05/15 16:23:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:02,855] {docker.py:276} INFO - 21/05/15 16:23:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267706665543822346127_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267706665543822346127_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267706665543822346127_0004}; taskId=attempt_202105151622267706665543822346127_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@232bb5de}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:02,855] {docker.py:276} INFO - 21/05/15 16:23:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622267706665543822346127_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267706665543822346127_0004_m_000028_316
[2021-05-15 13:23:02,855] {docker.py:276} INFO - 21/05/15 16:23:02 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-15 13:23:02,858] {docker.py:276} INFO - 21/05/15 16:23:02 INFO StagingCommitter: Task committer attempt_202105151622267706665543822346127_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267706665543822346127_0004_m_000028_316 : duration 0:00.004s
[2021-05-15 13:23:02,865] {docker.py:276} INFO - 21/05/15 16:23:02 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:02,868] {docker.py:276} INFO - 21/05/15 16:23:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:02,869] {docker.py:276} INFO - 21/05/15 16:23:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226358175133649533783_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226358175133649533783_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226358175133649533783_0004}; taskId=attempt_20210515162226358175133649533783_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@606c6b0a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:02,869] {docker.py:276} INFO - 21/05/15 16:23:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:02 INFO StagingCommitter: Starting: Task committer attempt_20210515162226358175133649533783_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226358175133649533783_0004_m_000029_317
[2021-05-15 13:23:02,873] {docker.py:276} INFO - 21/05/15 16:23:02 INFO StagingCommitter: Task committer attempt_20210515162226358175133649533783_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226358175133649533783_0004_m_000029_317 : duration 0:00.004s
[2021-05-15 13:23:03,086] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Starting: Task committer attempt_202105151622268022864531315544338_0004_m_000026_314: needsTaskCommit() Task attempt_202105151622268022864531315544338_0004_m_000026_314
[2021-05-15 13:23:03,087] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Task committer attempt_202105151622268022864531315544338_0004_m_000026_314: needsTaskCommit() Task attempt_202105151622268022864531315544338_0004_m_000026_314: duration 0:00.002s
21/05/15 16:23:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268022864531315544338_0004_m_000026_314
[2021-05-15 13:23:03,090] {docker.py:276} INFO - 21/05/15 16:23:03 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 4544 bytes result sent to driver
21/05/15 16:23:03 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (c776c6fed6bc, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:03,108] {docker.py:276} INFO - 21/05/15 16:23:03 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
21/05/15 16:23:03 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 2726 ms on c776c6fed6bc (executor driver) (27/200)
[2021-05-15 13:23:03,109] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Starting: Task committer attempt_202105151622262361321584504814072_0004_m_000027_315: needsTaskCommit() Task attempt_202105151622262361321584504814072_0004_m_000027_315
[2021-05-15 13:23:03,110] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Task committer attempt_202105151622262361321584504814072_0004_m_000027_315: needsTaskCommit() Task attempt_202105151622262361321584504814072_0004_m_000027_315: duration 0:00.000s
21/05/15 16:23:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262361321584504814072_0004_m_000027_315
[2021-05-15 13:23:03,110] {docker.py:276} INFO - 21/05/15 16:23:03 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 4544 bytes result sent to driver
[2021-05-15 13:23:03,111] {docker.py:276} INFO - 21/05/15 16:23:03 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (c776c6fed6bc, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:03,112] {docker.py:276} INFO - 21/05/15 16:23:03 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
[2021-05-15 13:23:03,112] {docker.py:276} INFO - 21/05/15 16:23:03 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 2664 ms on c776c6fed6bc (executor driver) (28/200)
[2021-05-15 13:23:03,113] {docker.py:276} INFO - 21/05/15 16:23:03 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:03,114] {docker.py:276} INFO - 21/05/15 16:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:03,114] {docker.py:276} INFO - 21/05/15 16:23:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:03,114] {docker.py:276} INFO - 21/05/15 16:23:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:03,115] {docker.py:276} INFO - 21/05/15 16:23:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261448927061620923765_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261448927061620923765_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261448927061620923765_0004}; taskId=attempt_202105151622261448927061620923765_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@125f95da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:03,115] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Starting: Task committer attempt_202105151622261448927061620923765_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261448927061620923765_0004_m_000030_318
[2021-05-15 13:23:03,117] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Task committer attempt_202105151622261448927061620923765_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261448927061620923765_0004_m_000030_318 : duration 0:00.006s
[2021-05-15 13:23:03,119] {docker.py:276} INFO - 21/05/15 16:23:03 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:03,119] {docker.py:276} INFO - 21/05/15 16:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-15 13:23:03,123] {docker.py:276} INFO - 21/05/15 16:23:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:03,123] {docker.py:276} INFO - 21/05/15 16:23:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226970207667133977600_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226970207667133977600_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226970207667133977600_0004}; taskId=attempt_20210515162226970207667133977600_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39654079}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:03 INFO StagingCommitter: Starting: Task committer attempt_20210515162226970207667133977600_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226970207667133977600_0004_m_000031_319
[2021-05-15 13:23:03,128] {docker.py:276} INFO - 21/05/15 16:23:03 INFO StagingCommitter: Task committer attempt_20210515162226970207667133977600_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226970207667133977600_0004_m_000031_319 : duration 0:00.005s
[2021-05-15 13:23:05,535] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622267706665543822346127_0004_m_000028_316: needsTaskCommit() Task attempt_202105151622267706665543822346127_0004_m_000028_316
[2021-05-15 13:23:05,537] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_202105151622267706665543822346127_0004_m_000028_316: needsTaskCommit() Task attempt_202105151622267706665543822346127_0004_m_000028_316: duration 0:00.002s
[2021-05-15 13:23:05,537] {docker.py:276} INFO - 21/05/15 16:23:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267706665543822346127_0004_m_000028_316
[2021-05-15 13:23:05,539] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 4587 bytes result sent to driver
[2021-05-15 13:23:05,541] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (c776c6fed6bc, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:05,542] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-15 13:23:05,543] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 2707 ms on c776c6fed6bc (executor driver) (29/200)
[2021-05-15 13:23:05,555] {docker.py:276} INFO - 21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:05,558] {docker.py:276} INFO - 21/05/15 16:23:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:05,558] {docker.py:276} INFO - 21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269014967061390385446_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269014967061390385446_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269014967061390385446_0004}; taskId=attempt_202105151622269014967061390385446_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ee0d9d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622269014967061390385446_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269014967061390385446_0004_m_000032_320
[2021-05-15 13:23:05,561] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_202105151622269014967061390385446_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269014967061390385446_0004_m_000032_320 : duration 0:00.004s
[2021-05-15 13:23:05,679] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_20210515162226358175133649533783_0004_m_000029_317: needsTaskCommit() Task attempt_20210515162226358175133649533783_0004_m_000029_317
[2021-05-15 13:23:05,680] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_20210515162226358175133649533783_0004_m_000029_317: needsTaskCommit() Task attempt_20210515162226358175133649533783_0004_m_000029_317: duration 0:00.001s
[2021-05-15 13:23:05,680] {docker.py:276} INFO - 21/05/15 16:23:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226358175133649533783_0004_m_000029_317
[2021-05-15 13:23:05,683] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 4587 bytes result sent to driver
[2021-05-15 13:23:05,684] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (c776c6fed6bc, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:05,684] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 2836 ms on c776c6fed6bc (executor driver) (30/200)
[2021-05-15 13:23:05,686] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-15 13:23:05,696] {docker.py:276} INFO - 21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:05,697] {docker.py:276} INFO - 21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:05,700] {docker.py:276} INFO - 21/05/15 16:23:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:05,700] {docker.py:276} INFO - 21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:05,701] {docker.py:276} INFO - 21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264164659163960529192_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264164659163960529192_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264164659163960529192_0004}; taskId=attempt_202105151622264164659163960529192_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d476ff7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:05,701] {docker.py:276} INFO - 21/05/15 16:23:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:05,702] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622264164659163960529192_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264164659163960529192_0004_m_000033_321
[2021-05-15 13:23:05,705] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_202105151622264164659163960529192_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264164659163960529192_0004_m_000033_321 : duration 0:00.004s
[2021-05-15 13:23:05,766] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622261448927061620923765_0004_m_000030_318: needsTaskCommit() Task attempt_202105151622261448927061620923765_0004_m_000030_318
[2021-05-15 13:23:05,767] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_202105151622261448927061620923765_0004_m_000030_318: needsTaskCommit() Task attempt_202105151622261448927061620923765_0004_m_000030_318: duration 0:00.001s
[2021-05-15 13:23:05,767] {docker.py:276} INFO - 21/05/15 16:23:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261448927061620923765_0004_m_000030_318
[2021-05-15 13:23:05,767] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 4587 bytes result sent to driver
[2021-05-15 13:23:05,769] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (c776c6fed6bc, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:05,770] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 2683 ms on c776c6fed6bc (executor driver) (31/200)
[2021-05-15 13:23:05,770] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
[2021-05-15 13:23:05,778] {docker.py:276} INFO - 21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:05,780] {docker.py:276} INFO - 21/05/15 16:23:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:05,781] {docker.py:276} INFO - 21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263035230893590528274_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263035230893590528274_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263035230893590528274_0004}; taskId=attempt_202105151622263035230893590528274_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e2080bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:05,781] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622263035230893590528274_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263035230893590528274_0004_m_000034_322
[2021-05-15 13:23:05,785] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_202105151622263035230893590528274_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263035230893590528274_0004_m_000034_322 : duration 0:00.005s
[2021-05-15 13:23:05,938] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_20210515162226970207667133977600_0004_m_000031_319: needsTaskCommit() Task attempt_20210515162226970207667133977600_0004_m_000031_319
[2021-05-15 13:23:05,939] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_20210515162226970207667133977600_0004_m_000031_319: needsTaskCommit() Task attempt_20210515162226970207667133977600_0004_m_000031_319: duration 0:00.002s
[2021-05-15 13:23:05,939] {docker.py:276} INFO - 21/05/15 16:23:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226970207667133977600_0004_m_000031_319
[2021-05-15 13:23:05,940] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 4587 bytes result sent to driver
[2021-05-15 13:23:05,941] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (c776c6fed6bc, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:05,941] {docker.py:276} INFO - 21/05/15 16:23:05 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
[2021-05-15 13:23:05,942] {docker.py:276} INFO - 21/05/15 16:23:05 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 2846 ms on c776c6fed6bc (executor driver) (32/200)
[2021-05-15 13:23:05,956] {docker.py:276} INFO - 21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:05,958] {docker.py:276} INFO - 21/05/15 16:23:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:05,959] {docker.py:276} INFO - 21/05/15 16:23:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267562820760998959932_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267562820760998959932_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267562820760998959932_0004}; taskId=attempt_202105151622267562820760998959932_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1323e869}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:05,960] {docker.py:276} INFO - 21/05/15 16:23:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622267562820760998959932_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267562820760998959932_0004_m_000035_323
[2021-05-15 13:23:05,963] {docker.py:276} INFO - 21/05/15 16:23:05 INFO StagingCommitter: Task committer attempt_202105151622267562820760998959932_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267562820760998959932_0004_m_000035_323 : duration 0:00.004s
[2021-05-15 13:23:08,208] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622269014967061390385446_0004_m_000032_320: needsTaskCommit() Task attempt_202105151622269014967061390385446_0004_m_000032_320
[2021-05-15 13:23:08,210] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622269014967061390385446_0004_m_000032_320: needsTaskCommit() Task attempt_202105151622269014967061390385446_0004_m_000032_320: duration 0:00.001s
21/05/15 16:23:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269014967061390385446_0004_m_000032_320
[2021-05-15 13:23:08,212] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 4544 bytes result sent to driver
[2021-05-15 13:23:08,214] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (c776c6fed6bc, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:08,215] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-15 13:23:08,216] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 2677 ms on c776c6fed6bc (executor driver) (33/200)
[2021-05-15 13:23:08,224] {docker.py:276} INFO - 21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:08,227] {docker.py:276} INFO - 21/05/15 16:23:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:08,227] {docker.py:276} INFO - 21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226231274217029650931_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226231274217029650931_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226231274217029650931_0004}; taskId=attempt_20210515162226231274217029650931_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@335c1052}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_20210515162226231274217029650931_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226231274217029650931_0004_m_000036_324
[2021-05-15 13:23:08,231] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_20210515162226231274217029650931_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226231274217029650931_0004_m_000036_324 : duration 0:00.004s
[2021-05-15 13:23:08,364] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622263035230893590528274_0004_m_000034_322: needsTaskCommit() Task attempt_202105151622263035230893590528274_0004_m_000034_322
[2021-05-15 13:23:08,365] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622263035230893590528274_0004_m_000034_322: needsTaskCommit() Task attempt_202105151622263035230893590528274_0004_m_000034_322: duration 0:00.001s
21/05/15 16:23:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263035230893590528274_0004_m_000034_322
[2021-05-15 13:23:08,368] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 4544 bytes result sent to driver
[2021-05-15 13:23:08,370] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (c776c6fed6bc, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:08,371] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 2605 ms on c776c6fed6bc (executor driver) (34/200)
[2021-05-15 13:23:08,372] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
[2021-05-15 13:23:08,382] {docker.py:276} INFO - 21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:08,385] {docker.py:276} INFO - 21/05/15 16:23:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:08,386] {docker.py:276} INFO - 21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264296986012084632321_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264296986012084632321_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264296986012084632321_0004}; taskId=attempt_202105151622264296986012084632321_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ce4d87e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:08,386] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622264296986012084632321_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264296986012084632321_0004_m_000037_325
[2021-05-15 13:23:08,389] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622264296986012084632321_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264296986012084632321_0004_m_000037_325 : duration 0:00.004s
[2021-05-15 13:23:08,398] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622264164659163960529192_0004_m_000033_321: needsTaskCommit() Task attempt_202105151622264164659163960529192_0004_m_000033_321
[2021-05-15 13:23:08,399] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622264164659163960529192_0004_m_000033_321: needsTaskCommit() Task attempt_202105151622264164659163960529192_0004_m_000033_321: duration 0:00.001s
21/05/15 16:23:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264164659163960529192_0004_m_000033_321
[2021-05-15 13:23:08,401] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 4544 bytes result sent to driver
[2021-05-15 13:23:08,402] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (c776c6fed6bc, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:08,403] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 2722 ms on c776c6fed6bc (executor driver) (35/200)
[2021-05-15 13:23:08,403] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
[2021-05-15 13:23:08,411] {docker.py:276} INFO - 21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:08,413] {docker.py:276} INFO - 21/05/15 16:23:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:08,414] {docker.py:276} INFO - 21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:08,414] {docker.py:276} INFO - 21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264777950609586290916_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264777950609586290916_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264777950609586290916_0004}; taskId=attempt_202105151622264777950609586290916_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@274f8791}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622264777950609586290916_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264777950609586290916_0004_m_000038_326
[2021-05-15 13:23:08,417] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622264777950609586290916_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264777950609586290916_0004_m_000038_326 : duration 0:00.003s
[2021-05-15 13:23:08,585] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622267562820760998959932_0004_m_000035_323: needsTaskCommit() Task attempt_202105151622267562820760998959932_0004_m_000035_323
[2021-05-15 13:23:08,586] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622267562820760998959932_0004_m_000035_323: needsTaskCommit() Task attempt_202105151622267562820760998959932_0004_m_000035_323: duration 0:00.001s
[2021-05-15 13:23:08,587] {docker.py:276} INFO - 21/05/15 16:23:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267562820760998959932_0004_m_000035_323
[2021-05-15 13:23:08,589] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 4544 bytes result sent to driver
[2021-05-15 13:23:08,590] {docker.py:276} INFO - 21/05/15 16:23:08 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (c776c6fed6bc, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:08,592] {docker.py:276} INFO - 21/05/15 16:23:08 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
21/05/15 16:23:08 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 2654 ms on c776c6fed6bc (executor driver) (36/200)
[2021-05-15 13:23:08,608] {docker.py:276} INFO - 21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:08,611] {docker.py:276} INFO - 21/05/15 16:23:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:08,612] {docker.py:276} INFO - 21/05/15 16:23:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261839145287191520507_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261839145287191520507_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261839145287191520507_0004}; taskId=attempt_202105151622261839145287191520507_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ecc7bcb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:08,612] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622261839145287191520507_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261839145287191520507_0004_m_000039_327
[2021-05-15 13:23:08,618] {docker.py:276} INFO - 21/05/15 16:23:08 INFO StagingCommitter: Task committer attempt_202105151622261839145287191520507_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261839145287191520507_0004_m_000039_327 : duration 0:00.006s
[2021-05-15 13:23:10,792] {docker.py:276} INFO - 21/05/15 16:23:10 INFO StagingCommitter: Starting: Task committer attempt_20210515162226231274217029650931_0004_m_000036_324: needsTaskCommit() Task attempt_20210515162226231274217029650931_0004_m_000036_324
[2021-05-15 13:23:10,793] {docker.py:276} INFO - 21/05/15 16:23:10 INFO StagingCommitter: Task committer attempt_20210515162226231274217029650931_0004_m_000036_324: needsTaskCommit() Task attempt_20210515162226231274217029650931_0004_m_000036_324: duration 0:00.001s
21/05/15 16:23:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226231274217029650931_0004_m_000036_324
[2021-05-15 13:23:10,795] {docker.py:276} INFO - 21/05/15 16:23:10 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-15 13:23:10,796] {docker.py:276} INFO - 21/05/15 16:23:10 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (c776c6fed6bc, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:10,796] {docker.py:276} INFO - 21/05/15 16:23:10 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 2587 ms on c776c6fed6bc (executor driver) (37/200)
[2021-05-15 13:23:10,797] {docker.py:276} INFO - 21/05/15 16:23:10 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
[2021-05-15 13:23:10,806] {docker.py:276} INFO - 21/05/15 16:23:10 INFO ShuffleBlockFetcherIterator: Getting 5 (22.3 KiB) non-empty blocks including 5 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:10,810] {docker.py:276} INFO - 21/05/15 16:23:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:10,810] {docker.py:276} INFO - 21/05/15 16:23:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263018904963167394293_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263018904963167394293_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263018904963167394293_0004}; taskId=attempt_202105151622263018904963167394293_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43e6907}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622263018904963167394293_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263018904963167394293_0004_m_000040_328
[2021-05-15 13:23:10,813] {docker.py:276} INFO - 21/05/15 16:23:10 INFO StagingCommitter: Task committer attempt_202105151622263018904963167394293_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263018904963167394293_0004_m_000040_328 : duration 0:00.003s
[2021-05-15 13:23:10,962] {docker.py:276} INFO - 21/05/15 16:23:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622264296986012084632321_0004_m_000037_325: needsTaskCommit() Task attempt_202105151622264296986012084632321_0004_m_000037_325
[2021-05-15 13:23:10,963] {docker.py:276} INFO - 21/05/15 16:23:10 INFO StagingCommitter: Task committer attempt_202105151622264296986012084632321_0004_m_000037_325: needsTaskCommit() Task attempt_202105151622264296986012084632321_0004_m_000037_325: duration 0:00.001s
21/05/15 16:23:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264296986012084632321_0004_m_000037_325
[2021-05-15 13:23:10,964] {docker.py:276} INFO - 21/05/15 16:23:10 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
[2021-05-15 13:23:10,965] {docker.py:276} INFO - 21/05/15 16:23:10 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (c776c6fed6bc, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:10,967] {docker.py:276} INFO - 21/05/15 16:23:10 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-15 13:23:10,968] {docker.py:276} INFO - 21/05/15 16:23:10 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 2601 ms on c776c6fed6bc (executor driver) (38/200)
[2021-05-15 13:23:10,978] {docker.py:276} INFO - 21/05/15 16:23:10 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:10,981] {docker.py:276} INFO - 21/05/15 16:23:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:10,982] {docker.py:276} INFO - 21/05/15 16:23:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265200228459531364115_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265200228459531364115_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265200228459531364115_0004}; taskId=attempt_202105151622265200228459531364115_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4828a5ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:10,982] {docker.py:276} INFO - 21/05/15 16:23:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622265200228459531364115_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265200228459531364115_0004_m_000041_329
[2021-05-15 13:23:10,986] {docker.py:276} INFO - 21/05/15 16:23:10 INFO StagingCommitter: Task committer attempt_202105151622265200228459531364115_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265200228459531364115_0004_m_000041_329 : duration 0:00.005s
[2021-05-15 13:23:11,255] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Starting: Task committer attempt_202105151622264777950609586290916_0004_m_000038_326: needsTaskCommit() Task attempt_202105151622264777950609586290916_0004_m_000038_326
[2021-05-15 13:23:11,256] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Task committer attempt_202105151622264777950609586290916_0004_m_000038_326: needsTaskCommit() Task attempt_202105151622264777950609586290916_0004_m_000038_326: duration 0:00.002s
[2021-05-15 13:23:11,257] {docker.py:276} INFO - 21/05/15 16:23:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264777950609586290916_0004_m_000038_326
[2021-05-15 13:23:11,258] {docker.py:276} INFO - 21/05/15 16:23:11 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 4544 bytes result sent to driver
[2021-05-15 13:23:11,260] {docker.py:276} INFO - 21/05/15 16:23:11 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (c776c6fed6bc, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:11,261] {docker.py:276} INFO - 21/05/15 16:23:11 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
[2021-05-15 13:23:11,262] {docker.py:276} INFO - 21/05/15 16:23:11 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 2862 ms on c776c6fed6bc (executor driver) (39/200)
[2021-05-15 13:23:11,272] {docker.py:276} INFO - 21/05/15 16:23:11 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:11,275] {docker.py:276} INFO - 21/05/15 16:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:11,275] {docker.py:276} INFO - 21/05/15 16:23:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:11,276] {docker.py:276} INFO - 21/05/15 16:23:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269101371472678980391_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269101371472678980391_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269101371472678980391_0004}; taskId=attempt_202105151622269101371472678980391_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63973036}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:11,276] {docker.py:276} INFO - 21/05/15 16:23:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:11,276] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Starting: Task committer attempt_202105151622269101371472678980391_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269101371472678980391_0004_m_000042_330
[2021-05-15 13:23:11,279] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Task committer attempt_202105151622269101371472678980391_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269101371472678980391_0004_m_000042_330 : duration 0:00.004s
[2021-05-15 13:23:11,292] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Starting: Task committer attempt_202105151622261839145287191520507_0004_m_000039_327: needsTaskCommit() Task attempt_202105151622261839145287191520507_0004_m_000039_327
[2021-05-15 13:23:11,292] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Task committer attempt_202105151622261839145287191520507_0004_m_000039_327: needsTaskCommit() Task attempt_202105151622261839145287191520507_0004_m_000039_327: duration 0:00.000s
21/05/15 16:23:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261839145287191520507_0004_m_000039_327
[2021-05-15 13:23:11,293] {docker.py:276} INFO - 21/05/15 16:23:11 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 4544 bytes result sent to driver
[2021-05-15 13:23:11,294] {docker.py:276} INFO - 21/05/15 16:23:11 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (c776c6fed6bc, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:11,295] {docker.py:276} INFO - 21/05/15 16:23:11 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 2708 ms on c776c6fed6bc (executor driver) (40/200)
[2021-05-15 13:23:11,295] {docker.py:276} INFO - 21/05/15 16:23:11 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
[2021-05-15 13:23:11,304] {docker.py:276} INFO - 21/05/15 16:23:11 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:11,306] {docker.py:276} INFO - 21/05/15 16:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267398971801474692211_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267398971801474692211_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267398971801474692211_0004}; taskId=attempt_202105151622267398971801474692211_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67d768c1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:11,307] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Starting: Task committer attempt_202105151622267398971801474692211_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267398971801474692211_0004_m_000043_331
[2021-05-15 13:23:11,314] {docker.py:276} INFO - 21/05/15 16:23:11 INFO StagingCommitter: Task committer attempt_202105151622267398971801474692211_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267398971801474692211_0004_m_000043_331 : duration 0:00.007s
[2021-05-15 13:23:13,401] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622263018904963167394293_0004_m_000040_328: needsTaskCommit() Task attempt_202105151622263018904963167394293_0004_m_000040_328
[2021-05-15 13:23:13,405] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622263018904963167394293_0004_m_000040_328: needsTaskCommit() Task attempt_202105151622263018904963167394293_0004_m_000040_328: duration 0:00.002s
21/05/15 16:23:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263018904963167394293_0004_m_000040_328
[2021-05-15 13:23:13,405] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 4544 bytes result sent to driver
[2021-05-15 13:23:13,406] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (c776c6fed6bc, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:13,409] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 2613 ms on c776c6fed6bc (executor driver) (41/200)
21/05/15 16:23:13 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-15 13:23:13,417] {docker.py:276} INFO - 21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:13,417] {docker.py:276} INFO - 21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:13,419] {docker.py:276} INFO - 21/05/15 16:23:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266438167344264916276_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266438167344264916276_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266438167344264916276_0004}; taskId=attempt_202105151622266438167344264916276_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77565f3d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622266438167344264916276_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266438167344264916276_0004_m_000044_332
[2021-05-15 13:23:13,423] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622266438167344264916276_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266438167344264916276_0004_m_000044_332 : duration 0:00.003s
[2021-05-15 13:23:13,600] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622265200228459531364115_0004_m_000041_329: needsTaskCommit() Task attempt_202105151622265200228459531364115_0004_m_000041_329
[2021-05-15 13:23:13,601] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622265200228459531364115_0004_m_000041_329: needsTaskCommit() Task attempt_202105151622265200228459531364115_0004_m_000041_329: duration 0:00.001s
21/05/15 16:23:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265200228459531364115_0004_m_000041_329
[2021-05-15 13:23:13,602] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 4544 bytes result sent to driver
[2021-05-15 13:23:13,604] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (c776c6fed6bc, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:13,605] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 2643 ms on c776c6fed6bc (executor driver) (42/200)
[2021-05-15 13:23:13,607] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-15 13:23:13,638] {docker.py:276} INFO - 21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:13,642] {docker.py:276} INFO - 21/05/15 16:23:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:13,643] {docker.py:276} INFO - 21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265428014534184569780_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265428014534184569780_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265428014534184569780_0004}; taskId=attempt_202105151622265428014534184569780_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6558895d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:13,643] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622265428014534184569780_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265428014534184569780_0004_m_000045_333
[2021-05-15 13:23:13,647] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622265428014534184569780_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265428014534184569780_0004_m_000045_333 : duration 0:00.005s
[2021-05-15 13:23:13,915] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622269101371472678980391_0004_m_000042_330: needsTaskCommit() Task attempt_202105151622269101371472678980391_0004_m_000042_330
[2021-05-15 13:23:13,916] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622269101371472678980391_0004_m_000042_330: needsTaskCommit() Task attempt_202105151622269101371472678980391_0004_m_000042_330: duration 0:00.001s
[2021-05-15 13:23:13,917] {docker.py:276} INFO - 21/05/15 16:23:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269101371472678980391_0004_m_000042_330
[2021-05-15 13:23:13,919] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 4587 bytes result sent to driver
[2021-05-15 13:23:13,920] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (c776c6fed6bc, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:13,921] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-15 13:23:13,922] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 2666 ms on c776c6fed6bc (executor driver) (43/200)
[2021-05-15 13:23:13,932] {docker.py:276} INFO - 21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Getting 5 (24.3 KiB) non-empty blocks including 5 (24.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:13,935] {docker.py:276} INFO - 21/05/15 16:23:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261203421959898610614_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261203421959898610614_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261203421959898610614_0004}; taskId=attempt_202105151622261203421959898610614_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bf1e15b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:13,937] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622261203421959898610614_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261203421959898610614_0004_m_000046_334
[2021-05-15 13:23:13,940] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622261203421959898610614_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261203421959898610614_0004_m_000046_334 : duration 0:00.005s
[2021-05-15 13:23:13,958] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622267398971801474692211_0004_m_000043_331: needsTaskCommit() Task attempt_202105151622267398971801474692211_0004_m_000043_331
[2021-05-15 13:23:13,959] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622267398971801474692211_0004_m_000043_331: needsTaskCommit() Task attempt_202105151622267398971801474692211_0004_m_000043_331: duration 0:00.000s
21/05/15 16:23:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267398971801474692211_0004_m_000043_331
[2021-05-15 13:23:13,960] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 4587 bytes result sent to driver
[2021-05-15 13:23:13,961] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (c776c6fed6bc, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:13,962] {docker.py:276} INFO - 21/05/15 16:23:13 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-15 13:23:13,962] {docker.py:276} INFO - 21/05/15 16:23:13 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 2672 ms on c776c6fed6bc (executor driver) (44/200)
[2021-05-15 13:23:13,970] {docker.py:276} INFO - 21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:13,972] {docker.py:276} INFO - 21/05/15 16:23:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:13,972] {docker.py:276} INFO - 21/05/15 16:23:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261825029959185980219_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261825029959185980219_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261825029959185980219_0004}; taskId=attempt_202105151622261825029959185980219_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d8cedf8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:13,972] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622261825029959185980219_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261825029959185980219_0004_m_000047_335
[2021-05-15 13:23:13,975] {docker.py:276} INFO - 21/05/15 16:23:13 INFO StagingCommitter: Task committer attempt_202105151622261825029959185980219_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261825029959185980219_0004_m_000047_335 : duration 0:00.003s
[2021-05-15 13:23:16,213] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622266438167344264916276_0004_m_000044_332: needsTaskCommit() Task attempt_202105151622266438167344264916276_0004_m_000044_332
[2021-05-15 13:23:16,214] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622266438167344264916276_0004_m_000044_332: needsTaskCommit() Task attempt_202105151622266438167344264916276_0004_m_000044_332: duration 0:00.001s
21/05/15 16:23:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266438167344264916276_0004_m_000044_332
[2021-05-15 13:23:16,216] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 4587 bytes result sent to driver
[2021-05-15 13:23:16,219] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (c776c6fed6bc, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:16,219] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
[2021-05-15 13:23:16,220] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 2819 ms on c776c6fed6bc (executor driver) (45/200)
[2021-05-15 13:23:16,230] {docker.py:276} INFO - 21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:16,232] {docker.py:276} INFO - 21/05/15 16:23:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:16,232] {docker.py:276} INFO - 21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262265379089659568866_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262265379089659568866_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262265379089659568866_0004}; taskId=attempt_202105151622262265379089659568866_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59704862}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622262265379089659568866_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262265379089659568866_0004_m_000048_336
[2021-05-15 13:23:16,236] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622262265379089659568866_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262265379089659568866_0004_m_000048_336 : duration 0:00.004s
[2021-05-15 13:23:16,320] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622265428014534184569780_0004_m_000045_333: needsTaskCommit() Task attempt_202105151622265428014534184569780_0004_m_000045_333
21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622265428014534184569780_0004_m_000045_333: needsTaskCommit() Task attempt_202105151622265428014534184569780_0004_m_000045_333: duration 0:00.000s
[2021-05-15 13:23:16,320] {docker.py:276} INFO - 21/05/15 16:23:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265428014534184569780_0004_m_000045_333
[2021-05-15 13:23:16,321] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 4587 bytes result sent to driver
[2021-05-15 13:23:16,323] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (c776c6fed6bc, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:16,324] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 2724 ms on c776c6fed6bc (executor driver) (46/200)
[2021-05-15 13:23:16,325] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-15 13:23:16,333] {docker.py:276} INFO - 21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:16,334] {docker.py:276} INFO - 21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:16,336] {docker.py:276} INFO - 21/05/15 16:23:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:16,337] {docker.py:276} INFO - 21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264933453431127230431_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264933453431127230431_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264933453431127230431_0004}; taskId=attempt_202105151622264933453431127230431_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37b32b22}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622264933453431127230431_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264933453431127230431_0004_m_000049_337
[2021-05-15 13:23:16,340] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622264933453431127230431_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264933453431127230431_0004_m_000049_337 : duration 0:00.003s
[2021-05-15 13:23:16,533] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622261203421959898610614_0004_m_000046_334: needsTaskCommit() Task attempt_202105151622261203421959898610614_0004_m_000046_334
[2021-05-15 13:23:16,535] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622261203421959898610614_0004_m_000046_334: needsTaskCommit() Task attempt_202105151622261203421959898610614_0004_m_000046_334: duration 0:00.001s
[2021-05-15 13:23:16,535] {docker.py:276} INFO - 21/05/15 16:23:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261203421959898610614_0004_m_000046_334
[2021-05-15 13:23:16,536] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 4544 bytes result sent to driver
[2021-05-15 13:23:16,538] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (c776c6fed6bc, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:16,540] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 2621 ms on c776c6fed6bc (executor driver) (47/200)
[2021-05-15 13:23:16,540] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-15 13:23:16,551] {docker.py:276} INFO - 21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:16,554] {docker.py:276} INFO - 21/05/15 16:23:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269010410841407153700_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269010410841407153700_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269010410841407153700_0004}; taskId=attempt_202105151622269010410841407153700_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c03a850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:16,554] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622269010410841407153700_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269010410841407153700_0004_m_000050_338
[2021-05-15 13:23:16,558] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622269010410841407153700_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269010410841407153700_0004_m_000050_338 : duration 0:00.004s
[2021-05-15 13:23:16,799] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622261825029959185980219_0004_m_000047_335: needsTaskCommit() Task attempt_202105151622261825029959185980219_0004_m_000047_335
21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622261825029959185980219_0004_m_000047_335: needsTaskCommit() Task attempt_202105151622261825029959185980219_0004_m_000047_335: duration 0:00.000s
21/05/15 16:23:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261825029959185980219_0004_m_000047_335
[2021-05-15 13:23:16,800] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 4544 bytes result sent to driver
[2021-05-15 13:23:16,801] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (c776c6fed6bc, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:16,802] {docker.py:276} INFO - 21/05/15 16:23:16 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-15 13:23:16,803] {docker.py:276} INFO - 21/05/15 16:23:16 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 2845 ms on c776c6fed6bc (executor driver) (48/200)
[2021-05-15 13:23:16,815] {docker.py:276} INFO - 21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:16,818] {docker.py:276} INFO - 21/05/15 16:23:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261100567730107103695_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261100567730107103695_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261100567730107103695_0004}; taskId=attempt_202105151622261100567730107103695_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c1205e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:16,819] {docker.py:276} INFO - 21/05/15 16:23:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622261100567730107103695_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261100567730107103695_0004_m_000051_339
[2021-05-15 13:23:16,823] {docker.py:276} INFO - 21/05/15 16:23:16 INFO StagingCommitter: Task committer attempt_202105151622261100567730107103695_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261100567730107103695_0004_m_000051_339 : duration 0:00.005s
[2021-05-15 13:23:18,817] {docker.py:276} INFO - 21/05/15 16:23:18 INFO StagingCommitter: Starting: Task committer attempt_202105151622262265379089659568866_0004_m_000048_336: needsTaskCommit() Task attempt_202105151622262265379089659568866_0004_m_000048_336
[2021-05-15 13:23:18,818] {docker.py:276} INFO - 21/05/15 16:23:18 INFO StagingCommitter: Task committer attempt_202105151622262265379089659568866_0004_m_000048_336: needsTaskCommit() Task attempt_202105151622262265379089659568866_0004_m_000048_336: duration 0:00.000s
[2021-05-15 13:23:18,818] {docker.py:276} INFO - 21/05/15 16:23:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262265379089659568866_0004_m_000048_336
[2021-05-15 13:23:18,820] {docker.py:276} INFO - 21/05/15 16:23:18 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 4544 bytes result sent to driver
[2021-05-15 13:23:18,821] {docker.py:276} INFO - 21/05/15 16:23:18 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (c776c6fed6bc, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:18,822] {docker.py:276} INFO - 21/05/15 16:23:18 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 2609 ms on c776c6fed6bc (executor driver) (49/200)
21/05/15 16:23:18 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
[2021-05-15 13:23:18,831] {docker.py:276} INFO - 21/05/15 16:23:18 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:18,834] {docker.py:276} INFO - 21/05/15 16:23:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:18,835] {docker.py:276} INFO - 21/05/15 16:23:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226192196933043610900_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226192196933043610900_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226192196933043610900_0004}; taskId=attempt_20210515162226192196933043610900_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ef47bb2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:18 INFO StagingCommitter: Starting: Task committer attempt_20210515162226192196933043610900_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226192196933043610900_0004_m_000052_340
[2021-05-15 13:23:18,837] {docker.py:276} INFO - 21/05/15 16:23:18 INFO StagingCommitter: Task committer attempt_20210515162226192196933043610900_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226192196933043610900_0004_m_000052_340 : duration 0:00.003s
[2021-05-15 13:23:19,051] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622264933453431127230431_0004_m_000049_337: needsTaskCommit() Task attempt_202105151622264933453431127230431_0004_m_000049_337
21/05/15 16:23:19 INFO StagingCommitter: Task committer attempt_202105151622264933453431127230431_0004_m_000049_337: needsTaskCommit() Task attempt_202105151622264933453431127230431_0004_m_000049_337: duration 0:00.000s
[2021-05-15 13:23:19,053] {docker.py:276} INFO - 21/05/15 16:23:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264933453431127230431_0004_m_000049_337
[2021-05-15 13:23:19,061] {docker.py:276} INFO - 21/05/15 16:23:19 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
21/05/15 16:23:19 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (c776c6fed6bc, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:19,062] {docker.py:276} INFO - 21/05/15 16:23:19 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 2737 ms on c776c6fed6bc (executor driver) (50/200)
21/05/15 16:23:19 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-15 13:23:19,079] {docker.py:276} INFO - 21/05/15 16:23:19 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:19,079] {docker.py:276} INFO - 21/05/15 16:23:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261524589005384002637_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261524589005384002637_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261524589005384002637_0004}; taskId=attempt_202105151622261524589005384002637_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cf0dfba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622261524589005384002637_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261524589005384002637_0004_m_000053_341
[2021-05-15 13:23:19,083] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Task committer attempt_202105151622261524589005384002637_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261524589005384002637_0004_m_000053_341 : duration 0:00.006s
[2021-05-15 13:23:19,221] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622269010410841407153700_0004_m_000050_338: needsTaskCommit() Task attempt_202105151622269010410841407153700_0004_m_000050_338
21/05/15 16:23:19 INFO StagingCommitter: Task committer attempt_202105151622269010410841407153700_0004_m_000050_338: needsTaskCommit() Task attempt_202105151622269010410841407153700_0004_m_000050_338: duration 0:00.001s
[2021-05-15 13:23:19,221] {docker.py:276} INFO - 21/05/15 16:23:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269010410841407153700_0004_m_000050_338
[2021-05-15 13:23:19,223] {docker.py:276} INFO - 21/05/15 16:23:19 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-15 13:23:19,225] {docker.py:276} INFO - 21/05/15 16:23:19 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (c776c6fed6bc, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:19,227] {docker.py:276} INFO - 21/05/15 16:23:19 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-15 13:23:19,228] {docker.py:276} INFO - 21/05/15 16:23:19 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 2694 ms on c776c6fed6bc (executor driver) (51/200)
[2021-05-15 13:23:19,237] {docker.py:276} INFO - 21/05/15 16:23:19 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:19,240] {docker.py:276} INFO - 21/05/15 16:23:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261633822416441695665_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261633822416441695665_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261633822416441695665_0004}; taskId=attempt_202105151622261633822416441695665_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4af7c948}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:19,240] {docker.py:276} INFO - 21/05/15 16:23:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622261633822416441695665_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261633822416441695665_0004_m_000054_342
[2021-05-15 13:23:19,244] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Task committer attempt_202105151622261633822416441695665_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261633822416441695665_0004_m_000054_342 : duration 0:00.005s
[2021-05-15 13:23:19,519] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622261100567730107103695_0004_m_000051_339: needsTaskCommit() Task attempt_202105151622261100567730107103695_0004_m_000051_339
[2021-05-15 13:23:19,520] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Task committer attempt_202105151622261100567730107103695_0004_m_000051_339: needsTaskCommit() Task attempt_202105151622261100567730107103695_0004_m_000051_339: duration 0:00.001s
21/05/15 16:23:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261100567730107103695_0004_m_000051_339
[2021-05-15 13:23:19,522] {docker.py:276} INFO - 21/05/15 16:23:19 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 4544 bytes result sent to driver
[2021-05-15 13:23:19,523] {docker.py:276} INFO - 21/05/15 16:23:19 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (c776c6fed6bc, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:19,524] {docker.py:276} INFO - 21/05/15 16:23:19 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-15 13:23:19,525] {docker.py:276} INFO - 21/05/15 16:23:19 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 2727 ms on c776c6fed6bc (executor driver) (52/200)
[2021-05-15 13:23:19,535] {docker.py:276} INFO - 21/05/15 16:23:19 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:19,536] {docker.py:276} INFO - 21/05/15 16:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:19,538] {docker.py:276} INFO - 21/05/15 16:23:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:19,539] {docker.py:276} INFO - 21/05/15 16:23:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:19,540] {docker.py:276} INFO - 21/05/15 16:23:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268948933688396019728_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268948933688396019728_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268948933688396019728_0004}; taskId=attempt_202105151622268948933688396019728_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c649fcb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:19,540] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622268948933688396019728_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268948933688396019728_0004_m_000055_343
[2021-05-15 13:23:19,543] {docker.py:276} INFO - 21/05/15 16:23:19 INFO StagingCommitter: Task committer attempt_202105151622268948933688396019728_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268948933688396019728_0004_m_000055_343 : duration 0:00.004s
[2021-05-15 13:23:21,478] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Starting: Task committer attempt_20210515162226192196933043610900_0004_m_000052_340: needsTaskCommit() Task attempt_20210515162226192196933043610900_0004_m_000052_340
[2021-05-15 13:23:21,479] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Task committer attempt_20210515162226192196933043610900_0004_m_000052_340: needsTaskCommit() Task attempt_20210515162226192196933043610900_0004_m_000052_340: duration 0:00.002s
21/05/15 16:23:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226192196933043610900_0004_m_000052_340
[2021-05-15 13:23:21,481] {docker.py:276} INFO - 21/05/15 16:23:21 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 4544 bytes result sent to driver
[2021-05-15 13:23:21,482] {docker.py:276} INFO - 21/05/15 16:23:21 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (c776c6fed6bc, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:21,483] {docker.py:276} INFO - 21/05/15 16:23:21 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
[2021-05-15 13:23:21,484] {docker.py:276} INFO - 21/05/15 16:23:21 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 2665 ms on c776c6fed6bc (executor driver) (53/200)
[2021-05-15 13:23:21,494] {docker.py:276} INFO - 21/05/15 16:23:21 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:21,494] {docker.py:276} INFO - 21/05/15 16:23:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:21,496] {docker.py:276} INFO - 21/05/15 16:23:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:21,497] {docker.py:276} INFO - 21/05/15 16:23:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:21,498] {docker.py:276} INFO - 21/05/15 16:23:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226263831345830282972_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226263831345830282972_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226263831345830282972_0004}; taskId=attempt_20210515162226263831345830282972_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c902518}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:21,498] {docker.py:276} INFO - 21/05/15 16:23:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:21,498] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Starting: Task committer attempt_20210515162226263831345830282972_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226263831345830282972_0004_m_000056_344
[2021-05-15 13:23:21,500] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Task committer attempt_20210515162226263831345830282972_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226263831345830282972_0004_m_000056_344 : duration 0:00.003s
[2021-05-15 13:23:21,844] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622261633822416441695665_0004_m_000054_342: needsTaskCommit() Task attempt_202105151622261633822416441695665_0004_m_000054_342
[2021-05-15 13:23:21,845] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Task committer attempt_202105151622261633822416441695665_0004_m_000054_342: needsTaskCommit() Task attempt_202105151622261633822416441695665_0004_m_000054_342: duration 0:00.001s
21/05/15 16:23:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261633822416441695665_0004_m_000054_342
[2021-05-15 13:23:21,847] {docker.py:276} INFO - 21/05/15 16:23:21 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-15 13:23:21,848] {docker.py:276} INFO - 21/05/15 16:23:21 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (c776c6fed6bc, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:21,849] {docker.py:276} INFO - 21/05/15 16:23:21 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
21/05/15 16:23:21 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 2627 ms on c776c6fed6bc (executor driver) (54/200)
[2021-05-15 13:23:21,860] {docker.py:276} INFO - 21/05/15 16:23:21 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:21,862] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622261524589005384002637_0004_m_000053_341: needsTaskCommit() Task attempt_202105151622261524589005384002637_0004_m_000053_341
[2021-05-15 13:23:21,863] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Task committer attempt_202105151622261524589005384002637_0004_m_000053_341: needsTaskCommit() Task attempt_202105151622261524589005384002637_0004_m_000053_341: duration 0:00.000s
21/05/15 16:23:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261524589005384002637_0004_m_000053_341
[2021-05-15 13:23:21,864] {docker.py:276} INFO - 21/05/15 16:23:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:21,864] {docker.py:276} INFO - 21/05/15 16:23:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:21 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 4544 bytes result sent to driver
[2021-05-15 13:23:21,865] {docker.py:276} INFO - 21/05/15 16:23:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262133508076272337136_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262133508076272337136_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262133508076272337136_0004}; taskId=attempt_202105151622262133508076272337136_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54c00310}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:21 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (c776c6fed6bc, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:21,866] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622262133508076272337136_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262133508076272337136_0004_m_000057_345
[2021-05-15 13:23:21,866] {docker.py:276} INFO - 21/05/15 16:23:21 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-15 13:23:21,866] {docker.py:276} INFO - 21/05/15 16:23:21 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 2815 ms on c776c6fed6bc (executor driver) (55/200)
[2021-05-15 13:23:21,869] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Task committer attempt_202105151622262133508076272337136_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262133508076272337136_0004_m_000057_345 : duration 0:00.005s
[2021-05-15 13:23:21,875] {docker.py:276} INFO - 21/05/15 16:23:21 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:21,878] {docker.py:276} INFO - 21/05/15 16:23:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:21,878] {docker.py:276} INFO - 21/05/15 16:23:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266825435302356309676_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266825435302356309676_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266825435302356309676_0004}; taskId=attempt_202105151622266825435302356309676_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a2dfe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622266825435302356309676_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266825435302356309676_0004_m_000058_346
[2021-05-15 13:23:21,882] {docker.py:276} INFO - 21/05/15 16:23:21 INFO StagingCommitter: Task committer attempt_202105151622266825435302356309676_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266825435302356309676_0004_m_000058_346 : duration 0:00.004s
[2021-05-15 13:23:22,360] {docker.py:276} INFO - 21/05/15 16:23:22 INFO StagingCommitter: Starting: Task committer attempt_202105151622268948933688396019728_0004_m_000055_343: needsTaskCommit() Task attempt_202105151622268948933688396019728_0004_m_000055_343
[2021-05-15 13:23:22,361] {docker.py:276} INFO - 21/05/15 16:23:22 INFO StagingCommitter: Task committer attempt_202105151622268948933688396019728_0004_m_000055_343: needsTaskCommit() Task attempt_202105151622268948933688396019728_0004_m_000055_343: duration 0:00.001s
21/05/15 16:23:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268948933688396019728_0004_m_000055_343
[2021-05-15 13:23:22,362] {docker.py:276} INFO - 21/05/15 16:23:22 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-15 13:23:22,364] {docker.py:276} INFO - 21/05/15 16:23:22 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (c776c6fed6bc, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:22,365] {docker.py:276} INFO - 21/05/15 16:23:22 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-15 13:23:22,366] {docker.py:276} INFO - 21/05/15 16:23:22 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 2846 ms on c776c6fed6bc (executor driver) (56/200)
[2021-05-15 13:23:22,389] {docker.py:276} INFO - 21/05/15 16:23:22 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:22,391] {docker.py:276} INFO - 21/05/15 16:23:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:22,392] {docker.py:276} INFO - 21/05/15 16:23:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266785310998683136575_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266785310998683136575_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266785310998683136575_0004}; taskId=attempt_202105151622266785310998683136575_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63cbdd45}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:22 INFO StagingCommitter: Starting: Task committer attempt_202105151622266785310998683136575_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266785310998683136575_0004_m_000059_347
[2021-05-15 13:23:22,395] {docker.py:276} INFO - 21/05/15 16:23:22 INFO StagingCommitter: Task committer attempt_202105151622266785310998683136575_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266785310998683136575_0004_m_000059_347 : duration 0:00.003s
[2021-05-15 13:23:24,071] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Starting: Task committer attempt_20210515162226263831345830282972_0004_m_000056_344: needsTaskCommit() Task attempt_20210515162226263831345830282972_0004_m_000056_344
[2021-05-15 13:23:24,072] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Task committer attempt_20210515162226263831345830282972_0004_m_000056_344: needsTaskCommit() Task attempt_20210515162226263831345830282972_0004_m_000056_344: duration 0:00.000s
21/05/15 16:23:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226263831345830282972_0004_m_000056_344
[2021-05-15 13:23:24,074] {docker.py:276} INFO - 21/05/15 16:23:24 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 4587 bytes result sent to driver
[2021-05-15 13:23:24,076] {docker.py:276} INFO - 21/05/15 16:23:24 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (c776c6fed6bc, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:24,077] {docker.py:276} INFO - 21/05/15 16:23:24 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 2598 ms on c776c6fed6bc (executor driver) (57/200)
[2021-05-15 13:23:24,078] {docker.py:276} INFO - 21/05/15 16:23:24 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
[2021-05-15 13:23:24,088] {docker.py:276} INFO - 21/05/15 16:23:24 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:24,091] {docker.py:276} INFO - 21/05/15 16:23:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:24,092] {docker.py:276} INFO - 21/05/15 16:23:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:24,093] {docker.py:276} INFO - 21/05/15 16:23:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266754469621381761829_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266754469621381761829_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266754469621381761829_0004}; taskId=attempt_202105151622266754469621381761829_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4fdf2ac7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622266754469621381761829_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266754469621381761829_0004_m_000060_348
[2021-05-15 13:23:24,097] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Task committer attempt_202105151622266754469621381761829_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266754469621381761829_0004_m_000060_348 : duration 0:00.004s
[2021-05-15 13:23:24,440] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622262133508076272337136_0004_m_000057_345: needsTaskCommit() Task attempt_202105151622262133508076272337136_0004_m_000057_345
21/05/15 16:23:24 INFO StagingCommitter: Task committer attempt_202105151622262133508076272337136_0004_m_000057_345: needsTaskCommit() Task attempt_202105151622262133508076272337136_0004_m_000057_345: duration 0:00.001s
21/05/15 16:23:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262133508076272337136_0004_m_000057_345
[2021-05-15 13:23:24,442] {docker.py:276} INFO - 21/05/15 16:23:24 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 4587 bytes result sent to driver
[2021-05-15 13:23:24,442] {docker.py:276} INFO - 21/05/15 16:23:24 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (c776c6fed6bc, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:24,443] {docker.py:276} INFO - 21/05/15 16:23:24 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 2598 ms on c776c6fed6bc (executor driver) (58/200)
[2021-05-15 13:23:24,444] {docker.py:276} INFO - 21/05/15 16:23:24 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-15 13:23:24,454] {docker.py:276} INFO - 21/05/15 16:23:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:24,457] {docker.py:276} INFO - 21/05/15 16:23:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265091459743528586109_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265091459743528586109_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265091459743528586109_0004}; taskId=attempt_202105151622265091459743528586109_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c55bdda}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:24,457] {docker.py:276} INFO - 21/05/15 16:23:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622265091459743528586109_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265091459743528586109_0004_m_000061_349
[2021-05-15 13:23:24,460] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Task committer attempt_202105151622265091459743528586109_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265091459743528586109_0004_m_000061_349 : duration 0:00.003s
[2021-05-15 13:23:24,588] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622266825435302356309676_0004_m_000058_346: needsTaskCommit() Task attempt_202105151622266825435302356309676_0004_m_000058_346
[2021-05-15 13:23:24,589] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Task committer attempt_202105151622266825435302356309676_0004_m_000058_346: needsTaskCommit() Task attempt_202105151622266825435302356309676_0004_m_000058_346: duration 0:00.001s
21/05/15 16:23:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266825435302356309676_0004_m_000058_346
[2021-05-15 13:23:24,591] {docker.py:276} INFO - 21/05/15 16:23:24 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-15 13:23:24,592] {docker.py:276} INFO - 21/05/15 16:23:24 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (c776c6fed6bc, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:24,593] {docker.py:276} INFO - 21/05/15 16:23:24 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-15 13:23:24,594] {docker.py:276} INFO - 21/05/15 16:23:24 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 2730 ms on c776c6fed6bc (executor driver) (59/200)
[2021-05-15 13:23:24,603] {docker.py:276} INFO - 21/05/15 16:23:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:24,604] {docker.py:276} INFO - 21/05/15 16:23:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:24,608] {docker.py:276} INFO - 21/05/15 16:23:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268867340395558448675_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268867340395558448675_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268867340395558448675_0004}; taskId=attempt_202105151622268867340395558448675_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@434ab055}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:24,608] {docker.py:276} INFO - 21/05/15 16:23:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:24,609] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622268867340395558448675_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268867340395558448675_0004_m_000062_350
[2021-05-15 13:23:24,612] {docker.py:276} INFO - 21/05/15 16:23:24 INFO StagingCommitter: Task committer attempt_202105151622268867340395558448675_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268867340395558448675_0004_m_000062_350 : duration 0:00.005s
[2021-05-15 13:23:25,057] {docker.py:276} INFO - 21/05/15 16:23:25 INFO StagingCommitter: Starting: Task committer attempt_202105151622266785310998683136575_0004_m_000059_347: needsTaskCommit() Task attempt_202105151622266785310998683136575_0004_m_000059_347
[2021-05-15 13:23:25,058] {docker.py:276} INFO - 21/05/15 16:23:25 INFO StagingCommitter: Task committer attempt_202105151622266785310998683136575_0004_m_000059_347: needsTaskCommit() Task attempt_202105151622266785310998683136575_0004_m_000059_347: duration 0:00.001s
21/05/15 16:23:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266785310998683136575_0004_m_000059_347
[2021-05-15 13:23:25,060] {docker.py:276} INFO - 21/05/15 16:23:25 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-15 13:23:25,062] {docker.py:276} INFO - 21/05/15 16:23:25 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (c776c6fed6bc, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:25,063] {docker.py:276} INFO - 21/05/15 16:23:25 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 2702 ms on c776c6fed6bc (executor driver) (60/200)
[2021-05-15 13:23:25,064] {docker.py:276} INFO - 21/05/15 16:23:25 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
[2021-05-15 13:23:25,074] {docker.py:276} INFO - 21/05/15 16:23:25 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:25,074] {docker.py:276} INFO - 21/05/15 16:23:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:25,076] {docker.py:276} INFO - 21/05/15 16:23:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:25,077] {docker.py:276} INFO - 21/05/15 16:23:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226895162777178678517_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226895162777178678517_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226895162777178678517_0004}; taskId=attempt_20210515162226895162777178678517_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ed7c5f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:25,077] {docker.py:276} INFO - 21/05/15 16:23:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:25,077] {docker.py:276} INFO - 21/05/15 16:23:25 INFO StagingCommitter: Starting: Task committer attempt_20210515162226895162777178678517_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226895162777178678517_0004_m_000063_351
[2021-05-15 13:23:25,080] {docker.py:276} INFO - 21/05/15 16:23:25 INFO StagingCommitter: Task committer attempt_20210515162226895162777178678517_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226895162777178678517_0004_m_000063_351 : duration 0:00.003s
[2021-05-15 13:23:26,875] {docker.py:276} INFO - 21/05/15 16:23:26 INFO StagingCommitter: Starting: Task committer attempt_202105151622266754469621381761829_0004_m_000060_348: needsTaskCommit() Task attempt_202105151622266754469621381761829_0004_m_000060_348
[2021-05-15 13:23:26,876] {docker.py:276} INFO - 21/05/15 16:23:26 INFO StagingCommitter: Task committer attempt_202105151622266754469621381761829_0004_m_000060_348: needsTaskCommit() Task attempt_202105151622266754469621381761829_0004_m_000060_348: duration 0:00.001s
[2021-05-15 13:23:26,877] {docker.py:276} INFO - 21/05/15 16:23:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266754469621381761829_0004_m_000060_348
[2021-05-15 13:23:26,880] {docker.py:276} INFO - 21/05/15 16:23:26 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 4544 bytes result sent to driver
[2021-05-15 13:23:26,881] {docker.py:276} INFO - 21/05/15 16:23:26 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (c776c6fed6bc, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:26,881] {docker.py:276} INFO - 21/05/15 16:23:26 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 2810 ms on c776c6fed6bc (executor driver) (61/200)
[2021-05-15 13:23:26,882] {docker.py:276} INFO - 21/05/15 16:23:26 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-15 13:23:26,893] {docker.py:276} INFO - 21/05/15 16:23:26 INFO ShuffleBlockFetcherIterator: Getting 5 (21.0 KiB) non-empty blocks including 5 (21.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:26,896] {docker.py:276} INFO - 21/05/15 16:23:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:26,896] {docker.py:276} INFO - 21/05/15 16:23:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226957380740949118042_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226957380740949118042_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226957380740949118042_0004}; taskId=attempt_20210515162226957380740949118042_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a7e9f3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:26 INFO StagingCommitter: Starting: Task committer attempt_20210515162226957380740949118042_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226957380740949118042_0004_m_000064_352
[2021-05-15 13:23:26,899] {docker.py:276} INFO - 21/05/15 16:23:26 INFO StagingCommitter: Task committer attempt_20210515162226957380740949118042_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226957380740949118042_0004_m_000064_352 : duration 0:00.003s
[2021-05-15 13:23:27,145] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Starting: Task committer attempt_202105151622265091459743528586109_0004_m_000061_349: needsTaskCommit() Task attempt_202105151622265091459743528586109_0004_m_000061_349
[2021-05-15 13:23:27,148] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Task committer attempt_202105151622265091459743528586109_0004_m_000061_349: needsTaskCommit() Task attempt_202105151622265091459743528586109_0004_m_000061_349: duration 0:00.000s
[2021-05-15 13:23:27,148] {docker.py:276} INFO - 21/05/15 16:23:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265091459743528586109_0004_m_000061_349
[2021-05-15 13:23:27,150] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Starting: Task committer attempt_202105151622268867340395558448675_0004_m_000062_350: needsTaskCommit() Task attempt_202105151622268867340395558448675_0004_m_000062_350
21/05/15 16:23:27 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 4544 bytes result sent to driver
[2021-05-15 13:23:27,152] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Task committer attempt_202105151622268867340395558448675_0004_m_000062_350: needsTaskCommit() Task attempt_202105151622268867340395558448675_0004_m_000062_350: duration 0:00.002s
21/05/15 16:23:27 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (c776c6fed6bc, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:23:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268867340395558448675_0004_m_000062_350
[2021-05-15 13:23:27,153] {docker.py:276} INFO - 21/05/15 16:23:27 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-15 13:23:27,153] {docker.py:276} INFO - 21/05/15 16:23:27 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 2714 ms on c776c6fed6bc (executor driver) (62/200)
[2021-05-15 13:23:27,154] {docker.py:276} INFO - 21/05/15 16:23:27 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-15 13:23:27,156] {docker.py:276} INFO - 21/05/15 16:23:27 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (c776c6fed6bc, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:27,157] {docker.py:276} INFO - 21/05/15 16:23:27 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 2570 ms on c776c6fed6bc (executor driver) (63/200)
21/05/15 16:23:27 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-15 13:23:27,166] {docker.py:276} INFO - 21/05/15 16:23:27 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:27,169] {docker.py:276} INFO - 21/05/15 16:23:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:27,170] {docker.py:276} INFO - 21/05/15 16:23:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267274708288917620252_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267274708288917620252_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267274708288917620252_0004}; taskId=attempt_202105151622267274708288917620252_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e061902}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:27,171] {docker.py:276} INFO - 21/05/15 16:23:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:27 INFO StagingCommitter: Starting: Task committer attempt_202105151622267274708288917620252_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267274708288917620252_0004_m_000065_353
[2021-05-15 13:23:27,171] {docker.py:276} INFO - 21/05/15 16:23:27 INFO ShuffleBlockFetcherIterator: Getting 5 (24.3 KiB) non-empty blocks including 5 (24.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:27,175] {docker.py:276} INFO - 21/05/15 16:23:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:27 INFO StagingCommitter: Task committer attempt_202105151622267274708288917620252_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267274708288917620252_0004_m_000065_353 : duration 0:00.005s
[2021-05-15 13:23:27,176] {docker.py:276} INFO - 21/05/15 16:23:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:27,179] {docker.py:276} INFO - 21/05/15 16:23:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:27,181] {docker.py:276} INFO - 21/05/15 16:23:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226111728343685486646_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226111728343685486646_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226111728343685486646_0004}; taskId=attempt_20210515162226111728343685486646_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7aa4b6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:27,182] {docker.py:276} INFO - 21/05/15 16:23:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:27,182] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Starting: Task committer attempt_20210515162226111728343685486646_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226111728343685486646_0004_m_000066_354
[2021-05-15 13:23:27,185] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Task committer attempt_20210515162226111728343685486646_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226111728343685486646_0004_m_000066_354 : duration 0:00.006s
[2021-05-15 13:23:27,676] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Starting: Task committer attempt_20210515162226895162777178678517_0004_m_000063_351: needsTaskCommit() Task attempt_20210515162226895162777178678517_0004_m_000063_351
[2021-05-15 13:23:27,677] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Task committer attempt_20210515162226895162777178678517_0004_m_000063_351: needsTaskCommit() Task attempt_20210515162226895162777178678517_0004_m_000063_351: duration 0:00.000s
21/05/15 16:23:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226895162777178678517_0004_m_000063_351
[2021-05-15 13:23:27,679] {docker.py:276} INFO - 21/05/15 16:23:27 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-15 13:23:27,680] {docker.py:276} INFO - 21/05/15 16:23:27 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (c776c6fed6bc, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:27,681] {docker.py:276} INFO - 21/05/15 16:23:27 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-15 13:23:27,682] {docker.py:276} INFO - 21/05/15 16:23:27 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 2625 ms on c776c6fed6bc (executor driver) (64/200)
[2021-05-15 13:23:27,695] {docker.py:276} INFO - 21/05/15 16:23:27 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:27,697] {docker.py:276} INFO - 21/05/15 16:23:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:27,698] {docker.py:276} INFO - 21/05/15 16:23:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265804479902136226042_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265804479902136226042_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265804479902136226042_0004}; taskId=attempt_202105151622265804479902136226042_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c0fbd64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:27 INFO StagingCommitter: Starting: Task committer attempt_202105151622265804479902136226042_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265804479902136226042_0004_m_000067_355
[2021-05-15 13:23:27,701] {docker.py:276} INFO - 21/05/15 16:23:27 INFO StagingCommitter: Task committer attempt_202105151622265804479902136226042_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265804479902136226042_0004_m_000067_355 : duration 0:00.003s
[2021-05-15 13:23:29,494] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Starting: Task committer attempt_20210515162226957380740949118042_0004_m_000064_352: needsTaskCommit() Task attempt_20210515162226957380740949118042_0004_m_000064_352
[2021-05-15 13:23:29,495] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Task committer attempt_20210515162226957380740949118042_0004_m_000064_352: needsTaskCommit() Task attempt_20210515162226957380740949118042_0004_m_000064_352: duration 0:00.002s
[2021-05-15 13:23:29,495] {docker.py:276} INFO - 21/05/15 16:23:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226957380740949118042_0004_m_000064_352
[2021-05-15 13:23:29,497] {docker.py:276} INFO - 21/05/15 16:23:29 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-15 13:23:29,499] {docker.py:276} INFO - 21/05/15 16:23:29 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (c776c6fed6bc, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:29,500] {docker.py:276} INFO - 21/05/15 16:23:29 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-15 13:23:29,501] {docker.py:276} INFO - 21/05/15 16:23:29 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 2588 ms on c776c6fed6bc (executor driver) (65/200)
[2021-05-15 13:23:29,511] {docker.py:276} INFO - 21/05/15 16:23:29 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:29,514] {docker.py:276} INFO - 21/05/15 16:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:29,514] {docker.py:276} INFO - 21/05/15 16:23:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264740209026599163799_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264740209026599163799_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264740209026599163799_0004}; taskId=attempt_202105151622264740209026599163799_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f69d1e1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622264740209026599163799_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264740209026599163799_0004_m_000068_356
[2021-05-15 13:23:29,518] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Task committer attempt_202105151622264740209026599163799_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264740209026599163799_0004_m_000068_356 : duration 0:00.004s
[2021-05-15 13:23:29,789] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Starting: Task committer attempt_20210515162226111728343685486646_0004_m_000066_354: needsTaskCommit() Task attempt_20210515162226111728343685486646_0004_m_000066_354
[2021-05-15 13:23:29,790] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Task committer attempt_20210515162226111728343685486646_0004_m_000066_354: needsTaskCommit() Task attempt_20210515162226111728343685486646_0004_m_000066_354: duration 0:00.001s
21/05/15 16:23:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226111728343685486646_0004_m_000066_354
[2021-05-15 13:23:29,791] {docker.py:276} INFO - 21/05/15 16:23:29 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-15 13:23:29,796] {docker.py:276} INFO - 21/05/15 16:23:29 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (c776c6fed6bc, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:29,797] {docker.py:276} INFO - 21/05/15 16:23:29 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 2608 ms on c776c6fed6bc (executor driver) (66/200)
[2021-05-15 13:23:29,797] {docker.py:276} INFO - 21/05/15 16:23:29 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-15 13:23:29,809] {docker.py:276} INFO - 21/05/15 16:23:29 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:29,814] {docker.py:276} INFO - 21/05/15 16:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:29,815] {docker.py:276} INFO - 21/05/15 16:23:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261070334385788030420_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261070334385788030420_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261070334385788030420_0004}; taskId=attempt_202105151622261070334385788030420_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55714e3e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622261070334385788030420_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261070334385788030420_0004_m_000069_357
[2021-05-15 13:23:29,819] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Task committer attempt_202105151622261070334385788030420_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261070334385788030420_0004_m_000069_357 : duration 0:00.004s
[2021-05-15 13:23:29,978] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622267274708288917620252_0004_m_000065_353: needsTaskCommit() Task attempt_202105151622267274708288917620252_0004_m_000065_353
[2021-05-15 13:23:29,979] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Task committer attempt_202105151622267274708288917620252_0004_m_000065_353: needsTaskCommit() Task attempt_202105151622267274708288917620252_0004_m_000065_353: duration 0:00.001s
21/05/15 16:23:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267274708288917620252_0004_m_000065_353
[2021-05-15 13:23:29,981] {docker.py:276} INFO - 21/05/15 16:23:29 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-15 13:23:29,983] {docker.py:276} INFO - 21/05/15 16:23:29 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (c776c6fed6bc, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:29,984] {docker.py:276} INFO - 21/05/15 16:23:29 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-15 13:23:29,985] {docker.py:276} INFO - 21/05/15 16:23:29 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 2802 ms on c776c6fed6bc (executor driver) (67/200)
[2021-05-15 13:23:29,994] {docker.py:276} INFO - 21/05/15 16:23:29 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:29,996] {docker.py:276} INFO - 21/05/15 16:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263485018045772109341_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263485018045772109341_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263485018045772109341_0004}; taskId=attempt_202105151622263485018045772109341_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5272952a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622263485018045772109341_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263485018045772109341_0004_m_000070_358
[2021-05-15 13:23:29,999] {docker.py:276} INFO - 21/05/15 16:23:29 INFO StagingCommitter: Task committer attempt_202105151622263485018045772109341_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263485018045772109341_0004_m_000070_358 : duration 0:00.003s
[2021-05-15 13:23:30,278] {docker.py:276} INFO - 21/05/15 16:23:30 INFO StagingCommitter: Starting: Task committer attempt_202105151622265804479902136226042_0004_m_000067_355: needsTaskCommit() Task attempt_202105151622265804479902136226042_0004_m_000067_355
[2021-05-15 13:23:30,278] {docker.py:276} INFO - 21/05/15 16:23:30 INFO StagingCommitter: Task committer attempt_202105151622265804479902136226042_0004_m_000067_355: needsTaskCommit() Task attempt_202105151622265804479902136226042_0004_m_000067_355: duration 0:00.001s
21/05/15 16:23:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265804479902136226042_0004_m_000067_355
[2021-05-15 13:23:30,281] {docker.py:276} INFO - 21/05/15 16:23:30 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 4544 bytes result sent to driver
[2021-05-15 13:23:30,282] {docker.py:276} INFO - 21/05/15 16:23:30 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (c776c6fed6bc, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:30,283] {docker.py:276} INFO - 21/05/15 16:23:30 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
21/05/15 16:23:30 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 2571 ms on c776c6fed6bc (executor driver) (68/200)
[2021-05-15 13:23:30,293] {docker.py:276} INFO - 21/05/15 16:23:30 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:30,295] {docker.py:276} INFO - 21/05/15 16:23:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:30,296] {docker.py:276} INFO - 21/05/15 16:23:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:30,296] {docker.py:276} INFO - 21/05/15 16:23:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266841663589090699773_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266841663589090699773_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266841663589090699773_0004}; taskId=attempt_202105151622266841663589090699773_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71a6b29e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:30,296] {docker.py:276} INFO - 21/05/15 16:23:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:30,297] {docker.py:276} INFO - 21/05/15 16:23:30 INFO StagingCommitter: Starting: Task committer attempt_202105151622266841663589090699773_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266841663589090699773_0004_m_000071_359
[2021-05-15 13:23:30,300] {docker.py:276} INFO - 21/05/15 16:23:30 INFO StagingCommitter: Task committer attempt_202105151622266841663589090699773_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266841663589090699773_0004_m_000071_359 : duration 0:00.004s
[2021-05-15 13:23:32,147] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622264740209026599163799_0004_m_000068_356: needsTaskCommit() Task attempt_202105151622264740209026599163799_0004_m_000068_356
[2021-05-15 13:23:32,148] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622264740209026599163799_0004_m_000068_356: needsTaskCommit() Task attempt_202105151622264740209026599163799_0004_m_000068_356: duration 0:00.001s
21/05/15 16:23:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264740209026599163799_0004_m_000068_356
[2021-05-15 13:23:32,151] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 4544 bytes result sent to driver
[2021-05-15 13:23:32,152] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (c776c6fed6bc, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:32,157] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-15 13:23:32,159] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 2660 ms on c776c6fed6bc (executor driver) (69/200)
[2021-05-15 13:23:32,192] {docker.py:276} INFO - 21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:32,227] {docker.py:276} INFO - 21/05/15 16:23:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:32,228] {docker.py:276} INFO - 21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266522399800897630155_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266522399800897630155_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266522399800897630155_0004}; taskId=attempt_202105151622266522399800897630155_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6699b12d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622266522399800897630155_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266522399800897630155_0004_m_000072_360
[2021-05-15 13:23:32,228] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622266522399800897630155_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266522399800897630155_0004_m_000072_360 : duration 0:00.004s
[2021-05-15 13:23:32,354] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622263485018045772109341_0004_m_000070_358: needsTaskCommit() Task attempt_202105151622263485018045772109341_0004_m_000070_358
[2021-05-15 13:23:32,355] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622263485018045772109341_0004_m_000070_358: needsTaskCommit() Task attempt_202105151622263485018045772109341_0004_m_000070_358: duration 0:00.001s
21/05/15 16:23:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263485018045772109341_0004_m_000070_358
[2021-05-15 13:23:32,356] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 4587 bytes result sent to driver
[2021-05-15 13:23:32,358] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (c776c6fed6bc, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:32,360] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 2379 ms on c776c6fed6bc (executor driver) (70/200)
[2021-05-15 13:23:32,360] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-15 13:23:32,370] {docker.py:276} INFO - 21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:32,371] {docker.py:276} INFO - 21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:32,373] {docker.py:276} INFO - 21/05/15 16:23:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:32,373] {docker.py:276} INFO - 21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:32,374] {docker.py:276} INFO - 21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261892693029687114593_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261892693029687114593_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261892693029687114593_0004}; taskId=attempt_202105151622261892693029687114593_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f35a4a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:32,374] {docker.py:276} INFO - 21/05/15 16:23:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:32,374] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622261892693029687114593_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261892693029687114593_0004_m_000073_361
[2021-05-15 13:23:32,377] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622261892693029687114593_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261892693029687114593_0004_m_000073_361 : duration 0:00.004s
[2021-05-15 13:23:32,460] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622261070334385788030420_0004_m_000069_357: needsTaskCommit() Task attempt_202105151622261070334385788030420_0004_m_000069_357
[2021-05-15 13:23:32,461] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622261070334385788030420_0004_m_000069_357: needsTaskCommit() Task attempt_202105151622261070334385788030420_0004_m_000069_357: duration 0:00.000s
[2021-05-15 13:23:32,461] {docker.py:276} INFO - 21/05/15 16:23:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261070334385788030420_0004_m_000069_357
[2021-05-15 13:23:32,463] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 4587 bytes result sent to driver
[2021-05-15 13:23:32,464] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (c776c6fed6bc, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:32,465] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 2675 ms on c776c6fed6bc (executor driver) (71/200)
[2021-05-15 13:23:32,466] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-15 13:23:32,475] {docker.py:276} INFO - 21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:32,476] {docker.py:276} INFO - 21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:32,478] {docker.py:276} INFO - 21/05/15 16:23:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:32,478] {docker.py:276} INFO - 21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:32,479] {docker.py:276} INFO - 21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267289384997785667386_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267289384997785667386_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267289384997785667386_0004}; taskId=attempt_202105151622267289384997785667386_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cacad93}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:32,479] {docker.py:276} INFO - 21/05/15 16:23:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:32,479] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622267289384997785667386_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267289384997785667386_0004_m_000074_362
[2021-05-15 13:23:32,481] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622267289384997785667386_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267289384997785667386_0004_m_000074_362 : duration 0:00.003s
[2021-05-15 13:23:32,883] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622266841663589090699773_0004_m_000071_359: needsTaskCommit() Task attempt_202105151622266841663589090699773_0004_m_000071_359
[2021-05-15 13:23:32,883] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622266841663589090699773_0004_m_000071_359: needsTaskCommit() Task attempt_202105151622266841663589090699773_0004_m_000071_359: duration 0:00.001s
21/05/15 16:23:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266841663589090699773_0004_m_000071_359
[2021-05-15 13:23:32,884] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 4587 bytes result sent to driver
[2021-05-15 13:23:32,885] {docker.py:276} INFO - 21/05/15 16:23:32 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (c776c6fed6bc, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:32,885] {docker.py:276} INFO - 21/05/15 16:23:32 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
21/05/15 16:23:32 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 2608 ms on c776c6fed6bc (executor driver) (72/200)
[2021-05-15 13:23:32,894] {docker.py:276} INFO - 21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:32,896] {docker.py:276} INFO - 21/05/15 16:23:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:32,897] {docker.py:276} INFO - 21/05/15 16:23:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261888605791089390186_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261888605791089390186_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261888605791089390186_0004}; taskId=attempt_202105151622261888605791089390186_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@696d9053}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:32 INFO StagingCommitter: Starting: Task committer attempt_202105151622261888605791089390186_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261888605791089390186_0004_m_000075_363
[2021-05-15 13:23:32,899] {docker.py:276} INFO - 21/05/15 16:23:32 INFO StagingCommitter: Task committer attempt_202105151622261888605791089390186_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261888605791089390186_0004_m_000075_363 : duration 0:00.003s
[2021-05-15 13:23:34,886] {docker.py:276} INFO - 21/05/15 16:23:34 INFO StagingCommitter: Starting: Task committer attempt_202105151622261892693029687114593_0004_m_000073_361: needsTaskCommit() Task attempt_202105151622261892693029687114593_0004_m_000073_361
[2021-05-15 13:23:34,887] {docker.py:276} INFO - 21/05/15 16:23:34 INFO StagingCommitter: Task committer attempt_202105151622261892693029687114593_0004_m_000073_361: needsTaskCommit() Task attempt_202105151622261892693029687114593_0004_m_000073_361: duration 0:00.001s
21/05/15 16:23:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261892693029687114593_0004_m_000073_361
[2021-05-15 13:23:34,889] {docker.py:276} INFO - 21/05/15 16:23:34 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 4544 bytes result sent to driver
[2021-05-15 13:23:34,890] {docker.py:276} INFO - 21/05/15 16:23:34 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (c776c6fed6bc, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:34,891] {docker.py:276} INFO - 21/05/15 16:23:34 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
21/05/15 16:23:34 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 2536 ms on c776c6fed6bc (executor driver) (73/200)
[2021-05-15 13:23:34,907] {docker.py:276} INFO - 21/05/15 16:23:34 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:34,912] {docker.py:276} INFO - 21/05/15 16:23:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:34,913] {docker.py:276} INFO - 21/05/15 16:23:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:34,914] {docker.py:276} INFO - 21/05/15 16:23:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263909812010282076861_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263909812010282076861_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263909812010282076861_0004}; taskId=attempt_202105151622263909812010282076861_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6076f0f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:34,915] {docker.py:276} INFO - 21/05/15 16:23:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:34 INFO StagingCommitter: Starting: Task committer attempt_202105151622263909812010282076861_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263909812010282076861_0004_m_000076_364
[2021-05-15 13:23:34,921] {docker.py:276} INFO - 21/05/15 16:23:34 INFO StagingCommitter: Task committer attempt_202105151622263909812010282076861_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263909812010282076861_0004_m_000076_364 : duration 0:00.006s
[2021-05-15 13:23:35,127] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622267289384997785667386_0004_m_000074_362: needsTaskCommit() Task attempt_202105151622267289384997785667386_0004_m_000074_362
[2021-05-15 13:23:35,128] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Task committer attempt_202105151622267289384997785667386_0004_m_000074_362: needsTaskCommit() Task attempt_202105151622267289384997785667386_0004_m_000074_362: duration 0:00.001s
21/05/15 16:23:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267289384997785667386_0004_m_000074_362
[2021-05-15 13:23:35,130] {docker.py:276} INFO - 21/05/15 16:23:35 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 4544 bytes result sent to driver
[2021-05-15 13:23:35,132] {docker.py:276} INFO - 21/05/15 16:23:35 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (c776c6fed6bc, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:35,133] {docker.py:276} INFO - 21/05/15 16:23:35 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
21/05/15 16:23:35 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 2672 ms on c776c6fed6bc (executor driver) (74/200)
[2021-05-15 13:23:35,144] {docker.py:276} INFO - 21/05/15 16:23:35 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:35,145] {docker.py:276} INFO - 21/05/15 16:23:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:35,147] {docker.py:276} INFO - 21/05/15 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:35,148] {docker.py:276} INFO - 21/05/15 16:23:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:35,149] {docker.py:276} INFO - 21/05/15 16:23:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226319771124972578745_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226319771124972578745_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226319771124972578745_0004}; taskId=attempt_20210515162226319771124972578745_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16423f5c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:35,149] {docker.py:276} INFO - 21/05/15 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:35 INFO StagingCommitter: Starting: Task committer attempt_20210515162226319771124972578745_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226319771124972578745_0004_m_000077_365
[2021-05-15 13:23:35,153] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Task committer attempt_20210515162226319771124972578745_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226319771124972578745_0004_m_000077_365 : duration 0:00.004s
[2021-05-15 13:23:35,406] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622266522399800897630155_0004_m_000072_360: needsTaskCommit() Task attempt_202105151622266522399800897630155_0004_m_000072_360
[2021-05-15 13:23:35,406] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Task committer attempt_202105151622266522399800897630155_0004_m_000072_360: needsTaskCommit() Task attempt_202105151622266522399800897630155_0004_m_000072_360: duration 0:00.000s
[2021-05-15 13:23:35,407] {docker.py:276} INFO - 21/05/15 16:23:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266522399800897630155_0004_m_000072_360
[2021-05-15 13:23:35,407] {docker.py:276} INFO - 21/05/15 16:23:35 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 4587 bytes result sent to driver
[2021-05-15 13:23:35,409] {docker.py:276} INFO - 21/05/15 16:23:35 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (c776c6fed6bc, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:35,409] {docker.py:276} INFO - 21/05/15 16:23:35 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 3262 ms on c776c6fed6bc (executor driver) (75/200)
[2021-05-15 13:23:35,410] {docker.py:276} INFO - 21/05/15 16:23:35 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-15 13:23:35,420] {docker.py:276} INFO - 21/05/15 16:23:35 INFO ShuffleBlockFetcherIterator: Getting 5 (22.3 KiB) non-empty blocks including 5 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:35,422] {docker.py:276} INFO - 21/05/15 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:35,423] {docker.py:276} INFO - 21/05/15 16:23:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263068046174021398921_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263068046174021398921_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263068046174021398921_0004}; taskId=attempt_202105151622263068046174021398921_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22938813}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622263068046174021398921_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263068046174021398921_0004_m_000078_366
[2021-05-15 13:23:35,427] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Task committer attempt_202105151622263068046174021398921_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263068046174021398921_0004_m_000078_366 : duration 0:00.003s
[2021-05-15 13:23:35,719] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622261888605791089390186_0004_m_000075_363: needsTaskCommit() Task attempt_202105151622261888605791089390186_0004_m_000075_363
[2021-05-15 13:23:35,720] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Task committer attempt_202105151622261888605791089390186_0004_m_000075_363: needsTaskCommit() Task attempt_202105151622261888605791089390186_0004_m_000075_363: duration 0:00.001s
21/05/15 16:23:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261888605791089390186_0004_m_000075_363
[2021-05-15 13:23:35,721] {docker.py:276} INFO - 21/05/15 16:23:35 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 4544 bytes result sent to driver
[2021-05-15 13:23:35,723] {docker.py:276} INFO - 21/05/15 16:23:35 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (c776c6fed6bc, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:35,724] {docker.py:276} INFO - 21/05/15 16:23:35 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 2843 ms on c776c6fed6bc (executor driver) (76/200)
[2021-05-15 13:23:35,724] {docker.py:276} INFO - 21/05/15 16:23:35 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
[2021-05-15 13:23:35,736] {docker.py:276} INFO - 21/05/15 16:23:35 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:35,739] {docker.py:276} INFO - 21/05/15 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:35,739] {docker.py:276} INFO - 21/05/15 16:23:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266142950860579447031_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266142950860579447031_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266142950860579447031_0004}; taskId=attempt_202105151622266142950860579447031_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@995684e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:35,740] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622266142950860579447031_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266142950860579447031_0004_m_000079_367
[2021-05-15 13:23:35,743] {docker.py:276} INFO - 21/05/15 16:23:35 INFO StagingCommitter: Task committer attempt_202105151622266142950860579447031_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266142950860579447031_0004_m_000079_367 : duration 0:00.003s
[2021-05-15 13:23:37,571] {docker.py:276} INFO - 21/05/15 16:23:37 INFO StagingCommitter: Starting: Task committer attempt_202105151622263909812010282076861_0004_m_000076_364: needsTaskCommit() Task attempt_202105151622263909812010282076861_0004_m_000076_364
21/05/15 16:23:37 INFO StagingCommitter: Task committer attempt_202105151622263909812010282076861_0004_m_000076_364: needsTaskCommit() Task attempt_202105151622263909812010282076861_0004_m_000076_364: duration 0:00.001s
21/05/15 16:23:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263909812010282076861_0004_m_000076_364
[2021-05-15 13:23:37,572] {docker.py:276} INFO - 21/05/15 16:23:37 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 4544 bytes result sent to driver
[2021-05-15 13:23:37,574] {docker.py:276} INFO - 21/05/15 16:23:37 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (c776c6fed6bc, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:37,575] {docker.py:276} INFO - 21/05/15 16:23:37 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 2690 ms on c776c6fed6bc (executor driver) (77/200)
[2021-05-15 13:23:37,576] {docker.py:276} INFO - 21/05/15 16:23:37 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-15 13:23:37,588] {docker.py:276} INFO - 21/05/15 16:23:37 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:37,589] {docker.py:276} INFO - 21/05/15 16:23:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:37,592] {docker.py:276} INFO - 21/05/15 16:23:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:37,592] {docker.py:276} INFO - 21/05/15 16:23:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261506339288004630140_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261506339288004630140_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261506339288004630140_0004}; taskId=attempt_202105151622261506339288004630140_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f2f6554}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:37,593] {docker.py:276} INFO - 21/05/15 16:23:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:37,594] {docker.py:276} INFO - 21/05/15 16:23:37 INFO StagingCommitter: Starting: Task committer attempt_202105151622261506339288004630140_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261506339288004630140_0004_m_000080_368
[2021-05-15 13:23:37,597] {docker.py:276} INFO - 21/05/15 16:23:37 INFO StagingCommitter: Task committer attempt_202105151622261506339288004630140_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261506339288004630140_0004_m_000080_368 : duration 0:00.006s
[2021-05-15 13:23:37,862] {docker.py:276} INFO - 21/05/15 16:23:37 INFO StagingCommitter: Starting: Task committer attempt_20210515162226319771124972578745_0004_m_000077_365: needsTaskCommit() Task attempt_20210515162226319771124972578745_0004_m_000077_365
[2021-05-15 13:23:37,863] {docker.py:276} INFO - 21/05/15 16:23:37 INFO StagingCommitter: Task committer attempt_20210515162226319771124972578745_0004_m_000077_365: needsTaskCommit() Task attempt_20210515162226319771124972578745_0004_m_000077_365: duration 0:00.001s
21/05/15 16:23:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226319771124972578745_0004_m_000077_365
[2021-05-15 13:23:37,867] {docker.py:276} INFO - 21/05/15 16:23:37 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 4544 bytes result sent to driver
[2021-05-15 13:23:37,868] {docker.py:276} INFO - 21/05/15 16:23:37 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (c776c6fed6bc, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:37,870] {docker.py:276} INFO - 21/05/15 16:23:37 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 2742 ms on c776c6fed6bc (executor driver) (78/200)
[2021-05-15 13:23:37,871] {docker.py:276} INFO - 21/05/15 16:23:37 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-15 13:23:37,886] {docker.py:276} INFO - 21/05/15 16:23:37 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:37,887] {docker.py:276} INFO - 21/05/15 16:23:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:37,890] {docker.py:276} INFO - 21/05/15 16:23:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268423277874209051329_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268423277874209051329_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268423277874209051329_0004}; taskId=attempt_202105151622268423277874209051329_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69232a88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:37 INFO StagingCommitter: Starting: Task committer attempt_202105151622268423277874209051329_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268423277874209051329_0004_m_000081_369
[2021-05-15 13:23:37,894] {docker.py:276} INFO - 21/05/15 16:23:37 INFO StagingCommitter: Task committer attempt_202105151622268423277874209051329_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268423277874209051329_0004_m_000081_369 : duration 0:00.003s
[2021-05-15 13:23:38,223] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Starting: Task committer attempt_202105151622263068046174021398921_0004_m_000078_366: needsTaskCommit() Task attempt_202105151622263068046174021398921_0004_m_000078_366
[2021-05-15 13:23:38,224] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Task committer attempt_202105151622263068046174021398921_0004_m_000078_366: needsTaskCommit() Task attempt_202105151622263068046174021398921_0004_m_000078_366: duration 0:00.001s
21/05/15 16:23:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263068046174021398921_0004_m_000078_366
[2021-05-15 13:23:38,228] {docker.py:276} INFO - 21/05/15 16:23:38 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 4544 bytes result sent to driver
[2021-05-15 13:23:38,230] {docker.py:276} INFO - 21/05/15 16:23:38 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (c776c6fed6bc, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:38,232] {docker.py:276} INFO - 21/05/15 16:23:38 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
21/05/15 16:23:38 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 2826 ms on c776c6fed6bc (executor driver) (79/200)
[2021-05-15 13:23:38,245] {docker.py:276} INFO - 21/05/15 16:23:38 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:38,245] {docker.py:276} INFO - 21/05/15 16:23:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:38,248] {docker.py:276} INFO - 21/05/15 16:23:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:38,249] {docker.py:276} INFO - 21/05/15 16:23:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268654624586893151528_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268654624586893151528_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268654624586893151528_0004}; taskId=attempt_202105151622268654624586893151528_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f248486}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:38,249] {docker.py:276} INFO - 21/05/15 16:23:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:38 INFO StagingCommitter: Starting: Task committer attempt_202105151622268654624586893151528_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268654624586893151528_0004_m_000082_370
[2021-05-15 13:23:38,255] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Task committer attempt_202105151622268654624586893151528_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268654624586893151528_0004_m_000082_370 : duration 0:00.004s
[2021-05-15 13:23:38,414] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Starting: Task committer attempt_202105151622266142950860579447031_0004_m_000079_367: needsTaskCommit() Task attempt_202105151622266142950860579447031_0004_m_000079_367
[2021-05-15 13:23:38,415] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Task committer attempt_202105151622266142950860579447031_0004_m_000079_367: needsTaskCommit() Task attempt_202105151622266142950860579447031_0004_m_000079_367: duration 0:00.000s
[2021-05-15 13:23:38,415] {docker.py:276} INFO - 21/05/15 16:23:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266142950860579447031_0004_m_000079_367
[2021-05-15 13:23:38,417] {docker.py:276} INFO - 21/05/15 16:23:38 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 4544 bytes result sent to driver
[2021-05-15 13:23:38,419] {docker.py:276} INFO - 21/05/15 16:23:38 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (c776c6fed6bc, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:38,420] {docker.py:276} INFO - 21/05/15 16:23:38 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 2700 ms on c776c6fed6bc (executor driver) (80/200)
21/05/15 16:23:38 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-15 13:23:38,432] {docker.py:276} INFO - 21/05/15 16:23:38 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:38,435] {docker.py:276} INFO - 21/05/15 16:23:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:38,435] {docker.py:276} INFO - 21/05/15 16:23:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:38,436] {docker.py:276} INFO - 21/05/15 16:23:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265252290806649323330_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265252290806649323330_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265252290806649323330_0004}; taskId=attempt_202105151622265252290806649323330_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3330a152}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:38,436] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Starting: Task committer attempt_202105151622265252290806649323330_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265252290806649323330_0004_m_000083_371
[2021-05-15 13:23:38,441] {docker.py:276} INFO - 21/05/15 16:23:38 INFO StagingCommitter: Task committer attempt_202105151622265252290806649323330_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265252290806649323330_0004_m_000083_371 : duration 0:00.005s
[2021-05-15 13:23:40,329] {docker.py:276} INFO - 21/05/15 16:23:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622261506339288004630140_0004_m_000080_368: needsTaskCommit() Task attempt_202105151622261506339288004630140_0004_m_000080_368
21/05/15 16:23:40 INFO StagingCommitter: Task committer attempt_202105151622261506339288004630140_0004_m_000080_368: needsTaskCommit() Task attempt_202105151622261506339288004630140_0004_m_000080_368: duration 0:00.000s
21/05/15 16:23:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261506339288004630140_0004_m_000080_368
[2021-05-15 13:23:40,333] {docker.py:276} INFO - 21/05/15 16:23:40 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 4544 bytes result sent to driver
[2021-05-15 13:23:40,335] {docker.py:276} INFO - 21/05/15 16:23:40 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (c776c6fed6bc, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:40,337] {docker.py:276} INFO - 21/05/15 16:23:40 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 2765 ms on c776c6fed6bc (executor driver) (81/200)
[2021-05-15 13:23:40,338] {docker.py:276} INFO - 21/05/15 16:23:40 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
[2021-05-15 13:23:40,357] {docker.py:276} INFO - 21/05/15 16:23:40 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:40,360] {docker.py:276} INFO - 21/05/15 16:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:40,361] {docker.py:276} INFO - 21/05/15 16:23:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265083313523035277502_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265083313523035277502_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265083313523035277502_0004}; taskId=attempt_202105151622265083313523035277502_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c419629}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622265083313523035277502_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265083313523035277502_0004_m_000084_372
[2021-05-15 13:23:40,367] {docker.py:276} INFO - 21/05/15 16:23:40 INFO StagingCommitter: Task committer attempt_202105151622265083313523035277502_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265083313523035277502_0004_m_000084_372 : duration 0:00.006s
[2021-05-15 13:23:40,593] {docker.py:276} INFO - 21/05/15 16:23:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622268423277874209051329_0004_m_000081_369: needsTaskCommit() Task attempt_202105151622268423277874209051329_0004_m_000081_369
[2021-05-15 13:23:40,594] {docker.py:276} INFO - 21/05/15 16:23:40 INFO StagingCommitter: Task committer attempt_202105151622268423277874209051329_0004_m_000081_369: needsTaskCommit() Task attempt_202105151622268423277874209051329_0004_m_000081_369: duration 0:00.002s
21/05/15 16:23:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268423277874209051329_0004_m_000081_369
[2021-05-15 13:23:40,596] {docker.py:276} INFO - 21/05/15 16:23:40 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 4544 bytes result sent to driver
[2021-05-15 13:23:40,598] {docker.py:276} INFO - 21/05/15 16:23:40 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (c776c6fed6bc, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:40,599] {docker.py:276} INFO - 21/05/15 16:23:40 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-15 13:23:40,601] {docker.py:276} INFO - 21/05/15 16:23:40 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 2735 ms on c776c6fed6bc (executor driver) (82/200)
[2021-05-15 13:23:40,611] {docker.py:276} INFO - 21/05/15 16:23:40 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:40,613] {docker.py:276} INFO - 21/05/15 16:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263808642984567510931_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263808642984567510931_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263808642984567510931_0004}; taskId=attempt_202105151622263808642984567510931_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71dc8ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622263808642984567510931_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263808642984567510931_0004_m_000085_373
[2021-05-15 13:23:40,617] {docker.py:276} INFO - 21/05/15 16:23:40 INFO StagingCommitter: Task committer attempt_202105151622263808642984567510931_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263808642984567510931_0004_m_000085_373 : duration 0:00.004s
[2021-05-15 13:23:41,086] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Starting: Task committer attempt_202105151622268654624586893151528_0004_m_000082_370: needsTaskCommit() Task attempt_202105151622268654624586893151528_0004_m_000082_370
[2021-05-15 13:23:41,088] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Task committer attempt_202105151622268654624586893151528_0004_m_000082_370: needsTaskCommit() Task attempt_202105151622268654624586893151528_0004_m_000082_370: duration 0:00.000s
21/05/15 16:23:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268654624586893151528_0004_m_000082_370
[2021-05-15 13:23:41,090] {docker.py:276} INFO - 21/05/15 16:23:41 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-15 13:23:41,091] {docker.py:276} INFO - 21/05/15 16:23:41 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (c776c6fed6bc, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:41,092] {docker.py:276} INFO - 21/05/15 16:23:41 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-15 13:23:41,093] {docker.py:276} INFO - 21/05/15 16:23:41 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 2867 ms on c776c6fed6bc (executor driver) (83/200)
[2021-05-15 13:23:41,113] {docker.py:276} INFO - 21/05/15 16:23:41 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:41,113] {docker.py:276} INFO - 21/05/15 16:23:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:41,115] {docker.py:276} INFO - 21/05/15 16:23:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:23:41,116] {docker.py:276} INFO - 21/05/15 16:23:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:41,116] {docker.py:276} INFO - 21/05/15 16:23:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:41,116] {docker.py:276} INFO - 21/05/15 16:23:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262949987489198799212_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262949987489198799212_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262949987489198799212_0004}; taskId=attempt_202105151622262949987489198799212_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6786f70f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:41,117] {docker.py:276} INFO - 21/05/15 16:23:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:41,117] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Starting: Task committer attempt_202105151622262949987489198799212_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262949987489198799212_0004_m_000086_374
[2021-05-15 13:23:41,121] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Task committer attempt_202105151622262949987489198799212_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262949987489198799212_0004_m_000086_374 : duration 0:00.004s
[2021-05-15 13:23:41,157] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Starting: Task committer attempt_202105151622265252290806649323330_0004_m_000083_371: needsTaskCommit() Task attempt_202105151622265252290806649323330_0004_m_000083_371
[2021-05-15 13:23:41,158] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Task committer attempt_202105151622265252290806649323330_0004_m_000083_371: needsTaskCommit() Task attempt_202105151622265252290806649323330_0004_m_000083_371: duration 0:00.000s
21/05/15 16:23:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265252290806649323330_0004_m_000083_371
[2021-05-15 13:23:41,160] {docker.py:276} INFO - 21/05/15 16:23:41 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 4587 bytes result sent to driver
[2021-05-15 13:23:41,161] {docker.py:276} INFO - 21/05/15 16:23:41 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (c776c6fed6bc, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:41,162] {docker.py:276} INFO - 21/05/15 16:23:41 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 2746 ms on c776c6fed6bc (executor driver) (84/200)
[2021-05-15 13:23:41,163] {docker.py:276} INFO - 21/05/15 16:23:41 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-15 13:23:41,172] {docker.py:276} INFO - 21/05/15 16:23:41 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:41,174] {docker.py:276} INFO - 21/05/15 16:23:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:23:41,175] {docker.py:276} INFO - 21/05/15 16:23:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:41,176] {docker.py:276} INFO - 21/05/15 16:23:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265052071851598553675_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265052071851598553675_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265052071851598553675_0004}; taskId=attempt_202105151622265052071851598553675_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e2606f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:41,176] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Starting: Task committer attempt_202105151622265052071851598553675_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265052071851598553675_0004_m_000087_375
[2021-05-15 13:23:41,179] {docker.py:276} INFO - 21/05/15 16:23:41 INFO StagingCommitter: Task committer attempt_202105151622265052071851598553675_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265052071851598553675_0004_m_000087_375 : duration 0:00.004s
[2021-05-15 13:23:43,085] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622265083313523035277502_0004_m_000084_372: needsTaskCommit() Task attempt_202105151622265083313523035277502_0004_m_000084_372
[2021-05-15 13:23:43,086] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622265083313523035277502_0004_m_000084_372: needsTaskCommit() Task attempt_202105151622265083313523035277502_0004_m_000084_372: duration 0:00.000s
21/05/15 16:23:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265083313523035277502_0004_m_000084_372
[2021-05-15 13:23:43,087] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 4587 bytes result sent to driver
[2021-05-15 13:23:43,088] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (c776c6fed6bc, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:43,090] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-15 13:23:43,092] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 2759 ms on c776c6fed6bc (executor driver) (85/200)
[2021-05-15 13:23:43,100] {docker.py:276} INFO - 21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Getting 5 (22.3 KiB) non-empty blocks including 5 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:43,102] {docker.py:276} INFO - 21/05/15 16:23:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:43,102] {docker.py:276} INFO - 21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261587176750931739446_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261587176750931739446_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261587176750931739446_0004}; taskId=attempt_202105151622261587176750931739446_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21e14a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:43,102] {docker.py:276} INFO - 21/05/15 16:23:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622261587176750931739446_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261587176750931739446_0004_m_000088_376
[2021-05-15 13:23:43,106] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622261587176750931739446_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261587176750931739446_0004_m_000088_376 : duration 0:00.002s
[2021-05-15 13:23:43,184] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622263808642984567510931_0004_m_000085_373: needsTaskCommit() Task attempt_202105151622263808642984567510931_0004_m_000085_373
[2021-05-15 13:23:43,185] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622263808642984567510931_0004_m_000085_373: needsTaskCommit() Task attempt_202105151622263808642984567510931_0004_m_000085_373: duration 0:00.001s
21/05/15 16:23:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263808642984567510931_0004_m_000085_373
[2021-05-15 13:23:43,188] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 4587 bytes result sent to driver
[2021-05-15 13:23:43,189] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (c776c6fed6bc, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:43,190] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
[2021-05-15 13:23:43,190] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 2597 ms on c776c6fed6bc (executor driver) (86/200)
[2021-05-15 13:23:43,199] {docker.py:276} INFO - 21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:43,201] {docker.py:276} INFO - 21/05/15 16:23:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:43,202] {docker.py:276} INFO - 21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266840595839606032935_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266840595839606032935_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266840595839606032935_0004}; taskId=attempt_202105151622266840595839606032935_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d7ae361}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:43,202] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622266840595839606032935_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266840595839606032935_0004_m_000089_377
[2021-05-15 13:23:43,205] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622266840595839606032935_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266840595839606032935_0004_m_000089_377 : duration 0:00.003s
[2021-05-15 13:23:43,369] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622262949987489198799212_0004_m_000086_374: needsTaskCommit() Task attempt_202105151622262949987489198799212_0004_m_000086_374
[2021-05-15 13:23:43,370] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622262949987489198799212_0004_m_000086_374: needsTaskCommit() Task attempt_202105151622262949987489198799212_0004_m_000086_374: duration 0:00.001s
21/05/15 16:23:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262949987489198799212_0004_m_000086_374
[2021-05-15 13:23:43,371] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 4587 bytes result sent to driver
[2021-05-15 13:23:43,372] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (c776c6fed6bc, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:43,373] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 2286 ms on c776c6fed6bc (executor driver) (87/200)
[2021-05-15 13:23:43,374] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-15 13:23:43,387] {docker.py:276} INFO - 21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:43,388] {docker.py:276} INFO - 21/05/15 16:23:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268953083150519180475_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268953083150519180475_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268953083150519180475_0004}; taskId=attempt_202105151622268953083150519180475_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d78da8b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:43,389] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622268953083150519180475_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268953083150519180475_0004_m_000090_378
[2021-05-15 13:23:43,393] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622268953083150519180475_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268953083150519180475_0004_m_000090_378 : duration 0:00.004s
[2021-05-15 13:23:43,856] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622265052071851598553675_0004_m_000087_375: needsTaskCommit() Task attempt_202105151622265052071851598553675_0004_m_000087_375
[2021-05-15 13:23:43,858] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622265052071851598553675_0004_m_000087_375: needsTaskCommit() Task attempt_202105151622265052071851598553675_0004_m_000087_375: duration 0:00.002s
21/05/15 16:23:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265052071851598553675_0004_m_000087_375
[2021-05-15 13:23:43,860] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
[2021-05-15 13:23:43,862] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (c776c6fed6bc, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:43,863] {docker.py:276} INFO - 21/05/15 16:23:43 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-15 13:23:43,864] {docker.py:276} INFO - 21/05/15 16:23:43 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 2707 ms on c776c6fed6bc (executor driver) (88/200)
[2021-05-15 13:23:43,873] {docker.py:276} INFO - 21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Getting 5 (23.6 KiB) non-empty blocks including 5 (23.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:43,875] {docker.py:276} INFO - 21/05/15 16:23:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:43,875] {docker.py:276} INFO - 21/05/15 16:23:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265397246832446829301_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265397246832446829301_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265397246832446829301_0004}; taskId=attempt_202105151622265397246832446829301_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63ac12a0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:43,876] {docker.py:276} INFO - 21/05/15 16:23:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622265397246832446829301_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265397246832446829301_0004_m_000091_379
[2021-05-15 13:23:43,879] {docker.py:276} INFO - 21/05/15 16:23:43 INFO StagingCommitter: Task committer attempt_202105151622265397246832446829301_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265397246832446829301_0004_m_000091_379 : duration 0:00.004s
[2021-05-15 13:23:45,846] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Starting: Task committer attempt_202105151622268953083150519180475_0004_m_000090_378: needsTaskCommit() Task attempt_202105151622268953083150519180475_0004_m_000090_378
[2021-05-15 13:23:45,847] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Task committer attempt_202105151622268953083150519180475_0004_m_000090_378: needsTaskCommit() Task attempt_202105151622268953083150519180475_0004_m_000090_378: duration 0:00.001s
21/05/15 16:23:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268953083150519180475_0004_m_000090_378
[2021-05-15 13:23:45,850] {docker.py:276} INFO - 21/05/15 16:23:45 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 4544 bytes result sent to driver
[2021-05-15 13:23:45,851] {docker.py:276} INFO - 21/05/15 16:23:45 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (c776c6fed6bc, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:45,853] {docker.py:276} INFO - 21/05/15 16:23:45 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-15 13:23:45,853] {docker.py:276} INFO - 21/05/15 16:23:45 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 2485 ms on c776c6fed6bc (executor driver) (89/200)
[2021-05-15 13:23:45,863] {docker.py:276} INFO - 21/05/15 16:23:45 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:45,865] {docker.py:276} INFO - 21/05/15 16:23:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:45,866] {docker.py:276} INFO - 21/05/15 16:23:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267139656888712795922_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267139656888712795922_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267139656888712795922_0004}; taskId=attempt_202105151622267139656888712795922_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5898d708}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:45,866] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Starting: Task committer attempt_202105151622267139656888712795922_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267139656888712795922_0004_m_000092_380
[2021-05-15 13:23:45,869] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Task committer attempt_202105151622267139656888712795922_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267139656888712795922_0004_m_000092_380 : duration 0:00.003s
[2021-05-15 13:23:45,879] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Starting: Task committer attempt_202105151622261587176750931739446_0004_m_000088_376: needsTaskCommit() Task attempt_202105151622261587176750931739446_0004_m_000088_376
[2021-05-15 13:23:45,880] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Task committer attempt_202105151622261587176750931739446_0004_m_000088_376: needsTaskCommit() Task attempt_202105151622261587176750931739446_0004_m_000088_376: duration 0:00.001s
21/05/15 16:23:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261587176750931739446_0004_m_000088_376
[2021-05-15 13:23:45,881] {docker.py:276} INFO - 21/05/15 16:23:45 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 4544 bytes result sent to driver
[2021-05-15 13:23:45,882] {docker.py:276} INFO - 21/05/15 16:23:45 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (c776c6fed6bc, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:45,882] {docker.py:276} INFO - 21/05/15 16:23:45 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
[2021-05-15 13:23:45,883] {docker.py:276} INFO - 21/05/15 16:23:45 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 2798 ms on c776c6fed6bc (executor driver) (90/200)
[2021-05-15 13:23:45,890] {docker.py:276} INFO - 21/05/15 16:23:45 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:45,892] {docker.py:276} INFO - 21/05/15 16:23:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226337394004135227761_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226337394004135227761_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226337394004135227761_0004}; taskId=attempt_20210515162226337394004135227761_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75e83261}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:45,893] {docker.py:276} INFO - 21/05/15 16:23:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:45 INFO StagingCommitter: Starting: Task committer attempt_20210515162226337394004135227761_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226337394004135227761_0004_m_000093_381
[2021-05-15 13:23:45,895] {docker.py:276} INFO - 21/05/15 16:23:45 INFO StagingCommitter: Task committer attempt_20210515162226337394004135227761_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226337394004135227761_0004_m_000093_381 : duration 0:00.003s
[2021-05-15 13:23:46,329] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622266840595839606032935_0004_m_000089_377: needsTaskCommit() Task attempt_202105151622266840595839606032935_0004_m_000089_377
[2021-05-15 13:23:46,330] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Task committer attempt_202105151622266840595839606032935_0004_m_000089_377: needsTaskCommit() Task attempt_202105151622266840595839606032935_0004_m_000089_377: duration 0:00.001s
21/05/15 16:23:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266840595839606032935_0004_m_000089_377
[2021-05-15 13:23:46,331] {docker.py:276} INFO - 21/05/15 16:23:46 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 4544 bytes result sent to driver
[2021-05-15 13:23:46,332] {docker.py:276} INFO - 21/05/15 16:23:46 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (c776c6fed6bc, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:46,332] {docker.py:276} INFO - 21/05/15 16:23:46 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 3148 ms on c776c6fed6bc (executor driver) (91/200)
[2021-05-15 13:23:46,333] {docker.py:276} INFO - 21/05/15 16:23:46 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-15 13:23:46,341] {docker.py:276} INFO - 21/05/15 16:23:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:46,343] {docker.py:276} INFO - 21/05/15 16:23:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:23:46,344] {docker.py:276} INFO - 21/05/15 16:23:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:46,344] {docker.py:276} INFO - 21/05/15 16:23:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:46,344] {docker.py:276} INFO - 21/05/15 16:23:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265228616917848521122_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265228616917848521122_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265228616917848521122_0004}; taskId=attempt_202105151622265228616917848521122_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@232df76a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:46,345] {docker.py:276} INFO - 21/05/15 16:23:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622265228616917848521122_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265228616917848521122_0004_m_000094_382
[2021-05-15 13:23:46,348] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Task committer attempt_202105151622265228616917848521122_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265228616917848521122_0004_m_000094_382 : duration 0:00.003s
[2021-05-15 13:23:46,627] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622265397246832446829301_0004_m_000091_379: needsTaskCommit() Task attempt_202105151622265397246832446829301_0004_m_000091_379
[2021-05-15 13:23:46,628] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Task committer attempt_202105151622265397246832446829301_0004_m_000091_379: needsTaskCommit() Task attempt_202105151622265397246832446829301_0004_m_000091_379: duration 0:00.000s
[2021-05-15 13:23:46,628] {docker.py:276} INFO - 21/05/15 16:23:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265397246832446829301_0004_m_000091_379
[2021-05-15 13:23:46,629] {docker.py:276} INFO - 21/05/15 16:23:46 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 4544 bytes result sent to driver
[2021-05-15 13:23:46,630] {docker.py:276} INFO - 21/05/15 16:23:46 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (c776c6fed6bc, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:46,631] {docker.py:276} INFO - 21/05/15 16:23:46 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 2774 ms on c776c6fed6bc (executor driver) (92/200)
[2021-05-15 13:23:46,632] {docker.py:276} INFO - 21/05/15 16:23:46 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-15 13:23:46,641] {docker.py:276} INFO - 21/05/15 16:23:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:46,643] {docker.py:276} INFO - 21/05/15 16:23:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:46,643] {docker.py:276} INFO - 21/05/15 16:23:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268666985126525155704_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268666985126525155704_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268666985126525155704_0004}; taskId=attempt_202105151622268666985126525155704_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46411b8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:46,644] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622268666985126525155704_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268666985126525155704_0004_m_000095_383
[2021-05-15 13:23:46,647] {docker.py:276} INFO - 21/05/15 16:23:46 INFO StagingCommitter: Task committer attempt_202105151622268666985126525155704_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268666985126525155704_0004_m_000095_383 : duration 0:00.004s
[2021-05-15 13:23:48,499] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622267139656888712795922_0004_m_000092_380: needsTaskCommit() Task attempt_202105151622267139656888712795922_0004_m_000092_380
21/05/15 16:23:48 INFO StagingCommitter: Task committer attempt_202105151622267139656888712795922_0004_m_000092_380: needsTaskCommit() Task attempt_202105151622267139656888712795922_0004_m_000092_380: duration 0:00.001s
[2021-05-15 13:23:48,500] {docker.py:276} INFO - 21/05/15 16:23:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267139656888712795922_0004_m_000092_380
[2021-05-15 13:23:48,503] {docker.py:276} INFO - 21/05/15 16:23:48 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 4544 bytes result sent to driver
[2021-05-15 13:23:48,504] {docker.py:276} INFO - 21/05/15 16:23:48 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (c776c6fed6bc, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:48,505] {docker.py:276} INFO - 21/05/15 16:23:48 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 2658 ms on c776c6fed6bc (executor driver) (93/200)
21/05/15 16:23:48 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-15 13:23:48,515] {docker.py:276} INFO - 21/05/15 16:23:48 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:48,517] {docker.py:276} INFO - 21/05/15 16:23:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:48,518] {docker.py:276} INFO - 21/05/15 16:23:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264783825620274444415_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264783825620274444415_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264783825620274444415_0004}; taskId=attempt_202105151622264783825620274444415_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@443f4afb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:48,518] {docker.py:276} INFO - 21/05/15 16:23:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622264783825620274444415_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264783825620274444415_0004_m_000096_384
[2021-05-15 13:23:48,521] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Task committer attempt_202105151622264783825620274444415_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264783825620274444415_0004_m_000096_384 : duration 0:00.003s
[2021-05-15 13:23:48,606] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Starting: Task committer attempt_20210515162226337394004135227761_0004_m_000093_381: needsTaskCommit() Task attempt_20210515162226337394004135227761_0004_m_000093_381
21/05/15 16:23:48 INFO StagingCommitter: Task committer attempt_20210515162226337394004135227761_0004_m_000093_381: needsTaskCommit() Task attempt_20210515162226337394004135227761_0004_m_000093_381: duration 0:00.000s
[2021-05-15 13:23:48,607] {docker.py:276} INFO - 21/05/15 16:23:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226337394004135227761_0004_m_000093_381
[2021-05-15 13:23:48,609] {docker.py:276} INFO - 21/05/15 16:23:48 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 4544 bytes result sent to driver
[2021-05-15 13:23:48,610] {docker.py:276} INFO - 21/05/15 16:23:48 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (c776c6fed6bc, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:48,611] {docker.py:276} INFO - 21/05/15 16:23:48 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 2732 ms on c776c6fed6bc (executor driver) (94/200)
[2021-05-15 13:23:48,613] {docker.py:276} INFO - 21/05/15 16:23:48 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
[2021-05-15 13:23:48,621] {docker.py:276} INFO - 21/05/15 16:23:48 INFO ShuffleBlockFetcherIterator: Getting 5 (24.3 KiB) non-empty blocks including 5 (24.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:48,623] {docker.py:276} INFO - 21/05/15 16:23:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262219090391030838281_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262219090391030838281_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262219090391030838281_0004}; taskId=attempt_202105151622262219090391030838281_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@251c635}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622262219090391030838281_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262219090391030838281_0004_m_000097_385
[2021-05-15 13:23:48,626] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Task committer attempt_202105151622262219090391030838281_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262219090391030838281_0004_m_000097_385 : duration 0:00.002s
[2021-05-15 13:23:48,884] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622265228616917848521122_0004_m_000094_382: needsTaskCommit() Task attempt_202105151622265228616917848521122_0004_m_000094_382
[2021-05-15 13:23:48,885] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Task committer attempt_202105151622265228616917848521122_0004_m_000094_382: needsTaskCommit() Task attempt_202105151622265228616917848521122_0004_m_000094_382: duration 0:00.001s
21/05/15 16:23:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265228616917848521122_0004_m_000094_382
[2021-05-15 13:23:48,890] {docker.py:276} INFO - 21/05/15 16:23:48 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 4544 bytes result sent to driver
[2021-05-15 13:23:48,892] {docker.py:276} INFO - 21/05/15 16:23:48 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (c776c6fed6bc, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:48,892] {docker.py:276} INFO - 21/05/15 16:23:48 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 2563 ms on c776c6fed6bc (executor driver) (95/200)
[2021-05-15 13:23:48,893] {docker.py:276} INFO - 21/05/15 16:23:48 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-15 13:23:48,903] {docker.py:276} INFO - 21/05/15 16:23:48 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:48,905] {docker.py:276} INFO - 21/05/15 16:23:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:48,906] {docker.py:276} INFO - 21/05/15 16:23:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268236244756392469382_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268236244756392469382_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268236244756392469382_0004}; taskId=attempt_202105151622268236244756392469382_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@368fdfce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:48,906] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622268236244756392469382_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268236244756392469382_0004_m_000098_386
[2021-05-15 13:23:48,909] {docker.py:276} INFO - 21/05/15 16:23:48 INFO StagingCommitter: Task committer attempt_202105151622268236244756392469382_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268236244756392469382_0004_m_000098_386 : duration 0:00.003s
[2021-05-15 13:23:49,221] {docker.py:276} INFO - 21/05/15 16:23:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622268666985126525155704_0004_m_000095_383: needsTaskCommit() Task attempt_202105151622268666985126525155704_0004_m_000095_383
[2021-05-15 13:23:49,222] {docker.py:276} INFO - 21/05/15 16:23:49 INFO StagingCommitter: Task committer attempt_202105151622268666985126525155704_0004_m_000095_383: needsTaskCommit() Task attempt_202105151622268666985126525155704_0004_m_000095_383: duration 0:00.001s
21/05/15 16:23:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268666985126525155704_0004_m_000095_383
[2021-05-15 13:23:49,225] {docker.py:276} INFO - 21/05/15 16:23:49 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 4544 bytes result sent to driver
[2021-05-15 13:23:49,227] {docker.py:276} INFO - 21/05/15 16:23:49 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (c776c6fed6bc, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:49,228] {docker.py:276} INFO - 21/05/15 16:23:49 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-15 13:23:49,229] {docker.py:276} INFO - 21/05/15 16:23:49 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 2601 ms on c776c6fed6bc (executor driver) (96/200)
[2021-05-15 13:23:49,239] {docker.py:276} INFO - 21/05/15 16:23:49 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:49,241] {docker.py:276} INFO - 21/05/15 16:23:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262674002378985175246_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262674002378985175246_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262674002378985175246_0004}; taskId=attempt_202105151622262674002378985175246_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47e0e0a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622262674002378985175246_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262674002378985175246_0004_m_000099_387
[2021-05-15 13:23:49,244] {docker.py:276} INFO - 21/05/15 16:23:49 INFO StagingCommitter: Task committer attempt_202105151622262674002378985175246_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262674002378985175246_0004_m_000099_387 : duration 0:00.003s
[2021-05-15 13:23:51,114] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622264783825620274444415_0004_m_000096_384: needsTaskCommit() Task attempt_202105151622264783825620274444415_0004_m_000096_384
[2021-05-15 13:23:51,115] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622264783825620274444415_0004_m_000096_384: needsTaskCommit() Task attempt_202105151622264783825620274444415_0004_m_000096_384: duration 0:00.001s
[2021-05-15 13:23:51,116] {docker.py:276} INFO - 21/05/15 16:23:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264783825620274444415_0004_m_000096_384
[2021-05-15 13:23:51,117] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 4544 bytes result sent to driver
[2021-05-15 13:23:51,119] {docker.py:276} INFO - 21/05/15 16:23:51 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (c776c6fed6bc, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:51,119] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
21/05/15 16:23:51 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 2620 ms on c776c6fed6bc (executor driver) (97/200)
[2021-05-15 13:23:51,143] {docker.py:276} INFO - 21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:51,146] {docker.py:276} INFO - 21/05/15 16:23:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:51,147] {docker.py:276} INFO - 21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267769229278884732382_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267769229278884732382_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267769229278884732382_0004}; taskId=attempt_202105151622267769229278884732382_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68ec09ca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:51,148] {docker.py:276} INFO - 21/05/15 16:23:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:51,148] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622267769229278884732382_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267769229278884732382_0004_m_000100_388
[2021-05-15 13:23:51,153] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622267769229278884732382_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267769229278884732382_0004_m_000100_388 : duration 0:00.005s
[2021-05-15 13:23:51,291] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622262219090391030838281_0004_m_000097_385: needsTaskCommit() Task attempt_202105151622262219090391030838281_0004_m_000097_385
[2021-05-15 13:23:51,293] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622262219090391030838281_0004_m_000097_385: needsTaskCommit() Task attempt_202105151622262219090391030838281_0004_m_000097_385: duration 0:00.001s
[2021-05-15 13:23:51,293] {docker.py:276} INFO - 21/05/15 16:23:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262219090391030838281_0004_m_000097_385
[2021-05-15 13:23:51,294] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 4587 bytes result sent to driver
[2021-05-15 13:23:51,296] {docker.py:276} INFO - 21/05/15 16:23:51 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (c776c6fed6bc, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:51,297] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
21/05/15 16:23:51 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 2690 ms on c776c6fed6bc (executor driver) (98/200)
[2021-05-15 13:23:51,306] {docker.py:276} INFO - 21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:51,310] {docker.py:276} INFO - 21/05/15 16:23:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:51,311] {docker.py:276} INFO - 21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:51,311] {docker.py:276} INFO - 21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226401127872885537146_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226401127872885537146_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226401127872885537146_0004}; taskId=attempt_20210515162226401127872885537146_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75e7c473}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:51,311] {docker.py:276} INFO - 21/05/15 16:23:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:51,312] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_20210515162226401127872885537146_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226401127872885537146_0004_m_000101_389
[2021-05-15 13:23:51,314] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_20210515162226401127872885537146_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226401127872885537146_0004_m_000101_389 : duration 0:00.003s
[2021-05-15 13:23:51,438] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622268236244756392469382_0004_m_000098_386: needsTaskCommit() Task attempt_202105151622268236244756392469382_0004_m_000098_386
[2021-05-15 13:23:51,439] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622268236244756392469382_0004_m_000098_386: needsTaskCommit() Task attempt_202105151622268236244756392469382_0004_m_000098_386: duration 0:00.001s
21/05/15 16:23:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268236244756392469382_0004_m_000098_386
[2021-05-15 13:23:51,441] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 4587 bytes result sent to driver
[2021-05-15 13:23:51,443] {docker.py:276} INFO - 21/05/15 16:23:51 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (c776c6fed6bc, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:51,444] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-15 13:23:51,444] {docker.py:276} INFO - 21/05/15 16:23:51 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 2556 ms on c776c6fed6bc (executor driver) (99/200)
[2021-05-15 13:23:51,453] {docker.py:276} INFO - 21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:51,456] {docker.py:276} INFO - 21/05/15 16:23:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265505325904573668866_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265505325904573668866_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265505325904573668866_0004}; taskId=attempt_202105151622265505325904573668866_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ab103bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622265505325904573668866_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265505325904573668866_0004_m_000102_390
[2021-05-15 13:23:51,459] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622265505325904573668866_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265505325904573668866_0004_m_000102_390 : duration 0:00.003s
[2021-05-15 13:23:51,850] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622262674002378985175246_0004_m_000099_387: needsTaskCommit() Task attempt_202105151622262674002378985175246_0004_m_000099_387
21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622262674002378985175246_0004_m_000099_387: needsTaskCommit() Task attempt_202105151622262674002378985175246_0004_m_000099_387: duration 0:00.000s
21/05/15 16:23:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262674002378985175246_0004_m_000099_387
[2021-05-15 13:23:51,852] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 4587 bytes result sent to driver
[2021-05-15 13:23:51,852] {docker.py:276} INFO - 21/05/15 16:23:51 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (c776c6fed6bc, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:51,854] {docker.py:276} INFO - 21/05/15 16:23:51 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 2632 ms on c776c6fed6bc (executor driver) (100/200)
[2021-05-15 13:23:51,856] {docker.py:276} INFO - 21/05/15 16:23:51 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-15 13:23:51,869] {docker.py:276} INFO - 21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:51,872] {docker.py:276} INFO - 21/05/15 16:23:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261842275914882061183_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261842275914882061183_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261842275914882061183_0004}; taskId=attempt_202105151622261842275914882061183_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35a59193}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:51,872] {docker.py:276} INFO - 21/05/15 16:23:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622261842275914882061183_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261842275914882061183_0004_m_000103_391
[2021-05-15 13:23:51,876] {docker.py:276} INFO - 21/05/15 16:23:51 INFO StagingCommitter: Task committer attempt_202105151622261842275914882061183_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261842275914882061183_0004_m_000103_391 : duration 0:00.004s
[2021-05-15 13:23:53,713] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Starting: Task committer attempt_202105151622267769229278884732382_0004_m_000100_388: needsTaskCommit() Task attempt_202105151622267769229278884732382_0004_m_000100_388
[2021-05-15 13:23:53,713] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Task committer attempt_202105151622267769229278884732382_0004_m_000100_388: needsTaskCommit() Task attempt_202105151622267769229278884732382_0004_m_000100_388: duration 0:00.000s
21/05/15 16:23:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267769229278884732382_0004_m_000100_388
[2021-05-15 13:23:53,714] {docker.py:276} INFO - 21/05/15 16:23:53 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 4587 bytes result sent to driver
[2021-05-15 13:23:53,715] {docker.py:276} INFO - 21/05/15 16:23:53 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (c776c6fed6bc, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:53,717] {docker.py:276} INFO - 21/05/15 16:23:53 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 2602 ms on c776c6fed6bc (executor driver) (101/200)
[2021-05-15 13:23:53,718] {docker.py:276} INFO - 21/05/15 16:23:53 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
[2021-05-15 13:23:53,726] {docker.py:276} INFO - 21/05/15 16:23:53 INFO ShuffleBlockFetcherIterator: Getting 5 (21.7 KiB) non-empty blocks including 5 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:53,729] {docker.py:276} INFO - 21/05/15 16:23:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265704573930234189666_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265704573930234189666_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265704573930234189666_0004}; taskId=attempt_202105151622265704573930234189666_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2966c4f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:53,729] {docker.py:276} INFO - 21/05/15 16:23:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:53 INFO StagingCommitter: Starting: Task committer attempt_202105151622265704573930234189666_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265704573930234189666_0004_m_000104_392
[2021-05-15 13:23:53,732] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Task committer attempt_202105151622265704573930234189666_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265704573930234189666_0004_m_000104_392 : duration 0:00.003s
[2021-05-15 13:23:53,873] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Starting: Task committer attempt_20210515162226401127872885537146_0004_m_000101_389: needsTaskCommit() Task attempt_20210515162226401127872885537146_0004_m_000101_389
[2021-05-15 13:23:53,874] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Task committer attempt_20210515162226401127872885537146_0004_m_000101_389: needsTaskCommit() Task attempt_20210515162226401127872885537146_0004_m_000101_389: duration 0:00.000s
[2021-05-15 13:23:53,874] {docker.py:276} INFO - 21/05/15 16:23:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226401127872885537146_0004_m_000101_389
[2021-05-15 13:23:53,876] {docker.py:276} INFO - 21/05/15 16:23:53 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 4544 bytes result sent to driver
[2021-05-15 13:23:53,878] {docker.py:276} INFO - 21/05/15 16:23:53 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (c776c6fed6bc, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:53,879] {docker.py:276} INFO - 21/05/15 16:23:53 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
21/05/15 16:23:53 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 2586 ms on c776c6fed6bc (executor driver) (102/200)
[2021-05-15 13:23:53,893] {docker.py:276} INFO - 21/05/15 16:23:53 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:53,895] {docker.py:276} INFO - 21/05/15 16:23:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:53,900] {docker.py:276} INFO - 21/05/15 16:23:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:23:53,901] {docker.py:276} INFO - 21/05/15 16:23:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:53,902] {docker.py:276} INFO - 21/05/15 16:23:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:53,903] {docker.py:276} INFO - 21/05/15 16:23:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226930175072872287633_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226930175072872287633_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226930175072872287633_0004}; taskId=attempt_20210515162226930175072872287633_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@681658b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:53,903] {docker.py:276} INFO - 21/05/15 16:23:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:53,904] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Starting: Task committer attempt_20210515162226930175072872287633_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226930175072872287633_0004_m_000105_393
[2021-05-15 13:23:53,909] {docker.py:276} INFO - 21/05/15 16:23:53 INFO StagingCommitter: Task committer attempt_20210515162226930175072872287633_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226930175072872287633_0004_m_000105_393 : duration 0:00.005s
[2021-05-15 13:23:54,023] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622265505325904573668866_0004_m_000102_390: needsTaskCommit() Task attempt_202105151622265505325904573668866_0004_m_000102_390
[2021-05-15 13:23:54,024] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Task committer attempt_202105151622265505325904573668866_0004_m_000102_390: needsTaskCommit() Task attempt_202105151622265505325904573668866_0004_m_000102_390: duration 0:00.001s
21/05/15 16:23:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265505325904573668866_0004_m_000102_390
[2021-05-15 13:23:54,026] {docker.py:276} INFO - 21/05/15 16:23:54 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 4544 bytes result sent to driver
[2021-05-15 13:23:54,027] {docker.py:276} INFO - 21/05/15 16:23:54 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (c776c6fed6bc, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:54,028] {docker.py:276} INFO - 21/05/15 16:23:54 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-15 13:23:54,029] {docker.py:276} INFO - 21/05/15 16:23:54 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 2589 ms on c776c6fed6bc (executor driver) (103/200)
[2021-05-15 13:23:54,040] {docker.py:276} INFO - 21/05/15 16:23:54 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:54,043] {docker.py:276} INFO - 21/05/15 16:23:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:54,044] {docker.py:276} INFO - 21/05/15 16:23:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262185348811490545196_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262185348811490545196_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262185348811490545196_0004}; taskId=attempt_202105151622262185348811490545196_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ec8ed6d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:54,045] {docker.py:276} INFO - 21/05/15 16:23:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:54,045] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622262185348811490545196_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262185348811490545196_0004_m_000106_394
[2021-05-15 13:23:54,049] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Task committer attempt_202105151622262185348811490545196_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262185348811490545196_0004_m_000106_394 : duration 0:00.005s
[2021-05-15 13:23:54,066] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622261842275914882061183_0004_m_000103_391: needsTaskCommit() Task attempt_202105151622261842275914882061183_0004_m_000103_391
[2021-05-15 13:23:54,067] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Task committer attempt_202105151622261842275914882061183_0004_m_000103_391: needsTaskCommit() Task attempt_202105151622261842275914882061183_0004_m_000103_391: duration 0:00.001s
21/05/15 16:23:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261842275914882061183_0004_m_000103_391
[2021-05-15 13:23:54,068] {docker.py:276} INFO - 21/05/15 16:23:54 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 4544 bytes result sent to driver
[2021-05-15 13:23:54,070] {docker.py:276} INFO - 21/05/15 16:23:54 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (c776c6fed6bc, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:54,071] {docker.py:276} INFO - 21/05/15 16:23:54 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
[2021-05-15 13:23:54,072] {docker.py:276} INFO - 21/05/15 16:23:54 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 2222 ms on c776c6fed6bc (executor driver) (104/200)
[2021-05-15 13:23:54,083] {docker.py:276} INFO - 21/05/15 16:23:54 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:54,085] {docker.py:276} INFO - 21/05/15 16:23:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:54,086] {docker.py:276} INFO - 21/05/15 16:23:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266041046113760164886_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266041046113760164886_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266041046113760164886_0004}; taskId=attempt_202105151622266041046113760164886_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c3b2b9e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:54,087] {docker.py:276} INFO - 21/05/15 16:23:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:54,087] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622266041046113760164886_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266041046113760164886_0004_m_000107_395
[2021-05-15 13:23:54,090] {docker.py:276} INFO - 21/05/15 16:23:54 INFO StagingCommitter: Task committer attempt_202105151622266041046113760164886_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266041046113760164886_0004_m_000107_395 : duration 0:00.004s
[2021-05-15 13:23:56,343] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622265704573930234189666_0004_m_000104_392: needsTaskCommit() Task attempt_202105151622265704573930234189666_0004_m_000104_392
[2021-05-15 13:23:56,344] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Task committer attempt_202105151622265704573930234189666_0004_m_000104_392: needsTaskCommit() Task attempt_202105151622265704573930234189666_0004_m_000104_392: duration 0:00.002s
21/05/15 16:23:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265704573930234189666_0004_m_000104_392
[2021-05-15 13:23:56,347] {docker.py:276} INFO - 21/05/15 16:23:56 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 4544 bytes result sent to driver
[2021-05-15 13:23:56,350] {docker.py:276} INFO - 21/05/15 16:23:56 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (c776c6fed6bc, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:56,351] {docker.py:276} INFO - 21/05/15 16:23:56 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
21/05/15 16:23:56 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 2636 ms on c776c6fed6bc (executor driver) (105/200)
[2021-05-15 13:23:56,372] {docker.py:276} INFO - 21/05/15 16:23:56 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:56,378] {docker.py:276} INFO - 21/05/15 16:23:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267958102248681980398_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267958102248681980398_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267958102248681980398_0004}; taskId=attempt_202105151622267958102248681980398_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d5bc4d2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622267958102248681980398_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267958102248681980398_0004_m_000108_396
[2021-05-15 13:23:56,384] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Task committer attempt_202105151622267958102248681980398_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267958102248681980398_0004_m_000108_396 : duration 0:00.006s
[2021-05-15 13:23:56,579] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622266041046113760164886_0004_m_000107_395: needsTaskCommit() Task attempt_202105151622266041046113760164886_0004_m_000107_395
[2021-05-15 13:23:56,579] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Task committer attempt_202105151622266041046113760164886_0004_m_000107_395: needsTaskCommit() Task attempt_202105151622266041046113760164886_0004_m_000107_395: duration 0:00.000s
[2021-05-15 13:23:56,579] {docker.py:276} INFO - 21/05/15 16:23:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266041046113760164886_0004_m_000107_395
[2021-05-15 13:23:56,580] {docker.py:276} INFO - 21/05/15 16:23:56 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 4544 bytes result sent to driver
[2021-05-15 13:23:56,581] {docker.py:276} INFO - 21/05/15 16:23:56 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (c776c6fed6bc, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:56,582] {docker.py:276} INFO - 21/05/15 16:23:56 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 2517 ms on c776c6fed6bc (executor driver) (106/200)
[2021-05-15 13:23:56,583] {docker.py:276} INFO - 21/05/15 16:23:56 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-15 13:23:56,593] {docker.py:276} INFO - 21/05/15 16:23:56 INFO ShuffleBlockFetcherIterator: Getting 5 (21.8 KiB) non-empty blocks including 5 (21.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:56,594] {docker.py:276} INFO - 21/05/15 16:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:56,596] {docker.py:276} INFO - 21/05/15 16:23:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:56,596] {docker.py:276} INFO - 21/05/15 16:23:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266528908370445606317_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266528908370445606317_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266528908370445606317_0004}; taskId=attempt_202105151622266528908370445606317_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@237417d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:56,597] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622266528908370445606317_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266528908370445606317_0004_m_000109_397
[2021-05-15 13:23:56,601] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Task committer attempt_202105151622266528908370445606317_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266528908370445606317_0004_m_000109_397 : duration 0:00.004s
[2021-05-15 13:23:56,765] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622262185348811490545196_0004_m_000106_394: needsTaskCommit() Task attempt_202105151622262185348811490545196_0004_m_000106_394
[2021-05-15 13:23:56,766] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Task committer attempt_202105151622262185348811490545196_0004_m_000106_394: needsTaskCommit() Task attempt_202105151622262185348811490545196_0004_m_000106_394: duration 0:00.001s
21/05/15 16:23:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262185348811490545196_0004_m_000106_394
[2021-05-15 13:23:56,767] {docker.py:276} INFO - 21/05/15 16:23:56 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 4544 bytes result sent to driver
[2021-05-15 13:23:56,768] {docker.py:276} INFO - 21/05/15 16:23:56 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (c776c6fed6bc, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:56,769] {docker.py:276} INFO - 21/05/15 16:23:56 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 2746 ms on c776c6fed6bc (executor driver) (107/200)
[2021-05-15 13:23:56,769] {docker.py:276} INFO - 21/05/15 16:23:56 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-15 13:23:56,781] {docker.py:276} INFO - 21/05/15 16:23:56 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:56,784] {docker.py:276} INFO - 21/05/15 16:23:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262182140955574367780_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262182140955574367780_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262182140955574367780_0004}; taskId=attempt_202105151622262182140955574367780_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@650a402e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:56,784] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622262182140955574367780_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262182140955574367780_0004_m_000110_398
[2021-05-15 13:23:56,789] {docker.py:276} INFO - 21/05/15 16:23:56 INFO StagingCommitter: Task committer attempt_202105151622262182140955574367780_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262182140955574367780_0004_m_000110_398 : duration 0:00.005s
[2021-05-15 13:23:57,192] {docker.py:276} INFO - 21/05/15 16:23:57 INFO StagingCommitter: Starting: Task committer attempt_20210515162226930175072872287633_0004_m_000105_393: needsTaskCommit() Task attempt_20210515162226930175072872287633_0004_m_000105_393
[2021-05-15 13:23:57,193] {docker.py:276} INFO - 21/05/15 16:23:57 INFO StagingCommitter: Task committer attempt_20210515162226930175072872287633_0004_m_000105_393: needsTaskCommit() Task attempt_20210515162226930175072872287633_0004_m_000105_393: duration 0:00.001s
21/05/15 16:23:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226930175072872287633_0004_m_000105_393
[2021-05-15 13:23:57,194] {docker.py:276} INFO - 21/05/15 16:23:57 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 4544 bytes result sent to driver
[2021-05-15 13:23:57,196] {docker.py:276} INFO - 21/05/15 16:23:57 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (c776c6fed6bc, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:57,197] {docker.py:276} INFO - 21/05/15 16:23:57 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-15 13:23:57,197] {docker.py:276} INFO - 21/05/15 16:23:57 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 3323 ms on c776c6fed6bc (executor driver) (108/200)
[2021-05-15 13:23:57,208] {docker.py:276} INFO - 21/05/15 16:23:57 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:57,212] {docker.py:276} INFO - 21/05/15 16:23:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:57,213] {docker.py:276} INFO - 21/05/15 16:23:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268426382350659598010_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268426382350659598010_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268426382350659598010_0004}; taskId=attempt_202105151622268426382350659598010_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65596d3d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:57,214] {docker.py:276} INFO - 21/05/15 16:23:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:57,214] {docker.py:276} INFO - 21/05/15 16:23:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622268426382350659598010_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268426382350659598010_0004_m_000111_399
[2021-05-15 13:23:57,218] {docker.py:276} INFO - 21/05/15 16:23:57 INFO StagingCommitter: Task committer attempt_202105151622268426382350659598010_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268426382350659598010_0004_m_000111_399 : duration 0:00.005s
[2021-05-15 13:23:58,968] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622267958102248681980398_0004_m_000108_396: needsTaskCommit() Task attempt_202105151622267958102248681980398_0004_m_000108_396
[2021-05-15 13:23:58,968] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622267958102248681980398_0004_m_000108_396: needsTaskCommit() Task attempt_202105151622267958102248681980398_0004_m_000108_396: duration 0:00.000s
[2021-05-15 13:23:58,969] {docker.py:276} INFO - 21/05/15 16:23:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267958102248681980398_0004_m_000108_396
[2021-05-15 13:23:58,971] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 4544 bytes result sent to driver
[2021-05-15 13:23:58,972] {docker.py:276} INFO - 21/05/15 16:23:59 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (c776c6fed6bc, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:58,973] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-15 13:23:58,974] {docker.py:276} INFO - 21/05/15 16:23:59 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 2631 ms on c776c6fed6bc (executor driver) (109/200)
[2021-05-15 13:23:58,987] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Getting 5 (22.0 KiB) non-empty blocks including 5 (22.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:58,988] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:58,991] {docker.py:276} INFO - 21/05/15 16:23:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:23:58,991] {docker.py:276} INFO - 21/05/15 16:23:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:58,992] {docker.py:276} INFO - 21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:58,993] {docker.py:276} INFO - 21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264187242815692334490_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264187242815692334490_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264187242815692334490_0004}; taskId=attempt_202105151622264187242815692334490_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6afe4747}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:58,993] {docker.py:276} INFO - 21/05/15 16:23:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:58,994] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622264187242815692334490_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264187242815692334490_0004_m_000112_400
[2021-05-15 13:23:58,998] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622264187242815692334490_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264187242815692334490_0004_m_000112_400 : duration 0:00.005s
[2021-05-15 13:23:59,291] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622266528908370445606317_0004_m_000109_397: needsTaskCommit() Task attempt_202105151622266528908370445606317_0004_m_000109_397
[2021-05-15 13:23:59,292] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622266528908370445606317_0004_m_000109_397: needsTaskCommit() Task attempt_202105151622266528908370445606317_0004_m_000109_397: duration 0:00.001s
[2021-05-15 13:23:59,293] {docker.py:276} INFO - 21/05/15 16:23:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266528908370445606317_0004_m_000109_397
[2021-05-15 13:23:59,296] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 4544 bytes result sent to driver
21/05/15 16:23:59 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (c776c6fed6bc, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:23:59 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 2717 ms on c776c6fed6bc (executor driver) (110/200)
[2021-05-15 13:23:59,297] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-15 13:23:59,306] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:59,307] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:59,310] {docker.py:276} INFO - 21/05/15 16:23:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:59,311] {docker.py:276} INFO - 21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265418807325060602859_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265418807325060602859_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265418807325060602859_0004}; taskId=attempt_202105151622265418807325060602859_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@132ab4b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:59,312] {docker.py:276} INFO - 21/05/15 16:23:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:59,312] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622265418807325060602859_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265418807325060602859_0004_m_000113_401
[2021-05-15 13:23:59,315] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622265418807325060602859_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265418807325060602859_0004_m_000113_401 : duration 0:00.005s
[2021-05-15 13:23:59,590] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622262182140955574367780_0004_m_000110_398: needsTaskCommit() Task attempt_202105151622262182140955574367780_0004_m_000110_398
[2021-05-15 13:23:59,591] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622262182140955574367780_0004_m_000110_398: needsTaskCommit() Task attempt_202105151622262182140955574367780_0004_m_000110_398: duration 0:00.001s
21/05/15 16:23:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262182140955574367780_0004_m_000110_398
[2021-05-15 13:23:59,594] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 4544 bytes result sent to driver
[2021-05-15 13:23:59,596] {docker.py:276} INFO - 21/05/15 16:23:59 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (c776c6fed6bc, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:59,597] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-15 13:23:59,598] {docker.py:276} INFO - 21/05/15 16:23:59 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 2796 ms on c776c6fed6bc (executor driver) (111/200)
[2021-05-15 13:23:59,610] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:23:59,612] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:23:59,616] {docker.py:276} INFO - 21/05/15 16:23:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:23:59,617] {docker.py:276} INFO - 21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:59,618] {docker.py:276} INFO - 21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266444842003162678227_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266444842003162678227_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266444842003162678227_0004}; taskId=attempt_202105151622266444842003162678227_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5aa9bd5b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:59,618] {docker.py:276} INFO - 21/05/15 16:23:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:23:59,619] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622266444842003162678227_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266444842003162678227_0004_m_000114_402
[2021-05-15 13:23:59,622] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622266444842003162678227_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266444842003162678227_0004_m_000114_402 : duration 0:00.005s
[2021-05-15 13:23:59,794] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622268426382350659598010_0004_m_000111_399: needsTaskCommit() Task attempt_202105151622268426382350659598010_0004_m_000111_399
[2021-05-15 13:23:59,794] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622268426382350659598010_0004_m_000111_399: needsTaskCommit() Task attempt_202105151622268426382350659598010_0004_m_000111_399: duration 0:00.000s
21/05/15 16:23:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268426382350659598010_0004_m_000111_399
[2021-05-15 13:23:59,798] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 4544 bytes result sent to driver
[2021-05-15 13:23:59,798] {docker.py:276} INFO - 21/05/15 16:23:59 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (c776c6fed6bc, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:23:59,799] {docker.py:276} INFO - 21/05/15 16:23:59 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
[2021-05-15 13:23:59,800] {docker.py:276} INFO - 21/05/15 16:23:59 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 2574 ms on c776c6fed6bc (executor driver) (112/200)
[2021-05-15 13:23:59,810] {docker.py:276} INFO - 21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:23:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:23:59,825] {docker.py:276} INFO - 21/05/15 16:23:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:23:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:23:59,826] {docker.py:276} INFO - 21/05/15 16:23:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261168233749510524752_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261168233749510524752_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261168233749510524752_0004}; taskId=attempt_202105151622261168233749510524752_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b09f5e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:23:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:23:59 INFO StagingCommitter: Starting: Task committer attempt_202105151622261168233749510524752_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261168233749510524752_0004_m_000115_403
[2021-05-15 13:23:59,829] {docker.py:276} INFO - 21/05/15 16:23:59 INFO StagingCommitter: Task committer attempt_202105151622261168233749510524752_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261168233749510524752_0004_m_000115_403 : duration 0:00.003s
[2021-05-15 13:24:01,583] {docker.py:276} INFO - 21/05/15 16:24:01 INFO StagingCommitter: Starting: Task committer attempt_202105151622264187242815692334490_0004_m_000112_400: needsTaskCommit() Task attempt_202105151622264187242815692334490_0004_m_000112_400
[2021-05-15 13:24:01,583] {docker.py:276} INFO - 21/05/15 16:24:01 INFO StagingCommitter: Task committer attempt_202105151622264187242815692334490_0004_m_000112_400: needsTaskCommit() Task attempt_202105151622264187242815692334490_0004_m_000112_400: duration 0:00.001s
21/05/15 16:24:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264187242815692334490_0004_m_000112_400
[2021-05-15 13:24:01,585] {docker.py:276} INFO - 21/05/15 16:24:01 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 4587 bytes result sent to driver
[2021-05-15 13:24:01,586] {docker.py:276} INFO - 21/05/15 16:24:01 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (c776c6fed6bc, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:01,586] {docker.py:276} INFO - 21/05/15 16:24:01 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 2583 ms on c776c6fed6bc (executor driver) (113/200)
[2021-05-15 13:24:01,586] {docker.py:276} INFO - 21/05/15 16:24:01 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
[2021-05-15 13:24:01,598] {docker.py:276} INFO - 21/05/15 16:24:01 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:01,602] {docker.py:276} INFO - 21/05/15 16:24:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266979569683613329716_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266979569683613329716_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266979569683613329716_0004}; taskId=attempt_202105151622266979569683613329716_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@394d250c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:01 INFO StagingCommitter: Starting: Task committer attempt_202105151622266979569683613329716_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266979569683613329716_0004_m_000116_404
[2021-05-15 13:24:01,606] {docker.py:276} INFO - 21/05/15 16:24:01 INFO StagingCommitter: Task committer attempt_202105151622266979569683613329716_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266979569683613329716_0004_m_000116_404 : duration 0:00.005s
[2021-05-15 13:24:01,956] {docker.py:276} INFO - 21/05/15 16:24:01 INFO StagingCommitter: Starting: Task committer attempt_202105151622265418807325060602859_0004_m_000113_401: needsTaskCommit() Task attempt_202105151622265418807325060602859_0004_m_000113_401
[2021-05-15 13:24:01,957] {docker.py:276} INFO - 21/05/15 16:24:01 INFO StagingCommitter: Task committer attempt_202105151622265418807325060602859_0004_m_000113_401: needsTaskCommit() Task attempt_202105151622265418807325060602859_0004_m_000113_401: duration 0:00.000s
21/05/15 16:24:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265418807325060602859_0004_m_000113_401
[2021-05-15 13:24:01,958] {docker.py:276} INFO - 21/05/15 16:24:01 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 4587 bytes result sent to driver
[2021-05-15 13:24:01,960] {docker.py:276} INFO - 21/05/15 16:24:01 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (c776c6fed6bc, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:01,961] {docker.py:276} INFO - 21/05/15 16:24:01 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
[2021-05-15 13:24:01,962] {docker.py:276} INFO - 21/05/15 16:24:01 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 2635 ms on c776c6fed6bc (executor driver) (114/200)
[2021-05-15 13:24:01,974] {docker.py:276} INFO - 21/05/15 16:24:01 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:01,974] {docker.py:276} INFO - 21/05/15 16:24:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:01,977] {docker.py:276} INFO - 21/05/15 16:24:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:01,977] {docker.py:276} INFO - 21/05/15 16:24:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:01,978] {docker.py:276} INFO - 21/05/15 16:24:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263613931509196181722_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263613931509196181722_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263613931509196181722_0004}; taskId=attempt_202105151622263613931509196181722_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36ebf66d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:01,979] {docker.py:276} INFO - 21/05/15 16:24:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:01 INFO StagingCommitter: Starting: Task committer attempt_202105151622263613931509196181722_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263613931509196181722_0004_m_000117_405
[2021-05-15 13:24:01,983] {docker.py:276} INFO - 21/05/15 16:24:01 INFO StagingCommitter: Task committer attempt_202105151622263613931509196181722_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263613931509196181722_0004_m_000117_405 : duration 0:00.004s
[2021-05-15 13:24:02,435] {docker.py:276} INFO - 21/05/15 16:24:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622261168233749510524752_0004_m_000115_403: needsTaskCommit() Task attempt_202105151622261168233749510524752_0004_m_000115_403
[2021-05-15 13:24:02,435] {docker.py:276} INFO - 21/05/15 16:24:02 INFO StagingCommitter: Task committer attempt_202105151622261168233749510524752_0004_m_000115_403: needsTaskCommit() Task attempt_202105151622261168233749510524752_0004_m_000115_403: duration 0:00.000s
[2021-05-15 13:24:02,436] {docker.py:276} INFO - 21/05/15 16:24:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261168233749510524752_0004_m_000115_403
[2021-05-15 13:24:02,437] {docker.py:276} INFO - 21/05/15 16:24:02 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 4587 bytes result sent to driver
[2021-05-15 13:24:02,438] {docker.py:276} INFO - 21/05/15 16:24:02 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (c776c6fed6bc, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:02,440] {docker.py:276} INFO - 21/05/15 16:24:02 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 2645 ms on c776c6fed6bc (executor driver) (115/200)
[2021-05-15 13:24:02,440] {docker.py:276} INFO - 21/05/15 16:24:02 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-15 13:24:02,453] {docker.py:276} INFO - 21/05/15 16:24:02 INFO ShuffleBlockFetcherIterator: Getting 5 (23.2 KiB) non-empty blocks including 5 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:02,454] {docker.py:276} INFO - 21/05/15 16:24:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:02,457] {docker.py:276} INFO - 21/05/15 16:24:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266994570040227964254_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266994570040227964254_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266994570040227964254_0004}; taskId=attempt_202105151622266994570040227964254_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b361ab0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:02,457] {docker.py:276} INFO - 21/05/15 16:24:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622266994570040227964254_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266994570040227964254_0004_m_000118_406
[2021-05-15 13:24:02,464] {docker.py:276} INFO - 21/05/15 16:24:02 INFO StagingCommitter: Task committer attempt_202105151622266994570040227964254_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266994570040227964254_0004_m_000118_406 : duration 0:00.007s
[2021-05-15 13:24:02,480] {docker.py:276} INFO - 21/05/15 16:24:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622266444842003162678227_0004_m_000114_402: needsTaskCommit() Task attempt_202105151622266444842003162678227_0004_m_000114_402
[2021-05-15 13:24:02,481] {docker.py:276} INFO - 21/05/15 16:24:02 INFO StagingCommitter: Task committer attempt_202105151622266444842003162678227_0004_m_000114_402: needsTaskCommit() Task attempt_202105151622266444842003162678227_0004_m_000114_402: duration 0:00.000s
21/05/15 16:24:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266444842003162678227_0004_m_000114_402
[2021-05-15 13:24:02,482] {docker.py:276} INFO - 21/05/15 16:24:02 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 4587 bytes result sent to driver
[2021-05-15 13:24:02,483] {docker.py:276} INFO - 21/05/15 16:24:02 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (c776c6fed6bc, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:02,483] {docker.py:276} INFO - 21/05/15 16:24:02 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 2893 ms on c776c6fed6bc (executor driver) (116/200)
21/05/15 16:24:02 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
[2021-05-15 13:24:02,493] {docker.py:276} INFO - 21/05/15 16:24:02 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:02,498] {docker.py:276} INFO - 21/05/15 16:24:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:24:02,499] {docker.py:276} INFO - 21/05/15 16:24:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:02,499] {docker.py:276} INFO - 21/05/15 16:24:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265539606924925138984_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265539606924925138984_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265539606924925138984_0004}; taskId=attempt_202105151622265539606924925138984_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f96dbd6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:02 INFO StagingCommitter: Starting: Task committer attempt_202105151622265539606924925138984_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265539606924925138984_0004_m_000119_407
[2021-05-15 13:24:02,503] {docker.py:276} INFO - 21/05/15 16:24:02 INFO StagingCommitter: Task committer attempt_202105151622265539606924925138984_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265539606924925138984_0004_m_000119_407 : duration 0:00.003s
[2021-05-15 13:24:04,129] {docker.py:276} INFO - 21/05/15 16:24:04 INFO StagingCommitter: Starting: Task committer attempt_202105151622266979569683613329716_0004_m_000116_404: needsTaskCommit() Task attempt_202105151622266979569683613329716_0004_m_000116_404
21/05/15 16:24:04 INFO StagingCommitter: Task committer attempt_202105151622266979569683613329716_0004_m_000116_404: needsTaskCommit() Task attempt_202105151622266979569683613329716_0004_m_000116_404: duration 0:00.000s
21/05/15 16:24:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266979569683613329716_0004_m_000116_404
[2021-05-15 13:24:04,130] {docker.py:276} INFO - 21/05/15 16:24:04 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 4544 bytes result sent to driver
[2021-05-15 13:24:04,131] {docker.py:276} INFO - 21/05/15 16:24:04 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (c776c6fed6bc, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:04,132] {docker.py:276} INFO - 21/05/15 16:24:04 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-15 13:24:04,132] {docker.py:276} INFO - 21/05/15 16:24:04 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 2550 ms on c776c6fed6bc (executor driver) (117/200)
[2021-05-15 13:24:04,141] {docker.py:276} INFO - 21/05/15 16:24:04 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:04,143] {docker.py:276} INFO - 21/05/15 16:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263197545482966724293_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263197545482966724293_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263197545482966724293_0004}; taskId=attempt_202105151622263197545482966724293_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@275117b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:04 INFO StagingCommitter: Starting: Task committer attempt_202105151622263197545482966724293_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263197545482966724293_0004_m_000120_408
[2021-05-15 13:24:04,147] {docker.py:276} INFO - 21/05/15 16:24:04 INFO StagingCommitter: Task committer attempt_202105151622263197545482966724293_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263197545482966724293_0004_m_000120_408 : duration 0:00.003s
[2021-05-15 13:24:04,607] {docker.py:276} INFO - 21/05/15 16:24:04 INFO StagingCommitter: Starting: Task committer attempt_202105151622263613931509196181722_0004_m_000117_405: needsTaskCommit() Task attempt_202105151622263613931509196181722_0004_m_000117_405
[2021-05-15 13:24:04,608] {docker.py:276} INFO - 21/05/15 16:24:04 INFO StagingCommitter: Task committer attempt_202105151622263613931509196181722_0004_m_000117_405: needsTaskCommit() Task attempt_202105151622263613931509196181722_0004_m_000117_405: duration 0:00.001s
21/05/15 16:24:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263613931509196181722_0004_m_000117_405
[2021-05-15 13:24:04,610] {docker.py:276} INFO - 21/05/15 16:24:04 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 4544 bytes result sent to driver
[2021-05-15 13:24:04,616] {docker.py:276} INFO - 21/05/15 16:24:04 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (c776c6fed6bc, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:04,616] {docker.py:276} INFO - 21/05/15 16:24:04 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
21/05/15 16:24:04 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 2657 ms on c776c6fed6bc (executor driver) (118/200)
[2021-05-15 13:24:04,633] {docker.py:276} INFO - 21/05/15 16:24:04 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:04,637] {docker.py:276} INFO - 21/05/15 16:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:04,638] {docker.py:276} INFO - 21/05/15 16:24:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:04,639] {docker.py:276} INFO - 21/05/15 16:24:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263235643079026938140_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263235643079026938140_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263235643079026938140_0004}; taskId=attempt_202105151622263235643079026938140_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c084fb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:04,639] {docker.py:276} INFO - 21/05/15 16:24:04 INFO StagingCommitter: Starting: Task committer attempt_202105151622263235643079026938140_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263235643079026938140_0004_m_000121_409
[2021-05-15 13:24:04,644] {docker.py:276} INFO - 21/05/15 16:24:04 INFO StagingCommitter: Task committer attempt_202105151622263235643079026938140_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263235643079026938140_0004_m_000121_409 : duration 0:00.006s
[2021-05-15 13:24:05,073] {docker.py:276} INFO - 21/05/15 16:24:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622266994570040227964254_0004_m_000118_406: needsTaskCommit() Task attempt_202105151622266994570040227964254_0004_m_000118_406
[2021-05-15 13:24:05,074] {docker.py:276} INFO - 21/05/15 16:24:05 INFO StagingCommitter: Task committer attempt_202105151622266994570040227964254_0004_m_000118_406: needsTaskCommit() Task attempt_202105151622266994570040227964254_0004_m_000118_406: duration 0:00.000s
21/05/15 16:24:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266994570040227964254_0004_m_000118_406
[2021-05-15 13:24:05,077] {docker.py:276} INFO - 21/05/15 16:24:05 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 4544 bytes result sent to driver
21/05/15 16:24:05 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (c776c6fed6bc, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:05,079] {docker.py:276} INFO - 21/05/15 16:24:05 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
21/05/15 16:24:05 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 2644 ms on c776c6fed6bc (executor driver) (119/200)
[2021-05-15 13:24:05,093] {docker.py:276} INFO - 21/05/15 16:24:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622265539606924925138984_0004_m_000119_407: needsTaskCommit() Task attempt_202105151622265539606924925138984_0004_m_000119_407
21/05/15 16:24:05 INFO StagingCommitter: Task committer attempt_202105151622265539606924925138984_0004_m_000119_407: needsTaskCommit() Task attempt_202105151622265539606924925138984_0004_m_000119_407: duration 0:00.000s
21/05/15 16:24:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265539606924925138984_0004_m_000119_407
21/05/15 16:24:05 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 4544 bytes result sent to driver
21/05/15 16:24:05 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (c776c6fed6bc, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:05,103] {docker.py:276} INFO - 21/05/15 16:24:05 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
21/05/15 16:24:05 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 2610 ms on c776c6fed6bc (executor driver) (120/200)
[2021-05-15 13:24:05,105] {docker.py:276} INFO - 21/05/15 16:24:05 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/15 16:24:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:05,108] {docker.py:276} INFO - 21/05/15 16:24:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265156481058284803238_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265156481058284803238_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265156481058284803238_0004}; taskId=attempt_202105151622265156481058284803238_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b3e1f8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622265156481058284803238_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265156481058284803238_0004_m_000122_410 
21/05/15 16:24:05 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:05,110] {docker.py:276} INFO - 21/05/15 16:24:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265364141328391618002_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265364141328391618002_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265364141328391618002_0004}; taskId=attempt_202105151622265364141328391618002_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50c88a88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:05 INFO StagingCommitter: Starting: Task committer attempt_202105151622265364141328391618002_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265364141328391618002_0004_m_000123_411
[2021-05-15 13:24:05,111] {docker.py:276} INFO - 21/05/15 16:24:05 INFO StagingCommitter: Task committer attempt_202105151622265156481058284803238_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265156481058284803238_0004_m_000122_410 : duration 0:00.007s
[2021-05-15 13:24:05,135] {docker.py:276} INFO - 21/05/15 16:24:05 INFO StagingCommitter: Task committer attempt_202105151622265364141328391618002_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265364141328391618002_0004_m_000123_411 : duration 0:00.026s
[2021-05-15 13:24:06,679] {docker.py:276} INFO - 21/05/15 16:24:06 INFO StagingCommitter: Starting: Task committer attempt_202105151622263197545482966724293_0004_m_000120_408: needsTaskCommit() Task attempt_202105151622263197545482966724293_0004_m_000120_408
[2021-05-15 13:24:06,680] {docker.py:276} INFO - 21/05/15 16:24:06 INFO StagingCommitter: Task committer attempt_202105151622263197545482966724293_0004_m_000120_408: needsTaskCommit() Task attempt_202105151622263197545482966724293_0004_m_000120_408: duration 0:00.001s
[2021-05-15 13:24:06,680] {docker.py:276} INFO - 21/05/15 16:24:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263197545482966724293_0004_m_000120_408
[2021-05-15 13:24:06,682] {docker.py:276} INFO - 21/05/15 16:24:06 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 4544 bytes result sent to driver
21/05/15 16:24:06 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (c776c6fed6bc, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:06,682] {docker.py:276} INFO - 21/05/15 16:24:06 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 2555 ms on c776c6fed6bc (executor driver) (121/200)
[2021-05-15 13:24:06,683] {docker.py:276} INFO - 21/05/15 16:24:06 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-15 13:24:06,692] {docker.py:276} INFO - 21/05/15 16:24:06 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:06,693] {docker.py:276} INFO - 21/05/15 16:24:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:24:06,696] {docker.py:276} INFO - 21/05/15 16:24:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:24:06,696] {docker.py:276} INFO - 21/05/15 16:24:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:06,697] {docker.py:276} INFO - 21/05/15 16:24:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:06,698] {docker.py:276} INFO - 21/05/15 16:24:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261261285444552384523_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261261285444552384523_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261261285444552384523_0004}; taskId=attempt_202105151622261261285444552384523_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f3b79cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:06 INFO StagingCommitter: Starting: Task committer attempt_202105151622261261285444552384523_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261261285444552384523_0004_m_000124_412
[2021-05-15 13:24:06,701] {docker.py:276} INFO - 21/05/15 16:24:06 INFO StagingCommitter: Task committer attempt_202105151622261261285444552384523_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261261285444552384523_0004_m_000124_412 : duration 0:00.003s
[2021-05-15 13:24:07,236] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Starting: Task committer attempt_202105151622263235643079026938140_0004_m_000121_409: needsTaskCommit() Task attempt_202105151622263235643079026938140_0004_m_000121_409
[2021-05-15 13:24:07,237] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Task committer attempt_202105151622263235643079026938140_0004_m_000121_409: needsTaskCommit() Task attempt_202105151622263235643079026938140_0004_m_000121_409: duration 0:00.001s
21/05/15 16:24:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263235643079026938140_0004_m_000121_409
[2021-05-15 13:24:07,239] {docker.py:276} INFO - 21/05/15 16:24:07 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 4544 bytes result sent to driver
[2021-05-15 13:24:07,240] {docker.py:276} INFO - 21/05/15 16:24:07 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (c776c6fed6bc, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:07,241] {docker.py:276} INFO - 21/05/15 16:24:07 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 2633 ms on c776c6fed6bc (executor driver) (122/200)
[2021-05-15 13:24:07,242] {docker.py:276} INFO - 21/05/15 16:24:07 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-15 13:24:07,252] {docker.py:276} INFO - 21/05/15 16:24:07 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:07,253] {docker.py:276} INFO - 21/05/15 16:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:07,255] {docker.py:276} INFO - 21/05/15 16:24:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:07,255] {docker.py:276} INFO - 21/05/15 16:24:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:07,257] {docker.py:276} INFO - 21/05/15 16:24:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267659020794681623924_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267659020794681623924_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267659020794681623924_0004}; taskId=attempt_202105151622267659020794681623924_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@782c23a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:07,257] {docker.py:276} INFO - 21/05/15 16:24:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:07,257] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Starting: Task committer attempt_202105151622267659020794681623924_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267659020794681623924_0004_m_000125_413
[2021-05-15 13:24:07,260] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Task committer attempt_202105151622267659020794681623924_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267659020794681623924_0004_m_000125_413 : duration 0:00.003s
[2021-05-15 13:24:07,731] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Starting: Task committer attempt_202105151622265156481058284803238_0004_m_000122_410: needsTaskCommit() Task attempt_202105151622265156481058284803238_0004_m_000122_410
[2021-05-15 13:24:07,732] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Task committer attempt_202105151622265156481058284803238_0004_m_000122_410: needsTaskCommit() Task attempt_202105151622265156481058284803238_0004_m_000122_410: duration 0:00.000s
21/05/15 16:24:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265156481058284803238_0004_m_000122_410
[2021-05-15 13:24:07,734] {docker.py:276} INFO - 21/05/15 16:24:07 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 4544 bytes result sent to driver
[2021-05-15 13:24:07,736] {docker.py:276} INFO - 21/05/15 16:24:07 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (c776c6fed6bc, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:07,737] {docker.py:276} INFO - 21/05/15 16:24:07 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 2664 ms on c776c6fed6bc (executor driver) (123/200)
[2021-05-15 13:24:07,738] {docker.py:276} INFO - 21/05/15 16:24:07 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
[2021-05-15 13:24:07,749] {docker.py:276} INFO - 21/05/15 16:24:07 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:07,752] {docker.py:276} INFO - 21/05/15 16:24:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266466740152393379128_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266466740152393379128_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266466740152393379128_0004}; taskId=attempt_202105151622266466740152393379128_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50ed0804}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:07 INFO StagingCommitter: Starting: Task committer attempt_202105151622266466740152393379128_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266466740152393379128_0004_m_000126_414
[2021-05-15 13:24:07,755] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Task committer attempt_202105151622266466740152393379128_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266466740152393379128_0004_m_000126_414 : duration 0:00.003s
[2021-05-15 13:24:07,947] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Starting: Task committer attempt_202105151622265364141328391618002_0004_m_000123_411: needsTaskCommit() Task attempt_202105151622265364141328391618002_0004_m_000123_411
[2021-05-15 13:24:07,948] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Task committer attempt_202105151622265364141328391618002_0004_m_000123_411: needsTaskCommit() Task attempt_202105151622265364141328391618002_0004_m_000123_411: duration 0:00.000s
21/05/15 16:24:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265364141328391618002_0004_m_000123_411
[2021-05-15 13:24:07,950] {docker.py:276} INFO - 21/05/15 16:24:07 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 4544 bytes result sent to driver
[2021-05-15 13:24:07,951] {docker.py:276} INFO - 21/05/15 16:24:07 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (c776c6fed6bc, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:07,952] {docker.py:276} INFO - 21/05/15 16:24:07 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 2868 ms on c776c6fed6bc (executor driver) (124/200)
21/05/15 16:24:07 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-15 13:24:07,960] {docker.py:276} INFO - 21/05/15 16:24:07 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:07,962] {docker.py:276} INFO - 21/05/15 16:24:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:07,963] {docker.py:276} INFO - 21/05/15 16:24:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264300065979155772744_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264300065979155772744_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264300065979155772744_0004}; taskId=attempt_202105151622264300065979155772744_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c986497}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:07,963] {docker.py:276} INFO - 21/05/15 16:24:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:07 INFO StagingCommitter: Starting: Task committer attempt_202105151622264300065979155772744_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264300065979155772744_0004_m_000127_415
[2021-05-15 13:24:07,966] {docker.py:276} INFO - 21/05/15 16:24:07 INFO StagingCommitter: Task committer attempt_202105151622264300065979155772744_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264300065979155772744_0004_m_000127_415 : duration 0:00.003s
[2021-05-15 13:24:08,406] {docker.py:276} INFO - 21/05/15 16:24:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622261261285444552384523_0004_m_000124_412: needsTaskCommit() Task attempt_202105151622261261285444552384523_0004_m_000124_412
[2021-05-15 13:24:08,407] {docker.py:276} INFO - 21/05/15 16:24:08 INFO StagingCommitter: Task committer attempt_202105151622261261285444552384523_0004_m_000124_412: needsTaskCommit() Task attempt_202105151622261261285444552384523_0004_m_000124_412: duration 0:00.000s
[2021-05-15 13:24:08,407] {docker.py:276} INFO - 21/05/15 16:24:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261261285444552384523_0004_m_000124_412
[2021-05-15 13:24:08,408] {docker.py:276} INFO - 21/05/15 16:24:08 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 4544 bytes result sent to driver
[2021-05-15 13:24:08,410] {docker.py:276} INFO - 21/05/15 16:24:08 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (c776c6fed6bc, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:08,411] {docker.py:276} INFO - 21/05/15 16:24:08 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 1731 ms on c776c6fed6bc (executor driver) (125/200)
[2021-05-15 13:24:08,411] {docker.py:276} INFO - 21/05/15 16:24:08 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
[2021-05-15 13:24:08,422] {docker.py:276} INFO - 21/05/15 16:24:08 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:08,425] {docker.py:276} INFO - 21/05/15 16:24:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262856291513415398137_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262856291513415398137_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262856291513415398137_0004}; taskId=attempt_202105151622262856291513415398137_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@160251ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:08 INFO StagingCommitter: Starting: Task committer attempt_202105151622262856291513415398137_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262856291513415398137_0004_m_000128_416
[2021-05-15 13:24:08,429] {docker.py:276} INFO - 21/05/15 16:24:08 INFO StagingCommitter: Task committer attempt_202105151622262856291513415398137_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262856291513415398137_0004_m_000128_416 : duration 0:00.005s
[2021-05-15 13:24:09,904] {docker.py:276} INFO - 21/05/15 16:24:09 INFO StagingCommitter: Starting: Task committer attempt_202105151622267659020794681623924_0004_m_000125_413: needsTaskCommit() Task attempt_202105151622267659020794681623924_0004_m_000125_413
[2021-05-15 13:24:09,905] {docker.py:276} INFO - 21/05/15 16:24:09 INFO StagingCommitter: Task committer attempt_202105151622267659020794681623924_0004_m_000125_413: needsTaskCommit() Task attempt_202105151622267659020794681623924_0004_m_000125_413: duration 0:00.000s
21/05/15 16:24:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267659020794681623924_0004_m_000125_413
[2021-05-15 13:24:09,907] {docker.py:276} INFO - 21/05/15 16:24:09 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 4544 bytes result sent to driver
[2021-05-15 13:24:09,908] {docker.py:276} INFO - 21/05/15 16:24:09 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (c776c6fed6bc, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:09,909] {docker.py:276} INFO - 21/05/15 16:24:09 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-15 13:24:09,910] {docker.py:276} INFO - 21/05/15 16:24:09 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 2672 ms on c776c6fed6bc (executor driver) (126/200)
[2021-05-15 13:24:09,924] {docker.py:276} INFO - 21/05/15 16:24:09 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:09,927] {docker.py:276} INFO - 21/05/15 16:24:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:09,927] {docker.py:276} INFO - 21/05/15 16:24:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268153641519560746912_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268153641519560746912_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268153641519560746912_0004}; taskId=attempt_202105151622268153641519560746912_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a2dd6fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:09,928] {docker.py:276} INFO - 21/05/15 16:24:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:09 INFO StagingCommitter: Starting: Task committer attempt_202105151622268153641519560746912_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268153641519560746912_0004_m_000129_417
[2021-05-15 13:24:09,930] {docker.py:276} INFO - 21/05/15 16:24:09 INFO StagingCommitter: Task committer attempt_202105151622268153641519560746912_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268153641519560746912_0004_m_000129_417 : duration 0:00.004s
[2021-05-15 13:24:10,600] {docker.py:276} INFO - 21/05/15 16:24:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622264300065979155772744_0004_m_000127_415: needsTaskCommit() Task attempt_202105151622264300065979155772744_0004_m_000127_415
[2021-05-15 13:24:10,601] {docker.py:276} INFO - 21/05/15 16:24:10 INFO StagingCommitter: Task committer attempt_202105151622264300065979155772744_0004_m_000127_415: needsTaskCommit() Task attempt_202105151622264300065979155772744_0004_m_000127_415: duration 0:00.000s
21/05/15 16:24:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264300065979155772744_0004_m_000127_415
[2021-05-15 13:24:10,603] {docker.py:276} INFO - 21/05/15 16:24:10 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 4587 bytes result sent to driver
[2021-05-15 13:24:10,603] {docker.py:276} INFO - 21/05/15 16:24:10 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (c776c6fed6bc, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:10,604] {docker.py:276} INFO - 21/05/15 16:24:10 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-15 13:24:10,605] {docker.py:276} INFO - 21/05/15 16:24:10 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 2658 ms on c776c6fed6bc (executor driver) (127/200)
[2021-05-15 13:24:10,617] {docker.py:276} INFO - 21/05/15 16:24:10 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:10,619] {docker.py:276} INFO - 21/05/15 16:24:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:10,620] {docker.py:276} INFO - 21/05/15 16:24:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263773298785059771293_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263773298785059771293_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263773298785059771293_0004}; taskId=attempt_202105151622263773298785059771293_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41723815}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622263773298785059771293_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263773298785059771293_0004_m_000130_418
[2021-05-15 13:24:10,625] {docker.py:276} INFO - 21/05/15 16:24:10 INFO StagingCommitter: Task committer attempt_202105151622263773298785059771293_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263773298785059771293_0004_m_000130_418 : duration 0:00.004s
[2021-05-15 13:24:10,882] {docker.py:276} INFO - 21/05/15 16:24:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622262856291513415398137_0004_m_000128_416: needsTaskCommit() Task attempt_202105151622262856291513415398137_0004_m_000128_416
[2021-05-15 13:24:10,883] {docker.py:276} INFO - 21/05/15 16:24:10 INFO StagingCommitter: Task committer attempt_202105151622262856291513415398137_0004_m_000128_416: needsTaskCommit() Task attempt_202105151622262856291513415398137_0004_m_000128_416: duration 0:00.001s
21/05/15 16:24:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262856291513415398137_0004_m_000128_416
[2021-05-15 13:24:10,884] {docker.py:276} INFO - 21/05/15 16:24:10 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 4587 bytes result sent to driver
[2021-05-15 13:24:10,885] {docker.py:276} INFO - 21/05/15 16:24:10 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (c776c6fed6bc, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:10,886] {docker.py:276} INFO - 21/05/15 16:24:10 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 2479 ms on c776c6fed6bc (executor driver) (128/200)
[2021-05-15 13:24:10,887] {docker.py:276} INFO - 21/05/15 16:24:10 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-15 13:24:10,896] {docker.py:276} INFO - 21/05/15 16:24:10 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:10,899] {docker.py:276} INFO - 21/05/15 16:24:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:10,899] {docker.py:276} INFO - 21/05/15 16:24:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268711586180504051164_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268711586180504051164_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268711586180504051164_0004}; taskId=attempt_202105151622268711586180504051164_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@146c36df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:10 INFO StagingCommitter: Starting: Task committer attempt_202105151622268711586180504051164_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268711586180504051164_0004_m_000131_419
[2021-05-15 13:24:10,903] {docker.py:276} INFO - 21/05/15 16:24:10 INFO StagingCommitter: Task committer attempt_202105151622268711586180504051164_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268711586180504051164_0004_m_000131_419 : duration 0:00.004s
[2021-05-15 13:24:11,025] {docker.py:276} INFO - 21/05/15 16:24:11 INFO StagingCommitter: Starting: Task committer attempt_202105151622266466740152393379128_0004_m_000126_414: needsTaskCommit() Task attempt_202105151622266466740152393379128_0004_m_000126_414
[2021-05-15 13:24:11,025] {docker.py:276} INFO - 21/05/15 16:24:11 INFO StagingCommitter: Task committer attempt_202105151622266466740152393379128_0004_m_000126_414: needsTaskCommit() Task attempt_202105151622266466740152393379128_0004_m_000126_414: duration 0:00.001s
[2021-05-15 13:24:11,026] {docker.py:276} INFO - 21/05/15 16:24:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266466740152393379128_0004_m_000126_414
[2021-05-15 13:24:11,028] {docker.py:276} INFO - 21/05/15 16:24:11 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 4587 bytes result sent to driver
[2021-05-15 13:24:11,031] {docker.py:276} INFO - 21/05/15 16:24:11 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (c776c6fed6bc, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:11,033] {docker.py:276} INFO - 21/05/15 16:24:11 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-15 13:24:11,033] {docker.py:276} INFO - 21/05/15 16:24:11 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 3300 ms on c776c6fed6bc (executor driver) (129/200)
[2021-05-15 13:24:11,048] {docker.py:276} INFO - 21/05/15 16:24:11 INFO ShuffleBlockFetcherIterator: Getting 5 (22.4 KiB) non-empty blocks including 5 (22.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:11,050] {docker.py:276} INFO - 21/05/15 16:24:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:11,051] {docker.py:276} INFO - 21/05/15 16:24:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226575951086307034358_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226575951086307034358_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226575951086307034358_0004}; taskId=attempt_20210515162226575951086307034358_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@678c6459}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:11,051] {docker.py:276} INFO - 21/05/15 16:24:11 INFO StagingCommitter: Starting: Task committer attempt_20210515162226575951086307034358_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226575951086307034358_0004_m_000132_420
[2021-05-15 13:24:11,056] {docker.py:276} INFO - 21/05/15 16:24:11 INFO StagingCommitter: Task committer attempt_20210515162226575951086307034358_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226575951086307034358_0004_m_000132_420 : duration 0:00.005s
[2021-05-15 13:24:12,523] {docker.py:276} INFO - 21/05/15 16:24:12 INFO StagingCommitter: Starting: Task committer attempt_202105151622268153641519560746912_0004_m_000129_417: needsTaskCommit() Task attempt_202105151622268153641519560746912_0004_m_000129_417
[2021-05-15 13:24:12,524] {docker.py:276} INFO - 21/05/15 16:24:12 INFO StagingCommitter: Task committer attempt_202105151622268153641519560746912_0004_m_000129_417: needsTaskCommit() Task attempt_202105151622268153641519560746912_0004_m_000129_417: duration 0:00.000s
21/05/15 16:24:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268153641519560746912_0004_m_000129_417
[2021-05-15 13:24:12,524] {docker.py:276} INFO - 21/05/15 16:24:12 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 4587 bytes result sent to driver
[2021-05-15 13:24:12,525] {docker.py:276} INFO - 21/05/15 16:24:12 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (c776c6fed6bc, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:12,526] {docker.py:276} INFO - 21/05/15 16:24:12 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-15 13:24:12,527] {docker.py:276} INFO - 21/05/15 16:24:12 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 2622 ms on c776c6fed6bc (executor driver) (130/200)
[2021-05-15 13:24:12,538] {docker.py:276} INFO - 21/05/15 16:24:12 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:12,541] {docker.py:276} INFO - 21/05/15 16:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265266126206245719382_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265266126206245719382_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265266126206245719382_0004}; taskId=attempt_202105151622265266126206245719382_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c56b603}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:12 INFO StagingCommitter: Starting: Task committer attempt_202105151622265266126206245719382_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265266126206245719382_0004_m_000133_421
[2021-05-15 13:24:12,546] {docker.py:276} INFO - 21/05/15 16:24:12 INFO StagingCommitter: Task committer attempt_202105151622265266126206245719382_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265266126206245719382_0004_m_000133_421 : duration 0:00.005s
[2021-05-15 13:24:13,326] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622263773298785059771293_0004_m_000130_418: needsTaskCommit() Task attempt_202105151622263773298785059771293_0004_m_000130_418
[2021-05-15 13:24:13,327] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Task committer attempt_202105151622263773298785059771293_0004_m_000130_418: needsTaskCommit() Task attempt_202105151622263773298785059771293_0004_m_000130_418: duration 0:00.000s
21/05/15 16:24:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263773298785059771293_0004_m_000130_418
[2021-05-15 13:24:13,327] {docker.py:276} INFO - 21/05/15 16:24:13 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 4544 bytes result sent to driver
[2021-05-15 13:24:13,328] {docker.py:276} INFO - 21/05/15 16:24:13 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (c776c6fed6bc, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:13,329] {docker.py:276} INFO - 21/05/15 16:24:13 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
[2021-05-15 13:24:13,330] {docker.py:276} INFO - 21/05/15 16:24:13 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 2730 ms on c776c6fed6bc (executor driver) (131/200)
[2021-05-15 13:24:13,337] {docker.py:276} INFO - 21/05/15 16:24:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:13,339] {docker.py:276} INFO - 21/05/15 16:24:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:13,340] {docker.py:276} INFO - 21/05/15 16:24:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226433042047745091271_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226433042047745091271_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226433042047745091271_0004}; taskId=attempt_20210515162226433042047745091271_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70d10070}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:13,340] {docker.py:276} INFO - 21/05/15 16:24:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:13,340] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Starting: Task committer attempt_20210515162226433042047745091271_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226433042047745091271_0004_m_000134_422
[2021-05-15 13:24:13,342] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Task committer attempt_20210515162226433042047745091271_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226433042047745091271_0004_m_000134_422 : duration 0:00.003s
[2021-05-15 13:24:13,585] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622268711586180504051164_0004_m_000131_419: needsTaskCommit() Task attempt_202105151622268711586180504051164_0004_m_000131_419
[2021-05-15 13:24:13,586] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Task committer attempt_202105151622268711586180504051164_0004_m_000131_419: needsTaskCommit() Task attempt_202105151622268711586180504051164_0004_m_000131_419: duration 0:00.001s
[2021-05-15 13:24:13,587] {docker.py:276} INFO - 21/05/15 16:24:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268711586180504051164_0004_m_000131_419
[2021-05-15 13:24:13,588] {docker.py:276} INFO - 21/05/15 16:24:13 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 4544 bytes result sent to driver
[2021-05-15 13:24:13,588] {docker.py:276} INFO - 21/05/15 16:24:13 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (c776c6fed6bc, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:13,589] {docker.py:276} INFO - 21/05/15 16:24:13 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-15 13:24:13,590] {docker.py:276} INFO - 21/05/15 16:24:13 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 2708 ms on c776c6fed6bc (executor driver) (132/200)
[2021-05-15 13:24:13,598] {docker.py:276} INFO - 21/05/15 16:24:13 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:13,601] {docker.py:276} INFO - 21/05/15 16:24:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:13,602] {docker.py:276} INFO - 21/05/15 16:24:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:13,603] {docker.py:276} INFO - 21/05/15 16:24:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265518524471647758137_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265518524471647758137_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265518524471647758137_0004}; taskId=attempt_202105151622265518524471647758137_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e818939}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:13,603] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622265518524471647758137_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265518524471647758137_0004_m_000135_423
[2021-05-15 13:24:13,607] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Task committer attempt_202105151622265518524471647758137_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265518524471647758137_0004_m_000135_423 : duration 0:00.005s
[2021-05-15 13:24:13,703] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Starting: Task committer attempt_20210515162226575951086307034358_0004_m_000132_420: needsTaskCommit() Task attempt_20210515162226575951086307034358_0004_m_000132_420
[2021-05-15 13:24:13,703] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Task committer attempt_20210515162226575951086307034358_0004_m_000132_420: needsTaskCommit() Task attempt_20210515162226575951086307034358_0004_m_000132_420: duration 0:00.000s
21/05/15 16:24:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226575951086307034358_0004_m_000132_420
[2021-05-15 13:24:13,704] {docker.py:276} INFO - 21/05/15 16:24:13 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 4544 bytes result sent to driver
[2021-05-15 13:24:13,705] {docker.py:276} INFO - 21/05/15 16:24:13 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (c776c6fed6bc, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:13,706] {docker.py:276} INFO - 21/05/15 16:24:13 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
[2021-05-15 13:24:13,707] {docker.py:276} INFO - 21/05/15 16:24:13 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 2681 ms on c776c6fed6bc (executor driver) (133/200)
[2021-05-15 13:24:13,715] {docker.py:276} INFO - 21/05/15 16:24:13 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:13,717] {docker.py:276} INFO - 21/05/15 16:24:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262519424464946915909_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262519424464946915909_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262519424464946915909_0004}; taskId=attempt_202105151622262519424464946915909_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43544fc8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:13,718] {docker.py:276} INFO - 21/05/15 16:24:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:13,718] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Starting: Task committer attempt_202105151622262519424464946915909_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262519424464946915909_0004_m_000136_424
[2021-05-15 13:24:13,722] {docker.py:276} INFO - 21/05/15 16:24:13 INFO StagingCommitter: Task committer attempt_202105151622262519424464946915909_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262519424464946915909_0004_m_000136_424 : duration 0:00.004s
[2021-05-15 13:24:15,238] {docker.py:276} INFO - 21/05/15 16:24:15 INFO StagingCommitter: Starting: Task committer attempt_202105151622265266126206245719382_0004_m_000133_421: needsTaskCommit() Task attempt_202105151622265266126206245719382_0004_m_000133_421
[2021-05-15 13:24:15,239] {docker.py:276} INFO - 21/05/15 16:24:15 INFO StagingCommitter: Task committer attempt_202105151622265266126206245719382_0004_m_000133_421: needsTaskCommit() Task attempt_202105151622265266126206245719382_0004_m_000133_421: duration 0:00.000s
[2021-05-15 13:24:15,240] {docker.py:276} INFO - 21/05/15 16:24:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265266126206245719382_0004_m_000133_421
[2021-05-15 13:24:15,240] {docker.py:276} INFO - 21/05/15 16:24:15 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 4544 bytes result sent to driver
[2021-05-15 13:24:15,241] {docker.py:276} INFO - 21/05/15 16:24:15 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (c776c6fed6bc, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:15,243] {docker.py:276} INFO - 21/05/15 16:24:15 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
21/05/15 16:24:15 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 2720 ms on c776c6fed6bc (executor driver) (134/200)
[2021-05-15 13:24:15,252] {docker.py:276} INFO - 21/05/15 16:24:15 INFO ShuffleBlockFetcherIterator: Getting 5 (21.9 KiB) non-empty blocks including 5 (21.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:15,255] {docker.py:276} INFO - 21/05/15 16:24:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:15,255] {docker.py:276} INFO - 21/05/15 16:24:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226869894256995562025_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226869894256995562025_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226869894256995562025_0004}; taskId=attempt_20210515162226869894256995562025_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@570f0bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:15,256] {docker.py:276} INFO - 21/05/15 16:24:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:15 INFO StagingCommitter: Starting: Task committer attempt_20210515162226869894256995562025_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226869894256995562025_0004_m_000137_425
[2021-05-15 13:24:15,260] {docker.py:276} INFO - 21/05/15 16:24:15 INFO StagingCommitter: Task committer attempt_20210515162226869894256995562025_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226869894256995562025_0004_m_000137_425 : duration 0:00.004s
[2021-05-15 13:24:16,049] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Starting: Task committer attempt_20210515162226433042047745091271_0004_m_000134_422: needsTaskCommit() Task attempt_20210515162226433042047745091271_0004_m_000134_422
[2021-05-15 13:24:16,050] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Task committer attempt_20210515162226433042047745091271_0004_m_000134_422: needsTaskCommit() Task attempt_20210515162226433042047745091271_0004_m_000134_422: duration 0:00.000s
21/05/15 16:24:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226433042047745091271_0004_m_000134_422
[2021-05-15 13:24:16,051] {docker.py:276} INFO - 21/05/15 16:24:16 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 4544 bytes result sent to driver
[2021-05-15 13:24:16,051] {docker.py:276} INFO - 21/05/15 16:24:16 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (c776c6fed6bc, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:16,052] {docker.py:276} INFO - 21/05/15 16:24:16 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 2727 ms on c776c6fed6bc (executor driver) (135/200)
21/05/15 16:24:16 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
[2021-05-15 13:24:16,060] {docker.py:276} INFO - 21/05/15 16:24:16 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:16,062] {docker.py:276} INFO - 21/05/15 16:24:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264974395126531178349_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264974395126531178349_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264974395126531178349_0004}; taskId=attempt_202105151622264974395126531178349_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29cc392}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622264974395126531178349_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264974395126531178349_0004_m_000138_426
[2021-05-15 13:24:16,066] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Task committer attempt_202105151622264974395126531178349_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264974395126531178349_0004_m_000138_426 : duration 0:00.004s
[2021-05-15 13:24:16,174] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622265518524471647758137_0004_m_000135_423: needsTaskCommit() Task attempt_202105151622265518524471647758137_0004_m_000135_423
[2021-05-15 13:24:16,174] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Task committer attempt_202105151622265518524471647758137_0004_m_000135_423: needsTaskCommit() Task attempt_202105151622265518524471647758137_0004_m_000135_423: duration 0:00.001s
21/05/15 16:24:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265518524471647758137_0004_m_000135_423
[2021-05-15 13:24:16,176] {docker.py:276} INFO - 21/05/15 16:24:16 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 4544 bytes result sent to driver
[2021-05-15 13:24:16,177] {docker.py:276} INFO - 21/05/15 16:24:16 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (c776c6fed6bc, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:16,179] {docker.py:276} INFO - 21/05/15 16:24:16 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 2593 ms on c776c6fed6bc (executor driver) (136/200)
21/05/15 16:24:16 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
[2021-05-15 13:24:16,190] {docker.py:276} INFO - 21/05/15 16:24:16 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:16,193] {docker.py:276} INFO - 21/05/15 16:24:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266166710236139021003_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266166710236139021003_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266166710236139021003_0004}; taskId=attempt_202105151622266166710236139021003_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61d5da96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622266166710236139021003_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266166710236139021003_0004_m_000139_427
[2021-05-15 13:24:16,196] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Task committer attempt_202105151622266166710236139021003_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266166710236139021003_0004_m_000139_427 : duration 0:00.003s
[2021-05-15 13:24:16,511] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622262519424464946915909_0004_m_000136_424: needsTaskCommit() Task attempt_202105151622262519424464946915909_0004_m_000136_424
[2021-05-15 13:24:16,512] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Task committer attempt_202105151622262519424464946915909_0004_m_000136_424: needsTaskCommit() Task attempt_202105151622262519424464946915909_0004_m_000136_424: duration 0:00.000s
[2021-05-15 13:24:16,512] {docker.py:276} INFO - 21/05/15 16:24:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262519424464946915909_0004_m_000136_424
[2021-05-15 13:24:16,513] {docker.py:276} INFO - 21/05/15 16:24:16 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 4544 bytes result sent to driver
[2021-05-15 13:24:16,514] {docker.py:276} INFO - 21/05/15 16:24:16 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (c776c6fed6bc, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:16,515] {docker.py:276} INFO - 21/05/15 16:24:16 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 2812 ms on c776c6fed6bc (executor driver) (137/200)
[2021-05-15 13:24:16,516] {docker.py:276} INFO - 21/05/15 16:24:16 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-15 13:24:16,523] {docker.py:276} INFO - 21/05/15 16:24:16 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:16,526] {docker.py:276} INFO - 21/05/15 16:24:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:16,526] {docker.py:276} INFO - 21/05/15 16:24:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269121003555100924147_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269121003555100924147_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269121003555100924147_0004}; taskId=attempt_202105151622269121003555100924147_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a7e2306}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:16,527] {docker.py:276} INFO - 21/05/15 16:24:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:16,527] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Starting: Task committer attempt_202105151622269121003555100924147_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269121003555100924147_0004_m_000140_428
[2021-05-15 13:24:16,530] {docker.py:276} INFO - 21/05/15 16:24:16 INFO StagingCommitter: Task committer attempt_202105151622269121003555100924147_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269121003555100924147_0004_m_000140_428 : duration 0:00.003s
[2021-05-15 13:24:17,808] {docker.py:276} INFO - 21/05/15 16:24:17 INFO StagingCommitter: Starting: Task committer attempt_20210515162226869894256995562025_0004_m_000137_425: needsTaskCommit() Task attempt_20210515162226869894256995562025_0004_m_000137_425
[2021-05-15 13:24:17,809] {docker.py:276} INFO - 21/05/15 16:24:17 INFO StagingCommitter: Task committer attempt_20210515162226869894256995562025_0004_m_000137_425: needsTaskCommit() Task attempt_20210515162226869894256995562025_0004_m_000137_425: duration 0:00.001s
[2021-05-15 13:24:17,809] {docker.py:276} INFO - 21/05/15 16:24:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226869894256995562025_0004_m_000137_425
[2021-05-15 13:24:17,810] {docker.py:276} INFO - 21/05/15 16:24:17 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 4544 bytes result sent to driver
[2021-05-15 13:24:17,811] {docker.py:276} INFO - 21/05/15 16:24:17 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (c776c6fed6bc, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:17,812] {docker.py:276} INFO - 21/05/15 16:24:17 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 2574 ms on c776c6fed6bc (executor driver) (138/200)
[2021-05-15 13:24:17,813] {docker.py:276} INFO - 21/05/15 16:24:17 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
[2021-05-15 13:24:17,821] {docker.py:276} INFO - 21/05/15 16:24:17 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:17,822] {docker.py:276} INFO - 21/05/15 16:24:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264371029316829157937_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264371029316829157937_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264371029316829157937_0004}; taskId=attempt_202105151622264371029316829157937_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8112ef0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:17,823] {docker.py:276} INFO - 21/05/15 16:24:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:17 INFO StagingCommitter: Starting: Task committer attempt_202105151622264371029316829157937_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264371029316829157937_0004_m_000141_429
[2021-05-15 13:24:17,826] {docker.py:276} INFO - 21/05/15 16:24:17 INFO StagingCommitter: Task committer attempt_202105151622264371029316829157937_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264371029316829157937_0004_m_000141_429 : duration 0:00.003s
[2021-05-15 13:24:18,615] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Starting: Task committer attempt_202105151622264974395126531178349_0004_m_000138_426: needsTaskCommit() Task attempt_202105151622264974395126531178349_0004_m_000138_426
[2021-05-15 13:24:18,616] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Task committer attempt_202105151622264974395126531178349_0004_m_000138_426: needsTaskCommit() Task attempt_202105151622264974395126531178349_0004_m_000138_426: duration 0:00.001s
21/05/15 16:24:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264974395126531178349_0004_m_000138_426
[2021-05-15 13:24:18,618] {docker.py:276} INFO - 21/05/15 16:24:18 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 4544 bytes result sent to driver
[2021-05-15 13:24:18,619] {docker.py:276} INFO - 21/05/15 16:24:18 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (c776c6fed6bc, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:18,620] {docker.py:276} INFO - 21/05/15 16:24:18 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-15 13:24:18,621] {docker.py:276} INFO - 21/05/15 16:24:18 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 2572 ms on c776c6fed6bc (executor driver) (139/200)
[2021-05-15 13:24:18,630] {docker.py:276} INFO - 21/05/15 16:24:18 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:18,631] {docker.py:276} INFO - 21/05/15 16:24:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:18,633] {docker.py:276} INFO - 21/05/15 16:24:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264071777810156353351_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264071777810156353351_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264071777810156353351_0004}; taskId=attempt_202105151622264071777810156353351_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b92726d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:18,634] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Starting: Task committer attempt_202105151622264071777810156353351_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264071777810156353351_0004_m_000142_430
[2021-05-15 13:24:18,637] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Task committer attempt_202105151622264071777810156353351_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264071777810156353351_0004_m_000142_430 : duration 0:00.003s
[2021-05-15 13:24:18,831] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Starting: Task committer attempt_202105151622266166710236139021003_0004_m_000139_427: needsTaskCommit() Task attempt_202105151622266166710236139021003_0004_m_000139_427
[2021-05-15 13:24:18,832] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Task committer attempt_202105151622266166710236139021003_0004_m_000139_427: needsTaskCommit() Task attempt_202105151622266166710236139021003_0004_m_000139_427: duration 0:00.001s
21/05/15 16:24:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266166710236139021003_0004_m_000139_427
[2021-05-15 13:24:18,834] {docker.py:276} INFO - 21/05/15 16:24:18 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 4544 bytes result sent to driver
[2021-05-15 13:24:18,835] {docker.py:276} INFO - 21/05/15 16:24:18 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (c776c6fed6bc, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:18,836] {docker.py:276} INFO - 21/05/15 16:24:18 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 2663 ms on c776c6fed6bc (executor driver) (140/200)
21/05/15 16:24:18 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
[2021-05-15 13:24:18,845] {docker.py:276} INFO - 21/05/15 16:24:18 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:18,847] {docker.py:276} INFO - 21/05/15 16:24:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:18,847] {docker.py:276} INFO - 21/05/15 16:24:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267433030403293294490_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267433030403293294490_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267433030403293294490_0004}; taskId=attempt_202105151622267433030403293294490_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62ca8970}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:18,847] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Starting: Task committer attempt_202105151622267433030403293294490_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267433030403293294490_0004_m_000143_431
[2021-05-15 13:24:18,852] {docker.py:276} INFO - 21/05/15 16:24:18 INFO StagingCommitter: Task committer attempt_202105151622267433030403293294490_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267433030403293294490_0004_m_000143_431 : duration 0:00.004s
[2021-05-15 13:24:19,310] {docker.py:276} INFO - 21/05/15 16:24:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622269121003555100924147_0004_m_000140_428: needsTaskCommit() Task attempt_202105151622269121003555100924147_0004_m_000140_428
[2021-05-15 13:24:19,311] {docker.py:276} INFO - 21/05/15 16:24:19 INFO StagingCommitter: Task committer attempt_202105151622269121003555100924147_0004_m_000140_428: needsTaskCommit() Task attempt_202105151622269121003555100924147_0004_m_000140_428: duration 0:00.001s
21/05/15 16:24:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269121003555100924147_0004_m_000140_428
[2021-05-15 13:24:19,312] {docker.py:276} INFO - 21/05/15 16:24:19 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 4544 bytes result sent to driver
[2021-05-15 13:24:19,315] {docker.py:276} INFO - 21/05/15 16:24:19 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (c776c6fed6bc, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:19,316] {docker.py:276} INFO - 21/05/15 16:24:19 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 2804 ms on c776c6fed6bc (executor driver) (141/200)
[2021-05-15 13:24:19,316] {docker.py:276} INFO - 21/05/15 16:24:19 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-15 13:24:19,328] {docker.py:276} INFO - 21/05/15 16:24:19 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-15 13:24:19,331] {docker.py:276} INFO - 21/05/15 16:24:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:19,331] {docker.py:276} INFO - 21/05/15 16:24:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:19,332] {docker.py:276} INFO - 21/05/15 16:24:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267205115433880151556_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267205115433880151556_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267205115433880151556_0004}; taskId=attempt_202105151622267205115433880151556_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c48069a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:19 INFO StagingCommitter: Starting: Task committer attempt_202105151622267205115433880151556_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267205115433880151556_0004_m_000144_432
[2021-05-15 13:24:19,336] {docker.py:276} INFO - 21/05/15 16:24:19 INFO StagingCommitter: Task committer attempt_202105151622267205115433880151556_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267205115433880151556_0004_m_000144_432 : duration 0:00.005s
[2021-05-15 13:24:20,634] {docker.py:276} INFO - 21/05/15 16:24:20 INFO StagingCommitter: Starting: Task committer attempt_202105151622264371029316829157937_0004_m_000141_429: needsTaskCommit() Task attempt_202105151622264371029316829157937_0004_m_000141_429
[2021-05-15 13:24:20,635] {docker.py:276} INFO - 21/05/15 16:24:20 INFO StagingCommitter: Task committer attempt_202105151622264371029316829157937_0004_m_000141_429: needsTaskCommit() Task attempt_202105151622264371029316829157937_0004_m_000141_429: duration 0:00.001s
[2021-05-15 13:24:20,636] {docker.py:276} INFO - 21/05/15 16:24:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264371029316829157937_0004_m_000141_429
[2021-05-15 13:24:20,637] {docker.py:276} INFO - 21/05/15 16:24:20 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 4587 bytes result sent to driver
[2021-05-15 13:24:20,637] {docker.py:276} INFO - 21/05/15 16:24:20 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (c776c6fed6bc, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:20,639] {docker.py:276} INFO - 21/05/15 16:24:20 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
21/05/15 16:24:20 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 2831 ms on c776c6fed6bc (executor driver) (142/200)
[2021-05-15 13:24:20,650] {docker.py:276} INFO - 21/05/15 16:24:20 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:20,652] {docker.py:276} INFO - 21/05/15 16:24:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267258939319262679903_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267258939319262679903_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267258939319262679903_0004}; taskId=attempt_202105151622267258939319262679903_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61c714bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:20 INFO StagingCommitter: Starting: Task committer attempt_202105151622267258939319262679903_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267258939319262679903_0004_m_000145_433
[2021-05-15 13:24:20,654] {docker.py:276} INFO - 21/05/15 16:24:20 INFO StagingCommitter: Task committer attempt_202105151622267258939319262679903_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267258939319262679903_0004_m_000145_433 : duration 0:00.003s
[2021-05-15 13:24:21,284] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622264071777810156353351_0004_m_000142_430: needsTaskCommit() Task attempt_202105151622264071777810156353351_0004_m_000142_430
[2021-05-15 13:24:21,285] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Task committer attempt_202105151622264071777810156353351_0004_m_000142_430: needsTaskCommit() Task attempt_202105151622264071777810156353351_0004_m_000142_430: duration 0:00.001s
[2021-05-15 13:24:21,286] {docker.py:276} INFO - 21/05/15 16:24:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264071777810156353351_0004_m_000142_430
[2021-05-15 13:24:21,286] {docker.py:276} INFO - 21/05/15 16:24:21 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 4587 bytes result sent to driver
[2021-05-15 13:24:21,288] {docker.py:276} INFO - 21/05/15 16:24:21 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (c776c6fed6bc, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:21,290] {docker.py:276} INFO - 21/05/15 16:24:21 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 2674 ms on c776c6fed6bc (executor driver) (143/200)
[2021-05-15 13:24:21,290] {docker.py:276} INFO - 21/05/15 16:24:21 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
[2021-05-15 13:24:21,304] {docker.py:276} INFO - 21/05/15 16:24:21 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:21,305] {docker.py:276} INFO - 21/05/15 16:24:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:21,308] {docker.py:276} INFO - 21/05/15 16:24:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:24:21,310] {docker.py:276} INFO - 21/05/15 16:24:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:21,312] {docker.py:276} INFO - 21/05/15 16:24:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:21,313] {docker.py:276} INFO - 21/05/15 16:24:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261839221734627717774_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261839221734627717774_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261839221734627717774_0004}; taskId=attempt_202105151622261839221734627717774_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56da5872}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:21,314] {docker.py:276} INFO - 21/05/15 16:24:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:21,315] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622261839221734627717774_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261839221734627717774_0004_m_000146_434
[2021-05-15 13:24:21,322] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Task committer attempt_202105151622261839221734627717774_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261839221734627717774_0004_m_000146_434 : duration 0:00.006s
[2021-05-15 13:24:21,464] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622267433030403293294490_0004_m_000143_431: needsTaskCommit() Task attempt_202105151622267433030403293294490_0004_m_000143_431
[2021-05-15 13:24:21,464] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Task committer attempt_202105151622267433030403293294490_0004_m_000143_431: needsTaskCommit() Task attempt_202105151622267433030403293294490_0004_m_000143_431: duration 0:00.000s
[2021-05-15 13:24:21,465] {docker.py:276} INFO - 21/05/15 16:24:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267433030403293294490_0004_m_000143_431
[2021-05-15 13:24:21,465] {docker.py:276} INFO - 21/05/15 16:24:21 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 4587 bytes result sent to driver
[2021-05-15 13:24:21,467] {docker.py:276} INFO - 21/05/15 16:24:21 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (c776c6fed6bc, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:21,467] {docker.py:276} INFO - 21/05/15 16:24:21 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-15 13:24:21,468] {docker.py:276} INFO - 21/05/15 16:24:21 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 2636 ms on c776c6fed6bc (executor driver) (144/200)
[2021-05-15 13:24:21,479] {docker.py:276} INFO - 21/05/15 16:24:21 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:21,479] {docker.py:276} INFO - 21/05/15 16:24:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:21,481] {docker.py:276} INFO - 21/05/15 16:24:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:21,482] {docker.py:276} INFO - 21/05/15 16:24:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:21,483] {docker.py:276} INFO - 21/05/15 16:24:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267849696096174550583_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267849696096174550583_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267849696096174550583_0004}; taskId=attempt_202105151622267849696096174550583_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66d6329a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:21,483] {docker.py:276} INFO - 21/05/15 16:24:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:21,483] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622267849696096174550583_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267849696096174550583_0004_m_000147_435
[2021-05-15 13:24:21,486] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Task committer attempt_202105151622267849696096174550583_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267849696096174550583_0004_m_000147_435 : duration 0:00.004s
[2021-05-15 13:24:21,874] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622267205115433880151556_0004_m_000144_432: needsTaskCommit() Task attempt_202105151622267205115433880151556_0004_m_000144_432
[2021-05-15 13:24:21,875] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Task committer attempt_202105151622267205115433880151556_0004_m_000144_432: needsTaskCommit() Task attempt_202105151622267205115433880151556_0004_m_000144_432: duration 0:00.001s
21/05/15 16:24:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267205115433880151556_0004_m_000144_432
[2021-05-15 13:24:21,876] {docker.py:276} INFO - 21/05/15 16:24:21 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 4587 bytes result sent to driver
[2021-05-15 13:24:21,877] {docker.py:276} INFO - 21/05/15 16:24:21 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (c776c6fed6bc, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:24:21 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
21/05/15 16:24:21 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 2567 ms on c776c6fed6bc (executor driver) (145/200)
[2021-05-15 13:24:21,888] {docker.py:276} INFO - 21/05/15 16:24:21 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:21,890] {docker.py:276} INFO - 21/05/15 16:24:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:21,890] {docker.py:276} INFO - 21/05/15 16:24:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261061664730215414058_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261061664730215414058_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261061664730215414058_0004}; taskId=attempt_202105151622261061664730215414058_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ed47df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:21 INFO StagingCommitter: Starting: Task committer attempt_202105151622261061664730215414058_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261061664730215414058_0004_m_000148_436
[2021-05-15 13:24:21,893] {docker.py:276} INFO - 21/05/15 16:24:21 INFO StagingCommitter: Task committer attempt_202105151622261061664730215414058_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261061664730215414058_0004_m_000148_436 : duration 0:00.003s
[2021-05-15 13:24:23,359] {docker.py:276} INFO - 21/05/15 16:24:23 INFO StagingCommitter: Starting: Task committer attempt_202105151622267258939319262679903_0004_m_000145_433: needsTaskCommit() Task attempt_202105151622267258939319262679903_0004_m_000145_433
21/05/15 16:24:23 INFO StagingCommitter: Task committer attempt_202105151622267258939319262679903_0004_m_000145_433: needsTaskCommit() Task attempt_202105151622267258939319262679903_0004_m_000145_433: duration 0:00.000s
21/05/15 16:24:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267258939319262679903_0004_m_000145_433
[2021-05-15 13:24:23,361] {docker.py:276} INFO - 21/05/15 16:24:23 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 4544 bytes result sent to driver
[2021-05-15 13:24:23,363] {docker.py:276} INFO - 21/05/15 16:24:23 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (c776c6fed6bc, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:23,365] {docker.py:276} INFO - 21/05/15 16:24:23 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 2730 ms on c776c6fed6bc (executor driver) (146/200)
21/05/15 16:24:23 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-15 13:24:23,374] {docker.py:276} INFO - 21/05/15 16:24:23 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:23,377] {docker.py:276} INFO - 21/05/15 16:24:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:23,377] {docker.py:276} INFO - 21/05/15 16:24:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261986816207698620446_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261986816207698620446_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261986816207698620446_0004}; taskId=attempt_202105151622261986816207698620446_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a7db411}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:23 INFO StagingCommitter: Starting: Task committer attempt_202105151622261986816207698620446_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261986816207698620446_0004_m_000149_437
[2021-05-15 13:24:23,380] {docker.py:276} INFO - 21/05/15 16:24:23 INFO StagingCommitter: Task committer attempt_202105151622261986816207698620446_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261986816207698620446_0004_m_000149_437 : duration 0:00.002s
[2021-05-15 13:24:23,958] {docker.py:276} INFO - 21/05/15 16:24:23 INFO StagingCommitter: Starting: Task committer attempt_202105151622261839221734627717774_0004_m_000146_434: needsTaskCommit() Task attempt_202105151622261839221734627717774_0004_m_000146_434
[2021-05-15 13:24:23,959] {docker.py:276} INFO - 21/05/15 16:24:23 INFO StagingCommitter: Task committer attempt_202105151622261839221734627717774_0004_m_000146_434: needsTaskCommit() Task attempt_202105151622261839221734627717774_0004_m_000146_434: duration 0:00.000s
21/05/15 16:24:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261839221734627717774_0004_m_000146_434
[2021-05-15 13:24:23,961] {docker.py:276} INFO - 21/05/15 16:24:23 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 4544 bytes result sent to driver
[2021-05-15 13:24:23,962] {docker.py:276} INFO - 21/05/15 16:24:23 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (c776c6fed6bc, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:23,962] {docker.py:276} INFO - 21/05/15 16:24:23 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 2678 ms on c776c6fed6bc (executor driver) (147/200)
[2021-05-15 13:24:23,964] {docker.py:276} INFO - 21/05/15 16:24:23 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-15 13:24:23,973] {docker.py:276} INFO - 21/05/15 16:24:24 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:23,977] {docker.py:276} INFO - 21/05/15 16:24:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266916817420299386697_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266916817420299386697_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266916817420299386697_0004}; taskId=attempt_202105151622266916817420299386697_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d08c9a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622266916817420299386697_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266916817420299386697_0004_m_000150_438
[2021-05-15 13:24:23,980] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Task committer attempt_202105151622266916817420299386697_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266916817420299386697_0004_m_000150_438 : duration 0:00.003s
[2021-05-15 13:24:24,050] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622267849696096174550583_0004_m_000147_435: needsTaskCommit() Task attempt_202105151622267849696096174550583_0004_m_000147_435
[2021-05-15 13:24:24,051] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Task committer attempt_202105151622267849696096174550583_0004_m_000147_435: needsTaskCommit() Task attempt_202105151622267849696096174550583_0004_m_000147_435: duration 0:00.001s
[2021-05-15 13:24:24,052] {docker.py:276} INFO - 21/05/15 16:24:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267849696096174550583_0004_m_000147_435
[2021-05-15 13:24:24,054] {docker.py:276} INFO - 21/05/15 16:24:24 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 4544 bytes result sent to driver
[2021-05-15 13:24:24,055] {docker.py:276} INFO - 21/05/15 16:24:24 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (c776c6fed6bc, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:24,056] {docker.py:276} INFO - 21/05/15 16:24:24 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 2593 ms on c776c6fed6bc (executor driver) (148/200)
21/05/15 16:24:24 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-15 13:24:24,065] {docker.py:276} INFO - 21/05/15 16:24:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:24,067] {docker.py:276} INFO - 21/05/15 16:24:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269020766429680073241_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269020766429680073241_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269020766429680073241_0004}; taskId=attempt_202105151622269020766429680073241_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7feecefa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:24,068] {docker.py:276} INFO - 21/05/15 16:24:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622269020766429680073241_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269020766429680073241_0004_m_000151_439
[2021-05-15 13:24:24,071] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Task committer attempt_202105151622269020766429680073241_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269020766429680073241_0004_m_000151_439 : duration 0:00.003s
[2021-05-15 13:24:24,461] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622261061664730215414058_0004_m_000148_436: needsTaskCommit() Task attempt_202105151622261061664730215414058_0004_m_000148_436
[2021-05-15 13:24:24,462] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Task committer attempt_202105151622261061664730215414058_0004_m_000148_436: needsTaskCommit() Task attempt_202105151622261061664730215414058_0004_m_000148_436: duration 0:00.001s
[2021-05-15 13:24:24,462] {docker.py:276} INFO - 21/05/15 16:24:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261061664730215414058_0004_m_000148_436
[2021-05-15 13:24:24,463] {docker.py:276} INFO - 21/05/15 16:24:24 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 4544 bytes result sent to driver
[2021-05-15 13:24:24,465] {docker.py:276} INFO - 21/05/15 16:24:24 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (c776c6fed6bc, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:24,468] {docker.py:276} INFO - 21/05/15 16:24:24 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
21/05/15 16:24:24 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 2593 ms on c776c6fed6bc (executor driver) (149/200)
[2021-05-15 13:24:24,480] {docker.py:276} INFO - 21/05/15 16:24:24 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:24,485] {docker.py:276} INFO - 21/05/15 16:24:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:24,486] {docker.py:276} INFO - 21/05/15 16:24:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266958421003131659020_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266958421003131659020_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266958421003131659020_0004}; taskId=attempt_202105151622266958421003131659020_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67c50b49}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:24 INFO StagingCommitter: Starting: Task committer attempt_202105151622266958421003131659020_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266958421003131659020_0004_m_000152_440
[2021-05-15 13:24:24,491] {docker.py:276} INFO - 21/05/15 16:24:24 INFO StagingCommitter: Task committer attempt_202105151622266958421003131659020_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266958421003131659020_0004_m_000152_440 : duration 0:00.006s
[2021-05-15 13:24:25,928] {docker.py:276} INFO - 21/05/15 16:24:25 INFO StagingCommitter: Starting: Task committer attempt_202105151622261986816207698620446_0004_m_000149_437: needsTaskCommit() Task attempt_202105151622261986816207698620446_0004_m_000149_437
21/05/15 16:24:25 INFO StagingCommitter: Task committer attempt_202105151622261986816207698620446_0004_m_000149_437: needsTaskCommit() Task attempt_202105151622261986816207698620446_0004_m_000149_437: duration 0:00.001s
21/05/15 16:24:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261986816207698620446_0004_m_000149_437
[2021-05-15 13:24:25,931] {docker.py:276} INFO - 21/05/15 16:24:25 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 4544 bytes result sent to driver
[2021-05-15 13:24:25,931] {docker.py:276} INFO - 21/05/15 16:24:25 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (c776c6fed6bc, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:25,932] {docker.py:276} INFO - 21/05/15 16:24:25 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 2572 ms on c776c6fed6bc (executor driver) (150/200)
[2021-05-15 13:24:25,933] {docker.py:276} INFO - 21/05/15 16:24:25 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-15 13:24:25,945] {docker.py:276} INFO - 21/05/15 16:24:25 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:25,948] {docker.py:276} INFO - 21/05/15 16:24:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:25,949] {docker.py:276} INFO - 21/05/15 16:24:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267063775265188446360_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267063775265188446360_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267063775265188446360_0004}; taskId=attempt_202105151622267063775265188446360_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b2c4cac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:25,949] {docker.py:276} INFO - 21/05/15 16:24:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:25 INFO StagingCommitter: Starting: Task committer attempt_202105151622267063775265188446360_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267063775265188446360_0004_m_000153_441
[2021-05-15 13:24:25,955] {docker.py:276} INFO - 21/05/15 16:24:25 INFO StagingCommitter: Task committer attempt_202105151622267063775265188446360_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267063775265188446360_0004_m_000153_441 : duration 0:00.005s
[2021-05-15 13:24:26,302] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Starting: Task committer attempt_202105151622266916817420299386697_0004_m_000150_438: needsTaskCommit() Task attempt_202105151622266916817420299386697_0004_m_000150_438
[2021-05-15 13:24:26,306] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Task committer attempt_202105151622266916817420299386697_0004_m_000150_438: needsTaskCommit() Task attempt_202105151622266916817420299386697_0004_m_000150_438: duration 0:00.001s
21/05/15 16:24:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266916817420299386697_0004_m_000150_438
[2021-05-15 13:24:26,307] {docker.py:276} INFO - 21/05/15 16:24:26 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 4544 bytes result sent to driver
[2021-05-15 13:24:26,309] {docker.py:276} INFO - 21/05/15 16:24:26 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (c776c6fed6bc, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:26,309] {docker.py:276} INFO - 21/05/15 16:24:26 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-15 13:24:26,310] {docker.py:276} INFO - 21/05/15 16:24:26 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 2350 ms on c776c6fed6bc (executor driver) (151/200)
[2021-05-15 13:24:26,322] {docker.py:276} INFO - 21/05/15 16:24:26 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:26,324] {docker.py:276} INFO - 21/05/15 16:24:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:26,325] {docker.py:276} INFO - 21/05/15 16:24:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226848081011900977975_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226848081011900977975_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226848081011900977975_0004}; taskId=attempt_20210515162226848081011900977975_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55cca350}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:26,325] {docker.py:276} INFO - 21/05/15 16:24:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:26 INFO StagingCommitter: Starting: Task committer attempt_20210515162226848081011900977975_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226848081011900977975_0004_m_000154_442
[2021-05-15 13:24:26,328] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Task committer attempt_20210515162226848081011900977975_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226848081011900977975_0004_m_000154_442 : duration 0:00.003s
[2021-05-15 13:24:26,709] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Starting: Task committer attempt_202105151622269020766429680073241_0004_m_000151_439: needsTaskCommit() Task attempt_202105151622269020766429680073241_0004_m_000151_439
[2021-05-15 13:24:26,710] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Task committer attempt_202105151622269020766429680073241_0004_m_000151_439: needsTaskCommit() Task attempt_202105151622269020766429680073241_0004_m_000151_439: duration 0:00.001s
21/05/15 16:24:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269020766429680073241_0004_m_000151_439
[2021-05-15 13:24:26,711] {docker.py:276} INFO - 21/05/15 16:24:26 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 4544 bytes result sent to driver
[2021-05-15 13:24:26,713] {docker.py:276} INFO - 21/05/15 16:24:26 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (c776c6fed6bc, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:26,714] {docker.py:276} INFO - 21/05/15 16:24:26 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 2662 ms on c776c6fed6bc (executor driver) (152/200)
21/05/15 16:24:26 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
[2021-05-15 13:24:26,725] {docker.py:276} INFO - 21/05/15 16:24:26 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:26,726] {docker.py:276} INFO - 21/05/15 16:24:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:26,727] {docker.py:276} INFO - 21/05/15 16:24:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268794651568893362843_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268794651568893362843_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268794651568893362843_0004}; taskId=attempt_202105151622268794651568893362843_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@414c5e02}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:26,727] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Starting: Task committer attempt_202105151622268794651568893362843_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268794651568893362843_0004_m_000155_443
[2021-05-15 13:24:26,730] {docker.py:276} INFO - 21/05/15 16:24:26 INFO StagingCommitter: Task committer attempt_202105151622268794651568893362843_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268794651568893362843_0004_m_000155_443 : duration 0:00.003s
[2021-05-15 13:24:27,120] {docker.py:276} INFO - 21/05/15 16:24:27 INFO StagingCommitter: Starting: Task committer attempt_202105151622266958421003131659020_0004_m_000152_440: needsTaskCommit() Task attempt_202105151622266958421003131659020_0004_m_000152_440
[2021-05-15 13:24:27,122] {docker.py:276} INFO - 21/05/15 16:24:27 INFO StagingCommitter: Task committer attempt_202105151622266958421003131659020_0004_m_000152_440: needsTaskCommit() Task attempt_202105151622266958421003131659020_0004_m_000152_440: duration 0:00.002s
[2021-05-15 13:24:27,123] {docker.py:276} INFO - 21/05/15 16:24:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266958421003131659020_0004_m_000152_440
[2021-05-15 13:24:27,123] {docker.py:276} INFO - 21/05/15 16:24:27 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 4544 bytes result sent to driver
[2021-05-15 13:24:27,125] {docker.py:276} INFO - 21/05/15 16:24:27 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (c776c6fed6bc, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:27,126] {docker.py:276} INFO - 21/05/15 16:24:27 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
21/05/15 16:24:27 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 2665 ms on c776c6fed6bc (executor driver) (153/200)
[2021-05-15 13:24:27,136] {docker.py:276} INFO - 21/05/15 16:24:27 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:27,137] {docker.py:276} INFO - 21/05/15 16:24:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262109233600834863428_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262109233600834863428_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262109233600834863428_0004}; taskId=attempt_202105151622262109233600834863428_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bade7c9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:27,138] {docker.py:276} INFO - 21/05/15 16:24:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:27 INFO StagingCommitter: Starting: Task committer attempt_202105151622262109233600834863428_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262109233600834863428_0004_m_000156_444
[2021-05-15 13:24:27,141] {docker.py:276} INFO - 21/05/15 16:24:27 INFO StagingCommitter: Task committer attempt_202105151622262109233600834863428_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262109233600834863428_0004_m_000156_444 : duration 0:00.003s
[2021-05-15 13:24:28,036] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Starting: Task committer attempt_20210515162226848081011900977975_0004_m_000154_442: needsTaskCommit() Task attempt_20210515162226848081011900977975_0004_m_000154_442
[2021-05-15 13:24:28,037] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Task committer attempt_20210515162226848081011900977975_0004_m_000154_442: needsTaskCommit() Task attempt_20210515162226848081011900977975_0004_m_000154_442: duration 0:00.003s
[2021-05-15 13:24:28,039] {docker.py:276} INFO - 21/05/15 16:24:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226848081011900977975_0004_m_000154_442
[2021-05-15 13:24:28,041] {docker.py:276} INFO - 21/05/15 16:24:28 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 4544 bytes result sent to driver
[2021-05-15 13:24:28,044] {docker.py:276} INFO - 21/05/15 16:24:28 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (c776c6fed6bc, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:28,045] {docker.py:276} INFO - 21/05/15 16:24:28 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-15 13:24:28,047] {docker.py:276} INFO - 21/05/15 16:24:28 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 1739 ms on c776c6fed6bc (executor driver) (154/200)
[2021-05-15 13:24:28,058] {docker.py:276} INFO - 21/05/15 16:24:28 INFO ShuffleBlockFetcherIterator: Getting 5 (22.8 KiB) non-empty blocks including 5 (22.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:28,058] {docker.py:276} INFO - 21/05/15 16:24:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:28,060] {docker.py:276} INFO - 21/05/15 16:24:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:28,061] {docker.py:276} INFO - 21/05/15 16:24:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267454021187528466491_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267454021187528466491_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267454021187528466491_0004}; taskId=attempt_202105151622267454021187528466491_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46a30967}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:28,061] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Starting: Task committer attempt_202105151622267454021187528466491_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267454021187528466491_0004_m_000157_445
[2021-05-15 13:24:28,065] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Task committer attempt_202105151622267454021187528466491_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267454021187528466491_0004_m_000157_445 : duration 0:00.004s
[2021-05-15 13:24:28,581] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Starting: Task committer attempt_202105151622267063775265188446360_0004_m_000153_441: needsTaskCommit() Task attempt_202105151622267063775265188446360_0004_m_000153_441
[2021-05-15 13:24:28,583] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Task committer attempt_202105151622267063775265188446360_0004_m_000153_441: needsTaskCommit() Task attempt_202105151622267063775265188446360_0004_m_000153_441: duration 0:00.000s
21/05/15 16:24:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267063775265188446360_0004_m_000153_441
[2021-05-15 13:24:28,584] {docker.py:276} INFO - 21/05/15 16:24:28 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 4544 bytes result sent to driver
[2021-05-15 13:24:28,586] {docker.py:276} INFO - 21/05/15 16:24:28 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (c776c6fed6bc, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:28,586] {docker.py:276} INFO - 21/05/15 16:24:28 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 2660 ms on c776c6fed6bc (executor driver) (155/200)
21/05/15 16:24:28 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-15 13:24:28,595] {docker.py:276} INFO - 21/05/15 16:24:28 INFO ShuffleBlockFetcherIterator: Getting 5 (21.6 KiB) non-empty blocks including 5 (21.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:28,598] {docker.py:276} INFO - 21/05/15 16:24:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:28,599] {docker.py:276} INFO - 21/05/15 16:24:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263325266227226966185_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263325266227226966185_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263325266227226966185_0004}; taskId=attempt_202105151622263325266227226966185_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75119f89}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:28,599] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Starting: Task committer attempt_202105151622263325266227226966185_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263325266227226966185_0004_m_000158_446
[2021-05-15 13:24:28,602] {docker.py:276} INFO - 21/05/15 16:24:28 INFO StagingCommitter: Task committer attempt_202105151622263325266227226966185_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263325266227226966185_0004_m_000158_446 : duration 0:00.004s
[2021-05-15 13:24:29,255] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622262109233600834863428_0004_m_000156_444: needsTaskCommit() Task attempt_202105151622262109233600834863428_0004_m_000156_444
[2021-05-15 13:24:29,256] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Task committer attempt_202105151622262109233600834863428_0004_m_000156_444: needsTaskCommit() Task attempt_202105151622262109233600834863428_0004_m_000156_444: duration 0:00.000s
21/05/15 16:24:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262109233600834863428_0004_m_000156_444
[2021-05-15 13:24:29,259] {docker.py:276} INFO - 21/05/15 16:24:29 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 4544 bytes result sent to driver
[2021-05-15 13:24:29,260] {docker.py:276} INFO - 21/05/15 16:24:29 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (c776c6fed6bc, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:29,262] {docker.py:276} INFO - 21/05/15 16:24:29 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
21/05/15 16:24:29 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 2140 ms on c776c6fed6bc (executor driver) (156/200)
[2021-05-15 13:24:29,287] {docker.py:276} INFO - 21/05/15 16:24:29 INFO ShuffleBlockFetcherIterator: Getting 5 (22.3 KiB) non-empty blocks including 5 (22.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:29,289] {docker.py:276} INFO - 21/05/15 16:24:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:29,290] {docker.py:276} INFO - 21/05/15 16:24:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268540338839823238106_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268540338839823238106_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268540338839823238106_0004}; taskId=attempt_202105151622268540338839823238106_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d877e9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:29,291] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622268540338839823238106_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268540338839823238106_0004_m_000159_447
[2021-05-15 13:24:29,294] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Task committer attempt_202105151622268540338839823238106_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268540338839823238106_0004_m_000159_447 : duration 0:00.003s
[2021-05-15 13:24:29,357] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622268794651568893362843_0004_m_000155_443: needsTaskCommit() Task attempt_202105151622268794651568893362843_0004_m_000155_443
[2021-05-15 13:24:29,358] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Task committer attempt_202105151622268794651568893362843_0004_m_000155_443: needsTaskCommit() Task attempt_202105151622268794651568893362843_0004_m_000155_443: duration 0:00.000s
21/05/15 16:24:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268794651568893362843_0004_m_000155_443
[2021-05-15 13:24:29,360] {docker.py:276} INFO - 21/05/15 16:24:29 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 4587 bytes result sent to driver
[2021-05-15 13:24:29,361] {docker.py:276} INFO - 21/05/15 16:24:29 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (c776c6fed6bc, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:29,362] {docker.py:276} INFO - 21/05/15 16:24:29 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
[2021-05-15 13:24:29,362] {docker.py:276} INFO - 21/05/15 16:24:29 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 2619 ms on c776c6fed6bc (executor driver) (157/200)
[2021-05-15 13:24:29,371] {docker.py:276} INFO - 21/05/15 16:24:29 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:29,373] {docker.py:276} INFO - 21/05/15 16:24:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266639382607968318387_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266639382607968318387_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266639382607968318387_0004}; taskId=attempt_202105151622266639382607968318387_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6020b0b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:29 INFO StagingCommitter: Starting: Task committer attempt_202105151622266639382607968318387_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266639382607968318387_0004_m_000160_448
[2021-05-15 13:24:29,376] {docker.py:276} INFO - 21/05/15 16:24:29 INFO StagingCommitter: Task committer attempt_202105151622266639382607968318387_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266639382607968318387_0004_m_000160_448 : duration 0:00.003s
[2021-05-15 13:24:30,474] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Starting: Task committer attempt_202105151622263325266227226966185_0004_m_000158_446: needsTaskCommit() Task attempt_202105151622263325266227226966185_0004_m_000158_446
[2021-05-15 13:24:30,475] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Task committer attempt_202105151622263325266227226966185_0004_m_000158_446: needsTaskCommit() Task attempt_202105151622263325266227226966185_0004_m_000158_446: duration 0:00.001s
21/05/15 16:24:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263325266227226966185_0004_m_000158_446
[2021-05-15 13:24:30,477] {docker.py:276} INFO - 21/05/15 16:24:30 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 4587 bytes result sent to driver
[2021-05-15 13:24:30,479] {docker.py:276} INFO - 21/05/15 16:24:30 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (c776c6fed6bc, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:30,480] {docker.py:276} INFO - 21/05/15 16:24:30 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
21/05/15 16:24:30 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 1862 ms on c776c6fed6bc (executor driver) (158/200)
[2021-05-15 13:24:30,492] {docker.py:276} INFO - 21/05/15 16:24:30 INFO ShuffleBlockFetcherIterator: Getting 5 (22.6 KiB) non-empty blocks including 5 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:30,495] {docker.py:276} INFO - 21/05/15 16:24:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263116333976413885153_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263116333976413885153_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263116333976413885153_0004}; taskId=attempt_202105151622263116333976413885153_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@efb589e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:30,496] {docker.py:276} INFO - 21/05/15 16:24:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:30,496] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Starting: Task committer attempt_202105151622263116333976413885153_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263116333976413885153_0004_m_000161_449
[2021-05-15 13:24:30,499] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Task committer attempt_202105151622263116333976413885153_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263116333976413885153_0004_m_000161_449 : duration 0:00.005s
[2021-05-15 13:24:30,814] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Starting: Task committer attempt_202105151622267454021187528466491_0004_m_000157_445: needsTaskCommit() Task attempt_202105151622267454021187528466491_0004_m_000157_445
[2021-05-15 13:24:30,815] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Task committer attempt_202105151622267454021187528466491_0004_m_000157_445: needsTaskCommit() Task attempt_202105151622267454021187528466491_0004_m_000157_445: duration 0:00.000s
21/05/15 16:24:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267454021187528466491_0004_m_000157_445
[2021-05-15 13:24:30,816] {docker.py:276} INFO - 21/05/15 16:24:30 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 4587 bytes result sent to driver
[2021-05-15 13:24:30,817] {docker.py:276} INFO - 21/05/15 16:24:30 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (c776c6fed6bc, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:30,818] {docker.py:276} INFO - 21/05/15 16:24:30 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 2743 ms on c776c6fed6bc (executor driver) (159/200)
[2021-05-15 13:24:30,818] {docker.py:276} INFO - 21/05/15 16:24:30 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-15 13:24:30,828] {docker.py:276} INFO - 21/05/15 16:24:30 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:30,831] {docker.py:276} INFO - 21/05/15 16:24:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266902643568301605728_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266902643568301605728_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266902643568301605728_0004}; taskId=attempt_202105151622266902643568301605728_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c10323b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:30 INFO StagingCommitter: Starting: Task committer attempt_202105151622266902643568301605728_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266902643568301605728_0004_m_000162_450
[2021-05-15 13:24:30,835] {docker.py:276} INFO - 21/05/15 16:24:30 INFO StagingCommitter: Task committer attempt_202105151622266902643568301605728_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266902643568301605728_0004_m_000162_450 : duration 0:00.004s
[2021-05-15 13:24:31,371] {docker.py:276} INFO - 21/05/15 16:24:31 INFO StagingCommitter: Starting: Task committer attempt_202105151622268540338839823238106_0004_m_000159_447: needsTaskCommit() Task attempt_202105151622268540338839823238106_0004_m_000159_447
[2021-05-15 13:24:31,371] {docker.py:276} INFO - 21/05/15 16:24:31 INFO StagingCommitter: Task committer attempt_202105151622268540338839823238106_0004_m_000159_447: needsTaskCommit() Task attempt_202105151622268540338839823238106_0004_m_000159_447: duration 0:00.000s
21/05/15 16:24:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268540338839823238106_0004_m_000159_447
[2021-05-15 13:24:31,372] {docker.py:276} INFO - 21/05/15 16:24:31 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 4587 bytes result sent to driver
[2021-05-15 13:24:31,373] {docker.py:276} INFO - 21/05/15 16:24:31 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (c776c6fed6bc, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:31,374] {docker.py:276} INFO - 21/05/15 16:24:31 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-15 13:24:31,375] {docker.py:276} INFO - 21/05/15 16:24:31 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 2083 ms on c776c6fed6bc (executor driver) (160/200)
[2021-05-15 13:24:31,387] {docker.py:276} INFO - 21/05/15 16:24:31 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:31,390] {docker.py:276} INFO - 21/05/15 16:24:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:31,391] {docker.py:276} INFO - 21/05/15 16:24:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268808467449136085487_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268808467449136085487_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268808467449136085487_0004}; taskId=attempt_202105151622268808467449136085487_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78263689}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:31 INFO StagingCommitter: Starting: Task committer attempt_202105151622268808467449136085487_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268808467449136085487_0004_m_000163_451
[2021-05-15 13:24:31,396] {docker.py:276} INFO - 21/05/15 16:24:31 INFO StagingCommitter: Task committer attempt_202105151622268808467449136085487_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268808467449136085487_0004_m_000163_451 : duration 0:00.004s
[2021-05-15 13:24:31,544] {docker.py:276} INFO - 21/05/15 16:24:31 INFO StagingCommitter: Starting: Task committer attempt_202105151622266639382607968318387_0004_m_000160_448: needsTaskCommit() Task attempt_202105151622266639382607968318387_0004_m_000160_448
[2021-05-15 13:24:31,545] {docker.py:276} INFO - 21/05/15 16:24:31 INFO StagingCommitter: Task committer attempt_202105151622266639382607968318387_0004_m_000160_448: needsTaskCommit() Task attempt_202105151622266639382607968318387_0004_m_000160_448: duration 0:00.002s
[2021-05-15 13:24:31,545] {docker.py:276} INFO - 21/05/15 16:24:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266639382607968318387_0004_m_000160_448
[2021-05-15 13:24:31,547] {docker.py:276} INFO - 21/05/15 16:24:31 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 4544 bytes result sent to driver
[2021-05-15 13:24:31,549] {docker.py:276} INFO - 21/05/15 16:24:31 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (c776c6fed6bc, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:31,550] {docker.py:276} INFO - 21/05/15 16:24:31 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 2190 ms on c776c6fed6bc (executor driver) (161/200)
[2021-05-15 13:24:31,551] {docker.py:276} INFO - 21/05/15 16:24:31 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-15 13:24:31,583] {docker.py:276} INFO - 21/05/15 16:24:31 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:31,584] {docker.py:276} INFO - 21/05/15 16:24:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-15 13:24:31,589] {docker.py:276} INFO - 21/05/15 16:24:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:31,591] {docker.py:276} INFO - 21/05/15 16:24:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268947014481243835739_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268947014481243835739_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268947014481243835739_0004}; taskId=attempt_202105151622268947014481243835739_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69715d4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:31,592] {docker.py:276} INFO - 21/05/15 16:24:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:31 INFO StagingCommitter: Starting: Task committer attempt_202105151622268947014481243835739_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268947014481243835739_0004_m_000164_452
[2021-05-15 13:24:31,601] {docker.py:276} INFO - 21/05/15 16:24:31 INFO StagingCommitter: Task committer attempt_202105151622268947014481243835739_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268947014481243835739_0004_m_000164_452 : duration 0:00.011s
[2021-05-15 13:24:33,099] {docker.py:276} INFO - 21/05/15 16:24:33 INFO StagingCommitter: Starting: Task committer attempt_202105151622263116333976413885153_0004_m_000161_449: needsTaskCommit() Task attempt_202105151622263116333976413885153_0004_m_000161_449
[2021-05-15 13:24:33,099] {docker.py:276} INFO - 21/05/15 16:24:33 INFO StagingCommitter: Task committer attempt_202105151622263116333976413885153_0004_m_000161_449: needsTaskCommit() Task attempt_202105151622263116333976413885153_0004_m_000161_449: duration 0:00.001s
21/05/15 16:24:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263116333976413885153_0004_m_000161_449
[2021-05-15 13:24:33,101] {docker.py:276} INFO - 21/05/15 16:24:33 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 4544 bytes result sent to driver
[2021-05-15 13:24:33,102] {docker.py:276} INFO - 21/05/15 16:24:33 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (c776c6fed6bc, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:33,104] {docker.py:276} INFO - 21/05/15 16:24:33 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 2629 ms on c776c6fed6bc (executor driver) (162/200)
21/05/15 16:24:33 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
[2021-05-15 13:24:33,116] {docker.py:276} INFO - 21/05/15 16:24:33 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:33,119] {docker.py:276} INFO - 21/05/15 16:24:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266243780602495470213_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266243780602495470213_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266243780602495470213_0004}; taskId=attempt_202105151622266243780602495470213_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78453514}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:33 INFO StagingCommitter: Starting: Task committer attempt_202105151622266243780602495470213_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266243780602495470213_0004_m_000165_453
[2021-05-15 13:24:33,123] {docker.py:276} INFO - 21/05/15 16:24:33 INFO StagingCommitter: Task committer attempt_202105151622266243780602495470213_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266243780602495470213_0004_m_000165_453 : duration 0:00.004s
[2021-05-15 13:24:33,635] {docker.py:276} INFO - 21/05/15 16:24:33 INFO StagingCommitter: Starting: Task committer attempt_202105151622266902643568301605728_0004_m_000162_450: needsTaskCommit() Task attempt_202105151622266902643568301605728_0004_m_000162_450
[2021-05-15 13:24:33,637] {docker.py:276} INFO - 21/05/15 16:24:33 INFO StagingCommitter: Task committer attempt_202105151622266902643568301605728_0004_m_000162_450: needsTaskCommit() Task attempt_202105151622266902643568301605728_0004_m_000162_450: duration 0:00.002s
21/05/15 16:24:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266902643568301605728_0004_m_000162_450
[2021-05-15 13:24:33,640] {docker.py:276} INFO - 21/05/15 16:24:33 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 4544 bytes result sent to driver
[2021-05-15 13:24:33,642] {docker.py:276} INFO - 21/05/15 16:24:33 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (c776c6fed6bc, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:33,644] {docker.py:276} INFO - 21/05/15 16:24:33 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 2830 ms on c776c6fed6bc (executor driver) (163/200)
[2021-05-15 13:24:33,645] {docker.py:276} INFO - 21/05/15 16:24:33 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
[2021-05-15 13:24:33,658] {docker.py:276} INFO - 21/05/15 16:24:33 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:33,661] {docker.py:276} INFO - 21/05/15 16:24:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:33,662] {docker.py:276} INFO - 21/05/15 16:24:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263923023106955057479_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263923023106955057479_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263923023106955057479_0004}; taskId=attempt_202105151622263923023106955057479_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53380cb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:33,662] {docker.py:276} INFO - 21/05/15 16:24:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:33 INFO StagingCommitter: Starting: Task committer attempt_202105151622263923023106955057479_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263923023106955057479_0004_m_000166_454
[2021-05-15 13:24:33,666] {docker.py:276} INFO - 21/05/15 16:24:33 INFO StagingCommitter: Task committer attempt_202105151622263923023106955057479_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263923023106955057479_0004_m_000166_454 : duration 0:00.004s
[2021-05-15 13:24:34,120] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Starting: Task committer attempt_202105151622268947014481243835739_0004_m_000164_452: needsTaskCommit() Task attempt_202105151622268947014481243835739_0004_m_000164_452
21/05/15 16:24:34 INFO StagingCommitter: Task committer attempt_202105151622268947014481243835739_0004_m_000164_452: needsTaskCommit() Task attempt_202105151622268947014481243835739_0004_m_000164_452: duration 0:00.001s
21/05/15 16:24:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268947014481243835739_0004_m_000164_452
[2021-05-15 13:24:34,122] {docker.py:276} INFO - 21/05/15 16:24:34 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 4544 bytes result sent to driver
[2021-05-15 13:24:34,124] {docker.py:276} INFO - 21/05/15 16:24:34 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (c776c6fed6bc, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:34,126] {docker.py:276} INFO - 21/05/15 16:24:34 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
21/05/15 16:24:34 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 2580 ms on c776c6fed6bc (executor driver) (164/200)
[2021-05-15 13:24:34,135] {docker.py:276} INFO - 21/05/15 16:24:34 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:34,137] {docker.py:276} INFO - 21/05/15 16:24:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226102092165102328592_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226102092165102328592_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226102092165102328592_0004}; taskId=attempt_20210515162226102092165102328592_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a0c64fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:34,138] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Starting: Task committer attempt_20210515162226102092165102328592_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226102092165102328592_0004_m_000167_455
[2021-05-15 13:24:34,140] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Task committer attempt_20210515162226102092165102328592_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226102092165102328592_0004_m_000167_455 : duration 0:00.003s
[2021-05-15 13:24:34,692] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Starting: Task committer attempt_202105151622268808467449136085487_0004_m_000163_451: needsTaskCommit() Task attempt_202105151622268808467449136085487_0004_m_000163_451
[2021-05-15 13:24:34,693] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Task committer attempt_202105151622268808467449136085487_0004_m_000163_451: needsTaskCommit() Task attempt_202105151622268808467449136085487_0004_m_000163_451: duration 0:00.000s
21/05/15 16:24:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268808467449136085487_0004_m_000163_451
[2021-05-15 13:24:34,694] {docker.py:276} INFO - 21/05/15 16:24:34 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 4544 bytes result sent to driver
[2021-05-15 13:24:34,695] {docker.py:276} INFO - 21/05/15 16:24:34 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (c776c6fed6bc, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:34,696] {docker.py:276} INFO - 21/05/15 16:24:34 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 3327 ms on c776c6fed6bc (executor driver) (165/200)
[2021-05-15 13:24:34,697] {docker.py:276} INFO - 21/05/15 16:24:34 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
[2021-05-15 13:24:34,709] {docker.py:276} INFO - 21/05/15 16:24:34 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:34,712] {docker.py:276} INFO - 21/05/15 16:24:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:34,712] {docker.py:276} INFO - 21/05/15 16:24:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261209332729999609472_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261209332729999609472_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261209332729999609472_0004}; taskId=attempt_202105151622261209332729999609472_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54d9cef0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:34,713] {docker.py:276} INFO - 21/05/15 16:24:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:34,714] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Starting: Task committer attempt_202105151622261209332729999609472_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261209332729999609472_0004_m_000168_456
[2021-05-15 13:24:34,718] {docker.py:276} INFO - 21/05/15 16:24:34 INFO StagingCommitter: Task committer attempt_202105151622261209332729999609472_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261209332729999609472_0004_m_000168_456 : duration 0:00.005s
[2021-05-15 13:24:35,357] {docker.py:276} INFO - 21/05/15 16:24:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622266243780602495470213_0004_m_000165_453: needsTaskCommit() Task attempt_202105151622266243780602495470213_0004_m_000165_453
[2021-05-15 13:24:35,357] {docker.py:276} INFO - 21/05/15 16:24:35 INFO StagingCommitter: Task committer attempt_202105151622266243780602495470213_0004_m_000165_453: needsTaskCommit() Task attempt_202105151622266243780602495470213_0004_m_000165_453: duration 0:00.001s
21/05/15 16:24:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266243780602495470213_0004_m_000165_453
[2021-05-15 13:24:35,358] {docker.py:276} INFO - 21/05/15 16:24:35 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 4544 bytes result sent to driver
[2021-05-15 13:24:35,359] {docker.py:276} INFO - 21/05/15 16:24:35 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (c776c6fed6bc, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:35,359] {docker.py:276} INFO - 21/05/15 16:24:35 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-15 13:24:35,360] {docker.py:276} INFO - 21/05/15 16:24:35 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 2260 ms on c776c6fed6bc (executor driver) (166/200)
[2021-05-15 13:24:35,368] {docker.py:276} INFO - 21/05/15 16:24:35 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:35,369] {docker.py:276} INFO - 21/05/15 16:24:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:35,369] {docker.py:276} INFO - 21/05/15 16:24:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268366109506123341144_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268366109506123341144_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268366109506123341144_0004}; taskId=attempt_202105151622268366109506123341144_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74b7ad3c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:35 INFO StagingCommitter: Starting: Task committer attempt_202105151622268366109506123341144_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268366109506123341144_0004_m_000169_457
[2021-05-15 13:24:35,373] {docker.py:276} INFO - 21/05/15 16:24:35 INFO StagingCommitter: Task committer attempt_202105151622268366109506123341144_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268366109506123341144_0004_m_000169_457 : duration 0:00.003s
[2021-05-15 13:24:36,299] {docker.py:276} INFO - 21/05/15 16:24:36 INFO StagingCommitter: Starting: Task committer attempt_202105151622263923023106955057479_0004_m_000166_454: needsTaskCommit() Task attempt_202105151622263923023106955057479_0004_m_000166_454
[2021-05-15 13:24:36,299] {docker.py:276} INFO - 21/05/15 16:24:36 INFO StagingCommitter: Task committer attempt_202105151622263923023106955057479_0004_m_000166_454: needsTaskCommit() Task attempt_202105151622263923023106955057479_0004_m_000166_454: duration 0:00.000s
21/05/15 16:24:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263923023106955057479_0004_m_000166_454
[2021-05-15 13:24:36,301] {docker.py:276} INFO - 21/05/15 16:24:36 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 4544 bytes result sent to driver
[2021-05-15 13:24:36,304] {docker.py:276} INFO - 21/05/15 16:24:36 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (c776c6fed6bc, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:24:36 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
21/05/15 16:24:36 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 2665 ms on c776c6fed6bc (executor driver) (167/200)
[2021-05-15 13:24:36,312] {docker.py:276} INFO - 21/05/15 16:24:36 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:36,313] {docker.py:276} INFO - 21/05/15 16:24:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:36,314] {docker.py:276} INFO - 21/05/15 16:24:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268629066561879183550_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268629066561879183550_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268629066561879183550_0004}; taskId=attempt_202105151622268629066561879183550_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@366b779d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:36 INFO StagingCommitter: Starting: Task committer attempt_202105151622268629066561879183550_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268629066561879183550_0004_m_000170_458
[2021-05-15 13:24:36,318] {docker.py:276} INFO - 21/05/15 16:24:36 INFO StagingCommitter: Task committer attempt_202105151622268629066561879183550_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268629066561879183550_0004_m_000170_458 : duration 0:00.004s
[2021-05-15 13:24:36,838] {docker.py:276} INFO - 21/05/15 16:24:36 INFO StagingCommitter: Starting: Task committer attempt_20210515162226102092165102328592_0004_m_000167_455: needsTaskCommit() Task attempt_20210515162226102092165102328592_0004_m_000167_455
[2021-05-15 13:24:36,839] {docker.py:276} INFO - 21/05/15 16:24:36 INFO StagingCommitter: Task committer attempt_20210515162226102092165102328592_0004_m_000167_455: needsTaskCommit() Task attempt_20210515162226102092165102328592_0004_m_000167_455: duration 0:00.001s
21/05/15 16:24:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226102092165102328592_0004_m_000167_455
[2021-05-15 13:24:36,841] {docker.py:276} INFO - 21/05/15 16:24:36 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 4544 bytes result sent to driver
[2021-05-15 13:24:36,842] {docker.py:276} INFO - 21/05/15 16:24:36 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (c776c6fed6bc, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:36,843] {docker.py:276} INFO - 21/05/15 16:24:36 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 2724 ms on c776c6fed6bc (executor driver) (168/200)
[2021-05-15 13:24:36,844] {docker.py:276} INFO - 21/05/15 16:24:36 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
[2021-05-15 13:24:36,855] {docker.py:276} INFO - 21/05/15 16:24:36 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:36,855] {docker.py:276} INFO - 21/05/15 16:24:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:36,858] {docker.py:276} INFO - 21/05/15 16:24:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:36,859] {docker.py:276} INFO - 21/05/15 16:24:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:36,859] {docker.py:276} INFO - 21/05/15 16:24:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268171234348861164918_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268171234348861164918_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268171234348861164918_0004}; taskId=attempt_202105151622268171234348861164918_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f4ee087}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:36,860] {docker.py:276} INFO - 21/05/15 16:24:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:36 INFO StagingCommitter: Starting: Task committer attempt_202105151622268171234348861164918_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268171234348861164918_0004_m_000171_459
[2021-05-15 13:24:36,864] {docker.py:276} INFO - 21/05/15 16:24:36 INFO StagingCommitter: Task committer attempt_202105151622268171234348861164918_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268171234348861164918_0004_m_000171_459 : duration 0:00.004s
[2021-05-15 13:24:37,424] {docker.py:276} INFO - 21/05/15 16:24:37 INFO StagingCommitter: Starting: Task committer attempt_202105151622261209332729999609472_0004_m_000168_456: needsTaskCommit() Task attempt_202105151622261209332729999609472_0004_m_000168_456
[2021-05-15 13:24:37,425] {docker.py:276} INFO - 21/05/15 16:24:37 INFO StagingCommitter: Task committer attempt_202105151622261209332729999609472_0004_m_000168_456: needsTaskCommit() Task attempt_202105151622261209332729999609472_0004_m_000168_456: duration 0:00.000s
[2021-05-15 13:24:37,425] {docker.py:276} INFO - 21/05/15 16:24:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261209332729999609472_0004_m_000168_456
[2021-05-15 13:24:37,427] {docker.py:276} INFO - 21/05/15 16:24:37 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 4544 bytes result sent to driver
[2021-05-15 13:24:37,428] {docker.py:276} INFO - 21/05/15 16:24:37 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (c776c6fed6bc, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:37,430] {docker.py:276} INFO - 21/05/15 16:24:37 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
21/05/15 16:24:37 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 2738 ms on c776c6fed6bc (executor driver) (169/200)
[2021-05-15 13:24:37,440] {docker.py:276} INFO - 21/05/15 16:24:37 INFO ShuffleBlockFetcherIterator: Getting 5 (22.5 KiB) non-empty blocks including 5 (22.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:37,443] {docker.py:276} INFO - 21/05/15 16:24:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:37,443] {docker.py:276} INFO - 21/05/15 16:24:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261822279125414662880_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261822279125414662880_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261822279125414662880_0004}; taskId=attempt_202105151622261822279125414662880_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f5faf86}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:37 INFO StagingCommitter: Starting: Task committer attempt_202105151622261822279125414662880_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261822279125414662880_0004_m_000172_460
[2021-05-15 13:24:37,447] {docker.py:276} INFO - 21/05/15 16:24:37 INFO StagingCommitter: Task committer attempt_202105151622261822279125414662880_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261822279125414662880_0004_m_000172_460 : duration 0:00.003s
[2021-05-15 13:24:37,989] {docker.py:276} INFO - 21/05/15 16:24:37 INFO StagingCommitter: Starting: Task committer attempt_202105151622268366109506123341144_0004_m_000169_457: needsTaskCommit() Task attempt_202105151622268366109506123341144_0004_m_000169_457
[2021-05-15 13:24:37,990] {docker.py:276} INFO - 21/05/15 16:24:37 INFO StagingCommitter: Task committer attempt_202105151622268366109506123341144_0004_m_000169_457: needsTaskCommit() Task attempt_202105151622268366109506123341144_0004_m_000169_457: duration 0:00.000s
21/05/15 16:24:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268366109506123341144_0004_m_000169_457
[2021-05-15 13:24:37,991] {docker.py:276} INFO - 21/05/15 16:24:38 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 4544 bytes result sent to driver
[2021-05-15 13:24:37,993] {docker.py:276} INFO - 21/05/15 16:24:38 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (c776c6fed6bc, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:37,994] {docker.py:276} INFO - 21/05/15 16:24:38 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
21/05/15 16:24:38 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 2638 ms on c776c6fed6bc (executor driver) (170/200)
[2021-05-15 13:24:38,001] {docker.py:276} INFO - 21/05/15 16:24:38 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:38,003] {docker.py:276} INFO - 21/05/15 16:24:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:38,004] {docker.py:276} INFO - 21/05/15 16:24:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:38,004] {docker.py:276} INFO - 21/05/15 16:24:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266095389733384108126_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266095389733384108126_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266095389733384108126_0004}; taskId=attempt_202105151622266095389733384108126_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fa1210a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:38,005] {docker.py:276} INFO - 21/05/15 16:24:38 INFO StagingCommitter: Starting: Task committer attempt_202105151622266095389733384108126_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266095389733384108126_0004_m_000173_461
[2021-05-15 13:24:38,007] {docker.py:276} INFO - 21/05/15 16:24:38 INFO StagingCommitter: Task committer attempt_202105151622266095389733384108126_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266095389733384108126_0004_m_000173_461 : duration 0:00.003s
[2021-05-15 13:24:39,026] {docker.py:276} INFO - 21/05/15 16:24:39 INFO StagingCommitter: Starting: Task committer attempt_202105151622268629066561879183550_0004_m_000170_458: needsTaskCommit() Task attempt_202105151622268629066561879183550_0004_m_000170_458
21/05/15 16:24:39 INFO StagingCommitter: Task committer attempt_202105151622268629066561879183550_0004_m_000170_458: needsTaskCommit() Task attempt_202105151622268629066561879183550_0004_m_000170_458: duration 0:00.000s
21/05/15 16:24:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268629066561879183550_0004_m_000170_458
[2021-05-15 13:24:39,028] {docker.py:276} INFO - 21/05/15 16:24:39 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 4544 bytes result sent to driver
21/05/15 16:24:39 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (c776c6fed6bc, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:39,029] {docker.py:276} INFO - 21/05/15 16:24:39 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 2730 ms on c776c6fed6bc (executor driver) (171/200)
[2021-05-15 13:24:39,030] {docker.py:276} INFO - 21/05/15 16:24:39 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-15 13:24:39,054] {docker.py:276} INFO - 21/05/15 16:24:39 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:39,055] {docker.py:276} INFO - 21/05/15 16:24:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:39,057] {docker.py:276} INFO - 21/05/15 16:24:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:39,057] {docker.py:276} INFO - 21/05/15 16:24:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268514957361548911281_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268514957361548911281_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268514957361548911281_0004}; taskId=attempt_202105151622268514957361548911281_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3654a73b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:39,058] {docker.py:276} INFO - 21/05/15 16:24:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:39,058] {docker.py:276} INFO - 21/05/15 16:24:39 INFO StagingCommitter: Starting: Task committer attempt_202105151622268514957361548911281_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268514957361548911281_0004_m_000174_462
[2021-05-15 13:24:39,063] {docker.py:276} INFO - 21/05/15 16:24:39 INFO StagingCommitter: Task committer attempt_202105151622268514957361548911281_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268514957361548911281_0004_m_000174_462 : duration 0:00.005s
[2021-05-15 13:24:39,430] {docker.py:276} INFO - 21/05/15 16:24:39 INFO StagingCommitter: Starting: Task committer attempt_202105151622268171234348861164918_0004_m_000171_459: needsTaskCommit() Task attempt_202105151622268171234348861164918_0004_m_000171_459
[2021-05-15 13:24:39,431] {docker.py:276} INFO - 21/05/15 16:24:39 INFO StagingCommitter: Task committer attempt_202105151622268171234348861164918_0004_m_000171_459: needsTaskCommit() Task attempt_202105151622268171234348861164918_0004_m_000171_459: duration 0:00.000s
21/05/15 16:24:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268171234348861164918_0004_m_000171_459
[2021-05-15 13:24:39,436] {docker.py:276} INFO - 21/05/15 16:24:39 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 4587 bytes result sent to driver
[2021-05-15 13:24:39,437] {docker.py:276} INFO - 21/05/15 16:24:39 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (c776c6fed6bc, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:39,437] {docker.py:276} INFO - 21/05/15 16:24:39 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 2596 ms on c776c6fed6bc (executor driver) (172/200)
21/05/15 16:24:39 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-15 13:24:39,447] {docker.py:276} INFO - 21/05/15 16:24:39 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:39,451] {docker.py:276} INFO - 21/05/15 16:24:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:39,452] {docker.py:276} INFO - 21/05/15 16:24:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263956003787230514535_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263956003787230514535_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263956003787230514535_0004}; taskId=attempt_202105151622263956003787230514535_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38b94f95}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:39 INFO StagingCommitter: Starting: Task committer attempt_202105151622263956003787230514535_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263956003787230514535_0004_m_000175_463
[2021-05-15 13:24:39,454] {docker.py:276} INFO - 21/05/15 16:24:39 INFO StagingCommitter: Task committer attempt_202105151622263956003787230514535_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263956003787230514535_0004_m_000175_463 : duration 0:00.004s
[2021-05-15 13:24:40,444] {docker.py:276} INFO - 21/05/15 16:24:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622261822279125414662880_0004_m_000172_460: needsTaskCommit() Task attempt_202105151622261822279125414662880_0004_m_000172_460
[2021-05-15 13:24:40,445] {docker.py:276} INFO - 21/05/15 16:24:40 INFO StagingCommitter: Task committer attempt_202105151622261822279125414662880_0004_m_000172_460: needsTaskCommit() Task attempt_202105151622261822279125414662880_0004_m_000172_460: duration 0:00.001s
21/05/15 16:24:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261822279125414662880_0004_m_000172_460
[2021-05-15 13:24:40,446] {docker.py:276} INFO - 21/05/15 16:24:40 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 4587 bytes result sent to driver
[2021-05-15 13:24:40,449] {docker.py:276} INFO - 21/05/15 16:24:40 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (c776c6fed6bc, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:40,449] {docker.py:276} INFO - 21/05/15 16:24:40 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
21/05/15 16:24:40 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 3024 ms on c776c6fed6bc (executor driver) (173/200)
[2021-05-15 13:24:40,458] {docker.py:276} INFO - 21/05/15 16:24:40 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:40,460] {docker.py:276} INFO - 21/05/15 16:24:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263984115621610291870_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263984115621610291870_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263984115621610291870_0004}; taskId=attempt_202105151622263984115621610291870_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c7d59e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622263984115621610291870_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263984115621610291870_0004_m_000176_464
[2021-05-15 13:24:40,463] {docker.py:276} INFO - 21/05/15 16:24:40 INFO StagingCommitter: Task committer attempt_202105151622263984115621610291870_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263984115621610291870_0004_m_000176_464 : duration 0:00.003s
[2021-05-15 13:24:40,539] {docker.py:276} INFO - 21/05/15 16:24:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622266095389733384108126_0004_m_000173_461: needsTaskCommit() Task attempt_202105151622266095389733384108126_0004_m_000173_461
21/05/15 16:24:40 INFO StagingCommitter: Task committer attempt_202105151622266095389733384108126_0004_m_000173_461: needsTaskCommit() Task attempt_202105151622266095389733384108126_0004_m_000173_461: duration 0:00.000s
21/05/15 16:24:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266095389733384108126_0004_m_000173_461
[2021-05-15 13:24:40,543] {docker.py:276} INFO - 21/05/15 16:24:40 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 4587 bytes result sent to driver
[2021-05-15 13:24:40,543] {docker.py:276} INFO - 21/05/15 16:24:40 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (c776c6fed6bc, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:40,544] {docker.py:276} INFO - 21/05/15 16:24:40 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
21/05/15 16:24:40 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 2553 ms on c776c6fed6bc (executor driver) (174/200)
[2021-05-15 13:24:40,553] {docker.py:276} INFO - 21/05/15 16:24:40 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:40,555] {docker.py:276} INFO - 21/05/15 16:24:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267219490373908363485_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267219490373908363485_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267219490373908363485_0004}; taskId=attempt_202105151622267219490373908363485_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6dc3c638}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:40 INFO StagingCommitter: Starting: Task committer attempt_202105151622267219490373908363485_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267219490373908363485_0004_m_000177_465
[2021-05-15 13:24:40,558] {docker.py:276} INFO - 21/05/15 16:24:40 INFO StagingCommitter: Task committer attempt_202105151622267219490373908363485_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267219490373908363485_0004_m_000177_465 : duration 0:00.003s
[2021-05-15 13:24:41,426] {docker.py:276} INFO - 21/05/15 16:24:41 INFO StagingCommitter: Starting: Task committer attempt_202105151622268514957361548911281_0004_m_000174_462: needsTaskCommit() Task attempt_202105151622268514957361548911281_0004_m_000174_462
[2021-05-15 13:24:41,427] {docker.py:276} INFO - 21/05/15 16:24:41 INFO StagingCommitter: Task committer attempt_202105151622268514957361548911281_0004_m_000174_462: needsTaskCommit() Task attempt_202105151622268514957361548911281_0004_m_000174_462: duration 0:00.000s
[2021-05-15 13:24:41,427] {docker.py:276} INFO - 21/05/15 16:24:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268514957361548911281_0004_m_000174_462
[2021-05-15 13:24:41,428] {docker.py:276} INFO - 21/05/15 16:24:41 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 4587 bytes result sent to driver
[2021-05-15 13:24:41,429] {docker.py:276} INFO - 21/05/15 16:24:41 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (c776c6fed6bc, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:41,431] {docker.py:276} INFO - 21/05/15 16:24:41 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 2406 ms on c776c6fed6bc (executor driver) (175/200)
[2021-05-15 13:24:41,431] {docker.py:276} INFO - 21/05/15 16:24:41 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-15 13:24:41,439] {docker.py:276} INFO - 21/05/15 16:24:41 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:41,441] {docker.py:276} INFO - 21/05/15 16:24:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:41,441] {docker.py:276} INFO - 21/05/15 16:24:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262385390652979764023_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262385390652979764023_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262385390652979764023_0004}; taskId=attempt_202105151622262385390652979764023_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e4a9d62}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:41,442] {docker.py:276} INFO - 21/05/15 16:24:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:41,442] {docker.py:276} INFO - 21/05/15 16:24:41 INFO StagingCommitter: Starting: Task committer attempt_202105151622262385390652979764023_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262385390652979764023_0004_m_000178_466
[2021-05-15 13:24:41,444] {docker.py:276} INFO - 21/05/15 16:24:41 INFO StagingCommitter: Task committer attempt_202105151622262385390652979764023_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262385390652979764023_0004_m_000178_466 : duration 0:00.003s
[2021-05-15 13:24:42,011] {docker.py:276} INFO - 21/05/15 16:24:42 INFO StagingCommitter: Starting: Task committer attempt_202105151622263956003787230514535_0004_m_000175_463: needsTaskCommit() Task attempt_202105151622263956003787230514535_0004_m_000175_463
21/05/15 16:24:42 INFO StagingCommitter: Task committer attempt_202105151622263956003787230514535_0004_m_000175_463: needsTaskCommit() Task attempt_202105151622263956003787230514535_0004_m_000175_463: duration 0:00.001s
21/05/15 16:24:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263956003787230514535_0004_m_000175_463
[2021-05-15 13:24:42,014] {docker.py:276} INFO - 21/05/15 16:24:42 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 4544 bytes result sent to driver
[2021-05-15 13:24:42,016] {docker.py:276} INFO - 21/05/15 16:24:42 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (c776c6fed6bc, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 16:24:42 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
21/05/15 16:24:42 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 2584 ms on c776c6fed6bc (executor driver) (176/200)
[2021-05-15 13:24:42,026] {docker.py:276} INFO - 21/05/15 16:24:42 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:42,028] {docker.py:276} INFO - 21/05/15 16:24:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264552778892878749639_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264552778892878749639_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264552778892878749639_0004}; taskId=attempt_202105151622264552778892878749639_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a2881ed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:42 INFO StagingCommitter: Starting: Task committer attempt_202105151622264552778892878749639_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264552778892878749639_0004_m_000179_467
[2021-05-15 13:24:42,031] {docker.py:276} INFO - 21/05/15 16:24:42 INFO StagingCommitter: Task committer attempt_202105151622264552778892878749639_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264552778892878749639_0004_m_000179_467 : duration 0:00.003s
[2021-05-15 13:24:43,168] {docker.py:276} INFO - 21/05/15 16:24:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622267219490373908363485_0004_m_000177_465: needsTaskCommit() Task attempt_202105151622267219490373908363485_0004_m_000177_465
[2021-05-15 13:24:43,170] {docker.py:276} INFO - 21/05/15 16:24:43 INFO StagingCommitter: Task committer attempt_202105151622267219490373908363485_0004_m_000177_465: needsTaskCommit() Task attempt_202105151622267219490373908363485_0004_m_000177_465: duration 0:00.000s
21/05/15 16:24:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267219490373908363485_0004_m_000177_465
[2021-05-15 13:24:43,172] {docker.py:276} INFO - 21/05/15 16:24:43 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 4544 bytes result sent to driver
21/05/15 16:24:43 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (c776c6fed6bc, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:43,173] {docker.py:276} INFO - 21/05/15 16:24:43 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
21/05/15 16:24:43 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 2635 ms on c776c6fed6bc (executor driver) (177/200)
[2021-05-15 13:24:43,183] {docker.py:276} INFO - 21/05/15 16:24:43 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:43,185] {docker.py:276} INFO - 21/05/15 16:24:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622267038249812827186532_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267038249812827186532_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622267038249812827186532_0004}; taskId=attempt_202105151622267038249812827186532_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3aafa6d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:43,186] {docker.py:276} INFO - 21/05/15 16:24:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622267038249812827186532_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267038249812827186532_0004_m_000180_468
[2021-05-15 13:24:43,189] {docker.py:276} INFO - 21/05/15 16:24:43 INFO StagingCommitter: Task committer attempt_202105151622267038249812827186532_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622267038249812827186532_0004_m_000180_468 : duration 0:00.003s
[2021-05-15 13:24:43,258] {docker.py:276} INFO - 21/05/15 16:24:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622263984115621610291870_0004_m_000176_464: needsTaskCommit() Task attempt_202105151622263984115621610291870_0004_m_000176_464
21/05/15 16:24:43 INFO StagingCommitter: Task committer attempt_202105151622263984115621610291870_0004_m_000176_464: needsTaskCommit() Task attempt_202105151622263984115621610291870_0004_m_000176_464: duration 0:00.001s
21/05/15 16:24:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263984115621610291870_0004_m_000176_464
[2021-05-15 13:24:43,260] {docker.py:276} INFO - 21/05/15 16:24:43 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 4544 bytes result sent to driver
21/05/15 16:24:43 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (c776c6fed6bc, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:43,261] {docker.py:276} INFO - 21/05/15 16:24:43 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 2817 ms on c776c6fed6bc (executor driver) (178/200)
[2021-05-15 13:24:43,264] {docker.py:276} INFO - 21/05/15 16:24:43 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
[2021-05-15 13:24:43,273] {docker.py:276} INFO - 21/05/15 16:24:43 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:43,275] {docker.py:276} INFO - 21/05/15 16:24:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622269172271419266845805_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269172271419266845805_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622269172271419266845805_0004}; taskId=attempt_202105151622269172271419266845805_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69a5095e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:43 INFO StagingCommitter: Starting: Task committer attempt_202105151622269172271419266845805_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269172271419266845805_0004_m_000181_469
[2021-05-15 13:24:43,278] {docker.py:276} INFO - 21/05/15 16:24:43 INFO StagingCommitter: Task committer attempt_202105151622269172271419266845805_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622269172271419266845805_0004_m_000181_469 : duration 0:00.003s
[2021-05-15 13:24:44,102] {docker.py:276} INFO - 21/05/15 16:24:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622262385390652979764023_0004_m_000178_466: needsTaskCommit() Task attempt_202105151622262385390652979764023_0004_m_000178_466
21/05/15 16:24:44 INFO StagingCommitter: Task committer attempt_202105151622262385390652979764023_0004_m_000178_466: needsTaskCommit() Task attempt_202105151622262385390652979764023_0004_m_000178_466: duration 0:00.001s
21/05/15 16:24:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262385390652979764023_0004_m_000178_466
[2021-05-15 13:24:44,103] {docker.py:276} INFO - 21/05/15 16:24:44 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 4544 bytes result sent to driver
[2021-05-15 13:24:44,104] {docker.py:276} INFO - 21/05/15 16:24:44 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (c776c6fed6bc, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:44,105] {docker.py:276} INFO - 21/05/15 16:24:44 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
[2021-05-15 13:24:44,106] {docker.py:276} INFO - 21/05/15 16:24:44 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 2679 ms on c776c6fed6bc (executor driver) (179/200)
[2021-05-15 13:24:44,114] {docker.py:276} INFO - 21/05/15 16:24:44 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:44,116] {docker.py:276} INFO - 21/05/15 16:24:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:44,117] {docker.py:276} INFO - 21/05/15 16:24:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226356980705173117717_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226356980705173117717_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226356980705173117717_0004}; taskId=attempt_20210515162226356980705173117717_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b91dd08}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:44,117] {docker.py:276} INFO - 21/05/15 16:24:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:44 INFO StagingCommitter: Starting: Task committer attempt_20210515162226356980705173117717_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226356980705173117717_0004_m_000182_470
[2021-05-15 13:24:44,121] {docker.py:276} INFO - 21/05/15 16:24:44 INFO StagingCommitter: Task committer attempt_20210515162226356980705173117717_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226356980705173117717_0004_m_000182_470 : duration 0:00.004s
[2021-05-15 13:24:44,653] {docker.py:276} INFO - 21/05/15 16:24:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622264552778892878749639_0004_m_000179_467: needsTaskCommit() Task attempt_202105151622264552778892878749639_0004_m_000179_467
[2021-05-15 13:24:44,654] {docker.py:276} INFO - 21/05/15 16:24:44 INFO StagingCommitter: Task committer attempt_202105151622264552778892878749639_0004_m_000179_467: needsTaskCommit() Task attempt_202105151622264552778892878749639_0004_m_000179_467: duration 0:00.001s
21/05/15 16:24:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264552778892878749639_0004_m_000179_467
[2021-05-15 13:24:44,656] {docker.py:276} INFO - 21/05/15 16:24:44 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 4544 bytes result sent to driver
[2021-05-15 13:24:44,657] {docker.py:276} INFO - 21/05/15 16:24:44 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (c776c6fed6bc, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:44,658] {docker.py:276} INFO - 21/05/15 16:24:44 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 2647 ms on c776c6fed6bc (executor driver) (180/200)
21/05/15 16:24:44 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-15 13:24:44,670] {docker.py:276} INFO - 21/05/15 16:24:44 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:44,673] {docker.py:276} INFO - 21/05/15 16:24:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263383296858274404297_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263383296858274404297_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263383296858274404297_0004}; taskId=attempt_202105151622263383296858274404297_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10c96149}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:44,673] {docker.py:276} INFO - 21/05/15 16:24:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:44 INFO StagingCommitter: Starting: Task committer attempt_202105151622263383296858274404297_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263383296858274404297_0004_m_000183_471
[2021-05-15 13:24:44,677] {docker.py:276} INFO - 21/05/15 16:24:44 INFO StagingCommitter: Task committer attempt_202105151622263383296858274404297_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263383296858274404297_0004_m_000183_471 : duration 0:00.004s
[2021-05-15 13:24:45,363] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Starting: Task committer attempt_202105151622267038249812827186532_0004_m_000180_468: needsTaskCommit() Task attempt_202105151622267038249812827186532_0004_m_000180_468
[2021-05-15 13:24:45,363] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Task committer attempt_202105151622267038249812827186532_0004_m_000180_468: needsTaskCommit() Task attempt_202105151622267038249812827186532_0004_m_000180_468: duration 0:00.000s
21/05/15 16:24:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622267038249812827186532_0004_m_000180_468
[2021-05-15 13:24:45,364] {docker.py:276} INFO - 21/05/15 16:24:45 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 4544 bytes result sent to driver
[2021-05-15 13:24:45,365] {docker.py:276} INFO - 21/05/15 16:24:45 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (c776c6fed6bc, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:45,367] {docker.py:276} INFO - 21/05/15 16:24:45 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 2197 ms on c776c6fed6bc (executor driver) (181/200)
[2021-05-15 13:24:45,367] {docker.py:276} INFO - 21/05/15 16:24:45 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-15 13:24:45,375] {docker.py:276} INFO - 21/05/15 16:24:45 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:45,377] {docker.py:276} INFO - 21/05/15 16:24:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515162226282072531255123936_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226282072531255123936_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515162226282072531255123936_0004}; taskId=attempt_20210515162226282072531255123936_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49a3e4d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:45,378] {docker.py:276} INFO - 21/05/15 16:24:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:45,378] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Starting: Task committer attempt_20210515162226282072531255123936_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226282072531255123936_0004_m_000184_472
[2021-05-15 13:24:45,381] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Task committer attempt_20210515162226282072531255123936_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_20210515162226282072531255123936_0004_m_000184_472 : duration 0:00.002s
[2021-05-15 13:24:45,856] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Starting: Task committer attempt_202105151622269172271419266845805_0004_m_000181_469: needsTaskCommit() Task attempt_202105151622269172271419266845805_0004_m_000181_469
[2021-05-15 13:24:45,857] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Task committer attempt_202105151622269172271419266845805_0004_m_000181_469: needsTaskCommit() Task attempt_202105151622269172271419266845805_0004_m_000181_469: duration 0:00.001s
21/05/15 16:24:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622269172271419266845805_0004_m_000181_469
[2021-05-15 13:24:45,859] {docker.py:276} INFO - 21/05/15 16:24:45 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 4544 bytes result sent to driver
[2021-05-15 13:24:45,860] {docker.py:276} INFO - 21/05/15 16:24:45 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (c776c6fed6bc, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:45,861] {docker.py:276} INFO - 21/05/15 16:24:45 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-15 13:24:45,862] {docker.py:276} INFO - 21/05/15 16:24:45 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 2605 ms on c776c6fed6bc (executor driver) (182/200)
[2021-05-15 13:24:45,872] {docker.py:276} INFO - 21/05/15 16:24:45 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:45,874] {docker.py:276} INFO - 21/05/15 16:24:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:45,874] {docker.py:276} INFO - 21/05/15 16:24:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268910423430947206159_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268910423430947206159_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268910423430947206159_0004}; taskId=attempt_202105151622268910423430947206159_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d1f4a71}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:45 INFO StagingCommitter: Starting: Task committer attempt_202105151622268910423430947206159_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268910423430947206159_0004_m_000185_473
[2021-05-15 13:24:45,877] {docker.py:276} INFO - 21/05/15 16:24:45 INFO StagingCommitter: Task committer attempt_202105151622268910423430947206159_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268910423430947206159_0004_m_000185_473 : duration 0:00.002s
[2021-05-15 13:24:46,780] {docker.py:276} INFO - 21/05/15 16:24:46 INFO StagingCommitter: Starting: Task committer attempt_20210515162226356980705173117717_0004_m_000182_470: needsTaskCommit() Task attempt_20210515162226356980705173117717_0004_m_000182_470
[2021-05-15 13:24:46,780] {docker.py:276} INFO - 21/05/15 16:24:46 INFO StagingCommitter: Task committer attempt_20210515162226356980705173117717_0004_m_000182_470: needsTaskCommit() Task attempt_20210515162226356980705173117717_0004_m_000182_470: duration 0:00.000s
21/05/15 16:24:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226356980705173117717_0004_m_000182_470
[2021-05-15 13:24:46,781] {docker.py:276} INFO - 21/05/15 16:24:46 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 4544 bytes result sent to driver
[2021-05-15 13:24:46,782] {docker.py:276} INFO - 21/05/15 16:24:46 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (c776c6fed6bc, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:46,783] {docker.py:276} INFO - 21/05/15 16:24:46 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
[2021-05-15 13:24:46,783] {docker.py:276} INFO - 21/05/15 16:24:46 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 2682 ms on c776c6fed6bc (executor driver) (183/200)
[2021-05-15 13:24:46,792] {docker.py:276} INFO - 21/05/15 16:24:46 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:46,793] {docker.py:276} INFO - 21/05/15 16:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:46,795] {docker.py:276} INFO - 21/05/15 16:24:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622264723275446920547144_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264723275446920547144_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622264723275446920547144_0004}; taskId=attempt_202105151622264723275446920547144_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@656aa411}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:46,795] {docker.py:276} INFO - 21/05/15 16:24:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:46 INFO StagingCommitter: Starting: Task committer attempt_202105151622264723275446920547144_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264723275446920547144_0004_m_000186_474
[2021-05-15 13:24:46,799] {docker.py:276} INFO - 21/05/15 16:24:46 INFO StagingCommitter: Task committer attempt_202105151622264723275446920547144_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622264723275446920547144_0004_m_000186_474 : duration 0:00.003s
[2021-05-15 13:24:46,980] {docker.py:276} INFO - 21/05/15 16:24:46 INFO StagingCommitter: Starting: Task committer attempt_20210515162226282072531255123936_0004_m_000184_472: needsTaskCommit() Task attempt_20210515162226282072531255123936_0004_m_000184_472
[2021-05-15 13:24:46,980] {docker.py:276} INFO - 21/05/15 16:24:47 INFO StagingCommitter: Task committer attempt_20210515162226282072531255123936_0004_m_000184_472: needsTaskCommit() Task attempt_20210515162226282072531255123936_0004_m_000184_472: duration 0:00.001s
21/05/15 16:24:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515162226282072531255123936_0004_m_000184_472
[2021-05-15 13:24:46,982] {docker.py:276} INFO - 21/05/15 16:24:47 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 4544 bytes result sent to driver
[2021-05-15 13:24:46,983] {docker.py:276} INFO - 21/05/15 16:24:47 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (c776c6fed6bc, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:46,984] {docker.py:276} INFO - 21/05/15 16:24:47 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 1621 ms on c776c6fed6bc (executor driver) (184/200)
[2021-05-15 13:24:46,985] {docker.py:276} INFO - 21/05/15 16:24:47 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
[2021-05-15 13:24:46,992] {docker.py:276} INFO - 21/05/15 16:24:47 INFO ShuffleBlockFetcherIterator: Getting 5 (23.1 KiB) non-empty blocks including 5 (23.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:46,994] {docker.py:276} INFO - 21/05/15 16:24:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:46,994] {docker.py:276} INFO - 21/05/15 16:24:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265786851398209776243_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265786851398209776243_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265786851398209776243_0004}; taskId=attempt_202105151622265786851398209776243_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d4c2e5a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:47 INFO StagingCommitter: Starting: Task committer attempt_202105151622265786851398209776243_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265786851398209776243_0004_m_000187_475
[2021-05-15 13:24:46,997] {docker.py:276} INFO - 21/05/15 16:24:47 INFO StagingCommitter: Task committer attempt_202105151622265786851398209776243_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265786851398209776243_0004_m_000187_475 : duration 0:00.003s
[2021-05-15 13:24:47,859] {docker.py:276} INFO - 21/05/15 16:24:47 INFO StagingCommitter: Starting: Task committer attempt_202105151622263383296858274404297_0004_m_000183_471: needsTaskCommit() Task attempt_202105151622263383296858274404297_0004_m_000183_471
[2021-05-15 13:24:47,860] {docker.py:276} INFO - 21/05/15 16:24:47 INFO StagingCommitter: Task committer attempt_202105151622263383296858274404297_0004_m_000183_471: needsTaskCommit() Task attempt_202105151622263383296858274404297_0004_m_000183_471: duration 0:00.000s
21/05/15 16:24:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263383296858274404297_0004_m_000183_471
[2021-05-15 13:24:47,862] {docker.py:276} INFO - 21/05/15 16:24:47 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 4544 bytes result sent to driver
[2021-05-15 13:24:47,863] {docker.py:276} INFO - 21/05/15 16:24:47 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (c776c6fed6bc, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:47,864] {docker.py:276} INFO - 21/05/15 16:24:47 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 3212 ms on c776c6fed6bc (executor driver) (185/200)
[2021-05-15 13:24:47,865] {docker.py:276} INFO - 21/05/15 16:24:47 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-15 13:24:47,877] {docker.py:276} INFO - 21/05/15 16:24:47 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:47,877] {docker.py:276} INFO - 21/05/15 16:24:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:47,879] {docker.py:276} INFO - 21/05/15 16:24:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 13:24:47,880] {docker.py:276} INFO - 21/05/15 16:24:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:47,881] {docker.py:276} INFO - 21/05/15 16:24:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:47,881] {docker.py:276} INFO - 21/05/15 16:24:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268314770249363821377_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268314770249363821377_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268314770249363821377_0004}; taskId=attempt_202105151622268314770249363821377_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72f5ff04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:47,881] {docker.py:276} INFO - 21/05/15 16:24:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:47,881] {docker.py:276} INFO - 21/05/15 16:24:47 INFO StagingCommitter: Starting: Task committer attempt_202105151622268314770249363821377_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268314770249363821377_0004_m_000188_476
[2021-05-15 13:24:47,883] {docker.py:276} INFO - 21/05/15 16:24:47 INFO StagingCommitter: Task committer attempt_202105151622268314770249363821377_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268314770249363821377_0004_m_000188_476 : duration 0:00.003s
[2021-05-15 13:24:48,504] {docker.py:276} INFO - 21/05/15 16:24:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622268910423430947206159_0004_m_000185_473: needsTaskCommit() Task attempt_202105151622268910423430947206159_0004_m_000185_473
[2021-05-15 13:24:48,505] {docker.py:276} INFO - 21/05/15 16:24:48 INFO StagingCommitter: Task committer attempt_202105151622268910423430947206159_0004_m_000185_473: needsTaskCommit() Task attempt_202105151622268910423430947206159_0004_m_000185_473: duration 0:00.001s
21/05/15 16:24:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268910423430947206159_0004_m_000185_473
[2021-05-15 13:24:48,508] {docker.py:276} INFO - 21/05/15 16:24:48 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 4544 bytes result sent to driver
21/05/15 16:24:48 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (c776c6fed6bc, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:48,509] {docker.py:276} INFO - 21/05/15 16:24:48 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 2652 ms on c776c6fed6bc (executor driver) (186/200)
[2021-05-15 13:24:48,510] {docker.py:276} INFO - 21/05/15 16:24:48 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-15 13:24:48,530] {docker.py:276} INFO - 21/05/15 16:24:48 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:48,532] {docker.py:276} INFO - 21/05/15 16:24:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:48,533] {docker.py:276} INFO - 21/05/15 16:24:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263774143493017610076_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263774143493017610076_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263774143493017610076_0004}; taskId=attempt_202105151622263774143493017610076_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18228539}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:48,533] {docker.py:276} INFO - 21/05/15 16:24:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:48 INFO StagingCommitter: Starting: Task committer attempt_202105151622263774143493017610076_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263774143493017610076_0004_m_000189_477
[2021-05-15 13:24:48,538] {docker.py:276} INFO - 21/05/15 16:24:48 INFO StagingCommitter: Task committer attempt_202105151622263774143493017610076_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263774143493017610076_0004_m_000189_477 : duration 0:00.004s
[2021-05-15 13:24:49,465] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622265786851398209776243_0004_m_000187_475: needsTaskCommit() Task attempt_202105151622265786851398209776243_0004_m_000187_475
[2021-05-15 13:24:49,465] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Task committer attempt_202105151622265786851398209776243_0004_m_000187_475: needsTaskCommit() Task attempt_202105151622265786851398209776243_0004_m_000187_475: duration 0:00.001s
21/05/15 16:24:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265786851398209776243_0004_m_000187_475
[2021-05-15 13:24:49,467] {docker.py:276} INFO - 21/05/15 16:24:49 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 4587 bytes result sent to driver
[2021-05-15 13:24:49,468] {docker.py:276} INFO - 21/05/15 16:24:49 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (c776c6fed6bc, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:49,469] {docker.py:276} INFO - 21/05/15 16:24:49 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 2489 ms on c776c6fed6bc (executor driver) (187/200)
[2021-05-15 13:24:49,470] {docker.py:276} INFO - 21/05/15 16:24:49 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-15 13:24:49,478] {docker.py:276} INFO - 21/05/15 16:24:49 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:49,480] {docker.py:276} INFO - 21/05/15 16:24:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:49,480] {docker.py:276} INFO - 21/05/15 16:24:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263290646774537659661_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263290646774537659661_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263290646774537659661_0004}; taskId=attempt_202105151622263290646774537659661_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62cf0f0c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622263290646774537659661_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263290646774537659661_0004_m_000190_478
[2021-05-15 13:24:49,484] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Task committer attempt_202105151622263290646774537659661_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263290646774537659661_0004_m_000190_478 : duration 0:00.003s
[2021-05-15 13:24:49,833] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622264723275446920547144_0004_m_000186_474: needsTaskCommit() Task attempt_202105151622264723275446920547144_0004_m_000186_474
[2021-05-15 13:24:49,833] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Task committer attempt_202105151622264723275446920547144_0004_m_000186_474: needsTaskCommit() Task attempt_202105151622264723275446920547144_0004_m_000186_474: duration 0:00.001s
21/05/15 16:24:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622264723275446920547144_0004_m_000186_474
[2021-05-15 13:24:49,835] {docker.py:276} INFO - 21/05/15 16:24:49 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 4587 bytes result sent to driver
[2021-05-15 13:24:49,836] {docker.py:276} INFO - 21/05/15 16:24:49 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (c776c6fed6bc, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:49,837] {docker.py:276} INFO - 21/05/15 16:24:49 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
21/05/15 16:24:49 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 3059 ms on c776c6fed6bc (executor driver) (188/200)
[2021-05-15 13:24:49,845] {docker.py:276} INFO - 21/05/15 16:24:49 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:49,847] {docker.py:276} INFO - 21/05/15 16:24:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622266663406219500342830_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266663406219500342830_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622266663406219500342830_0004}; taskId=attempt_202105151622266663406219500342830_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24bb805e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:49,848] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Starting: Task committer attempt_202105151622266663406219500342830_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266663406219500342830_0004_m_000191_479
[2021-05-15 13:24:49,851] {docker.py:276} INFO - 21/05/15 16:24:49 INFO StagingCommitter: Task committer attempt_202105151622266663406219500342830_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622266663406219500342830_0004_m_000191_479 : duration 0:00.004s
[2021-05-15 13:24:50,486] {docker.py:276} INFO - 21/05/15 16:24:50 INFO StagingCommitter: Starting: Task committer attempt_202105151622268314770249363821377_0004_m_000188_476: needsTaskCommit() Task attempt_202105151622268314770249363821377_0004_m_000188_476
[2021-05-15 13:24:50,487] {docker.py:276} INFO - 21/05/15 16:24:50 INFO StagingCommitter: Task committer attempt_202105151622268314770249363821377_0004_m_000188_476: needsTaskCommit() Task attempt_202105151622268314770249363821377_0004_m_000188_476: duration 0:00.000s
21/05/15 16:24:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268314770249363821377_0004_m_000188_476
[2021-05-15 13:24:50,488] {docker.py:276} INFO - 21/05/15 16:24:50 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 4587 bytes result sent to driver
[2021-05-15 13:24:50,489] {docker.py:276} INFO - 21/05/15 16:24:50 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (c776c6fed6bc, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:50,490] {docker.py:276} INFO - 21/05/15 16:24:50 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 2629 ms on c776c6fed6bc (executor driver) (189/200)
21/05/15 16:24:50 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-15 13:24:50,499] {docker.py:276} INFO - 21/05/15 16:24:50 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:50,501] {docker.py:276} INFO - 21/05/15 16:24:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263904766794505557570_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263904766794505557570_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263904766794505557570_0004}; taskId=attempt_202105151622263904766794505557570_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d60788f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:50 INFO StagingCommitter: Starting: Task committer attempt_202105151622263904766794505557570_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263904766794505557570_0004_m_000192_480
[2021-05-15 13:24:50,505] {docker.py:276} INFO - 21/05/15 16:24:50 INFO StagingCommitter: Task committer attempt_202105151622263904766794505557570_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263904766794505557570_0004_m_000192_480 : duration 0:00.004s
[2021-05-15 13:24:51,277] {docker.py:276} INFO - 21/05/15 16:24:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622263774143493017610076_0004_m_000189_477: needsTaskCommit() Task attempt_202105151622263774143493017610076_0004_m_000189_477
21/05/15 16:24:51 INFO StagingCommitter: Task committer attempt_202105151622263774143493017610076_0004_m_000189_477: needsTaskCommit() Task attempt_202105151622263774143493017610076_0004_m_000189_477: duration 0:00.000s
21/05/15 16:24:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263774143493017610076_0004_m_000189_477
[2021-05-15 13:24:51,279] {docker.py:276} INFO - 21/05/15 16:24:51 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 4587 bytes result sent to driver
[2021-05-15 13:24:51,279] {docker.py:276} INFO - 21/05/15 16:24:51 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (c776c6fed6bc, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:51,280] {docker.py:276} INFO - 21/05/15 16:24:51 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 2777 ms on c776c6fed6bc (executor driver) (190/200)
[2021-05-15 13:24:51,282] {docker.py:276} INFO - 21/05/15 16:24:51 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-15 13:24:51,292] {docker.py:276} INFO - 21/05/15 16:24:51 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 13:24:51,293] {docker.py:276} INFO - 21/05/15 16:24:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 13:24:51,295] {docker.py:276} INFO - 21/05/15 16:24:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265194323037359832244_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265194323037359832244_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265194323037359832244_0004}; taskId=attempt_202105151622265194323037359832244_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72c4b936}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:51,296] {docker.py:276} INFO - 21/05/15 16:24:51 INFO StagingCommitter: Starting: Task committer attempt_202105151622265194323037359832244_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265194323037359832244_0004_m_000193_481
[2021-05-15 13:24:51,299] {docker.py:276} INFO - 21/05/15 16:24:51 INFO StagingCommitter: Task committer attempt_202105151622265194323037359832244_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265194323037359832244_0004_m_000193_481 : duration 0:00.003s
[2021-05-15 13:24:52,038] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622263290646774537659661_0004_m_000190_478: needsTaskCommit() Task attempt_202105151622263290646774537659661_0004_m_000190_478
[2021-05-15 13:24:52,046] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Task committer attempt_202105151622263290646774537659661_0004_m_000190_478: needsTaskCommit() Task attempt_202105151622263290646774537659661_0004_m_000190_478: duration 0:00.001s
21/05/15 16:24:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263290646774537659661_0004_m_000190_478
[2021-05-15 13:24:52,047] {docker.py:276} INFO - 21/05/15 16:24:52 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 4544 bytes result sent to driver
21/05/15 16:24:52 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (c776c6fed6bc, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:52,048] {docker.py:276} INFO - 21/05/15 16:24:52 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 2576 ms on c776c6fed6bc (executor driver) (191/200)
21/05/15 16:24:52 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-15 13:24:52,054] {docker.py:276} INFO - 21/05/15 16:24:52 INFO ShuffleBlockFetcherIterator: Getting 5 (23.0 KiB) non-empty blocks including 5 (23.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:52,056] {docker.py:276} INFO - 21/05/15 16:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622261111684625953288578_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261111684625953288578_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622261111684625953288578_0004}; taskId=attempt_202105151622261111684625953288578_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bfc5439}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622261111684625953288578_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261111684625953288578_0004_m_000194_482
[2021-05-15 13:24:52,061] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Task committer attempt_202105151622261111684625953288578_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622261111684625953288578_0004_m_000194_482 : duration 0:00.004s
[2021-05-15 13:24:52,515] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622266663406219500342830_0004_m_000191_479: needsTaskCommit() Task attempt_202105151622266663406219500342830_0004_m_000191_479
[2021-05-15 13:24:52,516] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Task committer attempt_202105151622266663406219500342830_0004_m_000191_479: needsTaskCommit() Task attempt_202105151622266663406219500342830_0004_m_000191_479: duration 0:00.001s
[2021-05-15 13:24:52,517] {docker.py:276} INFO - 21/05/15 16:24:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622266663406219500342830_0004_m_000191_479
[2021-05-15 13:24:52,518] {docker.py:276} INFO - 21/05/15 16:24:52 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 4544 bytes result sent to driver
[2021-05-15 13:24:52,520] {docker.py:276} INFO - 21/05/15 16:24:52 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (c776c6fed6bc, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:52,521] {docker.py:276} INFO - 21/05/15 16:24:52 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-15 13:24:52,523] {docker.py:276} INFO - 21/05/15 16:24:52 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 2690 ms on c776c6fed6bc (executor driver) (192/200)
[2021-05-15 13:24:52,533] {docker.py:276} INFO - 21/05/15 16:24:52 INFO ShuffleBlockFetcherIterator: Getting 5 (22.7 KiB) non-empty blocks including 5 (22.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:52,535] {docker.py:276} INFO - 21/05/15 16:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262730162958410098888_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262730162958410098888_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262730162958410098888_0004}; taskId=attempt_202105151622262730162958410098888_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78656db4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:52,535] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Starting: Task committer attempt_202105151622262730162958410098888_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262730162958410098888_0004_m_000195_483
[2021-05-15 13:24:52,538] {docker.py:276} INFO - 21/05/15 16:24:52 INFO StagingCommitter: Task committer attempt_202105151622262730162958410098888_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262730162958410098888_0004_m_000195_483 : duration 0:00.003s
[2021-05-15 13:24:53,161] {docker.py:276} INFO - 21/05/15 16:24:53 INFO StagingCommitter: Starting: Task committer attempt_202105151622263904766794505557570_0004_m_000192_480: needsTaskCommit() Task attempt_202105151622263904766794505557570_0004_m_000192_480
21/05/15 16:24:53 INFO StagingCommitter: Task committer attempt_202105151622263904766794505557570_0004_m_000192_480: needsTaskCommit() Task attempt_202105151622263904766794505557570_0004_m_000192_480: duration 0:00.000s
21/05/15 16:24:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263904766794505557570_0004_m_000192_480
[2021-05-15 13:24:53,162] {docker.py:276} INFO - 21/05/15 16:24:53 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 4544 bytes result sent to driver
[2021-05-15 13:24:53,164] {docker.py:276} INFO - 21/05/15 16:24:53 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (c776c6fed6bc, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:53,165] {docker.py:276} INFO - 21/05/15 16:24:53 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 2679 ms on c776c6fed6bc (executor driver) (193/200)
21/05/15 16:24:53 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-15 13:24:53,175] {docker.py:276} INFO - 21/05/15 16:24:53 INFO ShuffleBlockFetcherIterator: Getting 5 (24.3 KiB) non-empty blocks including 5 (24.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:53,177] {docker.py:276} INFO - 21/05/15 16:24:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622263423329129539276017_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263423329129539276017_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622263423329129539276017_0004}; taskId=attempt_202105151622263423329129539276017_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b3a0c91}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 13:24:53,177] {docker.py:276} INFO - 21/05/15 16:24:53 INFO StagingCommitter: Starting: Task committer attempt_202105151622263423329129539276017_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263423329129539276017_0004_m_000196_484
[2021-05-15 13:24:53,180] {docker.py:276} INFO - 21/05/15 16:24:53 INFO StagingCommitter: Task committer attempt_202105151622263423329129539276017_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622263423329129539276017_0004_m_000196_484 : duration 0:00.003s
[2021-05-15 13:24:53,955] {docker.py:276} INFO - 21/05/15 16:24:53 INFO StagingCommitter: Starting: Task committer attempt_202105151622265194323037359832244_0004_m_000193_481: needsTaskCommit() Task attempt_202105151622265194323037359832244_0004_m_000193_481
[2021-05-15 13:24:53,956] {docker.py:276} INFO - 21/05/15 16:24:53 INFO StagingCommitter: Task committer attempt_202105151622265194323037359832244_0004_m_000193_481: needsTaskCommit() Task attempt_202105151622265194323037359832244_0004_m_000193_481: duration 0:00.002s
[2021-05-15 13:24:53,956] {docker.py:276} INFO - 21/05/15 16:24:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265194323037359832244_0004_m_000193_481
[2021-05-15 13:24:53,958] {docker.py:276} INFO - 21/05/15 16:24:53 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 4544 bytes result sent to driver
[2021-05-15 13:24:53,960] {docker.py:276} INFO - 21/05/15 16:24:53 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (c776c6fed6bc, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:53,961] {docker.py:276} INFO - 21/05/15 16:24:53 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-15 13:24:53,963] {docker.py:276} INFO - 21/05/15 16:24:53 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 2686 ms on c776c6fed6bc (executor driver) (194/200)
[2021-05-15 13:24:53,976] {docker.py:276} INFO - 21/05/15 16:24:54 INFO ShuffleBlockFetcherIterator: Getting 5 (22.1 KiB) non-empty blocks including 5 (22.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:53,978] {docker.py:276} INFO - 21/05/15 16:24:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622268836198749630323119_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268836198749630323119_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622268836198749630323119_0004}; taskId=attempt_202105151622268836198749630323119_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bf22022}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622268836198749630323119_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268836198749630323119_0004_m_000197_485
[2021-05-15 13:24:53,986] {docker.py:276} INFO - 21/05/15 16:24:54 INFO StagingCommitter: Task committer attempt_202105151622268836198749630323119_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622268836198749630323119_0004_m_000197_485 : duration 0:00.006s
[2021-05-15 13:24:54,679] {docker.py:276} INFO - 21/05/15 16:24:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622261111684625953288578_0004_m_000194_482: needsTaskCommit() Task attempt_202105151622261111684625953288578_0004_m_000194_482
[2021-05-15 13:24:54,680] {docker.py:276} INFO - 21/05/15 16:24:54 INFO StagingCommitter: Task committer attempt_202105151622261111684625953288578_0004_m_000194_482: needsTaskCommit() Task attempt_202105151622261111684625953288578_0004_m_000194_482: duration 0:00.001s
[2021-05-15 13:24:54,681] {docker.py:276} INFO - 21/05/15 16:24:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622261111684625953288578_0004_m_000194_482
[2021-05-15 13:24:54,682] {docker.py:276} INFO - 21/05/15 16:24:54 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 4544 bytes result sent to driver
[2021-05-15 13:24:54,684] {docker.py:276} INFO - 21/05/15 16:24:54 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (c776c6fed6bc, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:54,685] {docker.py:276} INFO - 21/05/15 16:24:54 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-15 13:24:54,687] {docker.py:276} INFO - 21/05/15 16:24:54 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 2649 ms on c776c6fed6bc (executor driver) (195/200)
[2021-05-15 13:24:54,702] {docker.py:276} INFO - 21/05/15 16:24:54 INFO ShuffleBlockFetcherIterator: Getting 5 (23.4 KiB) non-empty blocks including 5 (23.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:54,706] {docker.py:276} INFO - 21/05/15 16:24:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 13:24:54,707] {docker.py:276} INFO - 21/05/15 16:24:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:54,707] {docker.py:276} INFO - 21/05/15 16:24:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622262263124754407908371_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262263124754407908371_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622262263124754407908371_0004}; taskId=attempt_202105151622262263124754407908371_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bd7c886}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 16:24:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:54 INFO StagingCommitter: Starting: Task committer attempt_202105151622262263124754407908371_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262263124754407908371_0004_m_000198_486
[2021-05-15 13:24:54,714] {docker.py:276} INFO - 21/05/15 16:24:54 INFO StagingCommitter: Task committer attempt_202105151622262263124754407908371_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622262263124754407908371_0004_m_000198_486 : duration 0:00.006s
[2021-05-15 13:24:55,133] {docker.py:276} INFO - 21/05/15 16:24:55 INFO StagingCommitter: Starting: Task committer attempt_202105151622262730162958410098888_0004_m_000195_483: needsTaskCommit() Task attempt_202105151622262730162958410098888_0004_m_000195_483
21/05/15 16:24:55 INFO StagingCommitter: Task committer attempt_202105151622262730162958410098888_0004_m_000195_483: needsTaskCommit() Task attempt_202105151622262730162958410098888_0004_m_000195_483: duration 0:00.001s
21/05/15 16:24:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262730162958410098888_0004_m_000195_483
[2021-05-15 13:24:55,136] {docker.py:276} INFO - 21/05/15 16:24:55 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 4544 bytes result sent to driver
[2021-05-15 13:24:55,137] {docker.py:276} INFO - 21/05/15 16:24:55 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (c776c6fed6bc, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 13:24:55,138] {docker.py:276} INFO - 21/05/15 16:24:55 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 2621 ms on c776c6fed6bc (executor driver) (196/200)
[2021-05-15 13:24:55,139] {docker.py:276} INFO - 21/05/15 16:24:55 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-15 13:24:55,148] {docker.py:276} INFO - 21/05/15 16:24:55 INFO ShuffleBlockFetcherIterator: Getting 5 (23.5 KiB) non-empty blocks including 5 (23.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 16:24:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 13:24:55,151] {docker.py:276} INFO - 21/05/15 16:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 16:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 16:24:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 16:24:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151622265570306477345742746_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265570306477345742746_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151622265570306477345742746_0004}; taskId=attempt_202105151622265570306477345742746_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ed1f5a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/6894fed5-d186-4c8a-8e97-16f62bac58f9/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:55,151] {docker.py:276} INFO - 21/05/15 16:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 16:24:55 INFO StagingCommitter: Starting: Task committer attempt_202105151622265570306477345742746_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265570306477345742746_0004_m_000199_487
[2021-05-15 13:24:55,155] {docker.py:276} INFO - 21/05/15 16:24:55 INFO StagingCommitter: Task committer attempt_202105151622265570306477345742746_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/6894fed5-d186-4c8a-8e97-16f62bac58f9/_temporary/0/_temporary/attempt_202105151622265570306477345742746_0004_m_000199_487 : duration 0:00.003s
[2021-05-15 13:24:56,027] {docker.py:276} INFO - 21/05/15 16:24:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622263423329129539276017_0004_m_000196_484: needsTaskCommit() Task attempt_202105151622263423329129539276017_0004_m_000196_484
[2021-05-15 13:24:56,028] {docker.py:276} INFO - 21/05/15 16:24:56 INFO StagingCommitter: Task committer attempt_202105151622263423329129539276017_0004_m_000196_484: needsTaskCommit() Task attempt_202105151622263423329129539276017_0004_m_000196_484: duration 0:00.001s
21/05/15 16:24:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622263423329129539276017_0004_m_000196_484
[2021-05-15 13:24:56,030] {docker.py:276} INFO - 21/05/15 16:24:56 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 4544 bytes result sent to driver
[2021-05-15 13:24:56,033] {docker.py:276} INFO - 21/05/15 16:24:56 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 2872 ms on c776c6fed6bc (executor driver) (197/200)
[2021-05-15 13:24:56,606] {docker.py:276} INFO - 21/05/15 16:24:56 INFO StagingCommitter: Starting: Task committer attempt_202105151622268836198749630323119_0004_m_000197_485: needsTaskCommit() Task attempt_202105151622268836198749630323119_0004_m_000197_485
21/05/15 16:24:56 INFO StagingCommitter: Task committer attempt_202105151622268836198749630323119_0004_m_000197_485: needsTaskCommit() Task attempt_202105151622268836198749630323119_0004_m_000197_485: duration 0:00.000s
[2021-05-15 13:24:56,607] {docker.py:276} INFO - 21/05/15 16:24:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622268836198749630323119_0004_m_000197_485
[2021-05-15 13:24:56,607] {docker.py:276} INFO - 21/05/15 16:24:56 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 4544 bytes result sent to driver
[2021-05-15 13:24:56,609] {docker.py:276} INFO - 21/05/15 16:24:56 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 2652 ms on c776c6fed6bc (executor driver) (198/200)
[2021-05-15 13:24:56,971] {docker.py:276} INFO - 21/05/15 16:24:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622262263124754407908371_0004_m_000198_486: needsTaskCommit() Task attempt_202105151622262263124754407908371_0004_m_000198_486
[2021-05-15 13:24:56,971] {docker.py:276} INFO - 21/05/15 16:24:57 INFO StagingCommitter: Task committer attempt_202105151622262263124754407908371_0004_m_000198_486: needsTaskCommit() Task attempt_202105151622262263124754407908371_0004_m_000198_486: duration 0:00.000s
21/05/15 16:24:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622262263124754407908371_0004_m_000198_486
[2021-05-15 13:24:56,972] {docker.py:276} INFO - 21/05/15 16:24:57 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 4544 bytes result sent to driver
[2021-05-15 13:24:56,973] {docker.py:276} INFO - 21/05/15 16:24:57 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 2293 ms on c776c6fed6bc (executor driver) (199/200)
[2021-05-15 13:24:57,327] {docker.py:276} INFO - 21/05/15 16:24:57 INFO StagingCommitter: Starting: Task committer attempt_202105151622265570306477345742746_0004_m_000199_487: needsTaskCommit() Task attempt_202105151622265570306477345742746_0004_m_000199_487
[2021-05-15 13:24:57,328] {docker.py:276} INFO - 21/05/15 16:24:57 INFO StagingCommitter: Task committer attempt_202105151622265570306477345742746_0004_m_000199_487: needsTaskCommit() Task attempt_202105151622265570306477345742746_0004_m_000199_487: duration 0:00.000s
[2021-05-15 13:24:57,329] {docker.py:276} INFO - 21/05/15 16:24:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151622265570306477345742746_0004_m_000199_487
[2021-05-15 13:24:57,329] {docker.py:276} INFO - 21/05/15 16:24:57 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 4544 bytes result sent to driver
[2021-05-15 13:24:57,330] {docker.py:276} INFO - 21/05/15 16:24:57 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 2197 ms on c776c6fed6bc (executor driver) (200/200)
21/05/15 16:24:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-15 13:24:57,331] {docker.py:276} INFO - 21/05/15 16:24:57 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 133.052 s
[2021-05-15 13:24:57,332] {docker.py:276} INFO - 21/05/15 16:24:57 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 16:24:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-15 13:24:57,333] {docker.py:276} INFO - 21/05/15 16:24:57 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 150.819677 s
[2021-05-15 13:24:57,334] {docker.py:276} INFO - 21/05/15 16:24:57 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105151622263298027914728332583_0000_m_000000_0: commitJob((no job ID))
[2021-05-15 13:24:57,359] {docker.py:276} INFO - 21/05/15 16:24:57 WARN AbstractS3ACommitter: Task committer attempt_202105151622263298027914728332583_0000_m_000000_0: No pending uploads to commit
[2021-05-15 13:24:57,867] {docker.py:276} INFO - 21/05/15 16:24:57 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/15 16:24:57 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-15 13:24:58,045] {docker.py:276} INFO - 21/05/15 16:24:58 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.178s
21/05/15 16:24:58 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.178s
[2021-05-15 13:24:58,045] {docker.py:276} INFO - 21/05/15 16:24:58 INFO AbstractS3ACommitter: Task committer attempt_202105151622263298027914728332583_0000_m_000000_0: commitJob((no job ID)): duration 0:00.712s
[2021-05-15 13:24:58,563] {docker.py:276} INFO - 21/05/15 16:24:58 INFO FileFormatWriter: Write Job 6894fed5-d186-4c8a-8e97-16f62bac58f9 committed.
[2021-05-15 13:24:58,574] {docker.py:276} INFO - 21/05/15 16:24:58 INFO FileFormatWriter: Finished processing stats for write job 6894fed5-d186-4c8a-8e97-16f62bac58f9.
[2021-05-15 13:24:58,712] {docker.py:276} INFO - 21/05/15 16:24:58 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-15 13:24:58,728] {docker.py:276} INFO - 21/05/15 16:24:58 INFO SparkUI: Stopped Spark web UI at http://c776c6fed6bc:4040
[2021-05-15 13:24:58,759] {docker.py:276} INFO - 21/05/15 16:24:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-15 13:24:58,782] {docker.py:276} INFO - 21/05/15 16:24:58 INFO MemoryStore: MemoryStore cleared
[2021-05-15 13:24:58,783] {docker.py:276} INFO - 21/05/15 16:24:58 INFO BlockManager: BlockManager stopped
[2021-05-15 13:24:58,787] {docker.py:276} INFO - 21/05/15 16:24:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-15 13:24:58,793] {docker.py:276} INFO - 21/05/15 16:24:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-15 13:24:58,800] {docker.py:276} INFO - 21/05/15 16:24:58 INFO SparkContext: Successfully stopped SparkContext
[2021-05-15 13:24:58,802] {docker.py:276} INFO - 21/05/15 16:24:58 INFO ShutdownHookManager: Shutdown hook called
[2021-05-15 13:24:58,803] {docker.py:276} INFO - 21/05/15 16:24:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d963354-2f65-4d49-9240-efe812ea8ba1
[2021-05-15 13:24:58,806] {docker.py:276} INFO - 21/05/15 16:24:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0c8e9cca-f713-4000-bb52-dde875be0cb4
[2021-05-15 13:24:58,809] {docker.py:276} INFO - 21/05/15 16:24:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0c8e9cca-f713-4000-bb52-dde875be0cb4/pyspark-791c454f-38ee-48fb-8269-e964719747b4
[2021-05-15 13:24:58,816] {docker.py:276} INFO - 21/05/15 16:24:58 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-15 13:24:58,817] {docker.py:276} INFO - 21/05/15 16:24:58 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-15 13:24:58,818] {docker.py:276} INFO - 21/05/15 16:24:58 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-15 13:24:59,329] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210515T162043, start_date=20210515T162146, end_date=20210515T162459
[2021-05-15 13:24:59,406] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-15 13:24:59,459] {local_task_job.py:146} INFO - Task exited with return code 0
