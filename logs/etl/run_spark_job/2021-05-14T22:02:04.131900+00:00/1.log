[2021-05-14 19:02:54,101] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T22:02:04.131900+00:00 [queued]>
[2021-05-14 19:02:54,107] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T22:02:04.131900+00:00 [queued]>
[2021-05-14 19:02:54,107] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 19:02:54,107] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 19:02:54,107] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 19:02:54,112] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T22:02:04.131900+00:00
[2021-05-14 19:02:54,115] {standard_task_runner.py:52} INFO - Started process 25355 to run task
[2021-05-14 19:02:54,122] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T22:02:04.131900+00:00', '--job-id', '552', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmplcpo543r', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp5aovh4ks']
[2021-05-14 19:02:54,123] {standard_task_runner.py:77} INFO - Job 552: Subtask run_spark_job
[2021-05-14 19:02:54,152] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T22:02:04.131900+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-14 19:02:54,174] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T22:02:04.131900+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T22:02:04.131900+00:00
[2021-05-14 19:02:54,177] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 19:02:57,192] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 19:02:57,193] {docker.py:312} INFO - Digest: sha256:3ff139a9dbefaff7945c18f6cbdbc77460bc981cf21cbe1d495f99b227e826c8
[2021-05-14 19:02:57,193] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 19:02:57,198] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 19:02:59,223] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 19:03:00,069] {docker.py:276} INFO - 21/05/14 22:03:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 19:03:02,417] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 19:03:02,431] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 19:03:02,493] {docker.py:276} INFO - 21/05/14 22:03:02 INFO ResourceUtils: ==============================================================
[2021-05-14 19:03:02,494] {docker.py:276} INFO - 21/05/14 22:03:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 19:03:02,495] {docker.py:276} INFO - 21/05/14 22:03:02 INFO ResourceUtils: ==============================================================
[2021-05-14 19:03:02,496] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SparkContext: Submitted application: spark.py
[2021-05-14 19:03:02,529] {docker.py:276} INFO - 21/05/14 22:03:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 19:03:02,544] {docker.py:276} INFO - 21/05/14 22:03:02 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 19:03:02,545] {docker.py:276} INFO - 21/05/14 22:03:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 19:03:02,614] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-14 19:03:02,615] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 19:03:02,615] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SecurityManager: Changing view acls groups to:
[2021-05-14 19:03:02,616] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SecurityManager: Changing modify acls groups to:
[2021-05-14 19:03:02,616] {docker.py:276} INFO - 21/05/14 22:03:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 19:03:02,996] {docker.py:276} INFO - 21/05/14 22:03:03 INFO Utils: Successfully started service 'sparkDriver' on port 37443.
[2021-05-14 19:03:03,034] {docker.py:276} INFO - 21/05/14 22:03:03 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 19:03:03,081] {docker.py:276} INFO - 21/05/14 22:03:03 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 19:03:03,119] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 19:03:03,119] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 19:03:03,126] {docker.py:276} INFO - 21/05/14 22:03:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 19:03:03,143] {docker.py:276} INFO - 21/05/14 22:03:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-892f4e1a-b44a-419f-8307-093172fee809
[2021-05-14 19:03:03,171] {docker.py:276} INFO - 21/05/14 22:03:03 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 19:03:03,194] {docker.py:276} INFO - 21/05/14 22:03:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 19:03:03,488] {docker.py:276} INFO - 21/05/14 22:03:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 19:03:03,573] {docker.py:276} INFO - 21/05/14 22:03:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://028692ec38a6:4040
[2021-05-14 19:03:03,838] {docker.py:276} INFO - 21/05/14 22:03:03 INFO Executor: Starting executor ID driver on host 028692ec38a6
[2021-05-14 19:03:03,881] {docker.py:276} INFO - 21/05/14 22:03:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37159.
21/05/14 22:03:03 INFO NettyBlockTransferService: Server created on 028692ec38a6:37159
[2021-05-14 19:03:03,884] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 19:03:03,896] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 028692ec38a6, 37159, None)
[2021-05-14 19:03:03,905] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManagerMasterEndpoint: Registering block manager 028692ec38a6:37159 with 934.4 MiB RAM, BlockManagerId(driver, 028692ec38a6, 37159, None)
[2021-05-14 19:03:03,912] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 028692ec38a6, 37159, None)
[2021-05-14 19:03:03,914] {docker.py:276} INFO - 21/05/14 22:03:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 028692ec38a6, 37159, None)
[2021-05-14 19:03:04,533] {docker.py:276} INFO - 21/05/14 22:03:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 19:03:04,534] {docker.py:276} INFO - 21/05/14 22:03:04 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 19:03:05,638] {docker.py:276} INFO - 21/05/14 22:03:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 19:03:05,697] {docker.py:276} INFO - 21/05/14 22:03:05 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-14 19:03:05,697] {docker.py:276} INFO - 21/05/14 22:03:05 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 19:03:13,849] {docker.py:276} INFO - 21/05/14 22:03:13 INFO InMemoryFileIndex: It took 619 ms to list leaf files for 3 paths.
[2021-05-14 19:03:14,480] {docker.py:276} INFO - 21/05/14 22:03:14 INFO InMemoryFileIndex: It took 542 ms to list leaf files for 3 paths.
[2021-05-14 19:03:17,073] {docker.py:276} INFO - 21/05/14 22:03:17 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 19:03:17,078] {docker.py:276} INFO - 21/05/14 22:03:17 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 19:03:17,084] {docker.py:276} INFO - 21/05/14 22:03:17 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 19:03:17,888] {docker.py:276} INFO - 21/05/14 22:03:17 INFO CodeGenerator: Code generated in 270.6642 ms
[2021-05-14 19:03:17,961] {docker.py:276} INFO - 21/05/14 22:03:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-14 19:03:18,047] {docker.py:276} INFO - 21/05/14 22:03:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 19:03:18,051] {docker.py:276} INFO - 21/05/14 22:03:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 028692ec38a6:37159 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:03:18,055] {docker.py:276} INFO - 21/05/14 22:03:18 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:03:18,079] {docker.py:276} INFO - 21/05/14 22:03:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:03:18,239] {docker.py:276} INFO - 21/05/14 22:03:18 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:03:18,262] {docker.py:276} INFO - 21/05/14 22:03:18 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-14 19:03:18,263] {docker.py:276} INFO - 21/05/14 22:03:18 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 19:03:18,264] {docker.py:276} INFO - 21/05/14 22:03:18 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:03:18,266] {docker.py:276} INFO - 21/05/14 22:03:18 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:03:18,274] {docker.py:276} INFO - 21/05/14 22:03:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:03:18,357] {docker.py:276} INFO - 21/05/14 22:03:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 19:03:18,369] {docker.py:276} INFO - 21/05/14 22:03:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 19:03:18,370] {docker.py:276} INFO - 21/05/14 22:03:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 028692ec38a6:37159 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 19:03:18,372] {docker.py:276} INFO - 21/05/14 22:03:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:03:18,389] {docker.py:276} INFO - 21/05/14 22:03:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-14 19:03:18,391] {docker.py:276} INFO - 21/05/14 22:03:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-05-14 19:03:18,484] {docker.py:276} INFO - 21/05/14 22:03:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (028692ec38a6, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:18,508] {docker.py:276} INFO - 21/05/14 22:03:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 19:03:18,678] {docker.py:276} INFO - 21/05/14 22:03:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_02_46/from_1621026166_to_1621027966.csv, range: 0-111710, partition values: [empty row]
[2021-05-14 19:03:18,707] {docker.py:276} INFO - 21/05/14 22:03:18 INFO CodeGenerator: Code generated in 19.9092 ms
[2021-05-14 19:03:19,309] {docker.py:276} INFO - 21/05/14 22:03:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1564 bytes result sent to driver
[2021-05-14 19:03:19,322] {docker.py:276} INFO - 21/05/14 22:03:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 855 ms on 028692ec38a6 (executor driver) (1/1)
[2021-05-14 19:03:19,327] {docker.py:276} INFO - 21/05/14 22:03:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 19:03:19,334] {docker.py:276} INFO - 21/05/14 22:03:19 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.041 s
[2021-05-14 19:03:19,340] {docker.py:276} INFO - 21/05/14 22:03:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 19:03:19,340] {docker.py:276} INFO - 21/05/14 22:03:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 19:03:19,354] {docker.py:276} INFO - 21/05/14 22:03:19 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.114761 s
[2021-05-14 19:03:19,389] {docker.py:276} INFO - 21/05/14 22:03:19 INFO CodeGenerator: Code generated in 17.0866 ms
[2021-05-14 19:03:19,475] {docker.py:276} INFO - 21/05/14 22:03:19 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 19:03:19,476] {docker.py:276} INFO - 21/05/14 22:03:19 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 19:03:19,478] {docker.py:276} INFO - 21/05/14 22:03:19 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 19:03:19,492] {docker.py:276} INFO - 21/05/14 22:03:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-14 19:03:19,512] {docker.py:276} INFO - 21/05/14 22:03:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 19:03:19,529] {docker.py:276} INFO - 21/05/14 22:03:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 028692ec38a6:37159 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:03:19,531] {docker.py:276} INFO - 21/05/14 22:03:19 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:03:19,532] {docker.py:276} INFO - 21/05/14 22:03:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:03:19,548] {docker.py:276} INFO - 21/05/14 22:03:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 028692ec38a6:37159 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-14 19:03:20,249] {docker.py:276} INFO - 21/05/14 22:03:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 028692ec38a6:37159 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:03:20,263] {docker.py:276} INFO - 21/05/14 22:03:20 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 19:03:20,263] {docker.py:276} INFO - 21/05/14 22:03:20 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 19:03:20,264] {docker.py:276} INFO - 21/05/14 22:03:20 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 19:03:21,440] {docker.py:276} INFO - 21/05/14 22:03:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:21,444] {docker.py:276} INFO - 21/05/14 22:03:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:21,444] {docker.py:276} INFO - 21/05/14 22:03:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217546984503769496222_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217546984503769496222_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217546984503769496222_0000}; taskId=attempt_202105142203217546984503769496222_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29cb5a1b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:21,445] {docker.py:276} INFO - 21/05/14 22:03:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:21,480] {docker.py:276} INFO - 21/05/14 22:03:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 19:03:21,588] {docker.py:276} INFO - 21/05/14 22:03:21 INFO CodeGenerator: Code generated in 73.1298 ms
[2021-05-14 19:03:21,590] {docker.py:276} INFO - 21/05/14 22:03:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 19:03:21,634] {docker.py:276} INFO - 21/05/14 22:03:21 INFO CodeGenerator: Code generated in 36.4222 ms
[2021-05-14 19:03:21,639] {docker.py:276} INFO - 21/05/14 22:03:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.5 KiB, free 934.0 MiB)
[2021-05-14 19:03:21,656] {docker.py:276} INFO - 21/05/14 22:03:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 19:03:21,656] {docker.py:276} INFO - 21/05/14 22:03:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 028692ec38a6:37159 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:03:21,658] {docker.py:276} INFO - 21/05/14 22:03:21 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:03:21,663] {docker.py:276} INFO - 21/05/14 22:03:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:03:21,741] {docker.py:276} INFO - 21/05/14 22:03:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 028692ec38a6:37159 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:03:21,803] {docker.py:276} INFO - 21/05/14 22:03:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:03:21,808] {docker.py:276} INFO - 21/05/14 22:03:21 INFO DAGScheduler: Registering RDD 13 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 19:03:21,811] {docker.py:276} INFO - 21/05/14 22:03:21 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/14 22:03:21 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 22:03:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2021-05-14 19:03:21,814] {docker.py:276} INFO - 21/05/14 22:03:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
[2021-05-14 19:03:21,816] {docker.py:276} INFO - 21/05/14 22:03:21 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:03:21,833] {docker.py:276} INFO - 21/05/14 22:03:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 934.2 MiB)
[2021-05-14 19:03:21,850] {docker.py:276} INFO - 21/05/14 22:03:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.2 MiB)
[2021-05-14 19:03:21,851] {docker.py:276} INFO - 21/05/14 22:03:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 028692ec38a6:37159 (size: 12.0 KiB, free: 934.4 MiB)
[2021-05-14 19:03:21,852] {docker.py:276} INFO - 21/05/14 22:03:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:03:21,855] {docker.py:276} INFO - 21/05/14 22:03:21 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
21/05/14 22:03:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0
[2021-05-14 19:03:21,857] {docker.py:276} INFO - 21/05/14 22:03:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (028692ec38a6, executor driver, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:21,858] {docker.py:276} INFO - 21/05/14 22:03:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (028692ec38a6, executor driver, partition 1, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:21,859] {docker.py:276} INFO - 21/05/14 22:03:21 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (028692ec38a6, executor driver, partition 2, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:21,859] {docker.py:276} INFO - 21/05/14 22:03:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2021-05-14 19:03:21,873] {docker.py:276} INFO - 21/05/14 22:03:21 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2021-05-14 19:03:21,886] {docker.py:276} INFO - 21/05/14 22:03:21 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[2021-05-14 19:03:21,990] {docker.py:276} INFO - 21/05/14 22:03:21 INFO CodeGenerator: Code generated in 29.8816 ms
[2021-05-14 19:03:22,023] {docker.py:276} INFO - 21/05/14 22:03:22 INFO CodeGenerator: Code generated in 11.0758 ms
[2021-05-14 19:03:22,054] {docker.py:276} INFO - 21/05/14 22:03:22 INFO CodeGenerator: Code generated in 21.3459 ms
[2021-05-14 19:03:22,074] {docker.py:276} INFO - 21/05/14 22:03:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_02_46/from_1621026166_to_1621027966.csv, range: 0-111710, partition values: [empty row]
[2021-05-14 19:03:22,075] {docker.py:276} INFO - 21/05/14 22:03:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_02_46/from_1621026166_to_1621027966.csv, range: 0-104506, partition values: [empty row]
21/05/14 22:03:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_02_46/from_1621026166_to_1621027966.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:03:23,135] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2722 bytes result sent to driver
[2021-05-14 19:03:23,138] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1284 ms on 028692ec38a6 (executor driver) (1/3)
[2021-05-14 19:03:23,587] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2679 bytes result sent to driver
[2021-05-14 19:03:23,588] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2679 bytes result sent to driver
[2021-05-14 19:03:23,589] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1733 ms on 028692ec38a6 (executor driver) (2/3)
[2021-05-14 19:03:23,591] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1735 ms on 028692ec38a6 (executor driver) (3/3)
[2021-05-14 19:03:23,591] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 19:03:23,593] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.738 s
[2021-05-14 19:03:23,593] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 19:03:23,595] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: running: Set()
[2021-05-14 19:03:23,595] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2021-05-14 19:03:23,596] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: failed: Set()
[2021-05-14 19:03:23,601] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:03:23,665] {docker.py:276} INFO - 21/05/14 22:03:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.1 KiB, free 934.0 MiB)
[2021-05-14 19:03:23,671] {docker.py:276} INFO - 21/05/14 22:03:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 933.9 MiB)
[2021-05-14 19:03:23,671] {docker.py:276} INFO - 21/05/14 22:03:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 028692ec38a6:37159 (size: 74.1 KiB, free: 934.3 MiB)
[2021-05-14 19:03:23,672] {docker.py:276} INFO - 21/05/14 22:03:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:03:23,675] {docker.py:276} INFO - 21/05/14 22:03:23 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 22:03:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2021-05-14 19:03:23,683] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (028692ec38a6, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:23,683] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (028692ec38a6, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:23,684] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 6) (028692ec38a6, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:23,685] {docker.py:276} INFO - 21/05/14 22:03:23 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 7) (028692ec38a6, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:23,686] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)
[2021-05-14 19:03:23,687] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)
[2021-05-14 19:03:23,687] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Running task 2.0 in stage 2.0 (TID 6)
[2021-05-14 19:03:23,702] {docker.py:276} INFO - 21/05/14 22:03:23 INFO Executor: Running task 3.0 in stage 2.0 (TID 7)
[2021-05-14 19:03:23,846] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Getting 3 (836.0 B) non-empty blocks including 3 (836.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:23,849] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:23,850] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms
[2021-05-14 19:03:23,854] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:23,855] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 32 ms
[2021-05-14 19:03:23,855] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Getting 3 (1002.0 B) non-empty blocks including 3 (1002.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:23,857] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 30 ms
[2021-05-14 19:03:23,858] {docker.py:276} INFO - 21/05/14 22:03:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 028692ec38a6:37159 in memory (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-14 19:03:23,858] {docker.py:276} INFO - 21/05/14 22:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 32 ms
[2021-05-14 19:03:23,877] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:23,881] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:23,882] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:23,883] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,883] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211091794229676644448_0002_m_000002_6, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211091794229676644448_0002_m_000002_6}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211091794229676644448_0002}; taskId=attempt_202105142203211091794229676644448_0002_m_000002_6, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e235c68}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:23,886] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,886] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203219205320072197813322_0002_m_000001_5, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219205320072197813322_0002_m_000001_5}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203219205320072197813322_0002}; taskId=attempt_202105142203219205320072197813322_0002_m_000001_5, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70b65da3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,888] {docker.py:276} INFO - 21/05/14 22:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:23,889] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203219205320072197813322_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219205320072197813322_0002_m_000001_5
[2021-05-14 19:03:23,891] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:23,891] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:23,892] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203211091794229676644448_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211091794229676644448_0002_m_000002_6
[2021-05-14 19:03:23,893] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,894] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211846895649540017509_0002_m_000000_4, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211846895649540017509_0002_m_000000_4}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211846895649540017509_0002}; taskId=attempt_202105142203211846895649540017509_0002_m_000000_4, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d51bbb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,894] {docker.py:276} INFO - 21/05/14 22:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:23,895] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203211846895649540017509_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211846895649540017509_0002_m_000000_4
[2021-05-14 19:03:23,899] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:23,899] {docker.py:276} INFO - 21/05/14 22:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:23,900] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,901] {docker.py:276} INFO - 21/05/14 22:03:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217792652999120602709_0002_m_000003_7, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217792652999120602709_0002_m_000003_7}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217792652999120602709_0002}; taskId=attempt_202105142203217792652999120602709_0002_m_000003_7, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7637c9ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:23,901] {docker.py:276} INFO - 21/05/14 22:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:23,901] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203217792652999120602709_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217792652999120602709_0002_m_000003_7
[2021-05-14 19:03:23,939] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Task committer attempt_202105142203219205320072197813322_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219205320072197813322_0002_m_000001_5 : duration 0:00.042s
[2021-05-14 19:03:23,950] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Task committer attempt_202105142203211846895649540017509_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211846895649540017509_0002_m_000000_4 : duration 0:00.055s
[2021-05-14 19:03:23,961] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Task committer attempt_202105142203217792652999120602709_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217792652999120602709_0002_m_000003_7 : duration 0:00.060s
[2021-05-14 19:03:23,961] {docker.py:276} INFO - 21/05/14 22:03:23 INFO StagingCommitter: Task committer attempt_202105142203211091794229676644448_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211091794229676644448_0002_m_000002_6 : duration 0:00.069s
[2021-05-14 19:03:25,829] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203219205320072197813322_0002_m_000001_5: needsTaskCommit() Task attempt_202105142203219205320072197813322_0002_m_000001_5
[2021-05-14 19:03:25,832] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Task committer attempt_202105142203219205320072197813322_0002_m_000001_5: needsTaskCommit() Task attempt_202105142203219205320072197813322_0002_m_000001_5: duration 0:00.004s
[2021-05-14 19:03:25,833] {docker.py:276} INFO - 21/05/14 22:03:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203219205320072197813322_0002_m_000001_5
[2021-05-14 19:03:25,838] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203217792652999120602709_0002_m_000003_7: needsTaskCommit() Task attempt_202105142203217792652999120602709_0002_m_000003_7
[2021-05-14 19:03:25,839] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Task committer attempt_202105142203217792652999120602709_0002_m_000003_7: needsTaskCommit() Task attempt_202105142203217792652999120602709_0002_m_000003_7: duration 0:00.001s
[2021-05-14 19:03:25,840] {docker.py:276} INFO - 21/05/14 22:03:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217792652999120602709_0002_m_000003_7
[2021-05-14 19:03:25,850] {docker.py:276} INFO - 21/05/14 22:03:25 INFO Executor: Finished task 3.0 in stage 2.0 (TID 7). 4630 bytes result sent to driver
[2021-05-14 19:03:25,850] {docker.py:276} INFO - 21/05/14 22:03:25 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 4630 bytes result sent to driver
21/05/14 22:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203211846895649540017509_0002_m_000000_4: needsTaskCommit() Task attempt_202105142203211846895649540017509_0002_m_000000_4
[2021-05-14 19:03:25,851] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Task committer attempt_202105142203211846895649540017509_0002_m_000000_4: needsTaskCommit() Task attempt_202105142203211846895649540017509_0002_m_000000_4: duration 0:00.000s
[2021-05-14 19:03:25,852] {docker.py:276} INFO - 21/05/14 22:03:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211846895649540017509_0002_m_000000_4
[2021-05-14 19:03:25,852] {docker.py:276} INFO - 21/05/14 22:03:25 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 8) (028692ec38a6, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:25,853] {docker.py:276} INFO - 21/05/14 22:03:25 INFO Executor: Running task 4.0 in stage 2.0 (TID 8)
[2021-05-14 19:03:25,854] {docker.py:276} INFO - 21/05/14 22:03:25 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 9) (028692ec38a6, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:25,856] {docker.py:276} INFO - 21/05/14 22:03:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 4587 bytes result sent to driver
[2021-05-14 19:03:25,857] {docker.py:276} INFO - 21/05/14 22:03:25 INFO Executor: Running task 5.0 in stage 2.0 (TID 9)
[2021-05-14 19:03:25,864] {docker.py:276} INFO - 21/05/14 22:03:25 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2179 ms on 028692ec38a6 (executor driver) (1/200)
[2021-05-14 19:03:25,865] {docker.py:276} INFO - 21/05/14 22:03:25 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 10) (028692ec38a6, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:25,865] {docker.py:276} INFO - 21/05/14 22:03:25 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 7) in 2182 ms on 028692ec38a6 (executor driver) (2/200)
[2021-05-14 19:03:25,865] {docker.py:276} INFO - 21/05/14 22:03:25 INFO Executor: Running task 6.0 in stage 2.0 (TID 10)
[2021-05-14 19:03:25,873] {docker.py:276} INFO - 21/05/14 22:03:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2196 ms on 028692ec38a6 (executor driver) (3/200)
[2021-05-14 19:03:25,884] {docker.py:276} INFO - 21/05/14 22:03:25 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:25,885] {docker.py:276} INFO - 21/05/14 22:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:25,892] {docker.py:276} INFO - 21/05/14 22:03:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:25,893] {docker.py:276} INFO - 21/05/14 22:03:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:25,893] {docker.py:276} INFO - 21/05/14 22:03:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218710536186567743296_0002_m_000005_9, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218710536186567743296_0002_m_000005_9}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218710536186567743296_0002}; taskId=attempt_202105142203218710536186567743296_0002_m_000005_9, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@548c822a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:25,893] {docker.py:276} INFO - 21/05/14 22:03:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:25,894] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203218710536186567743296_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218710536186567743296_0002_m_000005_9
[2021-05-14 19:03:25,898] {docker.py:276} INFO - 21/05/14 22:03:25 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:25,902] {docker.py:276} INFO - 21/05/14 22:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-14 19:03:25,906] {docker.py:276} INFO - 21/05/14 22:03:25 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:25,907] {docker.py:276} INFO - 21/05/14 22:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2021-05-14 19:03:25,907] {docker.py:276} INFO - 21/05/14 22:03:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:25,912] {docker.py:276} INFO - 21/05/14 22:03:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:25,930] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Task committer attempt_202105142203218710536186567743296_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218710536186567743296_0002_m_000005_9 : duration 0:00.020s
[2021-05-14 19:03:25,931] {docker.py:276} INFO - 21/05/14 22:03:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:25,931] {docker.py:276} INFO - 21/05/14 22:03:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212411722201856802561_0002_m_000006_10, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212411722201856802561_0002_m_000006_10}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212411722201856802561_0002}; taskId=attempt_202105142203212411722201856802561_0002_m_000006_10, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16d481e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:25,932] {docker.py:276} INFO - 21/05/14 22:03:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:25,932] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203212411722201856802561_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212411722201856802561_0002_m_000006_10
[2021-05-14 19:03:25,933] {docker.py:276} INFO - 21/05/14 22:03:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:25,933] {docker.py:276} INFO - 21/05/14 22:03:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:25,933] {docker.py:276} INFO - 21/05/14 22:03:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218097093546623100107_0002_m_000004_8, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218097093546623100107_0002_m_000004_8}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218097093546623100107_0002}; taskId=attempt_202105142203218097093546623100107_0002_m_000004_8, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@422f1d0f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:25,934] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203218097093546623100107_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218097093546623100107_0002_m_000004_8
[2021-05-14 19:03:25,934] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Task committer attempt_202105142203212411722201856802561_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212411722201856802561_0002_m_000006_10 : duration 0:00.010s
[2021-05-14 19:03:25,934] {docker.py:276} INFO - 21/05/14 22:03:25 INFO StagingCommitter: Task committer attempt_202105142203218097093546623100107_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218097093546623100107_0002_m_000004_8 : duration 0:00.006s
[2021-05-14 19:03:26,248] {docker.py:276} INFO - 21/05/14 22:03:26 INFO StagingCommitter: Starting: Task committer attempt_202105142203211091794229676644448_0002_m_000002_6: needsTaskCommit() Task attempt_202105142203211091794229676644448_0002_m_000002_6
[2021-05-14 19:03:26,250] {docker.py:276} INFO - 21/05/14 22:03:26 INFO StagingCommitter: Task committer attempt_202105142203211091794229676644448_0002_m_000002_6: needsTaskCommit() Task attempt_202105142203211091794229676644448_0002_m_000002_6: duration 0:00.002s
21/05/14 22:03:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211091794229676644448_0002_m_000002_6
[2021-05-14 19:03:26,254] {docker.py:276} INFO - 21/05/14 22:03:26 INFO Executor: Finished task 2.0 in stage 2.0 (TID 6). 4587 bytes result sent to driver
[2021-05-14 19:03:26,255] {docker.py:276} INFO - 21/05/14 22:03:26 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 11) (028692ec38a6, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:26,256] {docker.py:276} INFO - 21/05/14 22:03:26 INFO Executor: Running task 7.0 in stage 2.0 (TID 11)
21/05/14 22:03:26 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 6) in 2576 ms on 028692ec38a6 (executor driver) (4/200)
[2021-05-14 19:03:26,273] {docker.py:276} INFO - 21/05/14 22:03:26 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:26,276] {docker.py:276} INFO - 21/05/14 22:03:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:26,276] {docker.py:276} INFO - 21/05/14 22:03:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:26,277] {docker.py:276} INFO - 21/05/14 22:03:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213328659484942725670_0002_m_000007_11, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213328659484942725670_0002_m_000007_11}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213328659484942725670_0002}; taskId=attempt_202105142203213328659484942725670_0002_m_000007_11, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19c38209}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:26,277] {docker.py:276} INFO - 21/05/14 22:03:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:26,278] {docker.py:276} INFO - 21/05/14 22:03:26 INFO StagingCommitter: Starting: Task committer attempt_202105142203213328659484942725670_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213328659484942725670_0002_m_000007_11
[2021-05-14 19:03:26,282] {docker.py:276} INFO - 21/05/14 22:03:26 INFO StagingCommitter: Task committer attempt_202105142203213328659484942725670_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213328659484942725670_0002_m_000007_11 : duration 0:00.005s
[2021-05-14 19:03:27,642] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203212411722201856802561_0002_m_000006_10: needsTaskCommit() Task attempt_202105142203212411722201856802561_0002_m_000006_10
[2021-05-14 19:03:27,643] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203212411722201856802561_0002_m_000006_10: needsTaskCommit() Task attempt_202105142203212411722201856802561_0002_m_000006_10: duration 0:00.004s
21/05/14 22:03:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212411722201856802561_0002_m_000006_10
[2021-05-14 19:03:27,645] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203218710536186567743296_0002_m_000005_9: needsTaskCommit() Task attempt_202105142203218710536186567743296_0002_m_000005_9
[2021-05-14 19:03:27,646] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203218097093546623100107_0002_m_000004_8: needsTaskCommit() Task attempt_202105142203218097093546623100107_0002_m_000004_8
[2021-05-14 19:03:27,647] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203218710536186567743296_0002_m_000005_9: needsTaskCommit() Task attempt_202105142203218710536186567743296_0002_m_000005_9: duration 0:00.002s
[2021-05-14 19:03:27,650] {docker.py:276} INFO - 21/05/14 22:03:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218710536186567743296_0002_m_000005_9
[2021-05-14 19:03:27,651] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203218097093546623100107_0002_m_000004_8: needsTaskCommit() Task attempt_202105142203218097093546623100107_0002_m_000004_8: duration 0:00.002s
[2021-05-14 19:03:27,652] {docker.py:276} INFO - 21/05/14 22:03:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218097093546623100107_0002_m_000004_8
[2021-05-14 19:03:27,653] {docker.py:276} INFO - 21/05/14 22:03:27 INFO Executor: Finished task 6.0 in stage 2.0 (TID 10). 4587 bytes result sent to driver
21/05/14 22:03:27 INFO Executor: Finished task 4.0 in stage 2.0 (TID 8). 4587 bytes result sent to driver
[2021-05-14 19:03:27,653] {docker.py:276} INFO - 21/05/14 22:03:27 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 12) (028692ec38a6, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:27,654] {docker.py:276} INFO - 21/05/14 22:03:27 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 10) in 1793 ms on 028692ec38a6 (executor driver) (5/200)
[2021-05-14 19:03:27,655] {docker.py:276} INFO - 21/05/14 22:03:27 INFO Executor: Finished task 5.0 in stage 2.0 (TID 9). 4587 bytes result sent to driver
[2021-05-14 19:03:27,657] {docker.py:276} INFO - 21/05/14 22:03:27 INFO Executor: Running task 8.0 in stage 2.0 (TID 12)
[2021-05-14 19:03:27,659] {docker.py:276} INFO - 21/05/14 22:03:27 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 13) (028692ec38a6, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:27,661] {docker.py:276} INFO - 21/05/14 22:03:27 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 14) (028692ec38a6, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:27,662] {docker.py:276} INFO - 21/05/14 22:03:27 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 8) in 1813 ms on 028692ec38a6 (executor driver) (6/200)
[2021-05-14 19:03:27,663] {docker.py:276} INFO - 21/05/14 22:03:27 INFO Executor: Running task 9.0 in stage 2.0 (TID 13)
[2021-05-14 19:03:27,664] {docker.py:276} INFO - 21/05/14 22:03:27 INFO Executor: Running task 10.0 in stage 2.0 (TID 14)
[2021-05-14 19:03:27,664] {docker.py:276} INFO - 21/05/14 22:03:27 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 9) in 1812 ms on 028692ec38a6 (executor driver) (7/200)
[2021-05-14 19:03:27,677] {docker.py:276} INFO - 21/05/14 22:03:27 INFO ShuffleBlockFetcherIterator: Getting 3 (856.0 B) non-empty blocks including 3 (856.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:27,679] {docker.py:276} INFO - 21/05/14 22:03:27 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:27,679] {docker.py:276} INFO - 21/05/14 22:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:03:27,681] {docker.py:276} INFO - 21/05/14 22:03:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:27,682] {docker.py:276} INFO - 21/05/14 22:03:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218867954784012545601_0002_m_000009_13, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218867954784012545601_0002_m_000009_13}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218867954784012545601_0002}; taskId=attempt_202105142203218867954784012545601_0002_m_000009_13, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f49c0f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203218867954784012545601_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218867954784012545601_0002_m_000009_13
[2021-05-14 19:03:27,688] {docker.py:276} INFO - 21/05/14 22:03:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:27,689] {docker.py:276} INFO - 21/05/14 22:03:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:27,690] {docker.py:276} INFO - 21/05/14 22:03:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217404890870137859757_0002_m_000010_14, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217404890870137859757_0002_m_000010_14}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217404890870137859757_0002}; taskId=attempt_202105142203217404890870137859757_0002_m_000010_14, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27e0c525}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:27,691] {docker.py:276} INFO - 21/05/14 22:03:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:27,693] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203217404890870137859757_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217404890870137859757_0002_m_000010_14
[2021-05-14 19:03:27,696] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203218867954784012545601_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218867954784012545601_0002_m_000009_13 : duration 0:00.011s
[2021-05-14 19:03:27,702] {docker.py:276} INFO - 21/05/14 22:03:27 INFO ShuffleBlockFetcherIterator: Getting 3 (861.0 B) non-empty blocks including 3 (861.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:27,705] {docker.py:276} INFO - 21/05/14 22:03:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:27,706] {docker.py:276} INFO - 21/05/14 22:03:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:27,706] {docker.py:276} INFO - 21/05/14 22:03:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211867433083893905519_0002_m_000008_12, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211867433083893905519_0002_m_000008_12}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211867433083893905519_0002}; taskId=attempt_202105142203211867433083893905519_0002_m_000008_12, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49db93ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:27,707] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203211867433083893905519_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211867433083893905519_0002_m_000008_12
[2021-05-14 19:03:27,709] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203217404890870137859757_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217404890870137859757_0002_m_000010_14 : duration 0:00.017s
[2021-05-14 19:03:27,719] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203211867433083893905519_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211867433083893905519_0002_m_000008_12 : duration 0:00.012s
[2021-05-14 19:03:27,989] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203213328659484942725670_0002_m_000007_11: needsTaskCommit() Task attempt_202105142203213328659484942725670_0002_m_000007_11
[2021-05-14 19:03:27,991] {docker.py:276} INFO - 21/05/14 22:03:27 INFO StagingCommitter: Task committer attempt_202105142203213328659484942725670_0002_m_000007_11: needsTaskCommit() Task attempt_202105142203213328659484942725670_0002_m_000007_11: duration 0:00.001s
[2021-05-14 19:03:27,991] {docker.py:276} INFO - 21/05/14 22:03:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213328659484942725670_0002_m_000007_11
[2021-05-14 19:03:27,995] {docker.py:276} INFO - 21/05/14 22:03:28 INFO Executor: Finished task 7.0 in stage 2.0 (TID 11). 4544 bytes result sent to driver
[2021-05-14 19:03:27,997] {docker.py:276} INFO - 21/05/14 22:03:28 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 15) (028692ec38a6, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:27,999] {docker.py:276} INFO - 21/05/14 22:03:28 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 11) in 1746 ms on 028692ec38a6 (executor driver) (8/200)
[2021-05-14 19:03:28,000] {docker.py:276} INFO - 21/05/14 22:03:28 INFO Executor: Running task 11.0 in stage 2.0 (TID 15)
[2021-05-14 19:03:28,017] {docker.py:276} INFO - 21/05/14 22:03:28 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:28,020] {docker.py:276} INFO - 21/05/14 22:03:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:28,021] {docker.py:276} INFO - 21/05/14 22:03:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212571667542971216582_0002_m_000011_15, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212571667542971216582_0002_m_000011_15}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212571667542971216582_0002}; taskId=attempt_202105142203212571667542971216582_0002_m_000011_15, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a305407}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:28,021] {docker.py:276} INFO - 21/05/14 22:03:28 INFO StagingCommitter: Starting: Task committer attempt_202105142203212571667542971216582_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212571667542971216582_0002_m_000011_15
[2021-05-14 19:03:28,025] {docker.py:276} INFO - 21/05/14 22:03:28 INFO StagingCommitter: Task committer attempt_202105142203212571667542971216582_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212571667542971216582_0002_m_000011_15 : duration 0:00.004s
[2021-05-14 19:03:29,278] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203212571667542971216582_0002_m_000011_15: needsTaskCommit() Task attempt_202105142203212571667542971216582_0002_m_000011_15
[2021-05-14 19:03:29,278] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203212571667542971216582_0002_m_000011_15: needsTaskCommit() Task attempt_202105142203212571667542971216582_0002_m_000011_15: duration 0:00.000s
21/05/14 22:03:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212571667542971216582_0002_m_000011_15
[2021-05-14 19:03:29,282] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Finished task 11.0 in stage 2.0 (TID 15). 4587 bytes result sent to driver
[2021-05-14 19:03:29,285] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 16) (028692ec38a6, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:29,287] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Running task 12.0 in stage 2.0 (TID 16)
[2021-05-14 19:03:29,287] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 15) in 1292 ms on 028692ec38a6 (executor driver) (9/200)
[2021-05-14 19:03:29,302] {docker.py:276} INFO - 21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Getting 3 (1043.0 B) non-empty blocks including 3 (1043.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:29,304] {docker.py:276} INFO - 21/05/14 22:03:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:29,304] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203219125720819931823316_0002_m_000012_16, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219125720819931823316_0002_m_000012_16}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203219125720819931823316_0002}; taskId=attempt_202105142203219125720819931823316_0002_m_000012_16, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a034b93}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:29,305] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203219125720819931823316_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219125720819931823316_0002_m_000012_16
[2021-05-14 19:03:29,309] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203219125720819931823316_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219125720819931823316_0002_m_000012_16 : duration 0:00.004s
[2021-05-14 19:03:29,414] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203217404890870137859757_0002_m_000010_14: needsTaskCommit() Task attempt_202105142203217404890870137859757_0002_m_000010_14
[2021-05-14 19:03:29,435] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203217404890870137859757_0002_m_000010_14: needsTaskCommit() Task attempt_202105142203217404890870137859757_0002_m_000010_14: duration 0:00.002s
[2021-05-14 19:03:29,436] {docker.py:276} INFO - 21/05/14 22:03:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217404890870137859757_0002_m_000010_14
[2021-05-14 19:03:29,436] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203218867954784012545601_0002_m_000009_13: needsTaskCommit() Task attempt_202105142203218867954784012545601_0002_m_000009_13
[2021-05-14 19:03:29,437] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203218867954784012545601_0002_m_000009_13: needsTaskCommit() Task attempt_202105142203218867954784012545601_0002_m_000009_13: duration 0:00.000s
[2021-05-14 19:03:29,437] {docker.py:276} INFO - 21/05/14 22:03:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218867954784012545601_0002_m_000009_13
[2021-05-14 19:03:29,438] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Finished task 10.0 in stage 2.0 (TID 14). 4587 bytes result sent to driver
[2021-05-14 19:03:29,438] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 17) (028692ec38a6, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:29,439] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Finished task 9.0 in stage 2.0 (TID 13). 4587 bytes result sent to driver
[2021-05-14 19:03:29,440] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 14) in 1763 ms on 028692ec38a6 (executor driver) (10/200)
21/05/14 22:03:29 INFO Executor: Running task 13.0 in stage 2.0 (TID 17)
[2021-05-14 19:03:29,440] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 13) in 1765 ms on 028692ec38a6 (executor driver) (11/200)
[2021-05-14 19:03:29,440] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 18) (028692ec38a6, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:29,441] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Running task 14.0 in stage 2.0 (TID 18)
[2021-05-14 19:03:29,442] {docker.py:276} INFO - 21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:29,442] {docker.py:276} INFO - 21/05/14 22:03:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:29,443] {docker.py:276} INFO - 21/05/14 22:03:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:29,443] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:29,443] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218329722847410371902_0002_m_000013_17, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218329722847410371902_0002_m_000013_17}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218329722847410371902_0002}; taskId=attempt_202105142203218329722847410371902_0002_m_000013_17, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d3ee2d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:29,444] {docker.py:276} INFO - 21/05/14 22:03:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:29,444] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203218329722847410371902_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218329722847410371902_0002_m_000013_17
[2021-05-14 19:03:29,445] {docker.py:276} INFO - 21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Getting 3 (1147.0 B) non-empty blocks including 3 (1147.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:29,445] {docker.py:276} INFO - 21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 19:03:29,447] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203218329722847410371902_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218329722847410371902_0002_m_000013_17 : duration 0:00.007s
[2021-05-14 19:03:29,451] {docker.py:276} INFO - 21/05/14 22:03:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:29,452] {docker.py:276} INFO - 21/05/14 22:03:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:29,453] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:29,453] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215224674020987000249_0002_m_000014_18, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215224674020987000249_0002_m_000014_18}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215224674020987000249_0002}; taskId=attempt_202105142203215224674020987000249_0002_m_000014_18, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c621364}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:29,453] {docker.py:276} INFO - 21/05/14 22:03:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:29,454] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203215224674020987000249_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215224674020987000249_0002_m_000014_18
[2021-05-14 19:03:29,456] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203211867433083893905519_0002_m_000008_12: needsTaskCommit() Task attempt_202105142203211867433083893905519_0002_m_000008_12
[2021-05-14 19:03:29,457] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203211867433083893905519_0002_m_000008_12: needsTaskCommit() Task attempt_202105142203211867433083893905519_0002_m_000008_12: duration 0:00.003s
[2021-05-14 19:03:29,457] {docker.py:276} INFO - 21/05/14 22:03:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211867433083893905519_0002_m_000008_12
[2021-05-14 19:03:29,464] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Finished task 8.0 in stage 2.0 (TID 12). 4630 bytes result sent to driver
[2021-05-14 19:03:29,465] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 19) (028692ec38a6, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:29,466] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203215224674020987000249_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215224674020987000249_0002_m_000014_18 : duration 0:00.012s
[2021-05-14 19:03:29,467] {docker.py:276} INFO - 21/05/14 22:03:29 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 12) in 1818 ms on 028692ec38a6 (executor driver) (12/200)
[2021-05-14 19:03:29,468] {docker.py:276} INFO - 21/05/14 22:03:29 INFO Executor: Running task 15.0 in stage 2.0 (TID 19)
[2021-05-14 19:03:29,486] {docker.py:276} INFO - 21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Getting 3 (838.0 B) non-empty blocks including 3 (838.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:29,489] {docker.py:276} INFO - 21/05/14 22:03:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:29,489] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:29,489] {docker.py:276} INFO - 21/05/14 22:03:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212024449542535238799_0002_m_000015_19, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212024449542535238799_0002_m_000015_19}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212024449542535238799_0002}; taskId=attempt_202105142203212024449542535238799_0002_m_000015_19, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d8163ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:29,490] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203212024449542535238799_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212024449542535238799_0002_m_000015_19
[2021-05-14 19:03:29,493] {docker.py:276} INFO - 21/05/14 22:03:29 INFO StagingCommitter: Task committer attempt_202105142203212024449542535238799_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212024449542535238799_0002_m_000015_19 : duration 0:00.004s
[2021-05-14 19:03:30,992] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203219125720819931823316_0002_m_000012_16: needsTaskCommit() Task attempt_202105142203219125720819931823316_0002_m_000012_16
[2021-05-14 19:03:30,993] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203219125720819931823316_0002_m_000012_16: needsTaskCommit() Task attempt_202105142203219125720819931823316_0002_m_000012_16: duration 0:00.001s
21/05/14 22:03:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203219125720819931823316_0002_m_000012_16
[2021-05-14 19:03:30,995] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Finished task 12.0 in stage 2.0 (TID 16). 4544 bytes result sent to driver
[2021-05-14 19:03:30,997] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 20) (028692ec38a6, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:30,998] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 16) in 1716 ms on 028692ec38a6 (executor driver) (13/200)
[2021-05-14 19:03:30,999] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Running task 16.0 in stage 2.0 (TID 20)
[2021-05-14 19:03:31,010] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:31,013] {docker.py:276} INFO - 21/05/14 22:03:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,013] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212693136354728264737_0002_m_000016_20, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212693136354728264737_0002_m_000016_20}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212693136354728264737_0002}; taskId=attempt_202105142203212693136354728264737_0002_m_000016_20, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7832c7b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,014] {docker.py:276} INFO - 21/05/14 22:03:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:31,014] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203212693136354728264737_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212693136354728264737_0002_m_000016_20
[2021-05-14 19:03:31,019] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203212693136354728264737_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212693136354728264737_0002_m_000016_20 : duration 0:00.005s
[2021-05-14 19:03:31,144] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203218329722847410371902_0002_m_000013_17: needsTaskCommit() Task attempt_202105142203218329722847410371902_0002_m_000013_17
[2021-05-14 19:03:31,145] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203218329722847410371902_0002_m_000013_17: needsTaskCommit() Task attempt_202105142203218329722847410371902_0002_m_000013_17: duration 0:00.002s
21/05/14 22:03:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218329722847410371902_0002_m_000013_17
[2021-05-14 19:03:31,148] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Finished task 13.0 in stage 2.0 (TID 17). 4544 bytes result sent to driver
[2021-05-14 19:03:31,150] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 21) (028692ec38a6, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:31,151] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Running task 17.0 in stage 2.0 (TID 21)
[2021-05-14 19:03:31,152] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 17) in 1734 ms on 028692ec38a6 (executor driver) (14/200)
[2021-05-14 19:03:31,159] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203212024449542535238799_0002_m_000015_19: needsTaskCommit() Task attempt_202105142203212024449542535238799_0002_m_000015_19
[2021-05-14 19:03:31,160] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203212024449542535238799_0002_m_000015_19: needsTaskCommit() Task attempt_202105142203212024449542535238799_0002_m_000015_19: duration 0:00.000s
[2021-05-14 19:03:31,160] {docker.py:276} INFO - 21/05/14 22:03:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212024449542535238799_0002_m_000015_19
[2021-05-14 19:03:31,162] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Finished task 15.0 in stage 2.0 (TID 19). 4544 bytes result sent to driver
[2021-05-14 19:03:31,163] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 22) (028692ec38a6, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:31,165] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 19) in 1702 ms on 028692ec38a6 (executor driver) (15/200)
[2021-05-14 19:03:31,166] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Running task 18.0 in stage 2.0 (TID 22)
[2021-05-14 19:03:31,173] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Getting 3 (1013.0 B) non-empty blocks including 3 (1013.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:31,174] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:31,175] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:31,175] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:31,176] {docker.py:276} INFO - 21/05/14 22:03:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:31,177] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,179] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217442415631111348174_0002_m_000017_21, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217442415631111348174_0002_m_000017_21}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217442415631111348174_0002}; taskId=attempt_202105142203217442415631111348174_0002_m_000017_21, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1671efc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:31,179] {docker.py:276} INFO - 21/05/14 22:03:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:31,181] {docker.py:276} INFO - 21/05/14 22:03:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:31,183] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,183] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215976814970466444593_0002_m_000018_22, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215976814970466444593_0002_m_000018_22}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215976814970466444593_0002}; taskId=attempt_202105142203215976814970466444593_0002_m_000018_22, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17ff69b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,184] {docker.py:276} INFO - 21/05/14 22:03:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:31,186] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203217442415631111348174_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217442415631111348174_0002_m_000017_21 
21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203215976814970466444593_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215976814970466444593_0002_m_000018_22
[2021-05-14 19:03:31,195] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203217442415631111348174_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217442415631111348174_0002_m_000017_21 : duration 0:00.013s
[2021-05-14 19:03:31,198] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203215976814970466444593_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215976814970466444593_0002_m_000018_22 : duration 0:00.014s
[2021-05-14 19:03:31,202] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203215224674020987000249_0002_m_000014_18: needsTaskCommit() Task attempt_202105142203215224674020987000249_0002_m_000014_18
[2021-05-14 19:03:31,202] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203215224674020987000249_0002_m_000014_18: needsTaskCommit() Task attempt_202105142203215224674020987000249_0002_m_000014_18: duration 0:00.000s
[2021-05-14 19:03:31,203] {docker.py:276} INFO - 21/05/14 22:03:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215224674020987000249_0002_m_000014_18
[2021-05-14 19:03:31,204] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Finished task 14.0 in stage 2.0 (TID 18). 4544 bytes result sent to driver
[2021-05-14 19:03:31,207] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 23) (028692ec38a6, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:31,208] {docker.py:276} INFO - 21/05/14 22:03:31 INFO Executor: Running task 19.0 in stage 2.0 (TID 23)
[2021-05-14 19:03:31,209] {docker.py:276} INFO - 21/05/14 22:03:31 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 18) in 1786 ms on 028692ec38a6 (executor driver) (16/200)
[2021-05-14 19:03:31,228] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:31,229] {docker.py:276} INFO - 21/05/14 22:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:31,231] {docker.py:276} INFO - 21/05/14 22:03:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:31,232] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,232] {docker.py:276} INFO - 21/05/14 22:03:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214552016357834913820_0002_m_000019_23, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214552016357834913820_0002_m_000019_23}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214552016357834913820_0002}; taskId=attempt_202105142203214552016357834913820_0002_m_000019_23, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@636d0fe8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:31,232] {docker.py:276} INFO - 21/05/14 22:03:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:31,233] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Starting: Task committer attempt_202105142203214552016357834913820_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214552016357834913820_0002_m_000019_23
[2021-05-14 19:03:31,236] {docker.py:276} INFO - 21/05/14 22:03:31 INFO StagingCommitter: Task committer attempt_202105142203214552016357834913820_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214552016357834913820_0002_m_000019_23 : duration 0:00.005s
[2021-05-14 19:03:32,751] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203212693136354728264737_0002_m_000016_20: needsTaskCommit() Task attempt_202105142203212693136354728264737_0002_m_000016_20
21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203212693136354728264737_0002_m_000016_20: needsTaskCommit() Task attempt_202105142203212693136354728264737_0002_m_000016_20: duration 0:00.003s
[2021-05-14 19:03:32,752] {docker.py:276} INFO - 21/05/14 22:03:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212693136354728264737_0002_m_000016_20
[2021-05-14 19:03:32,754] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Finished task 16.0 in stage 2.0 (TID 20). 4587 bytes result sent to driver
[2021-05-14 19:03:32,756] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 24) (028692ec38a6, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:32,757] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Running task 20.0 in stage 2.0 (TID 24)
[2021-05-14 19:03:32,758] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 20) in 1762 ms on 028692ec38a6 (executor driver) (17/200)
[2021-05-14 19:03:32,768] {docker.py:276} INFO - 21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Getting 3 (1070.0 B) non-empty blocks including 3 (1070.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:32,771] {docker.py:276} INFO - 21/05/14 22:03:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:32,771] {docker.py:276} INFO - 21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:32,772] {docker.py:276} INFO - 21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321129526773954197946_0002_m_000020_24, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321129526773954197946_0002_m_000020_24}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321129526773954197946_0002}; taskId=attempt_20210514220321129526773954197946_0002_m_000020_24, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fab9e35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:32,772] {docker.py:276} INFO - 21/05/14 22:03:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:32,772] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_20210514220321129526773954197946_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321129526773954197946_0002_m_000020_24
[2021-05-14 19:03:32,776] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_20210514220321129526773954197946_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321129526773954197946_0002_m_000020_24 : duration 0:00.004s
[2021-05-14 19:03:32,892] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203217442415631111348174_0002_m_000017_21: needsTaskCommit() Task attempt_202105142203217442415631111348174_0002_m_000017_21
[2021-05-14 19:03:32,905] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203217442415631111348174_0002_m_000017_21: needsTaskCommit() Task attempt_202105142203217442415631111348174_0002_m_000017_21: duration 0:00.002s
21/05/14 22:03:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217442415631111348174_0002_m_000017_21
[2021-05-14 19:03:32,906] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Finished task 17.0 in stage 2.0 (TID 21). 4587 bytes result sent to driver
[2021-05-14 19:03:32,906] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 25) (028692ec38a6, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:32,907] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Running task 21.0 in stage 2.0 (TID 25)
[2021-05-14 19:03:32,907] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 21) in 1750 ms on 028692ec38a6 (executor driver) (18/200)
[2021-05-14 19:03:32,910] {docker.py:276} INFO - 21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Getting 3 (808.0 B) non-empty blocks including 3 (808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:32,913] {docker.py:276} INFO - 21/05/14 22:03:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218357232096017588739_0002_m_000021_25, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218357232096017588739_0002_m_000021_25}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218357232096017588739_0002}; taskId=attempt_202105142203218357232096017588739_0002_m_000021_25, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e26b978}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:32,913] {docker.py:276} INFO - 21/05/14 22:03:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203218357232096017588739_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218357232096017588739_0002_m_000021_25
[2021-05-14 19:03:32,918] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203214552016357834913820_0002_m_000019_23: needsTaskCommit() Task attempt_202105142203214552016357834913820_0002_m_000019_23
[2021-05-14 19:03:32,920] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203214552016357834913820_0002_m_000019_23: needsTaskCommit() Task attempt_202105142203214552016357834913820_0002_m_000019_23: duration 0:00.001s
[2021-05-14 19:03:32,921] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203218357232096017588739_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218357232096017588739_0002_m_000021_25 : duration 0:00.007s
21/05/14 22:03:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214552016357834913820_0002_m_000019_23
[2021-05-14 19:03:32,923] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Finished task 19.0 in stage 2.0 (TID 23). 4587 bytes result sent to driver
[2021-05-14 19:03:32,925] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 23) in 1719 ms on 028692ec38a6 (executor driver) (19/200)
[2021-05-14 19:03:32,926] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 26) (028692ec38a6, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:32,928] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Running task 22.0 in stage 2.0 (TID 26)
[2021-05-14 19:03:32,944] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203215976814970466444593_0002_m_000018_22: needsTaskCommit() Task attempt_202105142203215976814970466444593_0002_m_000018_22
21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:32,945] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203215976814970466444593_0002_m_000018_22: needsTaskCommit() Task attempt_202105142203215976814970466444593_0002_m_000018_22: duration 0:00.001s
[2021-05-14 19:03:32,946] {docker.py:276} INFO - 21/05/14 22:03:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215976814970466444593_0002_m_000018_22
[2021-05-14 19:03:32,946] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Finished task 18.0 in stage 2.0 (TID 22). 4587 bytes result sent to driver
[2021-05-14 19:03:32,948] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 27) (028692ec38a6, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:32,949] {docker.py:276} INFO - 21/05/14 22:03:32 INFO Executor: Running task 23.0 in stage 2.0 (TID 27)
[2021-05-14 19:03:32,949] {docker.py:276} INFO - 21/05/14 22:03:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:32,950] {docker.py:276} INFO - 21/05/14 22:03:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:32,951] {docker.py:276} INFO - 21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:32,951] {docker.py:276} INFO - 21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211417658543485492386_0002_m_000022_26, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211417658543485492386_0002_m_000022_26}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211417658543485492386_0002}; taskId=attempt_202105142203211417658543485492386_0002_m_000022_26, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fc92766}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:32,952] {docker.py:276} INFO - 21/05/14 22:03:32 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 22) in 1787 ms on 028692ec38a6 (executor driver) (20/200)
[2021-05-14 19:03:32,953] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203211417658543485492386_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211417658543485492386_0002_m_000022_26
[2021-05-14 19:03:32,959] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203211417658543485492386_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211417658543485492386_0002_m_000022_26 : duration 0:00.010s
[2021-05-14 19:03:32,964] {docker.py:276} INFO - 21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Getting 3 (808.0 B) non-empty blocks including 3 (808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:32,970] {docker.py:276} INFO - 21/05/14 22:03:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:32,970] {docker.py:276} INFO - 21/05/14 22:03:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214457560191582540689_0002_m_000023_27, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214457560191582540689_0002_m_000023_27}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214457560191582540689_0002}; taskId=attempt_202105142203214457560191582540689_0002_m_000023_27, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30bf4539}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:32,971] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Starting: Task committer attempt_202105142203214457560191582540689_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214457560191582540689_0002_m_000023_27
[2021-05-14 19:03:32,976] {docker.py:276} INFO - 21/05/14 22:03:32 INFO StagingCommitter: Task committer attempt_202105142203214457560191582540689_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214457560191582540689_0002_m_000023_27 : duration 0:00.006s
[2021-05-14 19:03:34,233] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203211417658543485492386_0002_m_000022_26: needsTaskCommit() Task attempt_202105142203211417658543485492386_0002_m_000022_26
[2021-05-14 19:03:34,235] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203211417658543485492386_0002_m_000022_26: needsTaskCommit() Task attempt_202105142203211417658543485492386_0002_m_000022_26: duration 0:00.002s
21/05/14 22:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211417658543485492386_0002_m_000022_26
[2021-05-14 19:03:34,238] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Finished task 22.0 in stage 2.0 (TID 26). 4544 bytes result sent to driver
[2021-05-14 19:03:34,241] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 28) (028692ec38a6, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:34,241] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 26) in 1317 ms on 028692ec38a6 (executor driver) (21/200)
[2021-05-14 19:03:34,242] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Running task 24.0 in stage 2.0 (TID 28)
[2021-05-14 19:03:34,252] {docker.py:276} INFO - 21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Getting 3 (1006.0 B) non-empty blocks including 3 (1006.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:34,252] {docker.py:276} INFO - 21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:34,254] {docker.py:276} INFO - 21/05/14 22:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:34,255] {docker.py:276} INFO - 21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:34,255] {docker.py:276} INFO - 21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203219142462510804362137_0002_m_000024_28, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219142462510804362137_0002_m_000024_28}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203219142462510804362137_0002}; taskId=attempt_202105142203219142462510804362137_0002_m_000024_28, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40549438}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:34,255] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203219142462510804362137_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219142462510804362137_0002_m_000024_28
[2021-05-14 19:03:34,259] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203219142462510804362137_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219142462510804362137_0002_m_000024_28 : duration 0:00.004s
[2021-05-14 19:03:34,462] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_20210514220321129526773954197946_0002_m_000020_24: needsTaskCommit() Task attempt_20210514220321129526773954197946_0002_m_000020_24
[2021-05-14 19:03:34,463] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_20210514220321129526773954197946_0002_m_000020_24: needsTaskCommit() Task attempt_20210514220321129526773954197946_0002_m_000020_24: duration 0:00.003s
[2021-05-14 19:03:34,464] {docker.py:276} INFO - 21/05/14 22:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321129526773954197946_0002_m_000020_24
[2021-05-14 19:03:34,467] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Finished task 20.0 in stage 2.0 (TID 24). 4544 bytes result sent to driver
[2021-05-14 19:03:34,469] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 24) in 1715 ms on 028692ec38a6 (executor driver) (22/200)
[2021-05-14 19:03:34,471] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 29) (028692ec38a6, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:34,472] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Running task 25.0 in stage 2.0 (TID 29)
[2021-05-14 19:03:34,484] {docker.py:276} INFO - 21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Getting 3 (803.0 B) non-empty blocks including 3 (803.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:34,485] {docker.py:276} INFO - 21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:34,487] {docker.py:276} INFO - 21/05/14 22:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212478394053845848648_0002_m_000025_29, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212478394053845848648_0002_m_000025_29}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212478394053845848648_0002}; taskId=attempt_202105142203212478394053845848648_0002_m_000025_29, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@229a59b1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:34,487] {docker.py:276} INFO - 21/05/14 22:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:34,488] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203212478394053845848648_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212478394053845848648_0002_m_000025_29
[2021-05-14 19:03:34,492] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203212478394053845848648_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212478394053845848648_0002_m_000025_29 : duration 0:00.004s
[2021-05-14 19:03:34,677] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203214457560191582540689_0002_m_000023_27: needsTaskCommit() Task attempt_202105142203214457560191582540689_0002_m_000023_27
[2021-05-14 19:03:34,678] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203214457560191582540689_0002_m_000023_27: needsTaskCommit() Task attempt_202105142203214457560191582540689_0002_m_000023_27: duration 0:00.002s
21/05/14 22:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214457560191582540689_0002_m_000023_27
21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203218357232096017588739_0002_m_000021_25: needsTaskCommit() Task attempt_202105142203218357232096017588739_0002_m_000021_25
[2021-05-14 19:03:34,679] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203218357232096017588739_0002_m_000021_25: needsTaskCommit() Task attempt_202105142203218357232096017588739_0002_m_000021_25: duration 0:00.001s
21/05/14 22:03:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218357232096017588739_0002_m_000021_25
[2021-05-14 19:03:34,681] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Finished task 21.0 in stage 2.0 (TID 25). 4544 bytes result sent to driver
[2021-05-14 19:03:34,682] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Finished task 23.0 in stage 2.0 (TID 27). 4544 bytes result sent to driver
[2021-05-14 19:03:34,683] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 30) (028692ec38a6, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:34,685] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Running task 26.0 in stage 2.0 (TID 30)
[2021-05-14 19:03:34,686] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 31) (028692ec38a6, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:34,687] {docker.py:276} INFO - 21/05/14 22:03:34 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 27) in 1741 ms on 028692ec38a6 (executor driver) (23/200)
21/05/14 22:03:34 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 25) in 1792 ms on 028692ec38a6 (executor driver) (24/200)
[2021-05-14 19:03:34,688] {docker.py:276} INFO - 21/05/14 22:03:34 INFO Executor: Running task 27.0 in stage 2.0 (TID 31)
[2021-05-14 19:03:34,697] {docker.py:276} INFO - 21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Getting 3 (861.0 B) non-empty blocks including 3 (861.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:34,698] {docker.py:276} INFO - 21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:34,700] {docker.py:276} INFO - 21/05/14 22:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:34,700] {docker.py:276} INFO - 21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:34,701] {docker.py:276} INFO - 21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211890103549172635645_0002_m_000026_30, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211890103549172635645_0002_m_000026_30}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211890103549172635645_0002}; taskId=attempt_202105142203211890103549172635645_0002_m_000026_30, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62e97530}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:34,701] {docker.py:276} INFO - 21/05/14 22:03:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:34,702] {docker.py:276} INFO - 21/05/14 22:03:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:34,702] {docker.py:276} INFO - 21/05/14 22:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203211890103549172635645_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211890103549172635645_0002_m_000026_30
[2021-05-14 19:03:34,702] {docker.py:276} INFO - 21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:34,703] {docker.py:276} INFO - 21/05/14 22:03:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211108107385613672508_0002_m_000027_31, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211108107385613672508_0002_m_000027_31}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211108107385613672508_0002}; taskId=attempt_202105142203211108107385613672508_0002_m_000027_31, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c51fe8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:34,703] {docker.py:276} INFO - 21/05/14 22:03:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:34,704] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Starting: Task committer attempt_202105142203211108107385613672508_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211108107385613672508_0002_m_000027_31
[2021-05-14 19:03:34,706] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203211890103549172635645_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211890103549172635645_0002_m_000026_30 : duration 0:00.004s
[2021-05-14 19:03:34,707] {docker.py:276} INFO - 21/05/14 22:03:34 INFO StagingCommitter: Task committer attempt_202105142203211108107385613672508_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211108107385613672508_0002_m_000027_31 : duration 0:00.003s
[2021-05-14 19:03:35,939] {docker.py:276} INFO - 21/05/14 22:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105142203219142462510804362137_0002_m_000024_28: needsTaskCommit() Task attempt_202105142203219142462510804362137_0002_m_000024_28
[2021-05-14 19:03:35,939] {docker.py:276} INFO - 21/05/14 22:03:35 INFO StagingCommitter: Task committer attempt_202105142203219142462510804362137_0002_m_000024_28: needsTaskCommit() Task attempt_202105142203219142462510804362137_0002_m_000024_28: duration 0:00.001s
21/05/14 22:03:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203219142462510804362137_0002_m_000024_28
[2021-05-14 19:03:35,942] {docker.py:276} INFO - 21/05/14 22:03:35 INFO Executor: Finished task 24.0 in stage 2.0 (TID 28). 4544 bytes result sent to driver
[2021-05-14 19:03:35,943] {docker.py:276} INFO - 21/05/14 22:03:35 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 32) (028692ec38a6, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:35,945] {docker.py:276} INFO - 21/05/14 22:03:35 INFO Executor: Running task 28.0 in stage 2.0 (TID 32)
[2021-05-14 19:03:35,945] {docker.py:276} INFO - 21/05/14 22:03:35 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 28) in 1708 ms on 028692ec38a6 (executor driver) (25/200)
[2021-05-14 19:03:35,957] {docker.py:276} INFO - 21/05/14 22:03:35 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:35,958] {docker.py:276} INFO - 21/05/14 22:03:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:35,960] {docker.py:276} INFO - 21/05/14 22:03:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:35,962] {docker.py:276} INFO - 21/05/14 22:03:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:35,963] {docker.py:276} INFO - 21/05/14 22:03:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212472253730421352546_0002_m_000028_32, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212472253730421352546_0002_m_000028_32}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212472253730421352546_0002}; taskId=attempt_202105142203212472253730421352546_0002_m_000028_32, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48c7d5d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:35,963] {docker.py:276} INFO - 21/05/14 22:03:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:35,964] {docker.py:276} INFO - 21/05/14 22:03:35 INFO StagingCommitter: Starting: Task committer attempt_202105142203212472253730421352546_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212472253730421352546_0002_m_000028_32
[2021-05-14 19:03:35,967] {docker.py:276} INFO - 21/05/14 22:03:35 INFO StagingCommitter: Task committer attempt_202105142203212472253730421352546_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212472253730421352546_0002_m_000028_32 : duration 0:00.004s
[2021-05-14 19:03:36,156] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105142203212478394053845848648_0002_m_000025_29: needsTaskCommit() Task attempt_202105142203212478394053845848648_0002_m_000025_29
[2021-05-14 19:03:36,158] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Task committer attempt_202105142203212478394053845848648_0002_m_000025_29: needsTaskCommit() Task attempt_202105142203212478394053845848648_0002_m_000025_29: duration 0:00.003s
21/05/14 22:03:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212478394053845848648_0002_m_000025_29
[2021-05-14 19:03:36,160] {docker.py:276} INFO - 21/05/14 22:03:36 INFO Executor: Finished task 25.0 in stage 2.0 (TID 29). 4587 bytes result sent to driver
[2021-05-14 19:03:36,163] {docker.py:276} INFO - 21/05/14 22:03:36 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 33) (028692ec38a6, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:36,164] {docker.py:276} INFO - 21/05/14 22:03:36 INFO Executor: Running task 29.0 in stage 2.0 (TID 33)
[2021-05-14 19:03:36,165] {docker.py:276} INFO - 21/05/14 22:03:36 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 29) in 1697 ms on 028692ec38a6 (executor driver) (26/200)
[2021-05-14 19:03:36,174] {docker.py:276} INFO - 21/05/14 22:03:36 INFO ShuffleBlockFetcherIterator: Getting 3 (972.0 B) non-empty blocks including 3 (972.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:36,176] {docker.py:276} INFO - 21/05/14 22:03:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:36,176] {docker.py:276} INFO - 21/05/14 22:03:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217630871375665508377_0002_m_000029_33, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217630871375665508377_0002_m_000029_33}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217630871375665508377_0002}; taskId=attempt_202105142203217630871375665508377_0002_m_000029_33, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b49d388}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:36,176] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105142203217630871375665508377_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217630871375665508377_0002_m_000029_33
[2021-05-14 19:03:36,179] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Task committer attempt_202105142203217630871375665508377_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217630871375665508377_0002_m_000029_33 : duration 0:00.003s
[2021-05-14 19:03:36,390] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105142203211890103549172635645_0002_m_000026_30: needsTaskCommit() Task attempt_202105142203211890103549172635645_0002_m_000026_30
[2021-05-14 19:03:36,391] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Task committer attempt_202105142203211890103549172635645_0002_m_000026_30: needsTaskCommit() Task attempt_202105142203211890103549172635645_0002_m_000026_30: duration 0:00.002s
21/05/14 22:03:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211890103549172635645_0002_m_000026_30
[2021-05-14 19:03:36,393] {docker.py:276} INFO - 21/05/14 22:03:36 INFO Executor: Finished task 26.0 in stage 2.0 (TID 30). 4587 bytes result sent to driver
[2021-05-14 19:03:36,395] {docker.py:276} INFO - 21/05/14 22:03:36 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 34) (028692ec38a6, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:36,397] {docker.py:276} INFO - 21/05/14 22:03:36 INFO Executor: Running task 30.0 in stage 2.0 (TID 34)
21/05/14 22:03:36 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 30) in 1716 ms on 028692ec38a6 (executor driver) (27/200)
[2021-05-14 19:03:36,408] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105142203211108107385613672508_0002_m_000027_31: needsTaskCommit() Task attempt_202105142203211108107385613672508_0002_m_000027_31
[2021-05-14 19:03:36,409] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Task committer attempt_202105142203211108107385613672508_0002_m_000027_31: needsTaskCommit() Task attempt_202105142203211108107385613672508_0002_m_000027_31: duration 0:00.000s
[2021-05-14 19:03:36,409] {docker.py:276} INFO - 21/05/14 22:03:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211108107385613672508_0002_m_000027_31
[2021-05-14 19:03:36,410] {docker.py:276} INFO - 21/05/14 22:03:36 INFO ShuffleBlockFetcherIterator: Getting 3 (944.0 B) non-empty blocks including 3 (944.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:36,410] {docker.py:276} INFO - 21/05/14 22:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 19:03:36,411] {docker.py:276} INFO - 21/05/14 22:03:36 INFO Executor: Finished task 27.0 in stage 2.0 (TID 31). 4587 bytes result sent to driver
[2021-05-14 19:03:36,412] {docker.py:276} INFO - 21/05/14 22:03:36 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 35) (028692ec38a6, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:36,413] {docker.py:276} INFO - 21/05/14 22:03:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:36,414] {docker.py:276} INFO - 21/05/14 22:03:36 INFO Executor: Running task 31.0 in stage 2.0 (TID 35)
[2021-05-14 19:03:36,414] {docker.py:276} INFO - 21/05/14 22:03:36 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 31) in 1731 ms on 028692ec38a6 (executor driver) (28/200)
21/05/14 22:03:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:36,415] {docker.py:276} INFO - 21/05/14 22:03:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213232637285682742258_0002_m_000030_34, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213232637285682742258_0002_m_000030_34}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213232637285682742258_0002}; taskId=attempt_202105142203213232637285682742258_0002_m_000030_34, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f7bf5b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:36,415] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105142203213232637285682742258_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213232637285682742258_0002_m_000030_34
[2021-05-14 19:03:36,420] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Task committer attempt_202105142203213232637285682742258_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213232637285682742258_0002_m_000030_34 : duration 0:00.005s
[2021-05-14 19:03:36,425] {docker.py:276} INFO - 21/05/14 22:03:36 INFO ShuffleBlockFetcherIterator: Getting 3 (866.0 B) non-empty blocks including 3 (866.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:03:36,430] {docker.py:276} INFO - 21/05/14 22:03:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:36,430] {docker.py:276} INFO - 21/05/14 22:03:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:36,431] {docker.py:276} INFO - 21/05/14 22:03:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214828915156103256403_0002_m_000031_35, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214828915156103256403_0002_m_000031_35}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214828915156103256403_0002}; taskId=attempt_202105142203214828915156103256403_0002_m_000031_35, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19d560e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:36,431] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Starting: Task committer attempt_202105142203214828915156103256403_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214828915156103256403_0002_m_000031_35
[2021-05-14 19:03:36,435] {docker.py:276} INFO - 21/05/14 22:03:36 INFO StagingCommitter: Task committer attempt_202105142203214828915156103256403_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214828915156103256403_0002_m_000031_35 : duration 0:00.004s
[2021-05-14 19:03:37,694] {docker.py:276} INFO - 21/05/14 22:03:37 INFO StagingCommitter: Starting: Task committer attempt_202105142203212472253730421352546_0002_m_000028_32: needsTaskCommit() Task attempt_202105142203212472253730421352546_0002_m_000028_32
[2021-05-14 19:03:37,695] {docker.py:276} INFO - 21/05/14 22:03:37 INFO StagingCommitter: Task committer attempt_202105142203212472253730421352546_0002_m_000028_32: needsTaskCommit() Task attempt_202105142203212472253730421352546_0002_m_000028_32: duration 0:00.001s
21/05/14 22:03:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212472253730421352546_0002_m_000028_32
[2021-05-14 19:03:37,696] {docker.py:276} INFO - 21/05/14 22:03:37 INFO Executor: Finished task 28.0 in stage 2.0 (TID 32). 4587 bytes result sent to driver
[2021-05-14 19:03:37,698] {docker.py:276} INFO - 21/05/14 22:03:37 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 36) (028692ec38a6, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:37,699] {docker.py:276} INFO - 21/05/14 22:03:37 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 32) in 1758 ms on 028692ec38a6 (executor driver) (29/200)
[2021-05-14 19:03:37,700] {docker.py:276} INFO - 21/05/14 22:03:37 INFO Executor: Running task 32.0 in stage 2.0 (TID 36)
[2021-05-14 19:03:37,709] {docker.py:276} INFO - 21/05/14 22:03:37 INFO ShuffleBlockFetcherIterator: Getting 3 (919.0 B) non-empty blocks including 3 (919.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:37,711] {docker.py:276} INFO - 21/05/14 22:03:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:37,711] {docker.py:276} INFO - 21/05/14 22:03:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:37,712] {docker.py:276} INFO - 21/05/14 22:03:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:37,712] {docker.py:276} INFO - 21/05/14 22:03:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218931048043362939456_0002_m_000032_36, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218931048043362939456_0002_m_000032_36}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218931048043362939456_0002}; taskId=attempt_202105142203218931048043362939456_0002_m_000032_36, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53cea393}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:37,712] {docker.py:276} INFO - 21/05/14 22:03:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:37,713] {docker.py:276} INFO - 21/05/14 22:03:37 INFO StagingCommitter: Starting: Task committer attempt_202105142203218931048043362939456_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218931048043362939456_0002_m_000032_36
[2021-05-14 19:03:37,715] {docker.py:276} INFO - 21/05/14 22:03:37 INFO StagingCommitter: Task committer attempt_202105142203218931048043362939456_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218931048043362939456_0002_m_000032_36 : duration 0:00.003s
[2021-05-14 19:03:37,986] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105142203217630871375665508377_0002_m_000029_33: needsTaskCommit() Task attempt_202105142203217630871375665508377_0002_m_000029_33
[2021-05-14 19:03:37,987] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Task committer attempt_202105142203217630871375665508377_0002_m_000029_33: needsTaskCommit() Task attempt_202105142203217630871375665508377_0002_m_000029_33: duration 0:00.004s
21/05/14 22:03:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217630871375665508377_0002_m_000029_33
[2021-05-14 19:03:37,989] {docker.py:276} INFO - 21/05/14 22:03:38 INFO Executor: Finished task 29.0 in stage 2.0 (TID 33). 4544 bytes result sent to driver
[2021-05-14 19:03:37,991] {docker.py:276} INFO - 21/05/14 22:03:38 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 37) (028692ec38a6, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:37,993] {docker.py:276} INFO - 21/05/14 22:03:38 INFO Executor: Running task 33.0 in stage 2.0 (TID 37)
[2021-05-14 19:03:37,993] {docker.py:276} INFO - 21/05/14 22:03:38 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 33) in 1834 ms on 028692ec38a6 (executor driver) (30/200)
[2021-05-14 19:03:38,005] {docker.py:276} INFO - 21/05/14 22:03:38 INFO ShuffleBlockFetcherIterator: Getting 3 (942.0 B) non-empty blocks including 3 (942.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:38,007] {docker.py:276} INFO - 21/05/14 22:03:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:38,008] {docker.py:276} INFO - 21/05/14 22:03:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321517724725445464402_0002_m_000033_37, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321517724725445464402_0002_m_000033_37}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321517724725445464402_0002}; taskId=attempt_20210514220321517724725445464402_0002_m_000033_37, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e0facb2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:38 INFO StagingCommitter: Starting: Task committer attempt_20210514220321517724725445464402_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321517724725445464402_0002_m_000033_37
[2021-05-14 19:03:38,010] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Task committer attempt_20210514220321517724725445464402_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321517724725445464402_0002_m_000033_37 : duration 0:00.003s
[2021-05-14 19:03:38,120] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105142203214828915156103256403_0002_m_000031_35: needsTaskCommit() Task attempt_202105142203214828915156103256403_0002_m_000031_35
[2021-05-14 19:03:38,121] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105142203213232637285682742258_0002_m_000030_34: needsTaskCommit() Task attempt_202105142203213232637285682742258_0002_m_000030_34
[2021-05-14 19:03:38,123] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Task committer attempt_202105142203214828915156103256403_0002_m_000031_35: needsTaskCommit() Task attempt_202105142203214828915156103256403_0002_m_000031_35: duration 0:00.002s
[2021-05-14 19:03:38,124] {docker.py:276} INFO - 21/05/14 22:03:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214828915156103256403_0002_m_000031_35
[2021-05-14 19:03:38,124] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Task committer attempt_202105142203213232637285682742258_0002_m_000030_34: needsTaskCommit() Task attempt_202105142203213232637285682742258_0002_m_000030_34: duration 0:00.002s
[2021-05-14 19:03:38,125] {docker.py:276} INFO - 21/05/14 22:03:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213232637285682742258_0002_m_000030_34
[2021-05-14 19:03:38,125] {docker.py:276} INFO - 21/05/14 22:03:38 INFO Executor: Finished task 31.0 in stage 2.0 (TID 35). 4544 bytes result sent to driver
[2021-05-14 19:03:38,126] {docker.py:276} INFO - 21/05/14 22:03:38 INFO Executor: Finished task 30.0 in stage 2.0 (TID 34). 4544 bytes result sent to driver
21/05/14 22:03:38 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 38) (028692ec38a6, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:38,128] {docker.py:276} INFO - 21/05/14 22:03:38 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 39) (028692ec38a6, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:38,129] {docker.py:276} INFO - 21/05/14 22:03:38 INFO Executor: Running task 34.0 in stage 2.0 (TID 38)
[2021-05-14 19:03:38,130] {docker.py:276} INFO - 21/05/14 22:03:38 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 35) in 1719 ms on 028692ec38a6 (executor driver) (31/200)
[2021-05-14 19:03:38,131] {docker.py:276} INFO - 21/05/14 22:03:38 INFO Executor: Running task 35.0 in stage 2.0 (TID 39)
[2021-05-14 19:03:38,131] {docker.py:276} INFO - 21/05/14 22:03:38 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 34) in 1737 ms on 028692ec38a6 (executor driver) (32/200)
[2021-05-14 19:03:38,141] {docker.py:276} INFO - 21/05/14 22:03:38 INFO ShuffleBlockFetcherIterator: Getting 3 (856.0 B) non-empty blocks including 3 (856.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 22:03:38 INFO ShuffleBlockFetcherIterator: Getting 3 (1006.0 B) non-empty blocks including 3 (1006.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:38,144] {docker.py:276} INFO - 21/05/14 22:03:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:38,144] {docker.py:276} INFO - 21/05/14 22:03:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:38,145] {docker.py:276} INFO - 21/05/14 22:03:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:38,145] {docker.py:276} INFO - 21/05/14 22:03:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:38,145] {docker.py:276} INFO - 21/05/14 22:03:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211947584253085122279_0002_m_000034_38, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211947584253085122279_0002_m_000034_38}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211947584253085122279_0002}; taskId=attempt_202105142203211947584253085122279_0002_m_000034_38, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@602926af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217252462073354963393_0002_m_000035_39, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217252462073354963393_0002_m_000035_39}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217252462073354963393_0002}; taskId=attempt_202105142203217252462073354963393_0002_m_000035_39, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8a28910}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:38,145] {docker.py:276} INFO - 21/05/14 22:03:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:38,146] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105142203211947584253085122279_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211947584253085122279_0002_m_000034_38
[2021-05-14 19:03:38,146] {docker.py:276} INFO - 21/05/14 22:03:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:38 INFO StagingCommitter: Starting: Task committer attempt_202105142203217252462073354963393_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217252462073354963393_0002_m_000035_39
[2021-05-14 19:03:38,150] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Task committer attempt_202105142203217252462073354963393_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217252462073354963393_0002_m_000035_39 : duration 0:00.003s
[2021-05-14 19:03:38,152] {docker.py:276} INFO - 21/05/14 22:03:38 INFO StagingCommitter: Task committer attempt_202105142203211947584253085122279_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211947584253085122279_0002_m_000034_38 : duration 0:00.006s
[2021-05-14 19:03:39,186] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Starting: Task committer attempt_20210514220321517724725445464402_0002_m_000033_37: needsTaskCommit() Task attempt_20210514220321517724725445464402_0002_m_000033_37
21/05/14 22:03:39 INFO StagingCommitter: Task committer attempt_20210514220321517724725445464402_0002_m_000033_37: needsTaskCommit() Task attempt_20210514220321517724725445464402_0002_m_000033_37: duration 0:00.001s
21/05/14 22:03:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321517724725445464402_0002_m_000033_37
21/05/14 22:03:39 INFO Executor: Finished task 33.0 in stage 2.0 (TID 37). 4544 bytes result sent to driver
[2021-05-14 19:03:39,187] {docker.py:276} INFO - 21/05/14 22:03:39 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 40) (028692ec38a6, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:39,188] {docker.py:276} INFO - 21/05/14 22:03:39 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 37) in 1198 ms on 028692ec38a6 (executor driver) (33/200)
[2021-05-14 19:03:39,189] {docker.py:276} INFO - 21/05/14 22:03:39 INFO Executor: Running task 36.0 in stage 2.0 (TID 40)
[2021-05-14 19:03:39,201] {docker.py:276} INFO - 21/05/14 22:03:39 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:39,203] {docker.py:276} INFO - 21/05/14 22:03:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321374053832467834017_0002_m_000036_40, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321374053832467834017_0002_m_000036_40}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321374053832467834017_0002}; taskId=attempt_20210514220321374053832467834017_0002_m_000036_40, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d9a8c4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:39,203] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Starting: Task committer attempt_20210514220321374053832467834017_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321374053832467834017_0002_m_000036_40
[2021-05-14 19:03:39,207] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Task committer attempt_20210514220321374053832467834017_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321374053832467834017_0002_m_000036_40 : duration 0:00.003s
[2021-05-14 19:03:39,356] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105142203211947584253085122279_0002_m_000034_38: needsTaskCommit() Task attempt_202105142203211947584253085122279_0002_m_000034_38
[2021-05-14 19:03:39,357] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Task committer attempt_202105142203211947584253085122279_0002_m_000034_38: needsTaskCommit() Task attempt_202105142203211947584253085122279_0002_m_000034_38: duration 0:00.002s
21/05/14 22:03:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211947584253085122279_0002_m_000034_38
[2021-05-14 19:03:39,359] {docker.py:276} INFO - 21/05/14 22:03:39 INFO Executor: Finished task 34.0 in stage 2.0 (TID 38). 4544 bytes result sent to driver
[2021-05-14 19:03:39,360] {docker.py:276} INFO - 21/05/14 22:03:39 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 41) (028692ec38a6, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:39,361] {docker.py:276} INFO - 21/05/14 22:03:39 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 38) in 1237 ms on 028692ec38a6 (executor driver) (34/200)
[2021-05-14 19:03:39,361] {docker.py:276} INFO - 21/05/14 22:03:39 INFO Executor: Running task 37.0 in stage 2.0 (TID 41)
[2021-05-14 19:03:39,371] {docker.py:276} INFO - 21/05/14 22:03:39 INFO ShuffleBlockFetcherIterator: Getting 3 (912.0 B) non-empty blocks including 3 (912.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:39,373] {docker.py:276} INFO - 21/05/14 22:03:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:39,374] {docker.py:276} INFO - 21/05/14 22:03:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212048340382525716018_0002_m_000037_41, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212048340382525716018_0002_m_000037_41}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212048340382525716018_0002}; taskId=attempt_202105142203212048340382525716018_0002_m_000037_41, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49b5c9af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:39,374] {docker.py:276} INFO - 21/05/14 22:03:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:39,374] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105142203212048340382525716018_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212048340382525716018_0002_m_000037_41
[2021-05-14 19:03:39,377] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Task committer attempt_202105142203212048340382525716018_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212048340382525716018_0002_m_000037_41 : duration 0:00.003s
[2021-05-14 19:03:39,406] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105142203218931048043362939456_0002_m_000032_36: needsTaskCommit() Task attempt_202105142203218931048043362939456_0002_m_000032_36
[2021-05-14 19:03:39,407] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Task committer attempt_202105142203218931048043362939456_0002_m_000032_36: needsTaskCommit() Task attempt_202105142203218931048043362939456_0002_m_000032_36: duration 0:00.001s
21/05/14 22:03:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218931048043362939456_0002_m_000032_36
[2021-05-14 19:03:39,408] {docker.py:276} INFO - 21/05/14 22:03:39 INFO Executor: Finished task 32.0 in stage 2.0 (TID 36). 4544 bytes result sent to driver
[2021-05-14 19:03:39,409] {docker.py:276} INFO - 21/05/14 22:03:39 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 42) (028692ec38a6, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:39,410] {docker.py:276} INFO - 21/05/14 22:03:39 INFO Executor: Running task 38.0 in stage 2.0 (TID 42)
[2021-05-14 19:03:39,411] {docker.py:276} INFO - 21/05/14 22:03:39 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 36) in 1715 ms on 028692ec38a6 (executor driver) (35/200)
[2021-05-14 19:03:39,419] {docker.py:276} INFO - 21/05/14 22:03:39 INFO ShuffleBlockFetcherIterator: Getting 3 (836.0 B) non-empty blocks including 3 (836.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:39,419] {docker.py:276} INFO - 21/05/14 22:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:39,421] {docker.py:276} INFO - 21/05/14 22:03:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:39,421] {docker.py:276} INFO - 21/05/14 22:03:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:39,422] {docker.py:276} INFO - 21/05/14 22:03:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214323583863865447739_0002_m_000038_42, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214323583863865447739_0002_m_000038_42}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214323583863865447739_0002}; taskId=attempt_202105142203214323583863865447739_0002_m_000038_42, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3aac616d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:39,422] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Starting: Task committer attempt_202105142203214323583863865447739_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214323583863865447739_0002_m_000038_42
[2021-05-14 19:03:39,424] {docker.py:276} INFO - 21/05/14 22:03:39 INFO StagingCommitter: Task committer attempt_202105142203214323583863865447739_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214323583863865447739_0002_m_000038_42 : duration 0:00.002s
[2021-05-14 19:03:40,363] {docker.py:276} INFO - 21/05/14 22:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105142203217252462073354963393_0002_m_000035_39: needsTaskCommit() Task attempt_202105142203217252462073354963393_0002_m_000035_39
[2021-05-14 19:03:40,364] {docker.py:276} INFO - 21/05/14 22:03:40 INFO StagingCommitter: Task committer attempt_202105142203217252462073354963393_0002_m_000035_39: needsTaskCommit() Task attempt_202105142203217252462073354963393_0002_m_000035_39: duration 0:00.001s
21/05/14 22:03:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217252462073354963393_0002_m_000035_39
[2021-05-14 19:03:40,367] {docker.py:276} INFO - 21/05/14 22:03:40 INFO Executor: Finished task 35.0 in stage 2.0 (TID 39). 4544 bytes result sent to driver
[2021-05-14 19:03:40,370] {docker.py:276} INFO - 21/05/14 22:03:40 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 43) (028692ec38a6, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:40,371] {docker.py:276} INFO - 21/05/14 22:03:40 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 39) in 2246 ms on 028692ec38a6 (executor driver) (36/200)
[2021-05-14 19:03:40,372] {docker.py:276} INFO - 21/05/14 22:03:40 INFO Executor: Running task 39.0 in stage 2.0 (TID 43)
[2021-05-14 19:03:40,393] {docker.py:276} INFO - 21/05/14 22:03:40 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:40,394] {docker.py:276} INFO - 21/05/14 22:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:40,396] {docker.py:276} INFO - 21/05/14 22:03:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:40,397] {docker.py:276} INFO - 21/05/14 22:03:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:40,398] {docker.py:276} INFO - 21/05/14 22:03:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:40,399] {docker.py:276} INFO - 21/05/14 22:03:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215505105364390181749_0002_m_000039_43, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215505105364390181749_0002_m_000039_43}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215505105364390181749_0002}; taskId=attempt_202105142203215505105364390181749_0002_m_000039_43, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4180337f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:40,399] {docker.py:276} INFO - 21/05/14 22:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105142203215505105364390181749_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215505105364390181749_0002_m_000039_43
[2021-05-14 19:03:40,401] {docker.py:276} INFO - 21/05/14 22:03:40 INFO StagingCommitter: Task committer attempt_202105142203215505105364390181749_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215505105364390181749_0002_m_000039_43 : duration 0:00.003s
[2021-05-14 19:03:40,583] {docker.py:276} INFO - 21/05/14 22:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105142203212048340382525716018_0002_m_000037_41: needsTaskCommit() Task attempt_202105142203212048340382525716018_0002_m_000037_41
21/05/14 22:03:40 INFO StagingCommitter: Task committer attempt_202105142203212048340382525716018_0002_m_000037_41: needsTaskCommit() Task attempt_202105142203212048340382525716018_0002_m_000037_41: duration 0:00.001s
21/05/14 22:03:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212048340382525716018_0002_m_000037_41
[2021-05-14 19:03:40,586] {docker.py:276} INFO - 21/05/14 22:03:40 INFO Executor: Finished task 37.0 in stage 2.0 (TID 41). 4587 bytes result sent to driver
[2021-05-14 19:03:40,588] {docker.py:276} INFO - 21/05/14 22:03:40 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 44) (028692ec38a6, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:40,589] {docker.py:276} INFO - 21/05/14 22:03:40 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 41) in 1231 ms on 028692ec38a6 (executor driver) (37/200)
[2021-05-14 19:03:40,590] {docker.py:276} INFO - 21/05/14 22:03:40 INFO Executor: Running task 40.0 in stage 2.0 (TID 44)
[2021-05-14 19:03:40,602] {docker.py:276} INFO - 21/05/14 22:03:40 INFO ShuffleBlockFetcherIterator: Getting 3 (1015.0 B) non-empty blocks including 3 (1015.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:40,603] {docker.py:276} INFO - 21/05/14 22:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:40,605] {docker.py:276} INFO - 21/05/14 22:03:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214228589396934432144_0002_m_000040_44, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214228589396934432144_0002_m_000040_44}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214228589396934432144_0002}; taskId=attempt_202105142203214228589396934432144_0002_m_000040_44, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@349464bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:40,605] {docker.py:276} INFO - 21/05/14 22:03:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:40 INFO StagingCommitter: Starting: Task committer attempt_202105142203214228589396934432144_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214228589396934432144_0002_m_000040_44
[2021-05-14 19:03:40,609] {docker.py:276} INFO - 21/05/14 22:03:40 INFO StagingCommitter: Task committer attempt_202105142203214228589396934432144_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214228589396934432144_0002_m_000040_44 : duration 0:00.004s
[2021-05-14 19:03:41,121] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105142203214323583863865447739_0002_m_000038_42: needsTaskCommit() Task attempt_202105142203214323583863865447739_0002_m_000038_42
[2021-05-14 19:03:41,122] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Task committer attempt_202105142203214323583863865447739_0002_m_000038_42: needsTaskCommit() Task attempt_202105142203214323583863865447739_0002_m_000038_42: duration 0:00.002s
[2021-05-14 19:03:41,122] {docker.py:276} INFO - 21/05/14 22:03:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214323583863865447739_0002_m_000038_42
[2021-05-14 19:03:41,124] {docker.py:276} INFO - 21/05/14 22:03:41 INFO Executor: Finished task 38.0 in stage 2.0 (TID 42). 4587 bytes result sent to driver
[2021-05-14 19:03:41,125] {docker.py:276} INFO - 21/05/14 22:03:41 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 45) (028692ec38a6, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:41,127] {docker.py:276} INFO - 21/05/14 22:03:41 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 42) in 1719 ms on 028692ec38a6 (executor driver) (38/200)
[2021-05-14 19:03:41,128] {docker.py:276} INFO - 21/05/14 22:03:41 INFO Executor: Running task 41.0 in stage 2.0 (TID 45)
[2021-05-14 19:03:41,138] {docker.py:276} INFO - 21/05/14 22:03:41 INFO ShuffleBlockFetcherIterator: Getting 3 (944.0 B) non-empty blocks including 3 (944.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:41,140] {docker.py:276} INFO - 21/05/14 22:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:41,140] {docker.py:276} INFO - 21/05/14 22:03:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213629784106621727953_0002_m_000041_45, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213629784106621727953_0002_m_000041_45}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213629784106621727953_0002}; taskId=attempt_202105142203213629784106621727953_0002_m_000041_45, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c6c3885}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:41,140] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105142203213629784106621727953_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213629784106621727953_0002_m_000041_45
[2021-05-14 19:03:41,143] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Task committer attempt_202105142203213629784106621727953_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213629784106621727953_0002_m_000041_45 : duration 0:00.003s
[2021-05-14 19:03:41,410] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Starting: Task committer attempt_20210514220321374053832467834017_0002_m_000036_40: needsTaskCommit() Task attempt_20210514220321374053832467834017_0002_m_000036_40
[2021-05-14 19:03:41,410] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Task committer attempt_20210514220321374053832467834017_0002_m_000036_40: needsTaskCommit() Task attempt_20210514220321374053832467834017_0002_m_000036_40: duration 0:00.001s
21/05/14 22:03:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321374053832467834017_0002_m_000036_40
[2021-05-14 19:03:41,412] {docker.py:276} INFO - 21/05/14 22:03:41 INFO Executor: Finished task 36.0 in stage 2.0 (TID 40). 4587 bytes result sent to driver
[2021-05-14 19:03:41,413] {docker.py:276} INFO - 21/05/14 22:03:41 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 46) (028692ec38a6, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:41,414] {docker.py:276} INFO - 21/05/14 22:03:41 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 40) in 2232 ms on 028692ec38a6 (executor driver) (39/200)
21/05/14 22:03:41 INFO Executor: Running task 42.0 in stage 2.0 (TID 46)
[2021-05-14 19:03:41,423] {docker.py:276} INFO - 21/05/14 22:03:41 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:41,425] {docker.py:276} INFO - 21/05/14 22:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:41,426] {docker.py:276} INFO - 21/05/14 22:03:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217234461146766550454_0002_m_000042_46, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217234461146766550454_0002_m_000042_46}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217234461146766550454_0002}; taskId=attempt_202105142203217234461146766550454_0002_m_000042_46, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d21e0be}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:41,426] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105142203217234461146766550454_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217234461146766550454_0002_m_000042_46
[2021-05-14 19:03:41,429] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Task committer attempt_202105142203217234461146766550454_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217234461146766550454_0002_m_000042_46 : duration 0:00.003s
[2021-05-14 19:03:41,804] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105142203214228589396934432144_0002_m_000040_44: needsTaskCommit() Task attempt_202105142203214228589396934432144_0002_m_000040_44
[2021-05-14 19:03:41,805] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Task committer attempt_202105142203214228589396934432144_0002_m_000040_44: needsTaskCommit() Task attempt_202105142203214228589396934432144_0002_m_000040_44: duration 0:00.000s
21/05/14 22:03:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214228589396934432144_0002_m_000040_44
[2021-05-14 19:03:41,807] {docker.py:276} INFO - 21/05/14 22:03:41 INFO Executor: Finished task 40.0 in stage 2.0 (TID 44). 4544 bytes result sent to driver
[2021-05-14 19:03:41,808] {docker.py:276} INFO - 21/05/14 22:03:41 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 47) (028692ec38a6, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:41,810] {docker.py:276} INFO - 21/05/14 22:03:41 INFO Executor: Running task 43.0 in stage 2.0 (TID 47)
21/05/14 22:03:41 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 44) in 1223 ms on 028692ec38a6 (executor driver) (40/200)
[2021-05-14 19:03:41,819] {docker.py:276} INFO - 21/05/14 22:03:41 INFO ShuffleBlockFetcherIterator: Getting 3 (948.0 B) non-empty blocks including 3 (948.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:41,821] {docker.py:276} INFO - 21/05/14 22:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:41,821] {docker.py:276} INFO - 21/05/14 22:03:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216129016431626826337_0002_m_000043_47, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216129016431626826337_0002_m_000043_47}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216129016431626826337_0002}; taskId=attempt_202105142203216129016431626826337_0002_m_000043_47, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29e88787}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:41 INFO StagingCommitter: Starting: Task committer attempt_202105142203216129016431626826337_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216129016431626826337_0002_m_000043_47
[2021-05-14 19:03:41,824] {docker.py:276} INFO - 21/05/14 22:03:41 INFO StagingCommitter: Task committer attempt_202105142203216129016431626826337_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216129016431626826337_0002_m_000043_47 : duration 0:00.003s
[2021-05-14 19:03:42,560] {docker.py:276} INFO - 21/05/14 22:03:42 INFO StagingCommitter: Starting: Task committer attempt_202105142203215505105364390181749_0002_m_000039_43: needsTaskCommit() Task attempt_202105142203215505105364390181749_0002_m_000039_43
[2021-05-14 19:03:42,561] {docker.py:276} INFO - 21/05/14 22:03:42 INFO StagingCommitter: Task committer attempt_202105142203215505105364390181749_0002_m_000039_43: needsTaskCommit() Task attempt_202105142203215505105364390181749_0002_m_000039_43: duration 0:00.001s
21/05/14 22:03:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215505105364390181749_0002_m_000039_43
[2021-05-14 19:03:42,563] {docker.py:276} INFO - 21/05/14 22:03:42 INFO Executor: Finished task 39.0 in stage 2.0 (TID 43). 4587 bytes result sent to driver
[2021-05-14 19:03:42,564] {docker.py:276} INFO - 21/05/14 22:03:42 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 48) (028692ec38a6, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:42,565] {docker.py:276} INFO - 21/05/14 22:03:42 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 43) in 2198 ms on 028692ec38a6 (executor driver) (41/200)
[2021-05-14 19:03:42,566] {docker.py:276} INFO - 21/05/14 22:03:42 INFO Executor: Running task 44.0 in stage 2.0 (TID 48)
[2021-05-14 19:03:42,575] {docker.py:276} INFO - 21/05/14 22:03:42 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:42,577] {docker.py:276} INFO - 21/05/14 22:03:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:42,578] {docker.py:276} INFO - 21/05/14 22:03:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:42,578] {docker.py:276} INFO - 21/05/14 22:03:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218122586844002131752_0002_m_000044_48, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218122586844002131752_0002_m_000044_48}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218122586844002131752_0002}; taskId=attempt_202105142203218122586844002131752_0002_m_000044_48, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c069be1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:42,579] {docker.py:276} INFO - 21/05/14 22:03:42 INFO StagingCommitter: Starting: Task committer attempt_202105142203218122586844002131752_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218122586844002131752_0002_m_000044_48
[2021-05-14 19:03:42,582] {docker.py:276} INFO - 21/05/14 22:03:42 INFO StagingCommitter: Task committer attempt_202105142203218122586844002131752_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218122586844002131752_0002_m_000044_48 : duration 0:00.003s
[2021-05-14 19:03:43,092] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105142203217234461146766550454_0002_m_000042_46: needsTaskCommit() Task attempt_202105142203217234461146766550454_0002_m_000042_46
[2021-05-14 19:03:43,093] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Task committer attempt_202105142203217234461146766550454_0002_m_000042_46: needsTaskCommit() Task attempt_202105142203217234461146766550454_0002_m_000042_46: duration 0:00.001s
21/05/14 22:03:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217234461146766550454_0002_m_000042_46
[2021-05-14 19:03:43,094] {docker.py:276} INFO - 21/05/14 22:03:43 INFO Executor: Finished task 42.0 in stage 2.0 (TID 46). 4544 bytes result sent to driver
[2021-05-14 19:03:43,096] {docker.py:276} INFO - 21/05/14 22:03:43 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 49) (028692ec38a6, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:43,097] {docker.py:276} INFO - 21/05/14 22:03:43 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 46) in 1686 ms on 028692ec38a6 (executor driver) (42/200)
[2021-05-14 19:03:43,098] {docker.py:276} INFO - 21/05/14 22:03:43 INFO Executor: Running task 45.0 in stage 2.0 (TID 49)
[2021-05-14 19:03:43,107] {docker.py:276} INFO - 21/05/14 22:03:43 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:43,109] {docker.py:276} INFO - 21/05/14 22:03:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:43,110] {docker.py:276} INFO - 21/05/14 22:03:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212821723869086328666_0002_m_000045_49, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212821723869086328666_0002_m_000045_49}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212821723869086328666_0002}; taskId=attempt_202105142203212821723869086328666_0002_m_000045_49, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@150e2536}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:43,110] {docker.py:276} INFO - 21/05/14 22:03:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:43,111] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105142203212821723869086328666_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212821723869086328666_0002_m_000045_49
[2021-05-14 19:03:43,114] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Task committer attempt_202105142203212821723869086328666_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212821723869086328666_0002_m_000045_49 : duration 0:00.004s
[2021-05-14 19:03:43,275] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105142203213629784106621727953_0002_m_000041_45: needsTaskCommit() Task attempt_202105142203213629784106621727953_0002_m_000041_45
[2021-05-14 19:03:43,275] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Task committer attempt_202105142203213629784106621727953_0002_m_000041_45: needsTaskCommit() Task attempt_202105142203213629784106621727953_0002_m_000041_45: duration 0:00.000s
21/05/14 22:03:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213629784106621727953_0002_m_000041_45
[2021-05-14 19:03:43,278] {docker.py:276} INFO - 21/05/14 22:03:43 INFO Executor: Finished task 41.0 in stage 2.0 (TID 45). 4544 bytes result sent to driver
[2021-05-14 19:03:43,279] {docker.py:276} INFO - 21/05/14 22:03:43 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 50) (028692ec38a6, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:43,280] {docker.py:276} INFO - 21/05/14 22:03:43 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 45) in 2157 ms on 028692ec38a6 (executor driver) (43/200)
[2021-05-14 19:03:43,281] {docker.py:276} INFO - 21/05/14 22:03:43 INFO Executor: Running task 46.0 in stage 2.0 (TID 50)
[2021-05-14 19:03:43,291] {docker.py:276} INFO - 21/05/14 22:03:43 INFO ShuffleBlockFetcherIterator: Getting 3 (948.0 B) non-empty blocks including 3 (948.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:43,294] {docker.py:276} INFO - 21/05/14 22:03:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:43,295] {docker.py:276} INFO - 21/05/14 22:03:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214548925993006277586_0002_m_000046_50, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214548925993006277586_0002_m_000046_50}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214548925993006277586_0002}; taskId=attempt_202105142203214548925993006277586_0002_m_000046_50, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e98f6b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:43,295] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105142203214548925993006277586_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214548925993006277586_0002_m_000046_50
[2021-05-14 19:03:43,298] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Task committer attempt_202105142203214548925993006277586_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214548925993006277586_0002_m_000046_50 : duration 0:00.003s
[2021-05-14 19:03:43,528] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105142203216129016431626826337_0002_m_000043_47: needsTaskCommit() Task attempt_202105142203216129016431626826337_0002_m_000043_47
[2021-05-14 19:03:43,530] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Task committer attempt_202105142203216129016431626826337_0002_m_000043_47: needsTaskCommit() Task attempt_202105142203216129016431626826337_0002_m_000043_47: duration 0:00.002s
21/05/14 22:03:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216129016431626826337_0002_m_000043_47
[2021-05-14 19:03:43,531] {docker.py:276} INFO - 21/05/14 22:03:43 INFO Executor: Finished task 43.0 in stage 2.0 (TID 47). 4544 bytes result sent to driver
[2021-05-14 19:03:43,533] {docker.py:276} INFO - 21/05/14 22:03:43 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 51) (028692ec38a6, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:43,534] {docker.py:276} INFO - 21/05/14 22:03:43 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 47) in 1728 ms on 028692ec38a6 (executor driver) (44/200)
[2021-05-14 19:03:43,535] {docker.py:276} INFO - 21/05/14 22:03:43 INFO Executor: Running task 47.0 in stage 2.0 (TID 51)
[2021-05-14 19:03:43,546] {docker.py:276} INFO - 21/05/14 22:03:43 INFO ShuffleBlockFetcherIterator: Getting 3 (866.0 B) non-empty blocks including 3 (866.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:43,546] {docker.py:276} INFO - 21/05/14 22:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:43,548] {docker.py:276} INFO - 21/05/14 22:03:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:43,549] {docker.py:276} INFO - 21/05/14 22:03:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:43,549] {docker.py:276} INFO - 21/05/14 22:03:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203219207232612730143029_0002_m_000047_51, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219207232612730143029_0002_m_000047_51}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203219207232612730143029_0002}; taskId=attempt_202105142203219207232612730143029_0002_m_000047_51, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f21578e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:43,549] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Starting: Task committer attempt_202105142203219207232612730143029_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219207232612730143029_0002_m_000047_51
[2021-05-14 19:03:43,552] {docker.py:276} INFO - 21/05/14 22:03:43 INFO StagingCommitter: Task committer attempt_202105142203219207232612730143029_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219207232612730143029_0002_m_000047_51 : duration 0:00.004s
[2021-05-14 19:03:44,238] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105142203218122586844002131752_0002_m_000044_48: needsTaskCommit() Task attempt_202105142203218122586844002131752_0002_m_000044_48
[2021-05-14 19:03:44,238] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Task committer attempt_202105142203218122586844002131752_0002_m_000044_48: needsTaskCommit() Task attempt_202105142203218122586844002131752_0002_m_000044_48: duration 0:00.000s
[2021-05-14 19:03:44,239] {docker.py:276} INFO - 21/05/14 22:03:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218122586844002131752_0002_m_000044_48
[2021-05-14 19:03:44,239] {docker.py:276} INFO - 21/05/14 22:03:44 INFO Executor: Finished task 44.0 in stage 2.0 (TID 48). 4544 bytes result sent to driver
[2021-05-14 19:03:44,241] {docker.py:276} INFO - 21/05/14 22:03:44 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 52) (028692ec38a6, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:44,242] {docker.py:276} INFO - 21/05/14 22:03:44 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 48) in 1680 ms on 028692ec38a6 (executor driver) (45/200)
[2021-05-14 19:03:44,243] {docker.py:276} INFO - 21/05/14 22:03:44 INFO Executor: Running task 48.0 in stage 2.0 (TID 52)
[2021-05-14 19:03:44,252] {docker.py:276} INFO - 21/05/14 22:03:44 INFO ShuffleBlockFetcherIterator: Getting 3 (884.0 B) non-empty blocks including 3 (884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:44,254] {docker.py:276} INFO - 21/05/14 22:03:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:44,255] {docker.py:276} INFO - 21/05/14 22:03:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212105970475488054976_0002_m_000048_52, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212105970475488054976_0002_m_000048_52}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212105970475488054976_0002}; taskId=attempt_202105142203212105970475488054976_0002_m_000048_52, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30771616}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105142203212105970475488054976_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212105970475488054976_0002_m_000048_52
[2021-05-14 19:03:44,258] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Task committer attempt_202105142203212105970475488054976_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212105970475488054976_0002_m_000048_52 : duration 0:00.003s
[2021-05-14 19:03:44,801] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105142203212821723869086328666_0002_m_000045_49: needsTaskCommit() Task attempt_202105142203212821723869086328666_0002_m_000045_49
[2021-05-14 19:03:44,802] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Task committer attempt_202105142203212821723869086328666_0002_m_000045_49: needsTaskCommit() Task attempt_202105142203212821723869086328666_0002_m_000045_49: duration 0:00.001s
[2021-05-14 19:03:44,802] {docker.py:276} INFO - 21/05/14 22:03:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212821723869086328666_0002_m_000045_49
[2021-05-14 19:03:44,804] {docker.py:276} INFO - 21/05/14 22:03:44 INFO Executor: Finished task 45.0 in stage 2.0 (TID 49). 4544 bytes result sent to driver
[2021-05-14 19:03:44,805] {docker.py:276} INFO - 21/05/14 22:03:44 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 53) (028692ec38a6, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:44,806] {docker.py:276} INFO - 21/05/14 22:03:44 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 49) in 1712 ms on 028692ec38a6 (executor driver) (46/200)
21/05/14 22:03:44 INFO Executor: Running task 49.0 in stage 2.0 (TID 53)
[2021-05-14 19:03:44,814] {docker.py:276} INFO - 21/05/14 22:03:44 INFO ShuffleBlockFetcherIterator: Getting 3 (920.0 B) non-empty blocks including 3 (920.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:44,816] {docker.py:276} INFO - 21/05/14 22:03:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:44,817] {docker.py:276} INFO - 21/05/14 22:03:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213982314923918659069_0002_m_000049_53, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213982314923918659069_0002_m_000049_53}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213982314923918659069_0002}; taskId=attempt_202105142203213982314923918659069_0002_m_000049_53, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@259362a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:44,817] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Starting: Task committer attempt_202105142203213982314923918659069_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213982314923918659069_0002_m_000049_53
[2021-05-14 19:03:44,821] {docker.py:276} INFO - 21/05/14 22:03:44 INFO StagingCommitter: Task committer attempt_202105142203213982314923918659069_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213982314923918659069_0002_m_000049_53 : duration 0:00.004s
[2021-05-14 19:03:44,988] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105142203214548925993006277586_0002_m_000046_50: needsTaskCommit() Task attempt_202105142203214548925993006277586_0002_m_000046_50
21/05/14 22:03:45 INFO StagingCommitter: Task committer attempt_202105142203214548925993006277586_0002_m_000046_50: needsTaskCommit() Task attempt_202105142203214548925993006277586_0002_m_000046_50: duration 0:00.001s
[2021-05-14 19:03:44,989] {docker.py:276} INFO - 21/05/14 22:03:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214548925993006277586_0002_m_000046_50
[2021-05-14 19:03:44,992] {docker.py:276} INFO - 21/05/14 22:03:45 INFO Executor: Finished task 46.0 in stage 2.0 (TID 50). 4587 bytes result sent to driver
[2021-05-14 19:03:44,994] {docker.py:276} INFO - 21/05/14 22:03:45 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 54) (028692ec38a6, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:44,995] {docker.py:276} INFO - 21/05/14 22:03:45 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 50) in 1719 ms on 028692ec38a6 (executor driver) (47/200)
[2021-05-14 19:03:44,996] {docker.py:276} INFO - 21/05/14 22:03:45 INFO Executor: Running task 50.0 in stage 2.0 (TID 54)
[2021-05-14 19:03:45,005] {docker.py:276} INFO - 21/05/14 22:03:45 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:45,007] {docker.py:276} INFO - 21/05/14 22:03:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:45,008] {docker.py:276} INFO - 21/05/14 22:03:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321984310633878470852_0002_m_000050_54, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321984310633878470852_0002_m_000050_54}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321984310633878470852_0002}; taskId=attempt_20210514220321984310633878470852_0002_m_000050_54, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f198374}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:45,008] {docker.py:276} INFO - 21/05/14 22:03:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:45,009] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Starting: Task committer attempt_20210514220321984310633878470852_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321984310633878470852_0002_m_000050_54
[2021-05-14 19:03:45,012] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Task committer attempt_20210514220321984310633878470852_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321984310633878470852_0002_m_000050_54 : duration 0:00.004s
[2021-05-14 19:03:45,293] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105142203219207232612730143029_0002_m_000047_51: needsTaskCommit() Task attempt_202105142203219207232612730143029_0002_m_000047_51
[2021-05-14 19:03:45,294] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Task committer attempt_202105142203219207232612730143029_0002_m_000047_51: needsTaskCommit() Task attempt_202105142203219207232612730143029_0002_m_000047_51: duration 0:00.001s
[2021-05-14 19:03:45,294] {docker.py:276} INFO - 21/05/14 22:03:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203219207232612730143029_0002_m_000047_51
[2021-05-14 19:03:45,296] {docker.py:276} INFO - 21/05/14 22:03:45 INFO Executor: Finished task 47.0 in stage 2.0 (TID 51). 4587 bytes result sent to driver
[2021-05-14 19:03:45,299] {docker.py:276} INFO - 21/05/14 22:03:45 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 55) (028692ec38a6, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:45,300] {docker.py:276} INFO - 21/05/14 22:03:45 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 51) in 1769 ms on 028692ec38a6 (executor driver) (48/200)
[2021-05-14 19:03:45,301] {docker.py:276} INFO - 21/05/14 22:03:45 INFO Executor: Running task 51.0 in stage 2.0 (TID 55)
[2021-05-14 19:03:45,312] {docker.py:276} INFO - 21/05/14 22:03:45 INFO ShuffleBlockFetcherIterator: Getting 3 (930.0 B) non-empty blocks including 3 (930.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:45,314] {docker.py:276} INFO - 21/05/14 22:03:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214301587127321276986_0002_m_000051_55, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214301587127321276986_0002_m_000051_55}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214301587127321276986_0002}; taskId=attempt_202105142203214301587127321276986_0002_m_000051_55, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13decd7b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105142203214301587127321276986_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214301587127321276986_0002_m_000051_55
[2021-05-14 19:03:45,317] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Task committer attempt_202105142203214301587127321276986_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214301587127321276986_0002_m_000051_55 : duration 0:00.003s
[2021-05-14 19:03:45,475] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Starting: Task committer attempt_202105142203212105970475488054976_0002_m_000048_52: needsTaskCommit() Task attempt_202105142203212105970475488054976_0002_m_000048_52
[2021-05-14 19:03:45,477] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Task committer attempt_202105142203212105970475488054976_0002_m_000048_52: needsTaskCommit() Task attempt_202105142203212105970475488054976_0002_m_000048_52: duration 0:00.001s
21/05/14 22:03:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212105970475488054976_0002_m_000048_52
[2021-05-14 19:03:45,479] {docker.py:276} INFO - 21/05/14 22:03:45 INFO Executor: Finished task 48.0 in stage 2.0 (TID 52). 4587 bytes result sent to driver
[2021-05-14 19:03:45,481] {docker.py:276} INFO - 21/05/14 22:03:45 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 56) (028692ec38a6, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:45,483] {docker.py:276} INFO - 21/05/14 22:03:45 INFO Executor: Running task 52.0 in stage 2.0 (TID 56)
21/05/14 22:03:45 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 52) in 1243 ms on 028692ec38a6 (executor driver) (49/200)
[2021-05-14 19:03:45,493] {docker.py:276} INFO - 21/05/14 22:03:45 INFO ShuffleBlockFetcherIterator: Getting 3 (972.0 B) non-empty blocks including 3 (972.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:45,495] {docker.py:276} INFO - 21/05/14 22:03:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321956498921322336374_0002_m_000052_56, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321956498921322336374_0002_m_000052_56}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321956498921322336374_0002}; taskId=attempt_20210514220321956498921322336374_0002_m_000052_56, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a6e8031}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:45,496] {docker.py:276} INFO - 21/05/14 22:03:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:45 INFO StagingCommitter: Starting: Task committer attempt_20210514220321956498921322336374_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321956498921322336374_0002_m_000052_56
[2021-05-14 19:03:45,500] {docker.py:276} INFO - 21/05/14 22:03:45 INFO StagingCommitter: Task committer attempt_20210514220321956498921322336374_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321956498921322336374_0002_m_000052_56 : duration 0:00.003s
[2021-05-14 19:03:46,494] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105142203213982314923918659069_0002_m_000049_53: needsTaskCommit() Task attempt_202105142203213982314923918659069_0002_m_000049_53
[2021-05-14 19:03:46,495] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Task committer attempt_202105142203213982314923918659069_0002_m_000049_53: needsTaskCommit() Task attempt_202105142203213982314923918659069_0002_m_000049_53: duration 0:00.001s
[2021-05-14 19:03:46,496] {docker.py:276} INFO - 21/05/14 22:03:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213982314923918659069_0002_m_000049_53
[2021-05-14 19:03:46,497] {docker.py:276} INFO - 21/05/14 22:03:46 INFO Executor: Finished task 49.0 in stage 2.0 (TID 53). 4587 bytes result sent to driver
[2021-05-14 19:03:46,498] {docker.py:276} INFO - 21/05/14 22:03:46 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 57) (028692ec38a6, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:46,499] {docker.py:276} INFO - 21/05/14 22:03:46 INFO Executor: Running task 53.0 in stage 2.0 (TID 57)
21/05/14 22:03:46 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 53) in 1696 ms on 028692ec38a6 (executor driver) (50/200)
[2021-05-14 19:03:46,507] {docker.py:276} INFO - 21/05/14 22:03:46 INFO ShuffleBlockFetcherIterator: Getting 3 (1036.0 B) non-empty blocks including 3 (1036.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:46,509] {docker.py:276} INFO - 21/05/14 22:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:46,510] {docker.py:276} INFO - 21/05/14 22:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214499526380398138773_0002_m_000053_57, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214499526380398138773_0002_m_000053_57}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214499526380398138773_0002}; taskId=attempt_202105142203214499526380398138773_0002_m_000053_57, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25b2d11f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:46,510] {docker.py:276} INFO - 21/05/14 22:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105142203214499526380398138773_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214499526380398138773_0002_m_000053_57
[2021-05-14 19:03:46,513] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Task committer attempt_202105142203214499526380398138773_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214499526380398138773_0002_m_000053_57 : duration 0:00.003s
[2021-05-14 19:03:46,795] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Starting: Task committer attempt_20210514220321984310633878470852_0002_m_000050_54: needsTaskCommit() Task attempt_20210514220321984310633878470852_0002_m_000050_54
[2021-05-14 19:03:46,796] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Task committer attempt_20210514220321984310633878470852_0002_m_000050_54: needsTaskCommit() Task attempt_20210514220321984310633878470852_0002_m_000050_54: duration 0:00.000s
21/05/14 22:03:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321984310633878470852_0002_m_000050_54
[2021-05-14 19:03:46,798] {docker.py:276} INFO - 21/05/14 22:03:46 INFO Executor: Finished task 50.0 in stage 2.0 (TID 54). 4544 bytes result sent to driver
[2021-05-14 19:03:46,800] {docker.py:276} INFO - 21/05/14 22:03:46 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 58) (028692ec38a6, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:46,801] {docker.py:276} INFO - 21/05/14 22:03:46 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 54) in 1810 ms on 028692ec38a6 (executor driver) (51/200)
21/05/14 22:03:46 INFO Executor: Running task 54.0 in stage 2.0 (TID 58)
[2021-05-14 19:03:46,812] {docker.py:276} INFO - 21/05/14 22:03:46 INFO ShuffleBlockFetcherIterator: Getting 3 (884.0 B) non-empty blocks including 3 (884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:46,815] {docker.py:276} INFO - 21/05/14 22:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:46,815] {docker.py:276} INFO - 21/05/14 22:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:46,816] {docker.py:276} INFO - 21/05/14 22:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215101375032366942596_0002_m_000054_58, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215101375032366942596_0002_m_000054_58}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215101375032366942596_0002}; taskId=attempt_202105142203215101375032366942596_0002_m_000054_58, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d10bf0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:46,816] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105142203215101375032366942596_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215101375032366942596_0002_m_000054_58
[2021-05-14 19:03:46,818] {docker.py:276} INFO - 21/05/14 22:03:46 INFO StagingCommitter: Task committer attempt_202105142203215101375032366942596_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215101375032366942596_0002_m_000054_58 : duration 0:00.004s
[2021-05-14 19:03:47,162] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Starting: Task committer attempt_20210514220321956498921322336374_0002_m_000052_56: needsTaskCommit() Task attempt_20210514220321956498921322336374_0002_m_000052_56
[2021-05-14 19:03:47,163] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Task committer attempt_20210514220321956498921322336374_0002_m_000052_56: needsTaskCommit() Task attempt_20210514220321956498921322336374_0002_m_000052_56: duration 0:00.001s
[2021-05-14 19:03:47,163] {docker.py:276} INFO - 21/05/14 22:03:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321956498921322336374_0002_m_000052_56
[2021-05-14 19:03:47,166] {docker.py:276} INFO - 21/05/14 22:03:47 INFO Executor: Finished task 52.0 in stage 2.0 (TID 56). 4544 bytes result sent to driver
[2021-05-14 19:03:47,168] {docker.py:276} INFO - 21/05/14 22:03:47 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 59) (028692ec38a6, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:47,169] {docker.py:276} INFO - 21/05/14 22:03:47 INFO Executor: Running task 55.0 in stage 2.0 (TID 59)
[2021-05-14 19:03:47,170] {docker.py:276} INFO - 21/05/14 22:03:47 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 56) in 1691 ms on 028692ec38a6 (executor driver) (52/200)
[2021-05-14 19:03:47,178] {docker.py:276} INFO - 21/05/14 22:03:47 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:47,178] {docker.py:276} INFO - 21/05/14 22:03:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:47,180] {docker.py:276} INFO - 21/05/14 22:03:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:47,180] {docker.py:276} INFO - 21/05/14 22:03:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218140552618835568386_0002_m_000055_59, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218140552618835568386_0002_m_000055_59}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218140552618835568386_0002}; taskId=attempt_202105142203218140552618835568386_0002_m_000055_59, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e358698}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:47,180] {docker.py:276} INFO - 21/05/14 22:03:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:47 INFO StagingCommitter: Starting: Task committer attempt_202105142203218140552618835568386_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218140552618835568386_0002_m_000055_59
[2021-05-14 19:03:47,184] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Task committer attempt_202105142203218140552618835568386_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218140552618835568386_0002_m_000055_59 : duration 0:00.004s
[2021-05-14 19:03:47,496] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Starting: Task committer attempt_202105142203214301587127321276986_0002_m_000051_55: needsTaskCommit() Task attempt_202105142203214301587127321276986_0002_m_000051_55
[2021-05-14 19:03:47,496] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Task committer attempt_202105142203214301587127321276986_0002_m_000051_55: needsTaskCommit() Task attempt_202105142203214301587127321276986_0002_m_000051_55: duration 0:00.000s
21/05/14 22:03:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214301587127321276986_0002_m_000051_55
[2021-05-14 19:03:47,503] {docker.py:276} INFO - 21/05/14 22:03:47 INFO Executor: Finished task 51.0 in stage 2.0 (TID 55). 4544 bytes result sent to driver
[2021-05-14 19:03:47,504] {docker.py:276} INFO - 21/05/14 22:03:47 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 60) (028692ec38a6, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:47,506] {docker.py:276} INFO - 21/05/14 22:03:47 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 55) in 2211 ms on 028692ec38a6 (executor driver) (53/200)
21/05/14 22:03:47 INFO Executor: Running task 56.0 in stage 2.0 (TID 60)
[2021-05-14 19:03:47,515] {docker.py:276} INFO - 21/05/14 22:03:47 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:47,517] {docker.py:276} INFO - 21/05/14 22:03:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215185272003589667339_0002_m_000056_60, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215185272003589667339_0002_m_000056_60}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215185272003589667339_0002}; taskId=attempt_202105142203215185272003589667339_0002_m_000056_60, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78c4ed2e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:47,517] {docker.py:276} INFO - 21/05/14 22:03:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:47,517] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Starting: Task committer attempt_202105142203215185272003589667339_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215185272003589667339_0002_m_000056_60
[2021-05-14 19:03:47,520] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Task committer attempt_202105142203215185272003589667339_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215185272003589667339_0002_m_000056_60 : duration 0:00.002s
[2021-05-14 19:03:47,668] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Starting: Task committer attempt_202105142203214499526380398138773_0002_m_000053_57: needsTaskCommit() Task attempt_202105142203214499526380398138773_0002_m_000053_57
[2021-05-14 19:03:47,670] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Task committer attempt_202105142203214499526380398138773_0002_m_000053_57: needsTaskCommit() Task attempt_202105142203214499526380398138773_0002_m_000053_57: duration 0:00.001s
[2021-05-14 19:03:47,671] {docker.py:276} INFO - 21/05/14 22:03:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214499526380398138773_0002_m_000053_57
[2021-05-14 19:03:47,672] {docker.py:276} INFO - 21/05/14 22:03:47 INFO Executor: Finished task 53.0 in stage 2.0 (TID 57). 4544 bytes result sent to driver
[2021-05-14 19:03:47,674] {docker.py:276} INFO - 21/05/14 22:03:47 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 61) (028692ec38a6, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:47,676] {docker.py:276} INFO - 21/05/14 22:03:47 INFO Executor: Running task 57.0 in stage 2.0 (TID 61)
[2021-05-14 19:03:47,677] {docker.py:276} INFO - 21/05/14 22:03:47 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 57) in 1179 ms on 028692ec38a6 (executor driver) (54/200)
[2021-05-14 19:03:47,686] {docker.py:276} INFO - 21/05/14 22:03:47 INFO ShuffleBlockFetcherIterator: Getting 3 (1073.0 B) non-empty blocks including 3 (1073.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:47,688] {docker.py:276} INFO - 21/05/14 22:03:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321590002652994210791_0002_m_000057_61, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321590002652994210791_0002_m_000057_61}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321590002652994210791_0002}; taskId=attempt_20210514220321590002652994210791_0002_m_000057_61, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45024273}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:47,688] {docker.py:276} INFO - 21/05/14 22:03:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:47 INFO StagingCommitter: Starting: Task committer attempt_20210514220321590002652994210791_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321590002652994210791_0002_m_000057_61
[2021-05-14 19:03:47,691] {docker.py:276} INFO - 21/05/14 22:03:47 INFO StagingCommitter: Task committer attempt_20210514220321590002652994210791_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321590002652994210791_0002_m_000057_61 : duration 0:00.002s
[2021-05-14 19:03:48,481] {docker.py:276} INFO - 21/05/14 22:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105142203215101375032366942596_0002_m_000054_58: needsTaskCommit() Task attempt_202105142203215101375032366942596_0002_m_000054_58
[2021-05-14 19:03:48,482] {docker.py:276} INFO - 21/05/14 22:03:48 INFO StagingCommitter: Task committer attempt_202105142203215101375032366942596_0002_m_000054_58: needsTaskCommit() Task attempt_202105142203215101375032366942596_0002_m_000054_58: duration 0:00.002s
21/05/14 22:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215101375032366942596_0002_m_000054_58
[2021-05-14 19:03:48,485] {docker.py:276} INFO - 21/05/14 22:03:48 INFO Executor: Finished task 54.0 in stage 2.0 (TID 58). 4544 bytes result sent to driver
[2021-05-14 19:03:48,487] {docker.py:276} INFO - 21/05/14 22:03:48 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 62) (028692ec38a6, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:48,489] {docker.py:276} INFO - 21/05/14 22:03:48 INFO Executor: Running task 58.0 in stage 2.0 (TID 62)
21/05/14 22:03:48 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 58) in 1692 ms on 028692ec38a6 (executor driver) (55/200)
[2021-05-14 19:03:48,498] {docker.py:276} INFO - 21/05/14 22:03:48 INFO ShuffleBlockFetcherIterator: Getting 3 (944.0 B) non-empty blocks including 3 (944.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:48,500] {docker.py:276} INFO - 21/05/14 22:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211086795189291343443_0002_m_000058_62, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211086795189291343443_0002_m_000058_62}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211086795189291343443_0002}; taskId=attempt_202105142203211086795189291343443_0002_m_000058_62, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@289eddbd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:48,500] {docker.py:276} INFO - 21/05/14 22:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105142203211086795189291343443_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211086795189291343443_0002_m_000058_62
[2021-05-14 19:03:48,503] {docker.py:276} INFO - 21/05/14 22:03:48 INFO StagingCommitter: Task committer attempt_202105142203211086795189291343443_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211086795189291343443_0002_m_000058_62 : duration 0:00.003s
[2021-05-14 19:03:48,849] {docker.py:276} INFO - 21/05/14 22:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105142203218140552618835568386_0002_m_000055_59: needsTaskCommit() Task attempt_202105142203218140552618835568386_0002_m_000055_59
[2021-05-14 19:03:48,851] {docker.py:276} INFO - 21/05/14 22:03:48 INFO StagingCommitter: Task committer attempt_202105142203218140552618835568386_0002_m_000055_59: needsTaskCommit() Task attempt_202105142203218140552618835568386_0002_m_000055_59: duration 0:00.002s
21/05/14 22:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218140552618835568386_0002_m_000055_59
[2021-05-14 19:03:48,853] {docker.py:276} INFO - 21/05/14 22:03:48 INFO Executor: Finished task 55.0 in stage 2.0 (TID 59). 4544 bytes result sent to driver
[2021-05-14 19:03:48,855] {docker.py:276} INFO - 21/05/14 22:03:48 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 63) (028692ec38a6, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:48,856] {docker.py:276} INFO - 21/05/14 22:03:48 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 59) in 1690 ms on 028692ec38a6 (executor driver) (56/200)
[2021-05-14 19:03:48,857] {docker.py:276} INFO - 21/05/14 22:03:48 INFO Executor: Running task 59.0 in stage 2.0 (TID 63)
[2021-05-14 19:03:48,866] {docker.py:276} INFO - 21/05/14 22:03:48 INFO ShuffleBlockFetcherIterator: Getting 3 (803.0 B) non-empty blocks including 3 (803.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:48,870] {docker.py:276} INFO - 21/05/14 22:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321559226936027291173_0002_m_000059_63, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321559226936027291173_0002_m_000059_63}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321559226936027291173_0002}; taskId=attempt_20210514220321559226936027291173_0002_m_000059_63, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2379d807}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:48 INFO StagingCommitter: Starting: Task committer attempt_20210514220321559226936027291173_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321559226936027291173_0002_m_000059_63
[2021-05-14 19:03:48,872] {docker.py:276} INFO - 21/05/14 22:03:48 INFO StagingCommitter: Task committer attempt_20210514220321559226936027291173_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321559226936027291173_0002_m_000059_63 : duration 0:00.002s
[2021-05-14 19:03:49,374] {docker.py:276} INFO - 21/05/14 22:03:49 INFO StagingCommitter: Starting: Task committer attempt_20210514220321590002652994210791_0002_m_000057_61: needsTaskCommit() Task attempt_20210514220321590002652994210791_0002_m_000057_61
[2021-05-14 19:03:49,375] {docker.py:276} INFO - 21/05/14 22:03:49 INFO StagingCommitter: Task committer attempt_20210514220321590002652994210791_0002_m_000057_61: needsTaskCommit() Task attempt_20210514220321590002652994210791_0002_m_000057_61: duration 0:00.000s
21/05/14 22:03:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321590002652994210791_0002_m_000057_61
[2021-05-14 19:03:49,375] {docker.py:276} INFO - 21/05/14 22:03:49 INFO Executor: Finished task 57.0 in stage 2.0 (TID 61). 4544 bytes result sent to driver
[2021-05-14 19:03:49,377] {docker.py:276} INFO - 21/05/14 22:03:49 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 64) (028692ec38a6, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:49,378] {docker.py:276} INFO - 21/05/14 22:03:49 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 61) in 1707 ms on 028692ec38a6 (executor driver) (57/200)
[2021-05-14 19:03:49,379] {docker.py:276} INFO - 21/05/14 22:03:49 INFO Executor: Running task 60.0 in stage 2.0 (TID 64)
[2021-05-14 19:03:49,388] {docker.py:276} INFO - 21/05/14 22:03:49 INFO ShuffleBlockFetcherIterator: Getting 3 (838.0 B) non-empty blocks including 3 (838.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:49,389] {docker.py:276} INFO - 21/05/14 22:03:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214005516760133495876_0002_m_000060_64, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214005516760133495876_0002_m_000060_64}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214005516760133495876_0002}; taskId=attempt_202105142203214005516760133495876_0002_m_000060_64, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53f814ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105142203214005516760133495876_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214005516760133495876_0002_m_000060_64
[2021-05-14 19:03:49,392] {docker.py:276} INFO - 21/05/14 22:03:49 INFO StagingCommitter: Task committer attempt_202105142203214005516760133495876_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214005516760133495876_0002_m_000060_64 : duration 0:00.002s
[2021-05-14 19:03:49,686] {docker.py:276} INFO - 21/05/14 22:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105142203215185272003589667339_0002_m_000056_60: needsTaskCommit() Task attempt_202105142203215185272003589667339_0002_m_000056_60
[2021-05-14 19:03:49,687] {docker.py:276} INFO - 21/05/14 22:03:49 INFO StagingCommitter: Task committer attempt_202105142203215185272003589667339_0002_m_000056_60: needsTaskCommit() Task attempt_202105142203215185272003589667339_0002_m_000056_60: duration 0:00.001s
21/05/14 22:03:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215185272003589667339_0002_m_000056_60
[2021-05-14 19:03:49,688] {docker.py:276} INFO - 21/05/14 22:03:49 INFO Executor: Finished task 56.0 in stage 2.0 (TID 60). 4544 bytes result sent to driver
[2021-05-14 19:03:49,689] {docker.py:276} INFO - 21/05/14 22:03:49 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 60) in 2188 ms on 028692ec38a6 (executor driver) (58/200)
[2021-05-14 19:03:49,690] {docker.py:276} INFO - 21/05/14 22:03:49 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 65) (028692ec38a6, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:49,691] {docker.py:276} INFO - 21/05/14 22:03:49 INFO Executor: Running task 61.0 in stage 2.0 (TID 65)
[2021-05-14 19:03:49,710] {docker.py:276} INFO - 21/05/14 22:03:49 INFO ShuffleBlockFetcherIterator: Getting 3 (861.0 B) non-empty blocks including 3 (861.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:49,712] {docker.py:276} INFO - 21/05/14 22:03:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:49,713] {docker.py:276} INFO - 21/05/14 22:03:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216964651507347861726_0002_m_000061_65, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216964651507347861726_0002_m_000061_65}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216964651507347861726_0002}; taskId=attempt_202105142203216964651507347861726_0002_m_000061_65, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9c8115a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:49,713] {docker.py:276} INFO - 21/05/14 22:03:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:49 INFO StagingCommitter: Starting: Task committer attempt_202105142203216964651507347861726_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216964651507347861726_0002_m_000061_65
[2021-05-14 19:03:49,716] {docker.py:276} INFO - 21/05/14 22:03:49 INFO StagingCommitter: Task committer attempt_202105142203216964651507347861726_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216964651507347861726_0002_m_000061_65 : duration 0:00.003s
[2021-05-14 19:03:50,182] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105142203211086795189291343443_0002_m_000058_62: needsTaskCommit() Task attempt_202105142203211086795189291343443_0002_m_000058_62
[2021-05-14 19:03:50,183] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Task committer attempt_202105142203211086795189291343443_0002_m_000058_62: needsTaskCommit() Task attempt_202105142203211086795189291343443_0002_m_000058_62: duration 0:00.001s
[2021-05-14 19:03:50,184] {docker.py:276} INFO - 21/05/14 22:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211086795189291343443_0002_m_000058_62
[2021-05-14 19:03:50,186] {docker.py:276} INFO - 21/05/14 22:03:50 INFO Executor: Finished task 58.0 in stage 2.0 (TID 62). 4587 bytes result sent to driver
[2021-05-14 19:03:50,189] {docker.py:276} INFO - 21/05/14 22:03:50 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 66) (028692ec38a6, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:50,190] {docker.py:276} INFO - 21/05/14 22:03:50 INFO Executor: Running task 62.0 in stage 2.0 (TID 66)
[2021-05-14 19:03:50,190] {docker.py:276} INFO - 21/05/14 22:03:50 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 62) in 1706 ms on 028692ec38a6 (executor driver) (59/200)
[2021-05-14 19:03:50,199] {docker.py:276} INFO - 21/05/14 22:03:50 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:50,202] {docker.py:276} INFO - 21/05/14 22:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211138291922659677774_0002_m_000062_66, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211138291922659677774_0002_m_000062_66}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211138291922659677774_0002}; taskId=attempt_202105142203211138291922659677774_0002_m_000062_66, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bdd13ed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:50,202] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105142203211138291922659677774_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211138291922659677774_0002_m_000062_66
[2021-05-14 19:03:50,205] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Task committer attempt_202105142203211138291922659677774_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211138291922659677774_0002_m_000062_66 : duration 0:00.003s
[2021-05-14 19:03:50,539] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Starting: Task committer attempt_20210514220321559226936027291173_0002_m_000059_63: needsTaskCommit() Task attempt_20210514220321559226936027291173_0002_m_000059_63
[2021-05-14 19:03:50,540] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Task committer attempt_20210514220321559226936027291173_0002_m_000059_63: needsTaskCommit() Task attempt_20210514220321559226936027291173_0002_m_000059_63: duration 0:00.001s
21/05/14 22:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321559226936027291173_0002_m_000059_63
[2021-05-14 19:03:50,541] {docker.py:276} INFO - 21/05/14 22:03:50 INFO Executor: Finished task 59.0 in stage 2.0 (TID 63). 4587 bytes result sent to driver
[2021-05-14 19:03:50,543] {docker.py:276} INFO - 21/05/14 22:03:50 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 67) (028692ec38a6, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:50,544] {docker.py:276} INFO - 21/05/14 22:03:50 INFO Executor: Running task 63.0 in stage 2.0 (TID 67)
21/05/14 22:03:50 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 63) in 1692 ms on 028692ec38a6 (executor driver) (60/200)
[2021-05-14 19:03:50,555] {docker.py:276} INFO - 21/05/14 22:03:50 INFO ShuffleBlockFetcherIterator: Getting 3 (888.0 B) non-empty blocks including 3 (888.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:50,557] {docker.py:276} INFO - 21/05/14 22:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215434256437162572386_0002_m_000063_67, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215434256437162572386_0002_m_000063_67}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215434256437162572386_0002}; taskId=attempt_202105142203215434256437162572386_0002_m_000063_67, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18234640}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:50,558] {docker.py:276} INFO - 21/05/14 22:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:50,558] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105142203215434256437162572386_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215434256437162572386_0002_m_000063_67
[2021-05-14 19:03:50,561] {docker.py:276} INFO - 21/05/14 22:03:50 INFO StagingCommitter: Task committer attempt_202105142203215434256437162572386_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215434256437162572386_0002_m_000063_67 : duration 0:00.004s
[2021-05-14 19:03:51,060] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105142203214005516760133495876_0002_m_000060_64: needsTaskCommit() Task attempt_202105142203214005516760133495876_0002_m_000060_64
[2021-05-14 19:03:51,060] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Task committer attempt_202105142203214005516760133495876_0002_m_000060_64: needsTaskCommit() Task attempt_202105142203214005516760133495876_0002_m_000060_64: duration 0:00.000s
21/05/14 22:03:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214005516760133495876_0002_m_000060_64
[2021-05-14 19:03:51,061] {docker.py:276} INFO - 21/05/14 22:03:51 INFO Executor: Finished task 60.0 in stage 2.0 (TID 64). 4587 bytes result sent to driver
[2021-05-14 19:03:51,063] {docker.py:276} INFO - 21/05/14 22:03:51 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 68) (028692ec38a6, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:51,064] {docker.py:276} INFO - 21/05/14 22:03:51 INFO Executor: Running task 64.0 in stage 2.0 (TID 68)
[2021-05-14 19:03:51,064] {docker.py:276} INFO - 21/05/14 22:03:51 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 64) in 1688 ms on 028692ec38a6 (executor driver) (61/200)
[2021-05-14 19:03:51,073] {docker.py:276} INFO - 21/05/14 22:03:51 INFO ShuffleBlockFetcherIterator: Getting 3 (972.0 B) non-empty blocks including 3 (972.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:51,075] {docker.py:276} INFO - 21/05/14 22:03:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:51,076] {docker.py:276} INFO - 21/05/14 22:03:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203219134493550419193706_0002_m_000064_68, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219134493550419193706_0002_m_000064_68}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203219134493550419193706_0002}; taskId=attempt_202105142203219134493550419193706_0002_m_000064_68, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@265ee743}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:51,076] {docker.py:276} INFO - 21/05/14 22:03:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:51,076] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105142203219134493550419193706_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219134493550419193706_0002_m_000064_68
[2021-05-14 19:03:51,079] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Task committer attempt_202105142203219134493550419193706_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219134493550419193706_0002_m_000064_68 : duration 0:00.003s
[2021-05-14 19:03:51,339] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105142203216964651507347861726_0002_m_000061_65: needsTaskCommit() Task attempt_202105142203216964651507347861726_0002_m_000061_65
[2021-05-14 19:03:51,340] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Task committer attempt_202105142203216964651507347861726_0002_m_000061_65: needsTaskCommit() Task attempt_202105142203216964651507347861726_0002_m_000061_65: duration 0:00.000s
21/05/14 22:03:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216964651507347861726_0002_m_000061_65
[2021-05-14 19:03:51,341] {docker.py:276} INFO - 21/05/14 22:03:51 INFO Executor: Finished task 61.0 in stage 2.0 (TID 65). 4587 bytes result sent to driver
[2021-05-14 19:03:51,343] {docker.py:276} INFO - 21/05/14 22:03:51 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 69) (028692ec38a6, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:51,344] {docker.py:276} INFO - 21/05/14 22:03:51 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 65) in 1655 ms on 028692ec38a6 (executor driver) (62/200)
[2021-05-14 19:03:51,345] {docker.py:276} INFO - 21/05/14 22:03:51 INFO Executor: Running task 65.0 in stage 2.0 (TID 69)
[2021-05-14 19:03:51,357] {docker.py:276} INFO - 21/05/14 22:03:51 INFO ShuffleBlockFetcherIterator: Getting 3 (1070.0 B) non-empty blocks including 3 (1070.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:51,360] {docker.py:276} INFO - 21/05/14 22:03:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:51,361] {docker.py:276} INFO - 21/05/14 22:03:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212522496935244399420_0002_m_000065_69, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212522496935244399420_0002_m_000065_69}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212522496935244399420_0002}; taskId=attempt_202105142203212522496935244399420_0002_m_000065_69, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44f5b677}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:51,361] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105142203212522496935244399420_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212522496935244399420_0002_m_000065_69
[2021-05-14 19:03:51,365] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Task committer attempt_202105142203212522496935244399420_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212522496935244399420_0002_m_000065_69 : duration 0:00.004s
[2021-05-14 19:03:51,891] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105142203211138291922659677774_0002_m_000062_66: needsTaskCommit() Task attempt_202105142203211138291922659677774_0002_m_000062_66
[2021-05-14 19:03:51,892] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Task committer attempt_202105142203211138291922659677774_0002_m_000062_66: needsTaskCommit() Task attempt_202105142203211138291922659677774_0002_m_000062_66: duration 0:00.000s
21/05/14 22:03:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211138291922659677774_0002_m_000062_66
[2021-05-14 19:03:51,892] {docker.py:276} INFO - 21/05/14 22:03:51 INFO Executor: Finished task 62.0 in stage 2.0 (TID 66). 4544 bytes result sent to driver
[2021-05-14 19:03:51,893] {docker.py:276} INFO - 21/05/14 22:03:51 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 70) (028692ec38a6, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:51,894] {docker.py:276} INFO - 21/05/14 22:03:51 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 66) in 1674 ms on 028692ec38a6 (executor driver) (63/200)
21/05/14 22:03:51 INFO Executor: Running task 66.0 in stage 2.0 (TID 70)
[2021-05-14 19:03:51,903] {docker.py:276} INFO - 21/05/14 22:03:51 INFO ShuffleBlockFetcherIterator: Getting 3 (953.0 B) non-empty blocks including 3 (953.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:51,905] {docker.py:276} INFO - 21/05/14 22:03:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:51,906] {docker.py:276} INFO - 21/05/14 22:03:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217476070539311922150_0002_m_000066_70, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217476070539311922150_0002_m_000066_70}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217476070539311922150_0002}; taskId=attempt_202105142203217476070539311922150_0002_m_000066_70, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@315394df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:51,906] {docker.py:276} INFO - 21/05/14 22:03:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:51,906] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Starting: Task committer attempt_202105142203217476070539311922150_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217476070539311922150_0002_m_000066_70
[2021-05-14 19:03:51,910] {docker.py:276} INFO - 21/05/14 22:03:51 INFO StagingCommitter: Task committer attempt_202105142203217476070539311922150_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217476070539311922150_0002_m_000066_70 : duration 0:00.005s
[2021-05-14 19:03:52,238] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105142203215434256437162572386_0002_m_000063_67: needsTaskCommit() Task attempt_202105142203215434256437162572386_0002_m_000063_67
[2021-05-14 19:03:52,238] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Task committer attempt_202105142203215434256437162572386_0002_m_000063_67: needsTaskCommit() Task attempt_202105142203215434256437162572386_0002_m_000063_67: duration 0:00.001s
21/05/14 22:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215434256437162572386_0002_m_000063_67
[2021-05-14 19:03:52,240] {docker.py:276} INFO - 21/05/14 22:03:52 INFO Executor: Finished task 63.0 in stage 2.0 (TID 67). 4544 bytes result sent to driver
[2021-05-14 19:03:52,242] {docker.py:276} INFO - 21/05/14 22:03:52 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 71) (028692ec38a6, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:52,243] {docker.py:276} INFO - 21/05/14 22:03:52 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 67) in 1668 ms on 028692ec38a6 (executor driver) (64/200)
21/05/14 22:03:52 INFO Executor: Running task 67.0 in stage 2.0 (TID 71)
[2021-05-14 19:03:52,255] {docker.py:276} INFO - 21/05/14 22:03:52 INFO ShuffleBlockFetcherIterator: Getting 3 (808.0 B) non-empty blocks including 3 (808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:52,258] {docker.py:276} INFO - 21/05/14 22:03:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:52,258] {docker.py:276} INFO - 21/05/14 22:03:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216341177150290924437_0002_m_000067_71, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216341177150290924437_0002_m_000067_71}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216341177150290924437_0002}; taskId=attempt_202105142203216341177150290924437_0002_m_000067_71, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2194c3f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:52,259] {docker.py:276} INFO - 21/05/14 22:03:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:52,259] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105142203216341177150290924437_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216341177150290924437_0002_m_000067_71
[2021-05-14 19:03:52,261] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Task committer attempt_202105142203216341177150290924437_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216341177150290924437_0002_m_000067_71 : duration 0:00.003s
[2021-05-14 19:03:52,743] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105142203219134493550419193706_0002_m_000064_68: needsTaskCommit() Task attempt_202105142203219134493550419193706_0002_m_000064_68
[2021-05-14 19:03:52,743] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Task committer attempt_202105142203219134493550419193706_0002_m_000064_68: needsTaskCommit() Task attempt_202105142203219134493550419193706_0002_m_000064_68: duration 0:00.001s
[2021-05-14 19:03:52,744] {docker.py:276} INFO - 21/05/14 22:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203219134493550419193706_0002_m_000064_68
[2021-05-14 19:03:52,744] {docker.py:276} INFO - 21/05/14 22:03:52 INFO Executor: Finished task 64.0 in stage 2.0 (TID 68). 4544 bytes result sent to driver
[2021-05-14 19:03:52,746] {docker.py:276} INFO - 21/05/14 22:03:52 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 72) (028692ec38a6, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:52,746] {docker.py:276} INFO - 21/05/14 22:03:52 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 68) in 1651 ms on 028692ec38a6 (executor driver) (65/200)
[2021-05-14 19:03:52,747] {docker.py:276} INFO - 21/05/14 22:03:52 INFO Executor: Running task 68.0 in stage 2.0 (TID 72)
[2021-05-14 19:03:52,754] {docker.py:276} INFO - 21/05/14 22:03:52 INFO ShuffleBlockFetcherIterator: Getting 3 (948.0 B) non-empty blocks including 3 (948.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:52,756] {docker.py:276} INFO - 21/05/14 22:03:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:03:52,757] {docker.py:276} INFO - 21/05/14 22:03:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:52,757] {docker.py:276} INFO - 21/05/14 22:03:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:52,758] {docker.py:276} INFO - 21/05/14 22:03:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218044388673799839689_0002_m_000068_72, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218044388673799839689_0002_m_000068_72}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218044388673799839689_0002}; taskId=attempt_202105142203218044388673799839689_0002_m_000068_72, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c6ea22d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:52,758] {docker.py:276} INFO - 21/05/14 22:03:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:52,758] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105142203218044388673799839689_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218044388673799839689_0002_m_000068_72
[2021-05-14 19:03:52,761] {docker.py:276} INFO - 21/05/14 22:03:52 INFO StagingCommitter: Task committer attempt_202105142203218044388673799839689_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218044388673799839689_0002_m_000068_72 : duration 0:00.004s
[2021-05-14 19:03:53,058] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105142203212522496935244399420_0002_m_000065_69: needsTaskCommit() Task attempt_202105142203212522496935244399420_0002_m_000065_69
[2021-05-14 19:03:53,059] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Task committer attempt_202105142203212522496935244399420_0002_m_000065_69: needsTaskCommit() Task attempt_202105142203212522496935244399420_0002_m_000065_69: duration 0:00.001s
21/05/14 22:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212522496935244399420_0002_m_000065_69
[2021-05-14 19:03:53,062] {docker.py:276} INFO - 21/05/14 22:03:53 INFO Executor: Finished task 65.0 in stage 2.0 (TID 69). 4544 bytes result sent to driver
[2021-05-14 19:03:53,065] {docker.py:276} INFO - 21/05/14 22:03:53 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 73) (028692ec38a6, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:53,066] {docker.py:276} INFO - 21/05/14 22:03:53 INFO TaskSetManager: Finished task 65.0 in stage 2.0 (TID 69) in 1691 ms on 028692ec38a6 (executor driver) (66/200)
[2021-05-14 19:03:53,067] {docker.py:276} INFO - 21/05/14 22:03:53 INFO Executor: Running task 69.0 in stage 2.0 (TID 73)
[2021-05-14 19:03:53,076] {docker.py:276} INFO - 21/05/14 22:03:53 INFO ShuffleBlockFetcherIterator: Getting 3 (778.0 B) non-empty blocks including 3 (778.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:53,076] {docker.py:276} INFO - 21/05/14 22:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:53,077] {docker.py:276} INFO - 21/05/14 22:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:53,078] {docker.py:276} INFO - 21/05/14 22:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213611087068529976351_0002_m_000069_73, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213611087068529976351_0002_m_000069_73}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213611087068529976351_0002}; taskId=attempt_202105142203213611087068529976351_0002_m_000069_73, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bc0fdf8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:53,078] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105142203213611087068529976351_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213611087068529976351_0002_m_000069_73
[2021-05-14 19:03:53,081] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Task committer attempt_202105142203213611087068529976351_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213611087068529976351_0002_m_000069_73 : duration 0:00.003s
[2021-05-14 19:03:53,624] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105142203217476070539311922150_0002_m_000066_70: needsTaskCommit() Task attempt_202105142203217476070539311922150_0002_m_000066_70
[2021-05-14 19:03:53,624] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Task committer attempt_202105142203217476070539311922150_0002_m_000066_70: needsTaskCommit() Task attempt_202105142203217476070539311922150_0002_m_000066_70: duration 0:00.000s
21/05/14 22:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217476070539311922150_0002_m_000066_70
[2021-05-14 19:03:53,625] {docker.py:276} INFO - 21/05/14 22:03:53 INFO Executor: Finished task 66.0 in stage 2.0 (TID 70). 4544 bytes result sent to driver
[2021-05-14 19:03:53,626] {docker.py:276} INFO - 21/05/14 22:03:53 INFO TaskSetManager: Starting task 70.0 in stage 2.0 (TID 74) (028692ec38a6, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:53,627] {docker.py:276} INFO - 21/05/14 22:03:53 INFO TaskSetManager: Finished task 66.0 in stage 2.0 (TID 70) in 1736 ms on 028692ec38a6 (executor driver) (67/200)
[2021-05-14 19:03:53,628] {docker.py:276} INFO - 21/05/14 22:03:53 INFO Executor: Running task 70.0 in stage 2.0 (TID 74)
[2021-05-14 19:03:53,638] {docker.py:276} INFO - 21/05/14 22:03:53 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:53,639] {docker.py:276} INFO - 21/05/14 22:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:53,642] {docker.py:276} INFO - 21/05/14 22:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:53,642] {docker.py:276} INFO - 21/05/14 22:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218030158380505355504_0002_m_000070_74, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218030158380505355504_0002_m_000070_74}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218030158380505355504_0002}; taskId=attempt_202105142203218030158380505355504_0002_m_000070_74, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28c0fc84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:53,643] {docker.py:276} INFO - 21/05/14 22:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105142203218030158380505355504_0002_m_000070_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218030158380505355504_0002_m_000070_74
[2021-05-14 19:03:53,646] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Task committer attempt_202105142203218030158380505355504_0002_m_000070_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218030158380505355504_0002_m_000070_74 : duration 0:00.004s
[2021-05-14 19:03:53,920] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105142203216341177150290924437_0002_m_000067_71: needsTaskCommit() Task attempt_202105142203216341177150290924437_0002_m_000067_71
[2021-05-14 19:03:53,921] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Task committer attempt_202105142203216341177150290924437_0002_m_000067_71: needsTaskCommit() Task attempt_202105142203216341177150290924437_0002_m_000067_71: duration 0:00.001s
[2021-05-14 19:03:53,922] {docker.py:276} INFO - 21/05/14 22:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216341177150290924437_0002_m_000067_71
[2021-05-14 19:03:53,924] {docker.py:276} INFO - 21/05/14 22:03:53 INFO Executor: Finished task 67.0 in stage 2.0 (TID 71). 4544 bytes result sent to driver
[2021-05-14 19:03:53,925] {docker.py:276} INFO - 21/05/14 22:03:53 INFO TaskSetManager: Starting task 71.0 in stage 2.0 (TID 75) (028692ec38a6, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:53,927] {docker.py:276} INFO - 21/05/14 22:03:53 INFO Executor: Running task 71.0 in stage 2.0 (TID 75)
[2021-05-14 19:03:53,927] {docker.py:276} INFO - 21/05/14 22:03:53 INFO TaskSetManager: Finished task 67.0 in stage 2.0 (TID 71) in 1688 ms on 028692ec38a6 (executor driver) (68/200)
[2021-05-14 19:03:53,937] {docker.py:276} INFO - 21/05/14 22:03:53 INFO ShuffleBlockFetcherIterator: Getting 3 (861.0 B) non-empty blocks including 3 (861.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:53,938] {docker.py:276} INFO - 21/05/14 22:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:53,939] {docker.py:276} INFO - 21/05/14 22:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:53,940] {docker.py:276} INFO - 21/05/14 22:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218177684595779099557_0002_m_000071_75, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218177684595779099557_0002_m_000071_75}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218177684595779099557_0002}; taskId=attempt_202105142203218177684595779099557_0002_m_000071_75, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6667eff5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:53,940] {docker.py:276} INFO - 21/05/14 22:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:53,941] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105142203218177684595779099557_0002_m_000071_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218177684595779099557_0002_m_000071_75
[2021-05-14 19:03:53,944] {docker.py:276} INFO - 21/05/14 22:03:53 INFO StagingCommitter: Task committer attempt_202105142203218177684595779099557_0002_m_000071_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218177684595779099557_0002_m_000071_75 : duration 0:00.003s
[2021-05-14 19:03:54,499] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105142203218044388673799839689_0002_m_000068_72: needsTaskCommit() Task attempt_202105142203218044388673799839689_0002_m_000068_72
[2021-05-14 19:03:54,500] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Task committer attempt_202105142203218044388673799839689_0002_m_000068_72: needsTaskCommit() Task attempt_202105142203218044388673799839689_0002_m_000068_72: duration 0:00.000s
21/05/14 22:03:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218044388673799839689_0002_m_000068_72
[2021-05-14 19:03:54,503] {docker.py:276} INFO - 21/05/14 22:03:54 INFO Executor: Finished task 68.0 in stage 2.0 (TID 72). 4544 bytes result sent to driver
[2021-05-14 19:03:54,504] {docker.py:276} INFO - 21/05/14 22:03:54 INFO TaskSetManager: Starting task 72.0 in stage 2.0 (TID 76) (028692ec38a6, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:54,506] {docker.py:276} INFO - 21/05/14 22:03:54 INFO TaskSetManager: Finished task 68.0 in stage 2.0 (TID 72) in 1762 ms on 028692ec38a6 (executor driver) (69/200)
21/05/14 22:03:54 INFO Executor: Running task 72.0 in stage 2.0 (TID 76)
[2021-05-14 19:03:54,527] {docker.py:276} INFO - 21/05/14 22:03:54 INFO ShuffleBlockFetcherIterator: Getting 3 (942.0 B) non-empty blocks including 3 (942.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:54,529] {docker.py:276} INFO - 21/05/14 22:03:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:54,529] {docker.py:276} INFO - 21/05/14 22:03:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216190890023398299084_0002_m_000072_76, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216190890023398299084_0002_m_000072_76}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216190890023398299084_0002}; taskId=attempt_202105142203216190890023398299084_0002_m_000072_76, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64204aea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:54,530] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105142203216190890023398299084_0002_m_000072_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216190890023398299084_0002_m_000072_76
[2021-05-14 19:03:54,535] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Task committer attempt_202105142203216190890023398299084_0002_m_000072_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216190890023398299084_0002_m_000072_76 : duration 0:00.006s
[2021-05-14 19:03:54,766] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105142203213611087068529976351_0002_m_000069_73: needsTaskCommit() Task attempt_202105142203213611087068529976351_0002_m_000069_73
[2021-05-14 19:03:54,767] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Task committer attempt_202105142203213611087068529976351_0002_m_000069_73: needsTaskCommit() Task attempt_202105142203213611087068529976351_0002_m_000069_73: duration 0:00.001s
[2021-05-14 19:03:54,768] {docker.py:276} INFO - 21/05/14 22:03:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213611087068529976351_0002_m_000069_73
[2021-05-14 19:03:54,770] {docker.py:276} INFO - 21/05/14 22:03:54 INFO Executor: Finished task 69.0 in stage 2.0 (TID 73). 4587 bytes result sent to driver
[2021-05-14 19:03:54,772] {docker.py:276} INFO - 21/05/14 22:03:54 INFO TaskSetManager: Starting task 73.0 in stage 2.0 (TID 77) (028692ec38a6, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:54,773] {docker.py:276} INFO - 21/05/14 22:03:54 INFO TaskSetManager: Finished task 69.0 in stage 2.0 (TID 73) in 1711 ms on 028692ec38a6 (executor driver) (70/200)
[2021-05-14 19:03:54,773] {docker.py:276} INFO - 21/05/14 22:03:54 INFO Executor: Running task 73.0 in stage 2.0 (TID 77)
[2021-05-14 19:03:54,785] {docker.py:276} INFO - 21/05/14 22:03:54 INFO ShuffleBlockFetcherIterator: Getting 3 (942.0 B) non-empty blocks including 3 (942.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:54,786] {docker.py:276} INFO - 21/05/14 22:03:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:54,787] {docker.py:276} INFO - 21/05/14 22:03:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211265875126141273729_0002_m_000073_77, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211265875126141273729_0002_m_000073_77}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211265875126141273729_0002}; taskId=attempt_202105142203211265875126141273729_0002_m_000073_77, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@136618ca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105142203211265875126141273729_0002_m_000073_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211265875126141273729_0002_m_000073_77
[2021-05-14 19:03:54,791] {docker.py:276} INFO - 21/05/14 22:03:54 INFO StagingCommitter: Task committer attempt_202105142203211265875126141273729_0002_m_000073_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211265875126141273729_0002_m_000073_77 : duration 0:00.003s
[2021-05-14 19:03:55,320] {docker.py:276} INFO - 21/05/14 22:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105142203218030158380505355504_0002_m_000070_74: needsTaskCommit() Task attempt_202105142203218030158380505355504_0002_m_000070_74
[2021-05-14 19:03:55,321] {docker.py:276} INFO - 21/05/14 22:03:55 INFO StagingCommitter: Task committer attempt_202105142203218030158380505355504_0002_m_000070_74: needsTaskCommit() Task attempt_202105142203218030158380505355504_0002_m_000070_74: duration 0:00.001s
21/05/14 22:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218030158380505355504_0002_m_000070_74
[2021-05-14 19:03:55,323] {docker.py:276} INFO - 21/05/14 22:03:55 INFO Executor: Finished task 70.0 in stage 2.0 (TID 74). 4587 bytes result sent to driver
[2021-05-14 19:03:55,325] {docker.py:276} INFO - 21/05/14 22:03:55 INFO TaskSetManager: Starting task 74.0 in stage 2.0 (TID 78) (028692ec38a6, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:55,326] {docker.py:276} INFO - 21/05/14 22:03:55 INFO TaskSetManager: Finished task 70.0 in stage 2.0 (TID 74) in 1702 ms on 028692ec38a6 (executor driver) (71/200)
[2021-05-14 19:03:55,327] {docker.py:276} INFO - 21/05/14 22:03:55 INFO Executor: Running task 74.0 in stage 2.0 (TID 78)
[2021-05-14 19:03:55,336] {docker.py:276} INFO - 21/05/14 22:03:55 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:55,338] {docker.py:276} INFO - 21/05/14 22:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:55,339] {docker.py:276} INFO - 21/05/14 22:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218700258463958818803_0002_m_000074_78, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218700258463958818803_0002_m_000074_78}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218700258463958818803_0002}; taskId=attempt_202105142203218700258463958818803_0002_m_000074_78, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66712ff8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105142203218700258463958818803_0002_m_000074_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218700258463958818803_0002_m_000074_78
[2021-05-14 19:03:55,342] {docker.py:276} INFO - 21/05/14 22:03:55 INFO StagingCommitter: Task committer attempt_202105142203218700258463958818803_0002_m_000074_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218700258463958818803_0002_m_000074_78 : duration 0:00.004s
[2021-05-14 19:03:55,623] {docker.py:276} INFO - 21/05/14 22:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105142203218177684595779099557_0002_m_000071_75: needsTaskCommit() Task attempt_202105142203218177684595779099557_0002_m_000071_75
[2021-05-14 19:03:55,624] {docker.py:276} INFO - 21/05/14 22:03:55 INFO StagingCommitter: Task committer attempt_202105142203218177684595779099557_0002_m_000071_75: needsTaskCommit() Task attempt_202105142203218177684595779099557_0002_m_000071_75: duration 0:00.000s
21/05/14 22:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218177684595779099557_0002_m_000071_75
[2021-05-14 19:03:55,626] {docker.py:276} INFO - 21/05/14 22:03:55 INFO Executor: Finished task 71.0 in stage 2.0 (TID 75). 4587 bytes result sent to driver
[2021-05-14 19:03:55,628] {docker.py:276} INFO - 21/05/14 22:03:55 INFO TaskSetManager: Starting task 75.0 in stage 2.0 (TID 79) (028692ec38a6, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:55,629] {docker.py:276} INFO - 21/05/14 22:03:55 INFO TaskSetManager: Finished task 71.0 in stage 2.0 (TID 75) in 1706 ms on 028692ec38a6 (executor driver) (72/200)
[2021-05-14 19:03:55,631] {docker.py:276} INFO - 21/05/14 22:03:55 INFO Executor: Running task 75.0 in stage 2.0 (TID 79)
[2021-05-14 19:03:55,640] {docker.py:276} INFO - 21/05/14 22:03:55 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:55,642] {docker.py:276} INFO - 21/05/14 22:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217094878309137395718_0002_m_000075_79, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217094878309137395718_0002_m_000075_79}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217094878309137395718_0002}; taskId=attempt_202105142203217094878309137395718_0002_m_000075_79, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b22e33c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105142203217094878309137395718_0002_m_000075_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217094878309137395718_0002_m_000075_79
[2021-05-14 19:03:55,646] {docker.py:276} INFO - 21/05/14 22:03:55 INFO StagingCommitter: Task committer attempt_202105142203217094878309137395718_0002_m_000075_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217094878309137395718_0002_m_000075_79 : duration 0:00.004s
[2021-05-14 19:03:56,191] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105142203216190890023398299084_0002_m_000072_76: needsTaskCommit() Task attempt_202105142203216190890023398299084_0002_m_000072_76
[2021-05-14 19:03:56,192] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Task committer attempt_202105142203216190890023398299084_0002_m_000072_76: needsTaskCommit() Task attempt_202105142203216190890023398299084_0002_m_000072_76: duration 0:00.000s
[2021-05-14 19:03:56,192] {docker.py:276} INFO - 21/05/14 22:03:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216190890023398299084_0002_m_000072_76
[2021-05-14 19:03:56,194] {docker.py:276} INFO - 21/05/14 22:03:56 INFO Executor: Finished task 72.0 in stage 2.0 (TID 76). 4587 bytes result sent to driver
[2021-05-14 19:03:56,195] {docker.py:276} INFO - 21/05/14 22:03:56 INFO TaskSetManager: Starting task 76.0 in stage 2.0 (TID 80) (028692ec38a6, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:56,196] {docker.py:276} INFO - 21/05/14 22:03:56 INFO TaskSetManager: Finished task 72.0 in stage 2.0 (TID 76) in 1695 ms on 028692ec38a6 (executor driver) (73/200)
[2021-05-14 19:03:56,197] {docker.py:276} INFO - 21/05/14 22:03:56 INFO Executor: Running task 76.0 in stage 2.0 (TID 80)
[2021-05-14 19:03:56,205] {docker.py:276} INFO - 21/05/14 22:03:56 INFO ShuffleBlockFetcherIterator: Getting 3 (866.0 B) non-empty blocks including 3 (866.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:56,207] {docker.py:276} INFO - 21/05/14 22:03:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217423045195826452607_0002_m_000076_80, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217423045195826452607_0002_m_000076_80}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217423045195826452607_0002}; taskId=attempt_202105142203217423045195826452607_0002_m_000076_80, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f143b8d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:56,208] {docker.py:276} INFO - 21/05/14 22:03:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105142203217423045195826452607_0002_m_000076_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217423045195826452607_0002_m_000076_80
[2021-05-14 19:03:56,211] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Task committer attempt_202105142203217423045195826452607_0002_m_000076_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217423045195826452607_0002_m_000076_80 : duration 0:00.003s
[2021-05-14 19:03:56,436] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105142203211265875126141273729_0002_m_000073_77: needsTaskCommit() Task attempt_202105142203211265875126141273729_0002_m_000073_77
[2021-05-14 19:03:56,437] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Task committer attempt_202105142203211265875126141273729_0002_m_000073_77: needsTaskCommit() Task attempt_202105142203211265875126141273729_0002_m_000073_77: duration 0:00.001s
21/05/14 22:03:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211265875126141273729_0002_m_000073_77
[2021-05-14 19:03:56,438] {docker.py:276} INFO - 21/05/14 22:03:56 INFO Executor: Finished task 73.0 in stage 2.0 (TID 77). 4544 bytes result sent to driver
[2021-05-14 19:03:56,440] {docker.py:276} INFO - 21/05/14 22:03:56 INFO TaskSetManager: Starting task 77.0 in stage 2.0 (TID 81) (028692ec38a6, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:56,441] {docker.py:276} INFO - 21/05/14 22:03:56 INFO TaskSetManager: Finished task 73.0 in stage 2.0 (TID 77) in 1672 ms on 028692ec38a6 (executor driver) (74/200)
[2021-05-14 19:03:56,442] {docker.py:276} INFO - 21/05/14 22:03:56 INFO Executor: Running task 77.0 in stage 2.0 (TID 81)
[2021-05-14 19:03:56,452] {docker.py:276} INFO - 21/05/14 22:03:56 INFO ShuffleBlockFetcherIterator: Getting 3 (884.0 B) non-empty blocks including 3 (884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:56,452] {docker.py:276} INFO - 21/05/14 22:03:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:56,454] {docker.py:276} INFO - 21/05/14 22:03:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216064464573406114121_0002_m_000077_81, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216064464573406114121_0002_m_000077_81}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216064464573406114121_0002}; taskId=attempt_202105142203216064464573406114121_0002_m_000077_81, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8094890}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:56,454] {docker.py:276} INFO - 21/05/14 22:03:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105142203216064464573406114121_0002_m_000077_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216064464573406114121_0002_m_000077_81
[2021-05-14 19:03:56,457] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Task committer attempt_202105142203216064464573406114121_0002_m_000077_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216064464573406114121_0002_m_000077_81 : duration 0:00.003s
[2021-05-14 19:03:56,548] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105142203218700258463958818803_0002_m_000074_78: needsTaskCommit() Task attempt_202105142203218700258463958818803_0002_m_000074_78
[2021-05-14 19:03:56,549] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Task committer attempt_202105142203218700258463958818803_0002_m_000074_78: needsTaskCommit() Task attempt_202105142203218700258463958818803_0002_m_000074_78: duration 0:00.001s
21/05/14 22:03:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218700258463958818803_0002_m_000074_78
[2021-05-14 19:03:56,552] {docker.py:276} INFO - 21/05/14 22:03:56 INFO Executor: Finished task 74.0 in stage 2.0 (TID 78). 4544 bytes result sent to driver
[2021-05-14 19:03:56,553] {docker.py:276} INFO - 21/05/14 22:03:56 INFO TaskSetManager: Starting task 78.0 in stage 2.0 (TID 82) (028692ec38a6, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:56,553] {docker.py:276} INFO - 21/05/14 22:03:56 INFO Executor: Running task 78.0 in stage 2.0 (TID 82)
[2021-05-14 19:03:56,554] {docker.py:276} INFO - 21/05/14 22:03:56 INFO TaskSetManager: Finished task 74.0 in stage 2.0 (TID 78) in 1230 ms on 028692ec38a6 (executor driver) (75/200)
[2021-05-14 19:03:56,563] {docker.py:276} INFO - 21/05/14 22:03:56 INFO ShuffleBlockFetcherIterator: Getting 3 (932.0 B) non-empty blocks including 3 (932.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:56,563] {docker.py:276} INFO - 21/05/14 22:03:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:56,565] {docker.py:276} INFO - 21/05/14 22:03:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:56,565] {docker.py:276} INFO - 21/05/14 22:03:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:56,566] {docker.py:276} INFO - 21/05/14 22:03:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217456393153892147402_0002_m_000078_82, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217456393153892147402_0002_m_000078_82}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217456393153892147402_0002}; taskId=attempt_202105142203217456393153892147402_0002_m_000078_82, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cfd68f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:56,566] {docker.py:276} INFO - 21/05/14 22:03:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:56,567] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Starting: Task committer attempt_202105142203217456393153892147402_0002_m_000078_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217456393153892147402_0002_m_000078_82
[2021-05-14 19:03:56,569] {docker.py:276} INFO - 21/05/14 22:03:56 INFO StagingCommitter: Task committer attempt_202105142203217456393153892147402_0002_m_000078_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217456393153892147402_0002_m_000078_82 : duration 0:00.003s
[2021-05-14 19:03:57,304] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105142203217094878309137395718_0002_m_000075_79: needsTaskCommit() Task attempt_202105142203217094878309137395718_0002_m_000075_79
[2021-05-14 19:03:57,305] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Task committer attempt_202105142203217094878309137395718_0002_m_000075_79: needsTaskCommit() Task attempt_202105142203217094878309137395718_0002_m_000075_79: duration 0:00.001s
21/05/14 22:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217094878309137395718_0002_m_000075_79
[2021-05-14 19:03:57,307] {docker.py:276} INFO - 21/05/14 22:03:57 INFO Executor: Finished task 75.0 in stage 2.0 (TID 79). 4544 bytes result sent to driver
[2021-05-14 19:03:57,308] {docker.py:276} INFO - 21/05/14 22:03:57 INFO TaskSetManager: Starting task 79.0 in stage 2.0 (TID 83) (028692ec38a6, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:57,309] {docker.py:276} INFO - 21/05/14 22:03:57 INFO Executor: Running task 79.0 in stage 2.0 (TID 83)
[2021-05-14 19:03:57,310] {docker.py:276} INFO - 21/05/14 22:03:57 INFO TaskSetManager: Finished task 75.0 in stage 2.0 (TID 79) in 1684 ms on 028692ec38a6 (executor driver) (76/200)
[2021-05-14 19:03:57,318] {docker.py:276} INFO - 21/05/14 22:03:57 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:57,321] {docker.py:276} INFO - 21/05/14 22:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:57,321] {docker.py:276} INFO - 21/05/14 22:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214448332187321437855_0002_m_000079_83, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214448332187321437855_0002_m_000079_83}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214448332187321437855_0002}; taskId=attempt_202105142203214448332187321437855_0002_m_000079_83, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47c7d67d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105142203214448332187321437855_0002_m_000079_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214448332187321437855_0002_m_000079_83
[2021-05-14 19:03:57,326] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Task committer attempt_202105142203214448332187321437855_0002_m_000079_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214448332187321437855_0002_m_000079_83 : duration 0:00.005s
[2021-05-14 19:03:57,475] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105142203217423045195826452607_0002_m_000076_80: needsTaskCommit() Task attempt_202105142203217423045195826452607_0002_m_000076_80
[2021-05-14 19:03:57,475] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Task committer attempt_202105142203217423045195826452607_0002_m_000076_80: needsTaskCommit() Task attempt_202105142203217423045195826452607_0002_m_000076_80: duration 0:00.000s
21/05/14 22:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217423045195826452607_0002_m_000076_80
[2021-05-14 19:03:57,477] {docker.py:276} INFO - 21/05/14 22:03:57 INFO Executor: Finished task 76.0 in stage 2.0 (TID 80). 4544 bytes result sent to driver
[2021-05-14 19:03:57,478] {docker.py:276} INFO - 21/05/14 22:03:57 INFO TaskSetManager: Starting task 80.0 in stage 2.0 (TID 84) (028692ec38a6, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:57,479] {docker.py:276} INFO - 21/05/14 22:03:57 INFO Executor: Running task 80.0 in stage 2.0 (TID 84)
[2021-05-14 19:03:57,479] {docker.py:276} INFO - 21/05/14 22:03:57 INFO TaskSetManager: Finished task 76.0 in stage 2.0 (TID 80) in 1286 ms on 028692ec38a6 (executor driver) (77/200)
[2021-05-14 19:03:57,487] {docker.py:276} INFO - 21/05/14 22:03:57 INFO ShuffleBlockFetcherIterator: Getting 3 (859.0 B) non-empty blocks including 3 (859.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:57,489] {docker.py:276} INFO - 21/05/14 22:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212298676084057969148_0002_m_000080_84, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212298676084057969148_0002_m_000080_84}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212298676084057969148_0002}; taskId=attempt_202105142203212298676084057969148_0002_m_000080_84, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64fc93a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:57,490] {docker.py:276} INFO - 21/05/14 22:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105142203212298676084057969148_0002_m_000080_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212298676084057969148_0002_m_000080_84
[2021-05-14 19:03:57,493] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Task committer attempt_202105142203212298676084057969148_0002_m_000080_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212298676084057969148_0002_m_000080_84 : duration 0:00.003s
[2021-05-14 19:03:57,766] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105142203217456393153892147402_0002_m_000078_82: needsTaskCommit() Task attempt_202105142203217456393153892147402_0002_m_000078_82
[2021-05-14 19:03:57,767] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Task committer attempt_202105142203217456393153892147402_0002_m_000078_82: needsTaskCommit() Task attempt_202105142203217456393153892147402_0002_m_000078_82: duration 0:00.000s
21/05/14 22:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217456393153892147402_0002_m_000078_82
[2021-05-14 19:03:57,768] {docker.py:276} INFO - 21/05/14 22:03:57 INFO Executor: Finished task 78.0 in stage 2.0 (TID 82). 4544 bytes result sent to driver
[2021-05-14 19:03:57,770] {docker.py:276} INFO - 21/05/14 22:03:57 INFO TaskSetManager: Starting task 81.0 in stage 2.0 (TID 85) (028692ec38a6, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:57,771] {docker.py:276} INFO - 21/05/14 22:03:57 INFO TaskSetManager: Finished task 78.0 in stage 2.0 (TID 82) in 1221 ms on 028692ec38a6 (executor driver) (78/200)
[2021-05-14 19:03:57,772] {docker.py:276} INFO - 21/05/14 22:03:57 INFO Executor: Running task 81.0 in stage 2.0 (TID 85)
[2021-05-14 19:03:57,779] {docker.py:276} INFO - 21/05/14 22:03:57 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:57,782] {docker.py:276} INFO - 21/05/14 22:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211560177866416349685_0002_m_000081_85, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211560177866416349685_0002_m_000081_85}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211560177866416349685_0002}; taskId=attempt_202105142203211560177866416349685_0002_m_000081_85, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21b1573b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:57,782] {docker.py:276} INFO - 21/05/14 22:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105142203211560177866416349685_0002_m_000081_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211560177866416349685_0002_m_000081_85
[2021-05-14 19:03:57,784] {docker.py:276} INFO - 21/05/14 22:03:57 INFO StagingCommitter: Task committer attempt_202105142203211560177866416349685_0002_m_000081_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211560177866416349685_0002_m_000081_85 : duration 0:00.003s
[2021-05-14 19:03:58,095] {docker.py:276} INFO - 21/05/14 22:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105142203216064464573406114121_0002_m_000077_81: needsTaskCommit() Task attempt_202105142203216064464573406114121_0002_m_000077_81
[2021-05-14 19:03:58,096] {docker.py:276} INFO - 21/05/14 22:03:58 INFO StagingCommitter: Task committer attempt_202105142203216064464573406114121_0002_m_000077_81: needsTaskCommit() Task attempt_202105142203216064464573406114121_0002_m_000077_81: duration 0:00.001s
21/05/14 22:03:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216064464573406114121_0002_m_000077_81
[2021-05-14 19:03:58,100] {docker.py:276} INFO - 21/05/14 22:03:58 INFO Executor: Finished task 77.0 in stage 2.0 (TID 81). 4544 bytes result sent to driver
[2021-05-14 19:03:58,102] {docker.py:276} INFO - 21/05/14 22:03:58 INFO TaskSetManager: Starting task 82.0 in stage 2.0 (TID 86) (028692ec38a6, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:58,103] {docker.py:276} INFO - 21/05/14 22:03:58 INFO TaskSetManager: Finished task 77.0 in stage 2.0 (TID 81) in 1665 ms on 028692ec38a6 (executor driver) (79/200)
[2021-05-14 19:03:58,104] {docker.py:276} INFO - 21/05/14 22:03:58 INFO Executor: Running task 82.0 in stage 2.0 (TID 86)
[2021-05-14 19:03:58,126] {docker.py:276} INFO - 21/05/14 22:03:58 INFO ShuffleBlockFetcherIterator: Getting 3 (923.0 B) non-empty blocks including 3 (923.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:58,127] {docker.py:276} INFO - 21/05/14 22:03:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211813038914902161761_0002_m_000082_86, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211813038914902161761_0002_m_000082_86}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211813038914902161761_0002}; taskId=attempt_202105142203211813038914902161761_0002_m_000082_86, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14a8312b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:58,127] {docker.py:276} INFO - 21/05/14 22:03:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105142203211813038914902161761_0002_m_000082_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211813038914902161761_0002_m_000082_86
[2021-05-14 19:03:58,127] {docker.py:276} INFO - 21/05/14 22:03:58 INFO StagingCommitter: Task committer attempt_202105142203211813038914902161761_0002_m_000082_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211813038914902161761_0002_m_000082_86 : duration 0:00.003s
[2021-05-14 19:03:59,159] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203212298676084057969148_0002_m_000080_84: needsTaskCommit() Task attempt_202105142203212298676084057969148_0002_m_000080_84
[2021-05-14 19:03:59,160] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203212298676084057969148_0002_m_000080_84: needsTaskCommit() Task attempt_202105142203212298676084057969148_0002_m_000080_84: duration 0:00.000s
21/05/14 22:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212298676084057969148_0002_m_000080_84
[2021-05-14 19:03:59,161] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Finished task 80.0 in stage 2.0 (TID 84). 4544 bytes result sent to driver
[2021-05-14 19:03:59,162] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Starting task 83.0 in stage 2.0 (TID 87) (028692ec38a6, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:59,163] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Running task 83.0 in stage 2.0 (TID 87)
[2021-05-14 19:03:59,164] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Finished task 80.0 in stage 2.0 (TID 84) in 1688 ms on 028692ec38a6 (executor driver) (80/200)
[2021-05-14 19:03:59,172] {docker.py:276} INFO - 21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Getting 3 (948.0 B) non-empty blocks including 3 (948.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:59,175] {docker.py:276} INFO - 21/05/14 22:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211386393758331758042_0002_m_000083_87, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211386393758331758042_0002_m_000083_87}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211386393758331758042_0002}; taskId=attempt_202105142203211386393758331758042_0002_m_000083_87, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41c2e807}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:59,175] {docker.py:276} INFO - 21/05/14 22:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:03:59,176] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203211386393758331758042_0002_m_000083_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211386393758331758042_0002_m_000083_87
[2021-05-14 19:03:59,178] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203211386393758331758042_0002_m_000083_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211386393758331758042_0002_m_000083_87 : duration 0:00.004s
[2021-05-14 19:03:59,444] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203211560177866416349685_0002_m_000081_85: needsTaskCommit() Task attempt_202105142203211560177866416349685_0002_m_000081_85
21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203211560177866416349685_0002_m_000081_85: needsTaskCommit() Task attempt_202105142203211560177866416349685_0002_m_000081_85: duration 0:00.000s
21/05/14 22:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211560177866416349685_0002_m_000081_85
[2021-05-14 19:03:59,446] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Finished task 81.0 in stage 2.0 (TID 85). 4544 bytes result sent to driver
[2021-05-14 19:03:59,466] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Starting task 84.0 in stage 2.0 (TID 88) (028692ec38a6, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:03:59 INFO Executor: Running task 84.0 in stage 2.0 (TID 88)
21/05/14 22:03:59 INFO TaskSetManager: Finished task 81.0 in stage 2.0 (TID 85) in 1680 ms on 028692ec38a6 (executor driver) (81/200)
[2021-05-14 19:03:59,467] {docker.py:276} INFO - 21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Getting 3 (866.0 B) non-empty blocks including 3 (866.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:59,470] {docker.py:276} INFO - 21/05/14 22:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:59,470] {docker.py:276} INFO - 21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211478706198693925919_0002_m_000084_88, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211478706198693925919_0002_m_000084_88}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211478706198693925919_0002}; taskId=attempt_202105142203211478706198693925919_0002_m_000084_88, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1111430e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203211478706198693925919_0002_m_000084_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211478706198693925919_0002_m_000084_88
[2021-05-14 19:03:59,473] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203211478706198693925919_0002_m_000084_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211478706198693925919_0002_m_000084_88 : duration 0:00.003s
[2021-05-14 19:03:59,541] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203214448332187321437855_0002_m_000079_83: needsTaskCommit() Task attempt_202105142203214448332187321437855_0002_m_000079_83
[2021-05-14 19:03:59,542] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203214448332187321437855_0002_m_000079_83: needsTaskCommit() Task attempt_202105142203214448332187321437855_0002_m_000079_83: duration 0:00.001s
21/05/14 22:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214448332187321437855_0002_m_000079_83
[2021-05-14 19:03:59,545] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Finished task 79.0 in stage 2.0 (TID 83). 4587 bytes result sent to driver
[2021-05-14 19:03:59,546] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Starting task 85.0 in stage 2.0 (TID 89) (028692ec38a6, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:59,547] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Running task 85.0 in stage 2.0 (TID 89)
[2021-05-14 19:03:59,548] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Finished task 79.0 in stage 2.0 (TID 83) in 2241 ms on 028692ec38a6 (executor driver) (82/200)
[2021-05-14 19:03:59,557] {docker.py:276} INFO - 21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Getting 3 (912.0 B) non-empty blocks including 3 (912.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:59,557] {docker.py:276} INFO - 21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:59,559] {docker.py:276} INFO - 21/05/14 22:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213789138041187736121_0002_m_000085_89, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213789138041187736121_0002_m_000085_89}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213789138041187736121_0002}; taskId=attempt_202105142203213789138041187736121_0002_m_000085_89, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@158a747}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:03:59,560] {docker.py:276} INFO - 21/05/14 22:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203213789138041187736121_0002_m_000085_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213789138041187736121_0002_m_000085_89
[2021-05-14 19:03:59,562] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203213789138041187736121_0002_m_000085_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213789138041187736121_0002_m_000085_89 : duration 0:00.002s
[2021-05-14 19:03:59,805] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203211813038914902161761_0002_m_000082_86: needsTaskCommit() Task attempt_202105142203211813038914902161761_0002_m_000082_86
[2021-05-14 19:03:59,806] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203211813038914902161761_0002_m_000082_86: needsTaskCommit() Task attempt_202105142203211813038914902161761_0002_m_000082_86: duration 0:00.001s
21/05/14 22:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211813038914902161761_0002_m_000082_86
[2021-05-14 19:03:59,807] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Finished task 82.0 in stage 2.0 (TID 86). 4587 bytes result sent to driver
[2021-05-14 19:03:59,809] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Starting task 86.0 in stage 2.0 (TID 90) (028692ec38a6, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:03:59,809] {docker.py:276} INFO - 21/05/14 22:03:59 INFO Executor: Running task 86.0 in stage 2.0 (TID 90)
[2021-05-14 19:03:59,810] {docker.py:276} INFO - 21/05/14 22:03:59 INFO TaskSetManager: Finished task 82.0 in stage 2.0 (TID 86) in 1711 ms on 028692ec38a6 (executor driver) (83/200)
[2021-05-14 19:03:59,820] {docker.py:276} INFO - 21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Getting 3 (861.0 B) non-empty blocks including 3 (861.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:03:59,821] {docker.py:276} INFO - 21/05/14 22:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:03:59,822] {docker.py:276} INFO - 21/05/14 22:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:03:59,822] {docker.py:276} INFO - 21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217004900592723507143_0002_m_000086_90, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217004900592723507143_0002_m_000086_90}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217004900592723507143_0002}; taskId=attempt_202105142203217004900592723507143_0002_m_000086_90, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21161dfd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105142203217004900592723507143_0002_m_000086_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217004900592723507143_0002_m_000086_90
[2021-05-14 19:03:59,825] {docker.py:276} INFO - 21/05/14 22:03:59 INFO StagingCommitter: Task committer attempt_202105142203217004900592723507143_0002_m_000086_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217004900592723507143_0002_m_000086_90 : duration 0:00.003s
[2021-05-14 19:04:00,886] {docker.py:276} INFO - 21/05/14 22:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105142203211386393758331758042_0002_m_000083_87: needsTaskCommit() Task attempt_202105142203211386393758331758042_0002_m_000083_87
[2021-05-14 19:04:00,887] {docker.py:276} INFO - 21/05/14 22:04:00 INFO StagingCommitter: Task committer attempt_202105142203211386393758331758042_0002_m_000083_87: needsTaskCommit() Task attempt_202105142203211386393758331758042_0002_m_000083_87: duration 0:00.001s
21/05/14 22:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211386393758331758042_0002_m_000083_87
[2021-05-14 19:04:00,888] {docker.py:276} INFO - 21/05/14 22:04:00 INFO Executor: Finished task 83.0 in stage 2.0 (TID 87). 4587 bytes result sent to driver
[2021-05-14 19:04:00,890] {docker.py:276} INFO - 21/05/14 22:04:00 INFO TaskSetManager: Starting task 87.0 in stage 2.0 (TID 91) (028692ec38a6, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:00,892] {docker.py:276} INFO - 21/05/14 22:04:00 INFO Executor: Running task 87.0 in stage 2.0 (TID 91)
21/05/14 22:04:00 INFO TaskSetManager: Finished task 83.0 in stage 2.0 (TID 87) in 1731 ms on 028692ec38a6 (executor driver) (84/200)
[2021-05-14 19:04:00,902] {docker.py:276} INFO - 21/05/14 22:04:00 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:00,904] {docker.py:276} INFO - 21/05/14 22:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:00,904] {docker.py:276} INFO - 21/05/14 22:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211125356062580199137_0002_m_000087_91, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211125356062580199137_0002_m_000087_91}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211125356062580199137_0002}; taskId=attempt_202105142203211125356062580199137_0002_m_000087_91, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47b86d1f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:00,905] {docker.py:276} INFO - 21/05/14 22:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105142203211125356062580199137_0002_m_000087_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211125356062580199137_0002_m_000087_91
[2021-05-14 19:04:00,908] {docker.py:276} INFO - 21/05/14 22:04:00 INFO StagingCommitter: Task committer attempt_202105142203211125356062580199137_0002_m_000087_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211125356062580199137_0002_m_000087_91 : duration 0:00.003s
[2021-05-14 19:04:01,117] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105142203211478706198693925919_0002_m_000084_88: needsTaskCommit() Task attempt_202105142203211478706198693925919_0002_m_000084_88
[2021-05-14 19:04:01,118] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Task committer attempt_202105142203211478706198693925919_0002_m_000084_88: needsTaskCommit() Task attempt_202105142203211478706198693925919_0002_m_000084_88: duration 0:00.001s
21/05/14 22:04:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211478706198693925919_0002_m_000084_88
[2021-05-14 19:04:01,119] {docker.py:276} INFO - 21/05/14 22:04:01 INFO Executor: Finished task 84.0 in stage 2.0 (TID 88). 4587 bytes result sent to driver
[2021-05-14 19:04:01,120] {docker.py:276} INFO - 21/05/14 22:04:01 INFO TaskSetManager: Starting task 88.0 in stage 2.0 (TID 92) (028692ec38a6, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:01,122] {docker.py:276} INFO - 21/05/14 22:04:01 INFO TaskSetManager: Finished task 84.0 in stage 2.0 (TID 88) in 1677 ms on 028692ec38a6 (executor driver) (85/200)
21/05/14 22:04:01 INFO Executor: Running task 88.0 in stage 2.0 (TID 92)
[2021-05-14 19:04:01,130] {docker.py:276} INFO - 21/05/14 22:04:01 INFO ShuffleBlockFetcherIterator: Getting 3 (861.0 B) non-empty blocks including 3 (861.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:01,145] {docker.py:276} INFO - 21/05/14 22:04:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211971986220303444302_0002_m_000088_92, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211971986220303444302_0002_m_000088_92}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211971986220303444302_0002}; taskId=attempt_202105142203211971986220303444302_0002_m_000088_92, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73d11940}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:01,145] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105142203211971986220303444302_0002_m_000088_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211971986220303444302_0002_m_000088_92
[2021-05-14 19:04:01,145] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Task committer attempt_202105142203211971986220303444302_0002_m_000088_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211971986220303444302_0002_m_000088_92 : duration 0:00.003s
[2021-05-14 19:04:01,246] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105142203213789138041187736121_0002_m_000085_89: needsTaskCommit() Task attempt_202105142203213789138041187736121_0002_m_000085_89
[2021-05-14 19:04:01,246] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Task committer attempt_202105142203213789138041187736121_0002_m_000085_89: needsTaskCommit() Task attempt_202105142203213789138041187736121_0002_m_000085_89: duration 0:00.001s
21/05/14 22:04:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213789138041187736121_0002_m_000085_89
[2021-05-14 19:04:01,248] {docker.py:276} INFO - 21/05/14 22:04:01 INFO Executor: Finished task 85.0 in stage 2.0 (TID 89). 4544 bytes result sent to driver
[2021-05-14 19:04:01,250] {docker.py:276} INFO - 21/05/14 22:04:01 INFO TaskSetManager: Starting task 89.0 in stage 2.0 (TID 93) (028692ec38a6, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:01,251] {docker.py:276} INFO - 21/05/14 22:04:01 INFO TaskSetManager: Finished task 85.0 in stage 2.0 (TID 89) in 1707 ms on 028692ec38a6 (executor driver) (86/200)
[2021-05-14 19:04:01,252] {docker.py:276} INFO - 21/05/14 22:04:01 INFO Executor: Running task 89.0 in stage 2.0 (TID 93)
[2021-05-14 19:04:01,261] {docker.py:276} INFO - 21/05/14 22:04:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1024.0 B) non-empty blocks including 3 (1024.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:01,262] {docker.py:276} INFO - 21/05/14 22:04:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217659106255382736791_0002_m_000089_93, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217659106255382736791_0002_m_000089_93}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217659106255382736791_0002}; taskId=attempt_202105142203217659106255382736791_0002_m_000089_93, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e884d63}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:01,262] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105142203217659106255382736791_0002_m_000089_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217659106255382736791_0002_m_000089_93
[2021-05-14 19:04:01,266] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Task committer attempt_202105142203217659106255382736791_0002_m_000089_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217659106255382736791_0002_m_000089_93 : duration 0:00.003s
[2021-05-14 19:04:01,521] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105142203217004900592723507143_0002_m_000086_90: needsTaskCommit() Task attempt_202105142203217004900592723507143_0002_m_000086_90
[2021-05-14 19:04:01,523] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Task committer attempt_202105142203217004900592723507143_0002_m_000086_90: needsTaskCommit() Task attempt_202105142203217004900592723507143_0002_m_000086_90: duration 0:00.003s
21/05/14 22:04:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217004900592723507143_0002_m_000086_90
[2021-05-14 19:04:01,525] {docker.py:276} INFO - 21/05/14 22:04:01 INFO Executor: Finished task 86.0 in stage 2.0 (TID 90). 4544 bytes result sent to driver
[2021-05-14 19:04:01,526] {docker.py:276} INFO - 21/05/14 22:04:01 INFO TaskSetManager: Starting task 90.0 in stage 2.0 (TID 94) (028692ec38a6, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:01,527] {docker.py:276} INFO - 21/05/14 22:04:01 INFO TaskSetManager: Finished task 86.0 in stage 2.0 (TID 90) in 1720 ms on 028692ec38a6 (executor driver) (87/200)
21/05/14 22:04:01 INFO Executor: Running task 90.0 in stage 2.0 (TID 94)
[2021-05-14 19:04:01,537] {docker.py:276} INFO - 21/05/14 22:04:01 INFO ShuffleBlockFetcherIterator: Getting 3 (1083.0 B) non-empty blocks including 3 (1083.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:01,539] {docker.py:276} INFO - 21/05/14 22:04:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218878152664346882254_0002_m_000090_94, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218878152664346882254_0002_m_000090_94}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218878152664346882254_0002}; taskId=attempt_202105142203218878152664346882254_0002_m_000090_94, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f539f55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:01,540] {docker.py:276} INFO - 21/05/14 22:04:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:01 INFO StagingCommitter: Starting: Task committer attempt_202105142203218878152664346882254_0002_m_000090_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218878152664346882254_0002_m_000090_94
[2021-05-14 19:04:01,543] {docker.py:276} INFO - 21/05/14 22:04:01 INFO StagingCommitter: Task committer attempt_202105142203218878152664346882254_0002_m_000090_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218878152664346882254_0002_m_000090_94 : duration 0:00.003s
[2021-05-14 19:04:02,600] {docker.py:276} INFO - 21/05/14 22:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105142203211125356062580199137_0002_m_000087_91: needsTaskCommit() Task attempt_202105142203211125356062580199137_0002_m_000087_91
[2021-05-14 19:04:02,601] {docker.py:276} INFO - 21/05/14 22:04:02 INFO StagingCommitter: Task committer attempt_202105142203211125356062580199137_0002_m_000087_91: needsTaskCommit() Task attempt_202105142203211125356062580199137_0002_m_000087_91: duration 0:00.001s
[2021-05-14 19:04:02,602] {docker.py:276} INFO - 21/05/14 22:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211125356062580199137_0002_m_000087_91
[2021-05-14 19:04:02,603] {docker.py:276} INFO - 21/05/14 22:04:02 INFO Executor: Finished task 87.0 in stage 2.0 (TID 91). 4544 bytes result sent to driver
[2021-05-14 19:04:02,605] {docker.py:276} INFO - 21/05/14 22:04:02 INFO TaskSetManager: Starting task 91.0 in stage 2.0 (TID 95) (028692ec38a6, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:02,606] {docker.py:276} INFO - 21/05/14 22:04:02 INFO TaskSetManager: Finished task 87.0 in stage 2.0 (TID 91) in 1718 ms on 028692ec38a6 (executor driver) (88/200)
[2021-05-14 19:04:02,607] {docker.py:276} INFO - 21/05/14 22:04:02 INFO Executor: Running task 91.0 in stage 2.0 (TID 95)
[2021-05-14 19:04:02,618] {docker.py:276} INFO - 21/05/14 22:04:02 INFO ShuffleBlockFetcherIterator: Getting 3 (872.0 B) non-empty blocks including 3 (872.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:02,618] {docker.py:276} INFO - 21/05/14 22:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:02,620] {docker.py:276} INFO - 21/05/14 22:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:02,621] {docker.py:276} INFO - 21/05/14 22:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215431238076845065659_0002_m_000091_95, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215431238076845065659_0002_m_000091_95}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215431238076845065659_0002}; taskId=attempt_202105142203215431238076845065659_0002_m_000091_95, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15597bf2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105142203215431238076845065659_0002_m_000091_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215431238076845065659_0002_m_000091_95
[2021-05-14 19:04:02,624] {docker.py:276} INFO - 21/05/14 22:04:02 INFO StagingCommitter: Task committer attempt_202105142203215431238076845065659_0002_m_000091_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215431238076845065659_0002_m_000091_95 : duration 0:00.003s
[2021-05-14 19:04:02,895] {docker.py:276} INFO - 21/05/14 22:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105142203211971986220303444302_0002_m_000088_92: needsTaskCommit() Task attempt_202105142203211971986220303444302_0002_m_000088_92
[2021-05-14 19:04:02,896] {docker.py:276} INFO - 21/05/14 22:04:02 INFO StagingCommitter: Task committer attempt_202105142203211971986220303444302_0002_m_000088_92: needsTaskCommit() Task attempt_202105142203211971986220303444302_0002_m_000088_92: duration 0:00.000s
21/05/14 22:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211971986220303444302_0002_m_000088_92
[2021-05-14 19:04:02,898] {docker.py:276} INFO - 21/05/14 22:04:02 INFO Executor: Finished task 88.0 in stage 2.0 (TID 92). 4544 bytes result sent to driver
[2021-05-14 19:04:02,900] {docker.py:276} INFO - 21/05/14 22:04:02 INFO TaskSetManager: Finished task 88.0 in stage 2.0 (TID 92) in 1782 ms on 028692ec38a6 (executor driver) (89/200)
[2021-05-14 19:04:02,902] {docker.py:276} INFO - 21/05/14 22:04:02 INFO TaskSetManager: Starting task 92.0 in stage 2.0 (TID 96) (028692ec38a6, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:02,903] {docker.py:276} INFO - 21/05/14 22:04:02 INFO Executor: Running task 92.0 in stage 2.0 (TID 96)
[2021-05-14 19:04:02,914] {docker.py:276} INFO - 21/05/14 22:04:02 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:02,916] {docker.py:276} INFO - 21/05/14 22:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212276671465782063266_0002_m_000092_96, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212276671465782063266_0002_m_000092_96}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212276671465782063266_0002}; taskId=attempt_202105142203212276671465782063266_0002_m_000092_96, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10704c5f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105142203212276671465782063266_0002_m_000092_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212276671465782063266_0002_m_000092_96
[2021-05-14 19:04:02,920] {docker.py:276} INFO - 21/05/14 22:04:02 INFO StagingCommitter: Task committer attempt_202105142203212276671465782063266_0002_m_000092_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212276671465782063266_0002_m_000092_96 : duration 0:00.004s
[2021-05-14 19:04:03,180] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105142203217659106255382736791_0002_m_000089_93: needsTaskCommit() Task attempt_202105142203217659106255382736791_0002_m_000089_93
[2021-05-14 19:04:03,180] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Task committer attempt_202105142203217659106255382736791_0002_m_000089_93: needsTaskCommit() Task attempt_202105142203217659106255382736791_0002_m_000089_93: duration 0:00.001s
[2021-05-14 19:04:03,181] {docker.py:276} INFO - 21/05/14 22:04:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217659106255382736791_0002_m_000089_93
[2021-05-14 19:04:03,182] {docker.py:276} INFO - 21/05/14 22:04:03 INFO Executor: Finished task 89.0 in stage 2.0 (TID 93). 4544 bytes result sent to driver
[2021-05-14 19:04:03,183] {docker.py:276} INFO - 21/05/14 22:04:03 INFO TaskSetManager: Starting task 93.0 in stage 2.0 (TID 97) (028692ec38a6, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:03,185] {docker.py:276} INFO - 21/05/14 22:04:03 INFO TaskSetManager: Finished task 89.0 in stage 2.0 (TID 93) in 1938 ms on 028692ec38a6 (executor driver) (90/200)
21/05/14 22:04:03 INFO Executor: Running task 93.0 in stage 2.0 (TID 97)
[2021-05-14 19:04:03,194] {docker.py:276} INFO - 21/05/14 22:04:03 INFO ShuffleBlockFetcherIterator: Getting 3 (1006.0 B) non-empty blocks including 3 (1006.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:03,196] {docker.py:276} INFO - 21/05/14 22:04:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:03,196] {docker.py:276} INFO - 21/05/14 22:04:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:03,196] {docker.py:276} INFO - 21/05/14 22:04:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214845644283141727276_0002_m_000093_97, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214845644283141727276_0002_m_000093_97}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214845644283141727276_0002}; taskId=attempt_202105142203214845644283141727276_0002_m_000093_97, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5545d81a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105142203214845644283141727276_0002_m_000093_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214845644283141727276_0002_m_000093_97
[2021-05-14 19:04:03,200] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Task committer attempt_202105142203214845644283141727276_0002_m_000093_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214845644283141727276_0002_m_000093_97 : duration 0:00.003s
[2021-05-14 19:04:03,335] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105142203218878152664346882254_0002_m_000090_94: needsTaskCommit() Task attempt_202105142203218878152664346882254_0002_m_000090_94
[2021-05-14 19:04:03,336] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Task committer attempt_202105142203218878152664346882254_0002_m_000090_94: needsTaskCommit() Task attempt_202105142203218878152664346882254_0002_m_000090_94: duration 0:00.001s
21/05/14 22:04:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218878152664346882254_0002_m_000090_94
[2021-05-14 19:04:03,338] {docker.py:276} INFO - 21/05/14 22:04:03 INFO Executor: Finished task 90.0 in stage 2.0 (TID 94). 4544 bytes result sent to driver
[2021-05-14 19:04:03,339] {docker.py:276} INFO - 21/05/14 22:04:03 INFO TaskSetManager: Starting task 94.0 in stage 2.0 (TID 98) (028692ec38a6, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:03,341] {docker.py:276} INFO - 21/05/14 22:04:03 INFO TaskSetManager: Finished task 90.0 in stage 2.0 (TID 94) in 1817 ms on 028692ec38a6 (executor driver) (91/200)
21/05/14 22:04:03 INFO Executor: Running task 94.0 in stage 2.0 (TID 98)
[2021-05-14 19:04:03,350] {docker.py:276} INFO - 21/05/14 22:04:03 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:03,353] {docker.py:276} INFO - 21/05/14 22:04:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213351003889458369679_0002_m_000094_98, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213351003889458369679_0002_m_000094_98}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213351003889458369679_0002}; taskId=attempt_202105142203213351003889458369679_0002_m_000094_98, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c77fb33}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:03,353] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Starting: Task committer attempt_202105142203213351003889458369679_0002_m_000094_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213351003889458369679_0002_m_000094_98
[2021-05-14 19:04:03,356] {docker.py:276} INFO - 21/05/14 22:04:03 INFO StagingCommitter: Task committer attempt_202105142203213351003889458369679_0002_m_000094_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213351003889458369679_0002_m_000094_98 : duration 0:00.003s
[2021-05-14 19:04:04,294] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105142203215431238076845065659_0002_m_000091_95: needsTaskCommit() Task attempt_202105142203215431238076845065659_0002_m_000091_95
[2021-05-14 19:04:04,295] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Task committer attempt_202105142203215431238076845065659_0002_m_000091_95: needsTaskCommit() Task attempt_202105142203215431238076845065659_0002_m_000091_95: duration 0:00.001s
21/05/14 22:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215431238076845065659_0002_m_000091_95
[2021-05-14 19:04:04,301] {docker.py:276} INFO - 21/05/14 22:04:04 INFO Executor: Finished task 91.0 in stage 2.0 (TID 95). 4544 bytes result sent to driver
[2021-05-14 19:04:04,305] {docker.py:276} INFO - 21/05/14 22:04:04 INFO TaskSetManager: Starting task 95.0 in stage 2.0 (TID 99) (028692ec38a6, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:04,307] {docker.py:276} INFO - 21/05/14 22:04:04 INFO TaskSetManager: Finished task 91.0 in stage 2.0 (TID 95) in 1705 ms on 028692ec38a6 (executor driver) (92/200)
[2021-05-14 19:04:04,308] {docker.py:276} INFO - 21/05/14 22:04:04 INFO Executor: Running task 95.0 in stage 2.0 (TID 99)
[2021-05-14 19:04:04,318] {docker.py:276} INFO - 21/05/14 22:04:04 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:04,320] {docker.py:276} INFO - 21/05/14 22:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217341144767491448309_0002_m_000095_99, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217341144767491448309_0002_m_000095_99}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217341144767491448309_0002}; taskId=attempt_202105142203217341144767491448309_0002_m_000095_99, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49bc240a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105142203217341144767491448309_0002_m_000095_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217341144767491448309_0002_m_000095_99
[2021-05-14 19:04:04,324] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Task committer attempt_202105142203217341144767491448309_0002_m_000095_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217341144767491448309_0002_m_000095_99 : duration 0:00.003s
[2021-05-14 19:04:04,635] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105142203212276671465782063266_0002_m_000092_96: needsTaskCommit() Task attempt_202105142203212276671465782063266_0002_m_000092_96
[2021-05-14 19:04:04,636] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Task committer attempt_202105142203212276671465782063266_0002_m_000092_96: needsTaskCommit() Task attempt_202105142203212276671465782063266_0002_m_000092_96: duration 0:00.001s
21/05/14 22:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212276671465782063266_0002_m_000092_96
[2021-05-14 19:04:04,638] {docker.py:276} INFO - 21/05/14 22:04:04 INFO Executor: Finished task 92.0 in stage 2.0 (TID 96). 4587 bytes result sent to driver
[2021-05-14 19:04:04,639] {docker.py:276} INFO - 21/05/14 22:04:04 INFO TaskSetManager: Starting task 96.0 in stage 2.0 (TID 100) (028692ec38a6, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:04,640] {docker.py:276} INFO - 21/05/14 22:04:04 INFO TaskSetManager: Finished task 92.0 in stage 2.0 (TID 96) in 1741 ms on 028692ec38a6 (executor driver) (93/200)
[2021-05-14 19:04:04,640] {docker.py:276} INFO - 21/05/14 22:04:04 INFO Executor: Running task 96.0 in stage 2.0 (TID 100)
[2021-05-14 19:04:04,649] {docker.py:276} INFO - 21/05/14 22:04:04 INFO ShuffleBlockFetcherIterator: Getting 3 (859.0 B) non-empty blocks including 3 (859.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:04,651] {docker.py:276} INFO - 21/05/14 22:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:04,651] {docker.py:276} INFO - 21/05/14 22:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216387604801030657960_0002_m_000096_100, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216387604801030657960_0002_m_000096_100}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216387604801030657960_0002}; taskId=attempt_202105142203216387604801030657960_0002_m_000096_100, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@170544a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:04,652] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105142203216387604801030657960_0002_m_000096_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216387604801030657960_0002_m_000096_100
[2021-05-14 19:04:04,656] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Task committer attempt_202105142203216387604801030657960_0002_m_000096_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216387604801030657960_0002_m_000096_100 : duration 0:00.005s
[2021-05-14 19:04:04,899] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105142203214845644283141727276_0002_m_000093_97: needsTaskCommit() Task attempt_202105142203214845644283141727276_0002_m_000093_97
[2021-05-14 19:04:04,899] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Task committer attempt_202105142203214845644283141727276_0002_m_000093_97: needsTaskCommit() Task attempt_202105142203214845644283141727276_0002_m_000093_97: duration 0:00.000s
21/05/14 22:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214845644283141727276_0002_m_000093_97
[2021-05-14 19:04:04,900] {docker.py:276} INFO - 21/05/14 22:04:04 INFO Executor: Finished task 93.0 in stage 2.0 (TID 97). 4587 bytes result sent to driver
[2021-05-14 19:04:04,902] {docker.py:276} INFO - 21/05/14 22:04:04 INFO TaskSetManager: Starting task 97.0 in stage 2.0 (TID 101) (028692ec38a6, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:04,905] {docker.py:276} INFO - 21/05/14 22:04:04 INFO Executor: Running task 97.0 in stage 2.0 (TID 101)
21/05/14 22:04:04 INFO TaskSetManager: Finished task 93.0 in stage 2.0 (TID 97) in 1724 ms on 028692ec38a6 (executor driver) (94/200)
[2021-05-14 19:04:04,915] {docker.py:276} INFO - 21/05/14 22:04:04 INFO ShuffleBlockFetcherIterator: Getting 3 (972.0 B) non-empty blocks including 3 (972.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:04,917] {docker.py:276} INFO - 21/05/14 22:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216568943424636323623_0002_m_000097_101, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216568943424636323623_0002_m_000097_101}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216568943424636323623_0002}; taskId=attempt_202105142203216568943424636323623_0002_m_000097_101, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a3db95f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105142203216568943424636323623_0002_m_000097_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216568943424636323623_0002_m_000097_101
[2021-05-14 19:04:04,920] {docker.py:276} INFO - 21/05/14 22:04:04 INFO StagingCommitter: Task committer attempt_202105142203216568943424636323623_0002_m_000097_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216568943424636323623_0002_m_000097_101 : duration 0:00.003s
[2021-05-14 19:04:05,037] {docker.py:276} INFO - 21/05/14 22:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105142203213351003889458369679_0002_m_000094_98: needsTaskCommit() Task attempt_202105142203213351003889458369679_0002_m_000094_98
21/05/14 22:04:05 INFO StagingCommitter: Task committer attempt_202105142203213351003889458369679_0002_m_000094_98: needsTaskCommit() Task attempt_202105142203213351003889458369679_0002_m_000094_98: duration 0:00.001s
21/05/14 22:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203213351003889458369679_0002_m_000094_98
[2021-05-14 19:04:05,039] {docker.py:276} INFO - 21/05/14 22:04:05 INFO Executor: Finished task 94.0 in stage 2.0 (TID 98). 4587 bytes result sent to driver
[2021-05-14 19:04:05,041] {docker.py:276} INFO - 21/05/14 22:04:05 INFO TaskSetManager: Starting task 98.0 in stage 2.0 (TID 102) (028692ec38a6, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:05,041] {docker.py:276} INFO - 21/05/14 22:04:05 INFO Executor: Running task 98.0 in stage 2.0 (TID 102)
[2021-05-14 19:04:05,042] {docker.py:276} INFO - 21/05/14 22:04:05 INFO TaskSetManager: Finished task 94.0 in stage 2.0 (TID 98) in 1705 ms on 028692ec38a6 (executor driver) (95/200)
[2021-05-14 19:04:05,052] {docker.py:276} INFO - 21/05/14 22:04:05 INFO ShuffleBlockFetcherIterator: Getting 3 (856.0 B) non-empty blocks including 3 (856.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:05,052] {docker.py:276} INFO - 21/05/14 22:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:05,054] {docker.py:276} INFO - 21/05/14 22:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:05,054] {docker.py:276} INFO - 21/05/14 22:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:05,055] {docker.py:276} INFO - 21/05/14 22:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:05,055] {docker.py:276} INFO - 21/05/14 22:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217757649130039176477_0002_m_000098_102, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217757649130039176477_0002_m_000098_102}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217757649130039176477_0002}; taskId=attempt_202105142203217757649130039176477_0002_m_000098_102, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f3f910}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:05,055] {docker.py:276} INFO - 21/05/14 22:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105142203217757649130039176477_0002_m_000098_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217757649130039176477_0002_m_000098_102
[2021-05-14 19:04:05,058] {docker.py:276} INFO - 21/05/14 22:04:05 INFO StagingCommitter: Task committer attempt_202105142203217757649130039176477_0002_m_000098_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217757649130039176477_0002_m_000098_102 : duration 0:00.003s
[2021-05-14 19:04:06,011] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203217341144767491448309_0002_m_000095_99: needsTaskCommit() Task attempt_202105142203217341144767491448309_0002_m_000095_99
[2021-05-14 19:04:06,011] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203217341144767491448309_0002_m_000095_99: needsTaskCommit() Task attempt_202105142203217341144767491448309_0002_m_000095_99: duration 0:00.001s
21/05/14 22:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217341144767491448309_0002_m_000095_99
[2021-05-14 19:04:06,012] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Finished task 95.0 in stage 2.0 (TID 99). 4587 bytes result sent to driver
[2021-05-14 19:04:06,014] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Starting task 99.0 in stage 2.0 (TID 103) (028692ec38a6, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:06,014] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Running task 99.0 in stage 2.0 (TID 103)
[2021-05-14 19:04:06,016] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Finished task 95.0 in stage 2.0 (TID 99) in 1714 ms on 028692ec38a6 (executor driver) (96/200)
[2021-05-14 19:04:06,024] {docker.py:276} INFO - 21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Getting 3 (1036.0 B) non-empty blocks including 3 (1036.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:06,026] {docker.py:276} INFO - 21/05/14 22:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:06,026] {docker.py:276} INFO - 21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321241540310925514894_0002_m_000099_103, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321241540310925514894_0002_m_000099_103}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321241540310925514894_0002}; taskId=attempt_20210514220321241540310925514894_0002_m_000099_103, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5fc1e0b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_20210514220321241540310925514894_0002_m_000099_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321241540310925514894_0002_m_000099_103
[2021-05-14 19:04:06,030] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_20210514220321241540310925514894_0002_m_000099_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321241540310925514894_0002_m_000099_103 : duration 0:00.004s
[2021-05-14 19:04:06,316] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203216387604801030657960_0002_m_000096_100: needsTaskCommit() Task attempt_202105142203216387604801030657960_0002_m_000096_100
[2021-05-14 19:04:06,317] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203216387604801030657960_0002_m_000096_100: needsTaskCommit() Task attempt_202105142203216387604801030657960_0002_m_000096_100: duration 0:00.000s
21/05/14 22:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216387604801030657960_0002_m_000096_100
[2021-05-14 19:04:06,318] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Finished task 96.0 in stage 2.0 (TID 100). 4544 bytes result sent to driver
[2021-05-14 19:04:06,319] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Starting task 100.0 in stage 2.0 (TID 104) (028692ec38a6, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:06,320] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Running task 100.0 in stage 2.0 (TID 104)
[2021-05-14 19:04:06,321] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Finished task 96.0 in stage 2.0 (TID 100) in 1683 ms on 028692ec38a6 (executor driver) (97/200)
[2021-05-14 19:04:06,330] {docker.py:276} INFO - 21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:06,333] {docker.py:276} INFO - 21/05/14 22:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:06,333] {docker.py:276} INFO - 21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214652604698424818752_0002_m_000100_104, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214652604698424818752_0002_m_000100_104}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214652604698424818752_0002}; taskId=attempt_202105142203214652604698424818752_0002_m_000100_104, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54754d29}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203214652604698424818752_0002_m_000100_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214652604698424818752_0002_m_000100_104
[2021-05-14 19:04:06,337] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203214652604698424818752_0002_m_000100_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214652604698424818752_0002_m_000100_104 : duration 0:00.004s
[2021-05-14 19:04:06,588] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203216568943424636323623_0002_m_000097_101: needsTaskCommit() Task attempt_202105142203216568943424636323623_0002_m_000097_101
[2021-05-14 19:04:06,589] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203216568943424636323623_0002_m_000097_101: needsTaskCommit() Task attempt_202105142203216568943424636323623_0002_m_000097_101: duration 0:00.000s
21/05/14 22:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216568943424636323623_0002_m_000097_101
[2021-05-14 19:04:06,590] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Finished task 97.0 in stage 2.0 (TID 101). 4544 bytes result sent to driver
[2021-05-14 19:04:06,591] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Starting task 101.0 in stage 2.0 (TID 105) (028692ec38a6, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:06,592] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Finished task 97.0 in stage 2.0 (TID 101) in 1692 ms on 028692ec38a6 (executor driver) (98/200)
[2021-05-14 19:04:06,593] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Running task 101.0 in stage 2.0 (TID 105)
[2021-05-14 19:04:06,602] {docker.py:276} INFO - 21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Getting 3 (942.0 B) non-empty blocks including 3 (942.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:06,604] {docker.py:276} INFO - 21/05/14 22:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218635766264535677573_0002_m_000101_105, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218635766264535677573_0002_m_000101_105}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218635766264535677573_0002}; taskId=attempt_202105142203218635766264535677573_0002_m_000101_105, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59815ca8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:06,604] {docker.py:276} INFO - 21/05/14 22:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:06,605] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203218635766264535677573_0002_m_000101_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218635766264535677573_0002_m_000101_105
[2021-05-14 19:04:06,608] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203218635766264535677573_0002_m_000101_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218635766264535677573_0002_m_000101_105 : duration 0:00.003s
[2021-05-14 19:04:06,719] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203217757649130039176477_0002_m_000098_102: needsTaskCommit() Task attempt_202105142203217757649130039176477_0002_m_000098_102
[2021-05-14 19:04:06,720] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203217757649130039176477_0002_m_000098_102: needsTaskCommit() Task attempt_202105142203217757649130039176477_0002_m_000098_102: duration 0:00.000s
21/05/14 22:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217757649130039176477_0002_m_000098_102
[2021-05-14 19:04:06,721] {docker.py:276} INFO - 21/05/14 22:04:06 INFO Executor: Finished task 98.0 in stage 2.0 (TID 102). 4544 bytes result sent to driver
[2021-05-14 19:04:06,723] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Starting task 102.0 in stage 2.0 (TID 106) (028692ec38a6, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:06,724] {docker.py:276} INFO - 21/05/14 22:04:06 INFO TaskSetManager: Finished task 98.0 in stage 2.0 (TID 102) in 1685 ms on 028692ec38a6 (executor driver) (99/200)
21/05/14 22:04:06 INFO Executor: Running task 102.0 in stage 2.0 (TID 106)
[2021-05-14 19:04:06,734] {docker.py:276} INFO - 21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Getting 3 (923.0 B) non-empty blocks including 3 (923.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:06,734] {docker.py:276} INFO - 21/05/14 22:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:06,736] {docker.py:276} INFO - 21/05/14 22:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:06,736] {docker.py:276} INFO - 21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:06,737] {docker.py:276} INFO - 21/05/14 22:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217661712275870609430_0002_m_000102_106, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217661712275870609430_0002_m_000102_106}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217661712275870609430_0002}; taskId=attempt_202105142203217661712275870609430_0002_m_000102_106, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2969fe4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:06,737] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105142203217661712275870609430_0002_m_000102_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217661712275870609430_0002_m_000102_106
[2021-05-14 19:04:06,740] {docker.py:276} INFO - 21/05/14 22:04:06 INFO StagingCommitter: Task committer attempt_202105142203217661712275870609430_0002_m_000102_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217661712275870609430_0002_m_000102_106 : duration 0:00.004s
[2021-05-14 19:04:07,697] {docker.py:276} INFO - 21/05/14 22:04:07 INFO StagingCommitter: Starting: Task committer attempt_20210514220321241540310925514894_0002_m_000099_103: needsTaskCommit() Task attempt_20210514220321241540310925514894_0002_m_000099_103
[2021-05-14 19:04:07,697] {docker.py:276} INFO - 21/05/14 22:04:07 INFO StagingCommitter: Task committer attempt_20210514220321241540310925514894_0002_m_000099_103: needsTaskCommit() Task attempt_20210514220321241540310925514894_0002_m_000099_103: duration 0:00.001s
[2021-05-14 19:04:07,698] {docker.py:276} INFO - 21/05/14 22:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321241540310925514894_0002_m_000099_103
[2021-05-14 19:04:07,699] {docker.py:276} INFO - 21/05/14 22:04:07 INFO Executor: Finished task 99.0 in stage 2.0 (TID 103). 4544 bytes result sent to driver
[2021-05-14 19:04:07,700] {docker.py:276} INFO - 21/05/14 22:04:07 INFO TaskSetManager: Starting task 103.0 in stage 2.0 (TID 107) (028692ec38a6, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:07,701] {docker.py:276} INFO - 21/05/14 22:04:07 INFO TaskSetManager: Finished task 99.0 in stage 2.0 (TID 103) in 1690 ms on 028692ec38a6 (executor driver) (100/200)
[2021-05-14 19:04:07,702] {docker.py:276} INFO - 21/05/14 22:04:07 INFO Executor: Running task 103.0 in stage 2.0 (TID 107)
[2021-05-14 19:04:07,710] {docker.py:276} INFO - 21/05/14 22:04:07 INFO ShuffleBlockFetcherIterator: Getting 3 (976.0 B) non-empty blocks including 3 (976.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:07,711] {docker.py:276} INFO - 21/05/14 22:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214489585852150094379_0002_m_000103_107, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214489585852150094379_0002_m_000103_107}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214489585852150094379_0002}; taskId=attempt_202105142203214489585852150094379_0002_m_000103_107, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20755955}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105142203214489585852150094379_0002_m_000103_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214489585852150094379_0002_m_000103_107
[2021-05-14 19:04:07,714] {docker.py:276} INFO - 21/05/14 22:04:07 INFO StagingCommitter: Task committer attempt_202105142203214489585852150094379_0002_m_000103_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214489585852150094379_0002_m_000103_107 : duration 0:00.003s
[2021-05-14 19:04:08,101] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105142203214652604698424818752_0002_m_000100_104: needsTaskCommit() Task attempt_202105142203214652604698424818752_0002_m_000100_104
[2021-05-14 19:04:08,102] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Task committer attempt_202105142203214652604698424818752_0002_m_000100_104: needsTaskCommit() Task attempt_202105142203214652604698424818752_0002_m_000100_104: duration 0:00.001s
21/05/14 22:04:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214652604698424818752_0002_m_000100_104
[2021-05-14 19:04:08,103] {docker.py:276} INFO - 21/05/14 22:04:08 INFO Executor: Finished task 100.0 in stage 2.0 (TID 104). 4544 bytes result sent to driver
[2021-05-14 19:04:08,105] {docker.py:276} INFO - 21/05/14 22:04:08 INFO TaskSetManager: Starting task 104.0 in stage 2.0 (TID 108) (028692ec38a6, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:08,106] {docker.py:276} INFO - 21/05/14 22:04:08 INFO Executor: Running task 104.0 in stage 2.0 (TID 108)
[2021-05-14 19:04:08,106] {docker.py:276} INFO - 21/05/14 22:04:08 INFO TaskSetManager: Finished task 100.0 in stage 2.0 (TID 104) in 1789 ms on 028692ec38a6 (executor driver) (101/200)
[2021-05-14 19:04:08,115] {docker.py:276} INFO - 21/05/14 22:04:08 INFO ShuffleBlockFetcherIterator: Getting 3 (836.0 B) non-empty blocks including 3 (836.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:08,116] {docker.py:276} INFO - 21/05/14 22:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:08,117] {docker.py:276} INFO - 21/05/14 22:04:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321192995857535312051_0002_m_000104_108, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321192995857535312051_0002_m_000104_108}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321192995857535312051_0002}; taskId=attempt_20210514220321192995857535312051_0002_m_000104_108, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@137f09a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:08,117] {docker.py:276} INFO - 21/05/14 22:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:08,118] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Starting: Task committer attempt_20210514220321192995857535312051_0002_m_000104_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321192995857535312051_0002_m_000104_108
[2021-05-14 19:04:08,121] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Task committer attempt_20210514220321192995857535312051_0002_m_000104_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321192995857535312051_0002_m_000104_108 : duration 0:00.004s
[2021-05-14 19:04:08,320] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105142203218635766264535677573_0002_m_000101_105: needsTaskCommit() Task attempt_202105142203218635766264535677573_0002_m_000101_105
[2021-05-14 19:04:08,320] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Task committer attempt_202105142203218635766264535677573_0002_m_000101_105: needsTaskCommit() Task attempt_202105142203218635766264535677573_0002_m_000101_105: duration 0:00.000s
21/05/14 22:04:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218635766264535677573_0002_m_000101_105
[2021-05-14 19:04:08,321] {docker.py:276} INFO - 21/05/14 22:04:08 INFO Executor: Finished task 101.0 in stage 2.0 (TID 105). 4544 bytes result sent to driver
[2021-05-14 19:04:08,322] {docker.py:276} INFO - 21/05/14 22:04:08 INFO TaskSetManager: Starting task 105.0 in stage 2.0 (TID 109) (028692ec38a6, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:08,324] {docker.py:276} INFO - 21/05/14 22:04:08 INFO TaskSetManager: Finished task 101.0 in stage 2.0 (TID 105) in 1735 ms on 028692ec38a6 (executor driver) (102/200)
[2021-05-14 19:04:08,325] {docker.py:276} INFO - 21/05/14 22:04:08 INFO Executor: Running task 105.0 in stage 2.0 (TID 109)
[2021-05-14 19:04:08,336] {docker.py:276} INFO - 21/05/14 22:04:08 INFO ShuffleBlockFetcherIterator: Getting 3 (1043.0 B) non-empty blocks including 3 (1043.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:08,338] {docker.py:276} INFO - 21/05/14 22:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:08,339] {docker.py:276} INFO - 21/05/14 22:04:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214717984986531680734_0002_m_000105_109, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214717984986531680734_0002_m_000105_109}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214717984986531680734_0002}; taskId=attempt_202105142203214717984986531680734_0002_m_000105_109, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@aaac6aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105142203214717984986531680734_0002_m_000105_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214717984986531680734_0002_m_000105_109
[2021-05-14 19:04:08,344] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Task committer attempt_202105142203214717984986531680734_0002_m_000105_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214717984986531680734_0002_m_000105_109 : duration 0:00.006s
[2021-05-14 19:04:08,443] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105142203217661712275870609430_0002_m_000102_106: needsTaskCommit() Task attempt_202105142203217661712275870609430_0002_m_000102_106
[2021-05-14 19:04:08,444] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Task committer attempt_202105142203217661712275870609430_0002_m_000102_106: needsTaskCommit() Task attempt_202105142203217661712275870609430_0002_m_000102_106: duration 0:00.000s
21/05/14 22:04:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217661712275870609430_0002_m_000102_106
[2021-05-14 19:04:08,446] {docker.py:276} INFO - 21/05/14 22:04:08 INFO Executor: Finished task 102.0 in stage 2.0 (TID 106). 4544 bytes result sent to driver
[2021-05-14 19:04:08,449] {docker.py:276} INFO - 21/05/14 22:04:08 INFO TaskSetManager: Starting task 106.0 in stage 2.0 (TID 110) (028692ec38a6, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:08,450] {docker.py:276} INFO - 21/05/14 22:04:08 INFO TaskSetManager: Finished task 102.0 in stage 2.0 (TID 106) in 1730 ms on 028692ec38a6 (executor driver) (103/200)
[2021-05-14 19:04:08,451] {docker.py:276} INFO - 21/05/14 22:04:08 INFO Executor: Running task 106.0 in stage 2.0 (TID 110)
[2021-05-14 19:04:08,461] {docker.py:276} INFO - 21/05/14 22:04:08 INFO ShuffleBlockFetcherIterator: Getting 3 (866.0 B) non-empty blocks including 3 (866.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:08,462] {docker.py:276} INFO - 21/05/14 22:04:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:08,463] {docker.py:276} INFO - 21/05/14 22:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212076991274387494776_0002_m_000106_110, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212076991274387494776_0002_m_000106_110}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212076991274387494776_0002}; taskId=attempt_202105142203212076991274387494776_0002_m_000106_110, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54c58df4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:08,464] {docker.py:276} INFO - 21/05/14 22:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:08 INFO StagingCommitter: Starting: Task committer attempt_202105142203212076991274387494776_0002_m_000106_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212076991274387494776_0002_m_000106_110
[2021-05-14 19:04:08,467] {docker.py:276} INFO - 21/05/14 22:04:08 INFO StagingCommitter: Task committer attempt_202105142203212076991274387494776_0002_m_000106_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212076991274387494776_0002_m_000106_110 : duration 0:00.002s
[2021-05-14 19:04:09,388] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105142203214489585852150094379_0002_m_000103_107: needsTaskCommit() Task attempt_202105142203214489585852150094379_0002_m_000103_107
[2021-05-14 19:04:09,389] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Task committer attempt_202105142203214489585852150094379_0002_m_000103_107: needsTaskCommit() Task attempt_202105142203214489585852150094379_0002_m_000103_107: duration 0:00.001s
21/05/14 22:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214489585852150094379_0002_m_000103_107
[2021-05-14 19:04:09,389] {docker.py:276} INFO - 21/05/14 22:04:09 INFO Executor: Finished task 103.0 in stage 2.0 (TID 107). 4544 bytes result sent to driver
[2021-05-14 19:04:09,390] {docker.py:276} INFO - 21/05/14 22:04:09 INFO TaskSetManager: Starting task 107.0 in stage 2.0 (TID 111) (028692ec38a6, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:09,391] {docker.py:276} INFO - 21/05/14 22:04:09 INFO TaskSetManager: Finished task 103.0 in stage 2.0 (TID 107) in 1692 ms on 028692ec38a6 (executor driver) (104/200)
[2021-05-14 19:04:09,392] {docker.py:276} INFO - 21/05/14 22:04:09 INFO Executor: Running task 107.0 in stage 2.0 (TID 111)
[2021-05-14 19:04:09,412] {docker.py:276} INFO - 21/05/14 22:04:09 INFO ShuffleBlockFetcherIterator: Getting 3 (896.0 B) non-empty blocks including 3 (896.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:09,413] {docker.py:276} INFO - 21/05/14 22:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216694773076752974406_0002_m_000107_111, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216694773076752974406_0002_m_000107_111}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216694773076752974406_0002}; taskId=attempt_202105142203216694773076752974406_0002_m_000107_111, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79573468}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105142203216694773076752974406_0002_m_000107_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216694773076752974406_0002_m_000107_111
[2021-05-14 19:04:09,416] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Task committer attempt_202105142203216694773076752974406_0002_m_000107_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216694773076752974406_0002_m_000107_111 : duration 0:00.003s
[2021-05-14 19:04:09,567] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105142203214717984986531680734_0002_m_000105_109: needsTaskCommit() Task attempt_202105142203214717984986531680734_0002_m_000105_109
[2021-05-14 19:04:09,568] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Task committer attempt_202105142203214717984986531680734_0002_m_000105_109: needsTaskCommit() Task attempt_202105142203214717984986531680734_0002_m_000105_109: duration 0:00.000s
21/05/14 22:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214717984986531680734_0002_m_000105_109
[2021-05-14 19:04:09,568] {docker.py:276} INFO - 21/05/14 22:04:09 INFO Executor: Finished task 105.0 in stage 2.0 (TID 109). 4587 bytes result sent to driver
[2021-05-14 19:04:09,570] {docker.py:276} INFO - 21/05/14 22:04:09 INFO TaskSetManager: Starting task 108.0 in stage 2.0 (TID 112) (028692ec38a6, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:09,571] {docker.py:276} INFO - 21/05/14 22:04:09 INFO TaskSetManager: Finished task 105.0 in stage 2.0 (TID 109) in 1250 ms on 028692ec38a6 (executor driver) (105/200)
21/05/14 22:04:09 INFO Executor: Running task 108.0 in stage 2.0 (TID 112)
[2021-05-14 19:04:09,580] {docker.py:276} INFO - 21/05/14 22:04:09 INFO ShuffleBlockFetcherIterator: Getting 3 (859.0 B) non-empty blocks including 3 (859.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:09,581] {docker.py:276} INFO - 21/05/14 22:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:09,583] {docker.py:276} INFO - 21/05/14 22:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:09,584] {docker.py:276} INFO - 21/05/14 22:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:09,584] {docker.py:276} INFO - 21/05/14 22:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:09,585] {docker.py:276} INFO - 21/05/14 22:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218268015458426766707_0002_m_000108_112, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218268015458426766707_0002_m_000108_112}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218268015458426766707_0002}; taskId=attempt_202105142203218268015458426766707_0002_m_000108_112, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50d59682}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:09,585] {docker.py:276} INFO - 21/05/14 22:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105142203218268015458426766707_0002_m_000108_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218268015458426766707_0002_m_000108_112
[2021-05-14 19:04:09,588] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Task committer attempt_202105142203218268015458426766707_0002_m_000108_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218268015458426766707_0002_m_000108_112 : duration 0:00.004s
[2021-05-14 19:04:09,861] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Starting: Task committer attempt_20210514220321192995857535312051_0002_m_000104_108: needsTaskCommit() Task attempt_20210514220321192995857535312051_0002_m_000104_108
[2021-05-14 19:04:09,861] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Task committer attempt_20210514220321192995857535312051_0002_m_000104_108: needsTaskCommit() Task attempt_20210514220321192995857535312051_0002_m_000104_108: duration 0:00.001s
[2021-05-14 19:04:09,862] {docker.py:276} INFO - 21/05/14 22:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321192995857535312051_0002_m_000104_108
[2021-05-14 19:04:09,863] {docker.py:276} INFO - 21/05/14 22:04:09 INFO Executor: Finished task 104.0 in stage 2.0 (TID 108). 4587 bytes result sent to driver
[2021-05-14 19:04:09,865] {docker.py:276} INFO - 21/05/14 22:04:09 INFO TaskSetManager: Starting task 109.0 in stage 2.0 (TID 113) (028692ec38a6, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:09,865] {docker.py:276} INFO - 21/05/14 22:04:09 INFO Executor: Running task 109.0 in stage 2.0 (TID 113)
[2021-05-14 19:04:09,866] {docker.py:276} INFO - 21/05/14 22:04:09 INFO TaskSetManager: Finished task 104.0 in stage 2.0 (TID 108) in 1762 ms on 028692ec38a6 (executor driver) (106/200)
[2021-05-14 19:04:09,875] {docker.py:276} INFO - 21/05/14 22:04:09 INFO ShuffleBlockFetcherIterator: Getting 3 (976.0 B) non-empty blocks including 3 (976.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:09,878] {docker.py:276} INFO - 21/05/14 22:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212678321365463531315_0002_m_000109_113, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212678321365463531315_0002_m_000109_113}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212678321365463531315_0002}; taskId=attempt_202105142203212678321365463531315_0002_m_000109_113, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59ed6caa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:09,879] {docker.py:276} INFO - 21/05/14 22:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:09,879] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105142203212678321365463531315_0002_m_000109_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212678321365463531315_0002_m_000109_113
[2021-05-14 19:04:09,882] {docker.py:276} INFO - 21/05/14 22:04:09 INFO StagingCommitter: Task committer attempt_202105142203212678321365463531315_0002_m_000109_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212678321365463531315_0002_m_000109_113 : duration 0:00.003s
[2021-05-14 19:04:10,155] {docker.py:276} INFO - 21/05/14 22:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105142203212076991274387494776_0002_m_000106_110: needsTaskCommit() Task attempt_202105142203212076991274387494776_0002_m_000106_110
21/05/14 22:04:10 INFO StagingCommitter: Task committer attempt_202105142203212076991274387494776_0002_m_000106_110: needsTaskCommit() Task attempt_202105142203212076991274387494776_0002_m_000106_110: duration 0:00.000s
21/05/14 22:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212076991274387494776_0002_m_000106_110
[2021-05-14 19:04:10,157] {docker.py:276} INFO - 21/05/14 22:04:10 INFO Executor: Finished task 106.0 in stage 2.0 (TID 110). 4587 bytes result sent to driver
[2021-05-14 19:04:10,159] {docker.py:276} INFO - 21/05/14 22:04:10 INFO TaskSetManager: Starting task 110.0 in stage 2.0 (TID 114) (028692ec38a6, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:10,161] {docker.py:276} INFO - 21/05/14 22:04:10 INFO TaskSetManager: Finished task 106.0 in stage 2.0 (TID 110) in 1714 ms on 028692ec38a6 (executor driver) (107/200)
[2021-05-14 19:04:10,162] {docker.py:276} INFO - 21/05/14 22:04:10 INFO Executor: Running task 110.0 in stage 2.0 (TID 114)
[2021-05-14 19:04:10,173] {docker.py:276} INFO - 21/05/14 22:04:10 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:10,174] {docker.py:276} INFO - 21/05/14 22:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:10,177] {docker.py:276} INFO - 21/05/14 22:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:10,178] {docker.py:276} INFO - 21/05/14 22:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214552561451153458915_0002_m_000110_114, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214552561451153458915_0002_m_000110_114}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214552561451153458915_0002}; taskId=attempt_202105142203214552561451153458915_0002_m_000110_114, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ef2e5c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:10,178] {docker.py:276} INFO - 21/05/14 22:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:10,179] {docker.py:276} INFO - 21/05/14 22:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105142203214552561451153458915_0002_m_000110_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214552561451153458915_0002_m_000110_114
[2021-05-14 19:04:10,183] {docker.py:276} INFO - 21/05/14 22:04:10 INFO StagingCommitter: Task committer attempt_202105142203214552561451153458915_0002_m_000110_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214552561451153458915_0002_m_000110_114 : duration 0:00.005s
[2021-05-14 19:04:11,259] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203218268015458426766707_0002_m_000108_112: needsTaskCommit() Task attempt_202105142203218268015458426766707_0002_m_000108_112
[2021-05-14 19:04:11,262] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203218268015458426766707_0002_m_000108_112: needsTaskCommit() Task attempt_202105142203218268015458426766707_0002_m_000108_112: duration 0:00.001s
21/05/14 22:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218268015458426766707_0002_m_000108_112
[2021-05-14 19:04:11,263] {docker.py:276} INFO - 21/05/14 22:04:11 INFO Executor: Finished task 108.0 in stage 2.0 (TID 112). 4544 bytes result sent to driver
[2021-05-14 19:04:11,267] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Starting task 111.0 in stage 2.0 (TID 115) (028692ec38a6, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:11,268] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Finished task 108.0 in stage 2.0 (TID 112) in 1699 ms on 028692ec38a6 (executor driver) (108/200)
21/05/14 22:04:11 INFO Executor: Running task 111.0 in stage 2.0 (TID 115)
[2021-05-14 19:04:11,282] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:11,283] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:11,285] {docker.py:276} INFO - 21/05/14 22:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:11,285] {docker.py:276} INFO - 21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211935759072878591789_0002_m_000111_115, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211935759072878591789_0002_m_000111_115}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211935759072878591789_0002}; taskId=attempt_202105142203211935759072878591789_0002_m_000111_115, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@570afd25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:11,286] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203211935759072878591789_0002_m_000111_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211935759072878591789_0002_m_000111_115
[2021-05-14 19:04:11,290] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203211935759072878591789_0002_m_000111_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211935759072878591789_0002_m_000111_115 : duration 0:00.004s
[2021-05-14 19:04:11,432] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203214552561451153458915_0002_m_000110_114: needsTaskCommit() Task attempt_202105142203214552561451153458915_0002_m_000110_114
[2021-05-14 19:04:11,433] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203214552561451153458915_0002_m_000110_114: needsTaskCommit() Task attempt_202105142203214552561451153458915_0002_m_000110_114: duration 0:00.000s
21/05/14 22:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214552561451153458915_0002_m_000110_114
[2021-05-14 19:04:11,433] {docker.py:276} INFO - 21/05/14 22:04:11 INFO Executor: Finished task 110.0 in stage 2.0 (TID 114). 4544 bytes result sent to driver
[2021-05-14 19:04:11,435] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Starting task 112.0 in stage 2.0 (TID 116) (028692ec38a6, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:11,437] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Finished task 110.0 in stage 2.0 (TID 114) in 1280 ms on 028692ec38a6 (executor driver) (109/200)
21/05/14 22:04:11 INFO Executor: Running task 112.0 in stage 2.0 (TID 116)
[2021-05-14 19:04:11,450] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:11,451] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:11,456] {docker.py:276} INFO - 21/05/14 22:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:11,456] {docker.py:276} INFO - 21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:11,457] {docker.py:276} INFO - 21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211358688923733868588_0002_m_000112_116, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211358688923733868588_0002_m_000112_116}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211358688923733868588_0002}; taskId=attempt_202105142203211358688923733868588_0002_m_000112_116, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ab1357a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:11,457] {docker.py:276} INFO - 21/05/14 22:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203211358688923733868588_0002_m_000112_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211358688923733868588_0002_m_000112_116
[2021-05-14 19:04:11,460] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203211358688923733868588_0002_m_000112_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211358688923733868588_0002_m_000112_116 : duration 0:00.004s
[2021-05-14 19:04:11,537] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203212678321365463531315_0002_m_000109_113: needsTaskCommit() Task attempt_202105142203212678321365463531315_0002_m_000109_113
[2021-05-14 19:04:11,538] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203212678321365463531315_0002_m_000109_113: needsTaskCommit() Task attempt_202105142203212678321365463531315_0002_m_000109_113: duration 0:00.001s
21/05/14 22:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212678321365463531315_0002_m_000109_113
[2021-05-14 19:04:11,540] {docker.py:276} INFO - 21/05/14 22:04:11 INFO Executor: Finished task 109.0 in stage 2.0 (TID 113). 4544 bytes result sent to driver
[2021-05-14 19:04:11,543] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Starting task 113.0 in stage 2.0 (TID 117) (028692ec38a6, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:11,545] {docker.py:276} INFO - 21/05/14 22:04:11 INFO Executor: Running task 113.0 in stage 2.0 (TID 117)
[2021-05-14 19:04:11,546] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Finished task 109.0 in stage 2.0 (TID 113) in 1682 ms on 028692ec38a6 (executor driver) (110/200)
[2021-05-14 19:04:11,562] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Getting 3 (907.0 B) non-empty blocks including 3 (907.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:11,563] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:11,564] {docker.py:276} INFO - 21/05/14 22:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:11,565] {docker.py:276} INFO - 21/05/14 22:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:11,565] {docker.py:276} INFO - 21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:11,566] {docker.py:276} INFO - 21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218307517721218496567_0002_m_000113_117, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218307517721218496567_0002_m_000113_117}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218307517721218496567_0002}; taskId=attempt_202105142203218307517721218496567_0002_m_000113_117, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2231e540}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:11,566] {docker.py:276} INFO - 21/05/14 22:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:11,566] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203218307517721218496567_0002_m_000113_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218307517721218496567_0002_m_000113_117
[2021-05-14 19:04:11,570] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203218307517721218496567_0002_m_000113_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218307517721218496567_0002_m_000113_117 : duration 0:00.004s
[2021-05-14 19:04:11,620] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203216694773076752974406_0002_m_000107_111: needsTaskCommit() Task attempt_202105142203216694773076752974406_0002_m_000107_111
21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203216694773076752974406_0002_m_000107_111: needsTaskCommit() Task attempt_202105142203216694773076752974406_0002_m_000107_111: duration 0:00.000s
21/05/14 22:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216694773076752974406_0002_m_000107_111
[2021-05-14 19:04:11,621] {docker.py:276} INFO - 21/05/14 22:04:11 INFO Executor: Finished task 107.0 in stage 2.0 (TID 111). 4587 bytes result sent to driver
[2021-05-14 19:04:11,623] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Starting task 114.0 in stage 2.0 (TID 118) (028692ec38a6, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:11,624] {docker.py:276} INFO - 21/05/14 22:04:11 INFO Executor: Running task 114.0 in stage 2.0 (TID 118)
[2021-05-14 19:04:11,624] {docker.py:276} INFO - 21/05/14 22:04:11 INFO TaskSetManager: Finished task 107.0 in stage 2.0 (TID 111) in 2237 ms on 028692ec38a6 (executor driver) (111/200)
[2021-05-14 19:04:11,634] {docker.py:276} INFO - 21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Getting 3 (972.0 B) non-empty blocks including 3 (972.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:11,636] {docker.py:276} INFO - 21/05/14 22:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217960936594985780967_0002_m_000114_118, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217960936594985780967_0002_m_000114_118}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217960936594985780967_0002}; taskId=attempt_202105142203217960936594985780967_0002_m_000114_118, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45f39759}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105142203217960936594985780967_0002_m_000114_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217960936594985780967_0002_m_000114_118
[2021-05-14 19:04:11,640] {docker.py:276} INFO - 21/05/14 22:04:11 INFO StagingCommitter: Task committer attempt_202105142203217960936594985780967_0002_m_000114_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217960936594985780967_0002_m_000114_118 : duration 0:00.004s
[2021-05-14 19:04:13,140] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203211358688923733868588_0002_m_000112_116: needsTaskCommit() Task attempt_202105142203211358688923733868588_0002_m_000112_116
[2021-05-14 19:04:13,143] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203211358688923733868588_0002_m_000112_116: needsTaskCommit() Task attempt_202105142203211358688923733868588_0002_m_000112_116: duration 0:00.002s
21/05/14 22:04:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211358688923733868588_0002_m_000112_116
[2021-05-14 19:04:13,145] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Finished task 112.0 in stage 2.0 (TID 116). 4544 bytes result sent to driver
[2021-05-14 19:04:13,146] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Starting task 115.0 in stage 2.0 (TID 119) (028692ec38a6, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:13,147] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Running task 115.0 in stage 2.0 (TID 119)
[2021-05-14 19:04:13,149] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Finished task 112.0 in stage 2.0 (TID 116) in 1715 ms on 028692ec38a6 (executor driver) (112/200)
[2021-05-14 19:04:13,158] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:13,158] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:13,160] {docker.py:276} INFO - 21/05/14 22:04:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:13,161] {docker.py:276} INFO - 21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:13,161] {docker.py:276} INFO - 21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217352246504173666108_0002_m_000115_119, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217352246504173666108_0002_m_000115_119}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217352246504173666108_0002}; taskId=attempt_202105142203217352246504173666108_0002_m_000115_119, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12de2400}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:13,162] {docker.py:276} INFO - 21/05/14 22:04:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:13,162] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203217352246504173666108_0002_m_000115_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217352246504173666108_0002_m_000115_119
[2021-05-14 19:04:13,165] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203217352246504173666108_0002_m_000115_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217352246504173666108_0002_m_000115_119 : duration 0:00.003s
[2021-05-14 19:04:13,266] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203218307517721218496567_0002_m_000113_117: needsTaskCommit() Task attempt_202105142203218307517721218496567_0002_m_000113_117
21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203218307517721218496567_0002_m_000113_117: needsTaskCommit() Task attempt_202105142203218307517721218496567_0002_m_000113_117: duration 0:00.000s
[2021-05-14 19:04:13,267] {docker.py:276} INFO - 21/05/14 22:04:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218307517721218496567_0002_m_000113_117
[2021-05-14 19:04:13,268] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Finished task 113.0 in stage 2.0 (TID 117). 4544 bytes result sent to driver
21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203217960936594985780967_0002_m_000114_118: needsTaskCommit() Task attempt_202105142203217960936594985780967_0002_m_000114_118
[2021-05-14 19:04:13,269] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203217960936594985780967_0002_m_000114_118: needsTaskCommit() Task attempt_202105142203217960936594985780967_0002_m_000114_118: duration 0:00.001s
21/05/14 22:04:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217960936594985780967_0002_m_000114_118
[2021-05-14 19:04:13,270] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Starting task 116.0 in stage 2.0 (TID 120) (028692ec38a6, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:13,271] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Finished task 114.0 in stage 2.0 (TID 118). 4544 bytes result sent to driver
[2021-05-14 19:04:13,272] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Finished task 113.0 in stage 2.0 (TID 117) in 1732 ms on 028692ec38a6 (executor driver) (113/200)
[2021-05-14 19:04:13,273] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Running task 116.0 in stage 2.0 (TID 120)
[2021-05-14 19:04:13,274] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Starting task 117.0 in stage 2.0 (TID 121) (028692ec38a6, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:13,275] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Running task 117.0 in stage 2.0 (TID 121)
[2021-05-14 19:04:13,276] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Finished task 114.0 in stage 2.0 (TID 118) in 1654 ms on 028692ec38a6 (executor driver) (114/200)
[2021-05-14 19:04:13,284] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:13,284] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:13,285] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Getting 3 (806.0 B) non-empty blocks including 3 (806.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:13,285] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:04:13,288] {docker.py:276} INFO - 21/05/14 22:04:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:13,288] {docker.py:276} INFO - 21/05/14 22:04:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:13,289] {docker.py:276} INFO - 21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211534888991985773300_0002_m_000116_120, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211534888991985773300_0002_m_000116_120}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211534888991985773300_0002}; taskId=attempt_202105142203211534888991985773300_0002_m_000116_120, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20f9c2dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:13,289] {docker.py:276} INFO - 21/05/14 22:04:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203211534888991985773300_0002_m_000116_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211534888991985773300_0002_m_000116_120
[2021-05-14 19:04:13,289] {docker.py:276} INFO - 21/05/14 22:04:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:13,291] {docker.py:276} INFO - 21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:13,292] {docker.py:276} INFO - 21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214106619515129684398_0002_m_000117_121, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214106619515129684398_0002_m_000117_121}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214106619515129684398_0002}; taskId=attempt_202105142203214106619515129684398_0002_m_000117_121, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ce9d579}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:13,292] {docker.py:276} INFO - 21/05/14 22:04:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:13,292] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203214106619515129684398_0002_m_000117_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214106619515129684398_0002_m_000117_121
[2021-05-14 19:04:13,294] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203211534888991985773300_0002_m_000116_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211534888991985773300_0002_m_000116_120 : duration 0:00.006s
[2021-05-14 19:04:13,296] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203214106619515129684398_0002_m_000117_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214106619515129684398_0002_m_000117_121 : duration 0:00.004s
[2021-05-14 19:04:13,493] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_202105142203211935759072878591789_0002_m_000111_115: needsTaskCommit() Task attempt_202105142203211935759072878591789_0002_m_000111_115
[2021-05-14 19:04:13,494] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_202105142203211935759072878591789_0002_m_000111_115: needsTaskCommit() Task attempt_202105142203211935759072878591789_0002_m_000111_115: duration 0:00.001s
[2021-05-14 19:04:13,494] {docker.py:276} INFO - 21/05/14 22:04:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211935759072878591789_0002_m_000111_115
[2021-05-14 19:04:13,495] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Finished task 111.0 in stage 2.0 (TID 115). 4544 bytes result sent to driver
[2021-05-14 19:04:13,497] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Starting task 118.0 in stage 2.0 (TID 122) (028692ec38a6, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:13,499] {docker.py:276} INFO - 21/05/14 22:04:13 INFO TaskSetManager: Finished task 111.0 in stage 2.0 (TID 115) in 2237 ms on 028692ec38a6 (executor driver) (115/200)
[2021-05-14 19:04:13,500] {docker.py:276} INFO - 21/05/14 22:04:13 INFO Executor: Running task 118.0 in stage 2.0 (TID 122)
[2021-05-14 19:04:13,510] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Getting 3 (856.0 B) non-empty blocks including 3 (856.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:13,511] {docker.py:276} INFO - 21/05/14 22:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:13,512] {docker.py:276} INFO - 21/05/14 22:04:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:13,513] {docker.py:276} INFO - 21/05/14 22:04:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321740904594755955394_0002_m_000118_122, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321740904594755955394_0002_m_000118_122}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321740904594755955394_0002}; taskId=attempt_20210514220321740904594755955394_0002_m_000118_122, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1473ecca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:13 INFO StagingCommitter: Starting: Task committer attempt_20210514220321740904594755955394_0002_m_000118_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321740904594755955394_0002_m_000118_122
[2021-05-14 19:04:13,516] {docker.py:276} INFO - 21/05/14 22:04:13 INFO StagingCommitter: Task committer attempt_20210514220321740904594755955394_0002_m_000118_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321740904594755955394_0002_m_000118_122 : duration 0:00.004s
[2021-05-14 19:04:14,854] {docker.py:276} INFO - 21/05/14 22:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105142203217352246504173666108_0002_m_000115_119: needsTaskCommit() Task attempt_202105142203217352246504173666108_0002_m_000115_119
[2021-05-14 19:04:14,855] {docker.py:276} INFO - 21/05/14 22:04:14 INFO StagingCommitter: Task committer attempt_202105142203217352246504173666108_0002_m_000115_119: needsTaskCommit() Task attempt_202105142203217352246504173666108_0002_m_000115_119: duration 0:00.000s
[2021-05-14 19:04:14,856] {docker.py:276} INFO - 21/05/14 22:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217352246504173666108_0002_m_000115_119
[2021-05-14 19:04:14,857] {docker.py:276} INFO - 21/05/14 22:04:14 INFO Executor: Finished task 115.0 in stage 2.0 (TID 119). 4587 bytes result sent to driver
[2021-05-14 19:04:14,858] {docker.py:276} INFO - 21/05/14 22:04:14 INFO TaskSetManager: Starting task 119.0 in stage 2.0 (TID 123) (028692ec38a6, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:14,860] {docker.py:276} INFO - 21/05/14 22:04:14 INFO Executor: Running task 119.0 in stage 2.0 (TID 123)
[2021-05-14 19:04:14,861] {docker.py:276} INFO - 21/05/14 22:04:14 INFO TaskSetManager: Finished task 115.0 in stage 2.0 (TID 119) in 1717 ms on 028692ec38a6 (executor driver) (116/200)
[2021-05-14 19:04:14,875] {docker.py:276} INFO - 21/05/14 22:04:14 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:14,878] {docker.py:276} INFO - 21/05/14 22:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:14,879] {docker.py:276} INFO - 21/05/14 22:04:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215149143442731041019_0002_m_000119_123, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215149143442731041019_0002_m_000119_123}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215149143442731041019_0002}; taskId=attempt_202105142203215149143442731041019_0002_m_000119_123, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5011a7e7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:14,879] {docker.py:276} INFO - 21/05/14 22:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105142203215149143442731041019_0002_m_000119_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215149143442731041019_0002_m_000119_123
[2021-05-14 19:04:14,885] {docker.py:276} INFO - 21/05/14 22:04:14 INFO StagingCommitter: Task committer attempt_202105142203215149143442731041019_0002_m_000119_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215149143442731041019_0002_m_000119_123 : duration 0:00.007s
[2021-05-14 19:04:14,963] {docker.py:276} INFO - 21/05/14 22:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105142203211534888991985773300_0002_m_000116_120: needsTaskCommit() Task attempt_202105142203211534888991985773300_0002_m_000116_120
[2021-05-14 19:04:14,964] {docker.py:276} INFO - 21/05/14 22:04:14 INFO StagingCommitter: Task committer attempt_202105142203211534888991985773300_0002_m_000116_120: needsTaskCommit() Task attempt_202105142203211534888991985773300_0002_m_000116_120: duration 0:00.002s
[2021-05-14 19:04:14,965] {docker.py:276} INFO - 21/05/14 22:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211534888991985773300_0002_m_000116_120
[2021-05-14 19:04:14,967] {docker.py:276} INFO - 21/05/14 22:04:14 INFO Executor: Finished task 116.0 in stage 2.0 (TID 120). 4587 bytes result sent to driver
[2021-05-14 19:04:14,969] {docker.py:276} INFO - 21/05/14 22:04:14 INFO TaskSetManager: Starting task 120.0 in stage 2.0 (TID 124) (028692ec38a6, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:14,970] {docker.py:276} INFO - 21/05/14 22:04:14 INFO TaskSetManager: Finished task 116.0 in stage 2.0 (TID 120) in 1702 ms on 028692ec38a6 (executor driver) (117/200)
[2021-05-14 19:04:14,972] {docker.py:276} INFO - 21/05/14 22:04:14 INFO Executor: Running task 120.0 in stage 2.0 (TID 124)
[2021-05-14 19:04:14,982] {docker.py:276} INFO - 21/05/14 22:04:15 INFO ShuffleBlockFetcherIterator: Getting 3 (884.0 B) non-empty blocks including 3 (884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:14,984] {docker.py:276} INFO - 21/05/14 22:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:14,984] {docker.py:276} INFO - 21/05/14 22:04:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215961705052290131917_0002_m_000120_124, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215961705052290131917_0002_m_000120_124}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215961705052290131917_0002}; taskId=attempt_202105142203215961705052290131917_0002_m_000120_124, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c0f30f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:14,984] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105142203215961705052290131917_0002_m_000120_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215961705052290131917_0002_m_000120_124
[2021-05-14 19:04:14,987] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Task committer attempt_202105142203215961705052290131917_0002_m_000120_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215961705052290131917_0002_m_000120_124 : duration 0:00.003s
[2021-05-14 19:04:15,046] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105142203214106619515129684398_0002_m_000117_121: needsTaskCommit() Task attempt_202105142203214106619515129684398_0002_m_000117_121
[2021-05-14 19:04:15,047] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Task committer attempt_202105142203214106619515129684398_0002_m_000117_121: needsTaskCommit() Task attempt_202105142203214106619515129684398_0002_m_000117_121: duration 0:00.001s
21/05/14 22:04:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214106619515129684398_0002_m_000117_121
[2021-05-14 19:04:15,049] {docker.py:276} INFO - 21/05/14 22:04:15 INFO Executor: Finished task 117.0 in stage 2.0 (TID 121). 4587 bytes result sent to driver
[2021-05-14 19:04:15,051] {docker.py:276} INFO - 21/05/14 22:04:15 INFO TaskSetManager: Starting task 121.0 in stage 2.0 (TID 125) (028692ec38a6, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:15,053] {docker.py:276} INFO - 21/05/14 22:04:15 INFO TaskSetManager: Finished task 117.0 in stage 2.0 (TID 121) in 1782 ms on 028692ec38a6 (executor driver) (118/200)
[2021-05-14 19:04:15,053] {docker.py:276} INFO - 21/05/14 22:04:15 INFO Executor: Running task 121.0 in stage 2.0 (TID 125)
[2021-05-14 19:04:15,063] {docker.py:276} INFO - 21/05/14 22:04:15 INFO ShuffleBlockFetcherIterator: Getting 3 (889.0 B) non-empty blocks including 3 (889.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:15,065] {docker.py:276} INFO - 21/05/14 22:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211164063777529922841_0002_m_000121_125, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211164063777529922841_0002_m_000121_125}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211164063777529922841_0002}; taskId=attempt_202105142203211164063777529922841_0002_m_000121_125, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fa5385}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:15,065] {docker.py:276} INFO - 21/05/14 22:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105142203211164063777529922841_0002_m_000121_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211164063777529922841_0002_m_000121_125
[2021-05-14 19:04:15,068] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Task committer attempt_202105142203211164063777529922841_0002_m_000121_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211164063777529922841_0002_m_000121_125 : duration 0:00.003s
[2021-05-14 19:04:15,213] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Starting: Task committer attempt_20210514220321740904594755955394_0002_m_000118_122: needsTaskCommit() Task attempt_20210514220321740904594755955394_0002_m_000118_122
[2021-05-14 19:04:15,214] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Task committer attempt_20210514220321740904594755955394_0002_m_000118_122: needsTaskCommit() Task attempt_20210514220321740904594755955394_0002_m_000118_122: duration 0:00.001s
21/05/14 22:04:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321740904594755955394_0002_m_000118_122
[2021-05-14 19:04:15,217] {docker.py:276} INFO - 21/05/14 22:04:15 INFO Executor: Finished task 118.0 in stage 2.0 (TID 122). 4587 bytes result sent to driver
[2021-05-14 19:04:15,219] {docker.py:276} INFO - 21/05/14 22:04:15 INFO TaskSetManager: Starting task 122.0 in stage 2.0 (TID 126) (028692ec38a6, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:15,221] {docker.py:276} INFO - 21/05/14 22:04:15 INFO Executor: Running task 122.0 in stage 2.0 (TID 126)
21/05/14 22:04:15 INFO TaskSetManager: Finished task 118.0 in stage 2.0 (TID 122) in 1725 ms on 028692ec38a6 (executor driver) (119/200)
[2021-05-14 19:04:15,232] {docker.py:276} INFO - 21/05/14 22:04:15 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:15,232] {docker.py:276} INFO - 21/05/14 22:04:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:15,234] {docker.py:276} INFO - 21/05/14 22:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:15,235] {docker.py:276} INFO - 21/05/14 22:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:15,235] {docker.py:276} INFO - 21/05/14 22:04:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:15,235] {docker.py:276} INFO - 21/05/14 22:04:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214991541696600760472_0002_m_000122_126, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214991541696600760472_0002_m_000122_126}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214991541696600760472_0002}; taskId=attempt_202105142203214991541696600760472_0002_m_000122_126, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44009c85}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:15,236] {docker.py:276} INFO - 21/05/14 22:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:15,236] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Starting: Task committer attempt_202105142203214991541696600760472_0002_m_000122_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214991541696600760472_0002_m_000122_126
[2021-05-14 19:04:15,239] {docker.py:276} INFO - 21/05/14 22:04:15 INFO StagingCommitter: Task committer attempt_202105142203214991541696600760472_0002_m_000122_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214991541696600760472_0002_m_000122_126 : duration 0:00.003s
[2021-05-14 19:04:16,558] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203215149143442731041019_0002_m_000119_123: needsTaskCommit() Task attempt_202105142203215149143442731041019_0002_m_000119_123
[2021-05-14 19:04:16,559] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203215149143442731041019_0002_m_000119_123: needsTaskCommit() Task attempt_202105142203215149143442731041019_0002_m_000119_123: duration 0:00.001s
21/05/14 22:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215149143442731041019_0002_m_000119_123
[2021-05-14 19:04:16,562] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Finished task 119.0 in stage 2.0 (TID 123). 4544 bytes result sent to driver
[2021-05-14 19:04:16,563] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Starting task 123.0 in stage 2.0 (TID 127) (028692ec38a6, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:16,564] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Finished task 119.0 in stage 2.0 (TID 123) in 1708 ms on 028692ec38a6 (executor driver) (120/200)
[2021-05-14 19:04:16,566] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Running task 123.0 in stage 2.0 (TID 127)
[2021-05-14 19:04:16,576] {docker.py:276} INFO - 21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Getting 3 (778.0 B) non-empty blocks including 3 (778.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:16,578] {docker.py:276} INFO - 21/05/14 22:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216976411313397329282_0002_m_000123_127, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216976411313397329282_0002_m_000123_127}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216976411313397329282_0002}; taskId=attempt_202105142203216976411313397329282_0002_m_000123_127, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7272878a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:16,578] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203216976411313397329282_0002_m_000123_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216976411313397329282_0002_m_000123_127
[2021-05-14 19:04:16,581] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203216976411313397329282_0002_m_000123_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216976411313397329282_0002_m_000123_127 : duration 0:00.003s
[2021-05-14 19:04:16,695] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203215961705052290131917_0002_m_000120_124: needsTaskCommit() Task attempt_202105142203215961705052290131917_0002_m_000120_124
[2021-05-14 19:04:16,696] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203215961705052290131917_0002_m_000120_124: needsTaskCommit() Task attempt_202105142203215961705052290131917_0002_m_000120_124: duration 0:00.000s
21/05/14 22:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215961705052290131917_0002_m_000120_124
[2021-05-14 19:04:16,697] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Finished task 120.0 in stage 2.0 (TID 124). 4544 bytes result sent to driver
[2021-05-14 19:04:16,700] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Starting task 124.0 in stage 2.0 (TID 128) (028692ec38a6, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:16,701] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Finished task 120.0 in stage 2.0 (TID 124) in 1735 ms on 028692ec38a6 (executor driver) (121/200)
[2021-05-14 19:04:16,702] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Running task 124.0 in stage 2.0 (TID 128)
[2021-05-14 19:04:16,711] {docker.py:276} INFO - 21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Getting 3 (859.0 B) non-empty blocks including 3 (859.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:16,713] {docker.py:276} INFO - 21/05/14 22:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:16,713] {docker.py:276} INFO - 21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216344540564548742076_0002_m_000124_128, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216344540564548742076_0002_m_000124_128}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216344540564548742076_0002}; taskId=attempt_202105142203216344540564548742076_0002_m_000124_128, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55a680b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203216344540564548742076_0002_m_000124_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216344540564548742076_0002_m_000124_128
[2021-05-14 19:04:16,716] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203216344540564548742076_0002_m_000124_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216344540564548742076_0002_m_000124_128 : duration 0:00.003s
[2021-05-14 19:04:16,759] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203211164063777529922841_0002_m_000121_125: needsTaskCommit() Task attempt_202105142203211164063777529922841_0002_m_000121_125
[2021-05-14 19:04:16,760] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203211164063777529922841_0002_m_000121_125: needsTaskCommit() Task attempt_202105142203211164063777529922841_0002_m_000121_125: duration 0:00.000s
[2021-05-14 19:04:16,760] {docker.py:276} INFO - 21/05/14 22:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211164063777529922841_0002_m_000121_125
[2021-05-14 19:04:16,761] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Finished task 121.0 in stage 2.0 (TID 125). 4544 bytes result sent to driver
[2021-05-14 19:04:16,762] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Starting task 125.0 in stage 2.0 (TID 129) (028692ec38a6, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:16,763] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Running task 125.0 in stage 2.0 (TID 129)
[2021-05-14 19:04:16,764] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Finished task 121.0 in stage 2.0 (TID 125) in 1715 ms on 028692ec38a6 (executor driver) (122/200)
[2021-05-14 19:04:16,771] {docker.py:276} INFO - 21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Getting 3 (972.0 B) non-empty blocks including 3 (972.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:16,773] {docker.py:276} INFO - 21/05/14 22:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:16,774] {docker.py:276} INFO - 21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216959250762184324668_0002_m_000125_129, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216959250762184324668_0002_m_000125_129}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216959250762184324668_0002}; taskId=attempt_202105142203216959250762184324668_0002_m_000125_129, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28c13b9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203216959250762184324668_0002_m_000125_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216959250762184324668_0002_m_000125_129
[2021-05-14 19:04:16,777] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203216959250762184324668_0002_m_000125_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216959250762184324668_0002_m_000125_129 : duration 0:00.003s
[2021-05-14 19:04:16,928] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203214991541696600760472_0002_m_000122_126: needsTaskCommit() Task attempt_202105142203214991541696600760472_0002_m_000122_126
21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203214991541696600760472_0002_m_000122_126: needsTaskCommit() Task attempt_202105142203214991541696600760472_0002_m_000122_126: duration 0:00.001s
21/05/14 22:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214991541696600760472_0002_m_000122_126
[2021-05-14 19:04:16,930] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Finished task 122.0 in stage 2.0 (TID 126). 4544 bytes result sent to driver
[2021-05-14 19:04:16,931] {docker.py:276} INFO - 21/05/14 22:04:16 INFO TaskSetManager: Starting task 126.0 in stage 2.0 (TID 130) (028692ec38a6, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:16,932] {docker.py:276} INFO - 21/05/14 22:04:16 INFO Executor: Running task 126.0 in stage 2.0 (TID 130)
21/05/14 22:04:16 INFO TaskSetManager: Finished task 122.0 in stage 2.0 (TID 126) in 1715 ms on 028692ec38a6 (executor driver) (123/200)
[2021-05-14 19:04:16,946] {docker.py:276} INFO - 21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Getting 3 (803.0 B) non-empty blocks including 3 (803.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:16,949] {docker.py:276} INFO - 21/05/14 22:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:16,950] {docker.py:276} INFO - 21/05/14 22:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211639695301142270224_0002_m_000126_130, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211639695301142270224_0002_m_000126_130}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211639695301142270224_0002}; taskId=attempt_202105142203211639695301142270224_0002_m_000126_130, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63df42e4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:16,951] {docker.py:276} INFO - 21/05/14 22:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:16,951] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105142203211639695301142270224_0002_m_000126_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211639695301142270224_0002_m_000126_130
[2021-05-14 19:04:16,955] {docker.py:276} INFO - 21/05/14 22:04:16 INFO StagingCommitter: Task committer attempt_202105142203211639695301142270224_0002_m_000126_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211639695301142270224_0002_m_000126_130 : duration 0:00.005s
[2021-05-14 19:04:18,254] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203216976411313397329282_0002_m_000123_127: needsTaskCommit() Task attempt_202105142203216976411313397329282_0002_m_000123_127
21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203216976411313397329282_0002_m_000123_127: needsTaskCommit() Task attempt_202105142203216976411313397329282_0002_m_000123_127: duration 0:00.000s
21/05/14 22:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216976411313397329282_0002_m_000123_127
[2021-05-14 19:04:18,258] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Finished task 123.0 in stage 2.0 (TID 127). 4544 bytes result sent to driver
[2021-05-14 19:04:18,259] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Starting task 127.0 in stage 2.0 (TID 131) (028692ec38a6, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:18,261] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Finished task 123.0 in stage 2.0 (TID 127) in 1700 ms on 028692ec38a6 (executor driver) (124/200)
[2021-05-14 19:04:18,262] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Running task 127.0 in stage 2.0 (TID 131)
[2021-05-14 19:04:18,271] {docker.py:276} INFO - 21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Getting 3 (870.0 B) non-empty blocks including 3 (870.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:18,273] {docker.py:276} INFO - 21/05/14 22:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215485356723565319310_0002_m_000127_131, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215485356723565319310_0002_m_000127_131}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215485356723565319310_0002}; taskId=attempt_202105142203215485356723565319310_0002_m_000127_131, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@753ab858}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203215485356723565319310_0002_m_000127_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215485356723565319310_0002_m_000127_131
[2021-05-14 19:04:18,276] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203215485356723565319310_0002_m_000127_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215485356723565319310_0002_m_000127_131 : duration 0:00.002s
[2021-05-14 19:04:18,408] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203216344540564548742076_0002_m_000124_128: needsTaskCommit() Task attempt_202105142203216344540564548742076_0002_m_000124_128
[2021-05-14 19:04:18,409] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203216344540564548742076_0002_m_000124_128: needsTaskCommit() Task attempt_202105142203216344540564548742076_0002_m_000124_128: duration 0:00.001s
21/05/14 22:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216344540564548742076_0002_m_000124_128
[2021-05-14 19:04:18,412] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Finished task 124.0 in stage 2.0 (TID 128). 4544 bytes result sent to driver
[2021-05-14 19:04:18,413] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Starting task 128.0 in stage 2.0 (TID 132) (028692ec38a6, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:18,414] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Finished task 124.0 in stage 2.0 (TID 128) in 1717 ms on 028692ec38a6 (executor driver) (125/200)
[2021-05-14 19:04:18,415] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Running task 128.0 in stage 2.0 (TID 132)
[2021-05-14 19:04:18,425] {docker.py:276} INFO - 21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Getting 3 (900.0 B) non-empty blocks including 3 (900.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:18,427] {docker.py:276} INFO - 21/05/14 22:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214631444798662075574_0002_m_000128_132, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214631444798662075574_0002_m_000128_132}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214631444798662075574_0002}; taskId=attempt_202105142203214631444798662075574_0002_m_000128_132, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42f717}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:18,427] {docker.py:276} INFO - 21/05/14 22:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203214631444798662075574_0002_m_000128_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214631444798662075574_0002_m_000128_132
[2021-05-14 19:04:18,429] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203214631444798662075574_0002_m_000128_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214631444798662075574_0002_m_000128_132 : duration 0:00.002s
[2021-05-14 19:04:18,460] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203216959250762184324668_0002_m_000125_129: needsTaskCommit() Task attempt_202105142203216959250762184324668_0002_m_000125_129
[2021-05-14 19:04:18,461] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203216959250762184324668_0002_m_000125_129: needsTaskCommit() Task attempt_202105142203216959250762184324668_0002_m_000125_129: duration 0:00.001s
21/05/14 22:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216959250762184324668_0002_m_000125_129
[2021-05-14 19:04:18,462] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Finished task 125.0 in stage 2.0 (TID 129). 4544 bytes result sent to driver
[2021-05-14 19:04:18,464] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Starting task 129.0 in stage 2.0 (TID 133) (028692ec38a6, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:18,465] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Running task 129.0 in stage 2.0 (TID 133)
[2021-05-14 19:04:18,466] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Finished task 125.0 in stage 2.0 (TID 129) in 1705 ms on 028692ec38a6 (executor driver) (126/200)
[2021-05-14 19:04:18,475] {docker.py:276} INFO - 21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Getting 3 (923.0 B) non-empty blocks including 3 (923.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:18,477] {docker.py:276} INFO - 21/05/14 22:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211658684125611338541_0002_m_000129_133, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211658684125611338541_0002_m_000129_133}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211658684125611338541_0002}; taskId=attempt_202105142203211658684125611338541_0002_m_000129_133, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50b700c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:18,477] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203211658684125611338541_0002_m_000129_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211658684125611338541_0002_m_000129_133
[2021-05-14 19:04:18,480] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203211658684125611338541_0002_m_000129_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211658684125611338541_0002_m_000129_133 : duration 0:00.003s
[2021-05-14 19:04:18,667] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203211639695301142270224_0002_m_000126_130: needsTaskCommit() Task attempt_202105142203211639695301142270224_0002_m_000126_130
[2021-05-14 19:04:18,668] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203211639695301142270224_0002_m_000126_130: needsTaskCommit() Task attempt_202105142203211639695301142270224_0002_m_000126_130: duration 0:00.000s
21/05/14 22:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211639695301142270224_0002_m_000126_130
[2021-05-14 19:04:18,669] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Finished task 126.0 in stage 2.0 (TID 130). 4544 bytes result sent to driver
[2021-05-14 19:04:18,671] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Starting task 130.0 in stage 2.0 (TID 134) (028692ec38a6, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:18,672] {docker.py:276} INFO - 21/05/14 22:04:18 INFO TaskSetManager: Finished task 126.0 in stage 2.0 (TID 130) in 1744 ms on 028692ec38a6 (executor driver) (127/200)
[2021-05-14 19:04:18,673] {docker.py:276} INFO - 21/05/14 22:04:18 INFO Executor: Running task 130.0 in stage 2.0 (TID 134)
[2021-05-14 19:04:18,683] {docker.py:276} INFO - 21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Getting 3 (976.0 B) non-empty blocks including 3 (976.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:18,685] {docker.py:276} INFO - 21/05/14 22:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218989621375255277644_0002_m_000130_134, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218989621375255277644_0002_m_000130_134}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218989621375255277644_0002}; taskId=attempt_202105142203218989621375255277644_0002_m_000130_134, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d47433}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:18,686] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105142203218989621375255277644_0002_m_000130_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218989621375255277644_0002_m_000130_134
[2021-05-14 19:04:18,688] {docker.py:276} INFO - 21/05/14 22:04:18 INFO StagingCommitter: Task committer attempt_202105142203218989621375255277644_0002_m_000130_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218989621375255277644_0002_m_000130_134 : duration 0:00.002s
[2021-05-14 19:04:20,002] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203215485356723565319310_0002_m_000127_131: needsTaskCommit() Task attempt_202105142203215485356723565319310_0002_m_000127_131
[2021-05-14 19:04:20,002] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203215485356723565319310_0002_m_000127_131: needsTaskCommit() Task attempt_202105142203215485356723565319310_0002_m_000127_131: duration 0:00.001s
21/05/14 22:04:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215485356723565319310_0002_m_000127_131
[2021-05-14 19:04:20,004] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Finished task 127.0 in stage 2.0 (TID 131). 4587 bytes result sent to driver
[2021-05-14 19:04:20,005] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Starting task 131.0 in stage 2.0 (TID 135) (028692ec38a6, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:20,007] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Finished task 127.0 in stage 2.0 (TID 131) in 1750 ms on 028692ec38a6 (executor driver) (128/200)
[2021-05-14 19:04:20,008] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Running task 131.0 in stage 2.0 (TID 135)
[2021-05-14 19:04:20,025] {docker.py:276} INFO - 21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Getting 3 (859.0 B) non-empty blocks including 3 (859.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:20,028] {docker.py:276} INFO - 21/05/14 22:04:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:20,029] {docker.py:276} INFO - 21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:20,031] {docker.py:276} INFO - 21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203219041575200536964758_0002_m_000131_135, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219041575200536964758_0002_m_000131_135}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203219041575200536964758_0002}; taskId=attempt_202105142203219041575200536964758_0002_m_000131_135, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f9420f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:20,032] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203219041575200536964758_0002_m_000131_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219041575200536964758_0002_m_000131_135
[2021-05-14 19:04:20,037] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203219041575200536964758_0002_m_000131_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203219041575200536964758_0002_m_000131_135 : duration 0:00.007s
[2021-05-14 19:04:20,110] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203214631444798662075574_0002_m_000128_132: needsTaskCommit() Task attempt_202105142203214631444798662075574_0002_m_000128_132
[2021-05-14 19:04:20,111] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203214631444798662075574_0002_m_000128_132: needsTaskCommit() Task attempt_202105142203214631444798662075574_0002_m_000128_132: duration 0:00.001s
21/05/14 22:04:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214631444798662075574_0002_m_000128_132
[2021-05-14 19:04:20,112] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Finished task 128.0 in stage 2.0 (TID 132). 4587 bytes result sent to driver
[2021-05-14 19:04:20,114] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Starting task 132.0 in stage 2.0 (TID 136) (028692ec38a6, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:20,115] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Finished task 128.0 in stage 2.0 (TID 132) in 1704 ms on 028692ec38a6 (executor driver) (129/200)
[2021-05-14 19:04:20,116] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Running task 132.0 in stage 2.0 (TID 136)
[2021-05-14 19:04:20,126] {docker.py:276} INFO - 21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Getting 3 (813.0 B) non-empty blocks including 3 (813.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:20,128] {docker.py:276} INFO - 21/05/14 22:04:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:20,129] {docker.py:276} INFO - 21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218000448361163344125_0002_m_000132_136, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218000448361163344125_0002_m_000132_136}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218000448361163344125_0002}; taskId=attempt_202105142203218000448361163344125_0002_m_000132_136, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@110ff928}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203218000448361163344125_0002_m_000132_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218000448361163344125_0002_m_000132_136
[2021-05-14 19:04:20,132] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203218000448361163344125_0002_m_000132_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218000448361163344125_0002_m_000132_136 : duration 0:00.004s
[2021-05-14 19:04:20,172] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203211658684125611338541_0002_m_000129_133: needsTaskCommit() Task attempt_202105142203211658684125611338541_0002_m_000129_133
[2021-05-14 19:04:20,172] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203211658684125611338541_0002_m_000129_133: needsTaskCommit() Task attempt_202105142203211658684125611338541_0002_m_000129_133: duration 0:00.000s
21/05/14 22:04:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211658684125611338541_0002_m_000129_133
[2021-05-14 19:04:20,173] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Finished task 129.0 in stage 2.0 (TID 133). 4587 bytes result sent to driver
[2021-05-14 19:04:20,175] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Starting task 133.0 in stage 2.0 (TID 137) (028692ec38a6, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:20,176] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Running task 133.0 in stage 2.0 (TID 137)
21/05/14 22:04:20 INFO TaskSetManager: Finished task 129.0 in stage 2.0 (TID 133) in 1715 ms on 028692ec38a6 (executor driver) (130/200)
[2021-05-14 19:04:20,185] {docker.py:276} INFO - 21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:20,186] {docker.py:276} INFO - 21/05/14 22:04:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321705870182530930394_0002_m_000133_137, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321705870182530930394_0002_m_000133_137}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321705870182530930394_0002}; taskId=attempt_20210514220321705870182530930394_0002_m_000133_137, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@632273f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:20,186] {docker.py:276} INFO - 21/05/14 22:04:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_20210514220321705870182530930394_0002_m_000133_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321705870182530930394_0002_m_000133_137
[2021-05-14 19:04:20,189] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_20210514220321705870182530930394_0002_m_000133_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321705870182530930394_0002_m_000133_137 : duration 0:00.003s
[2021-05-14 19:04:20,351] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203218989621375255277644_0002_m_000130_134: needsTaskCommit() Task attempt_202105142203218989621375255277644_0002_m_000130_134
[2021-05-14 19:04:20,354] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203218989621375255277644_0002_m_000130_134: needsTaskCommit() Task attempt_202105142203218989621375255277644_0002_m_000130_134: duration 0:00.003s
21/05/14 22:04:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218989621375255277644_0002_m_000130_134
[2021-05-14 19:04:20,355] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Finished task 130.0 in stage 2.0 (TID 134). 4587 bytes result sent to driver
[2021-05-14 19:04:20,357] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Starting task 134.0 in stage 2.0 (TID 138) (028692ec38a6, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:20,359] {docker.py:276} INFO - 21/05/14 22:04:20 INFO Executor: Running task 134.0 in stage 2.0 (TID 138)
[2021-05-14 19:04:20,361] {docker.py:276} INFO - 21/05/14 22:04:20 INFO TaskSetManager: Finished task 130.0 in stage 2.0 (TID 134) in 1692 ms on 028692ec38a6 (executor driver) (131/200)
[2021-05-14 19:04:20,369] {docker.py:276} INFO - 21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:20,371] {docker.py:276} INFO - 21/05/14 22:04:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211102702450800639083_0002_m_000134_138, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211102702450800639083_0002_m_000134_138}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211102702450800639083_0002}; taskId=attempt_202105142203211102702450800639083_0002_m_000134_138, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f508046}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:20 INFO StagingCommitter: Starting: Task committer attempt_202105142203211102702450800639083_0002_m_000134_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211102702450800639083_0002_m_000134_138
[2021-05-14 19:04:20,374] {docker.py:276} INFO - 21/05/14 22:04:20 INFO StagingCommitter: Task committer attempt_202105142203211102702450800639083_0002_m_000134_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211102702450800639083_0002_m_000134_138 : duration 0:00.003s
[2021-05-14 19:04:21,728] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105142203219041575200536964758_0002_m_000131_135: needsTaskCommit() Task attempt_202105142203219041575200536964758_0002_m_000131_135
[2021-05-14 19:04:21,729] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Task committer attempt_202105142203219041575200536964758_0002_m_000131_135: needsTaskCommit() Task attempt_202105142203219041575200536964758_0002_m_000131_135: duration 0:00.000s
21/05/14 22:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203219041575200536964758_0002_m_000131_135
[2021-05-14 19:04:21,730] {docker.py:276} INFO - 21/05/14 22:04:21 INFO Executor: Finished task 131.0 in stage 2.0 (TID 135). 4544 bytes result sent to driver
[2021-05-14 19:04:21,731] {docker.py:276} INFO - 21/05/14 22:04:21 INFO TaskSetManager: Starting task 135.0 in stage 2.0 (TID 139) (028692ec38a6, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:21,732] {docker.py:276} INFO - 21/05/14 22:04:21 INFO TaskSetManager: Finished task 131.0 in stage 2.0 (TID 135) in 1729 ms on 028692ec38a6 (executor driver) (132/200)
21/05/14 22:04:21 INFO Executor: Running task 135.0 in stage 2.0 (TID 139)
[2021-05-14 19:04:21,740] {docker.py:276} INFO - 21/05/14 22:04:21 INFO ShuffleBlockFetcherIterator: Getting 3 (836.0 B) non-empty blocks including 3 (836.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:21,742] {docker.py:276} INFO - 21/05/14 22:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212813117968215317817_0002_m_000135_139, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212813117968215317817_0002_m_000135_139}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212813117968215317817_0002}; taskId=attempt_202105142203212813117968215317817_0002_m_000135_139, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79cc7295}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:21,743] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105142203212813117968215317817_0002_m_000135_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212813117968215317817_0002_m_000135_139
[2021-05-14 19:04:21,746] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Task committer attempt_202105142203212813117968215317817_0002_m_000135_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212813117968215317817_0002_m_000135_139 : duration 0:00.003s
[2021-05-14 19:04:21,847] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Starting: Task committer attempt_20210514220321705870182530930394_0002_m_000133_137: needsTaskCommit() Task attempt_20210514220321705870182530930394_0002_m_000133_137
[2021-05-14 19:04:21,848] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Task committer attempt_20210514220321705870182530930394_0002_m_000133_137: needsTaskCommit() Task attempt_20210514220321705870182530930394_0002_m_000133_137: duration 0:00.001s
21/05/14 22:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321705870182530930394_0002_m_000133_137
[2021-05-14 19:04:21,850] {docker.py:276} INFO - 21/05/14 22:04:21 INFO Executor: Finished task 133.0 in stage 2.0 (TID 137). 4544 bytes result sent to driver
[2021-05-14 19:04:21,851] {docker.py:276} INFO - 21/05/14 22:04:21 INFO TaskSetManager: Starting task 136.0 in stage 2.0 (TID 140) (028692ec38a6, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:21,852] {docker.py:276} INFO - 21/05/14 22:04:21 INFO TaskSetManager: Finished task 133.0 in stage 2.0 (TID 137) in 1647 ms on 028692ec38a6 (executor driver) (133/200)
[2021-05-14 19:04:21,853] {docker.py:276} INFO - 21/05/14 22:04:21 INFO Executor: Running task 136.0 in stage 2.0 (TID 140)
[2021-05-14 19:04:21,866] {docker.py:276} INFO - 21/05/14 22:04:21 INFO ShuffleBlockFetcherIterator: Getting 3 (886.0 B) non-empty blocks including 3 (886.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:21,867] {docker.py:276} INFO - 21/05/14 22:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:21,868] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105142203218000448361163344125_0002_m_000132_136: needsTaskCommit() Task attempt_202105142203218000448361163344125_0002_m_000132_136
[2021-05-14 19:04:21,869] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Task committer attempt_202105142203218000448361163344125_0002_m_000132_136: needsTaskCommit() Task attempt_202105142203218000448361163344125_0002_m_000132_136: duration 0:00.001s
21/05/14 22:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218000448361163344125_0002_m_000132_136
[2021-05-14 19:04:21,870] {docker.py:276} INFO - 21/05/14 22:04:21 INFO Executor: Finished task 132.0 in stage 2.0 (TID 136). 4544 bytes result sent to driver
[2021-05-14 19:04:21,870] {docker.py:276} INFO - 21/05/14 22:04:21 INFO TaskSetManager: Starting task 137.0 in stage 2.0 (TID 141) (028692ec38a6, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:21,872] {docker.py:276} INFO - 21/05/14 22:04:21 INFO TaskSetManager: Finished task 132.0 in stage 2.0 (TID 136) in 1727 ms on 028692ec38a6 (executor driver) (134/200)
[2021-05-14 19:04:21,873] {docker.py:276} INFO - 21/05/14 22:04:21 INFO Executor: Running task 137.0 in stage 2.0 (TID 141)
21/05/14 22:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:21,875] {docker.py:276} INFO - 21/05/14 22:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321184983492683589638_0002_m_000136_140, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321184983492683589638_0002_m_000136_140}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321184983492683589638_0002}; taskId=attempt_20210514220321184983492683589638_0002_m_000136_140, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41509ae5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:21 INFO StagingCommitter: Starting: Task committer attempt_20210514220321184983492683589638_0002_m_000136_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321184983492683589638_0002_m_000136_140
[2021-05-14 19:04:21,878] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Task committer attempt_20210514220321184983492683589638_0002_m_000136_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321184983492683589638_0002_m_000136_140 : duration 0:00.004s
[2021-05-14 19:04:21,885] {docker.py:276} INFO - 21/05/14 22:04:21 INFO ShuffleBlockFetcherIterator: Getting 3 (778.0 B) non-empty blocks including 3 (778.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:21,887] {docker.py:276} INFO - 21/05/14 22:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:21,887] {docker.py:276} INFO - 21/05/14 22:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212368839571186625675_0002_m_000137_141, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212368839571186625675_0002_m_000137_141}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212368839571186625675_0002}; taskId=attempt_202105142203212368839571186625675_0002_m_000137_141, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7642f0a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:21,888] {docker.py:276} INFO - 21/05/14 22:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:21,888] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105142203212368839571186625675_0002_m_000137_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212368839571186625675_0002_m_000137_141
[2021-05-14 19:04:21,892] {docker.py:276} INFO - 21/05/14 22:04:21 INFO StagingCommitter: Task committer attempt_202105142203212368839571186625675_0002_m_000137_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212368839571186625675_0002_m_000137_141 : duration 0:00.004s
[2021-05-14 19:04:22,068] {docker.py:276} INFO - 21/05/14 22:04:22 INFO StagingCommitter: Starting: Task committer attempt_202105142203211102702450800639083_0002_m_000134_138: needsTaskCommit() Task attempt_202105142203211102702450800639083_0002_m_000134_138
21/05/14 22:04:22 INFO StagingCommitter: Task committer attempt_202105142203211102702450800639083_0002_m_000134_138: needsTaskCommit() Task attempt_202105142203211102702450800639083_0002_m_000134_138: duration 0:00.002s
21/05/14 22:04:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211102702450800639083_0002_m_000134_138
[2021-05-14 19:04:22,069] {docker.py:276} INFO - 21/05/14 22:04:22 INFO Executor: Finished task 134.0 in stage 2.0 (TID 138). 4544 bytes result sent to driver
[2021-05-14 19:04:22,072] {docker.py:276} INFO - 21/05/14 22:04:22 INFO TaskSetManager: Starting task 138.0 in stage 2.0 (TID 142) (028692ec38a6, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:22,073] {docker.py:276} INFO - 21/05/14 22:04:22 INFO Executor: Running task 138.0 in stage 2.0 (TID 142)
21/05/14 22:04:22 INFO TaskSetManager: Finished task 134.0 in stage 2.0 (TID 138) in 1684 ms on 028692ec38a6 (executor driver) (135/200)
[2021-05-14 19:04:22,099] {docker.py:276} INFO - 21/05/14 22:04:22 INFO ShuffleBlockFetcherIterator: Getting 3 (884.0 B) non-empty blocks including 3 (884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:22,100] {docker.py:276} INFO - 21/05/14 22:04:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:22,107] {docker.py:276} INFO - 21/05/14 22:04:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:22,107] {docker.py:276} INFO - 21/05/14 22:04:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214893891866036744630_0002_m_000138_142, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214893891866036744630_0002_m_000138_142}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214893891866036744630_0002}; taskId=attempt_202105142203214893891866036744630_0002_m_000138_142, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a5470b1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:22,108] {docker.py:276} INFO - 21/05/14 22:04:22 INFO StagingCommitter: Starting: Task committer attempt_202105142203214893891866036744630_0002_m_000138_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214893891866036744630_0002_m_000138_142
[2021-05-14 19:04:22,114] {docker.py:276} INFO - 21/05/14 22:04:22 INFO StagingCommitter: Task committer attempt_202105142203214893891866036744630_0002_m_000138_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214893891866036744630_0002_m_000138_142 : duration 0:00.006s
[2021-05-14 19:04:23,513] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203212813117968215317817_0002_m_000135_139: needsTaskCommit() Task attempt_202105142203212813117968215317817_0002_m_000135_139
[2021-05-14 19:04:23,514] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203212813117968215317817_0002_m_000135_139: needsTaskCommit() Task attempt_202105142203212813117968215317817_0002_m_000135_139: duration 0:00.001s
21/05/14 22:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212813117968215317817_0002_m_000135_139
[2021-05-14 19:04:23,515] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Finished task 135.0 in stage 2.0 (TID 139). 4544 bytes result sent to driver
[2021-05-14 19:04:23,517] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Starting task 139.0 in stage 2.0 (TID 143) (028692ec38a6, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:23,519] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Finished task 135.0 in stage 2.0 (TID 139) in 1756 ms on 028692ec38a6 (executor driver) (136/200)
[2021-05-14 19:04:23,519] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Running task 139.0 in stage 2.0 (TID 143)
[2021-05-14 19:04:23,534] {docker.py:276} INFO - 21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Getting 3 (944.0 B) non-empty blocks including 3 (944.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:23,536] {docker.py:276} INFO - 21/05/14 22:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:23,536] {docker.py:276} INFO - 21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:23,537] {docker.py:276} INFO - 21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203217054583322826594941_0002_m_000139_143, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217054583322826594941_0002_m_000139_143}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203217054583322826594941_0002}; taskId=attempt_202105142203217054583322826594941_0002_m_000139_143, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@209c3c37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203217054583322826594941_0002_m_000139_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217054583322826594941_0002_m_000139_143
[2021-05-14 19:04:23,539] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203217054583322826594941_0002_m_000139_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203217054583322826594941_0002_m_000139_143 : duration 0:00.003s
[2021-05-14 19:04:23,561] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203212368839571186625675_0002_m_000137_141: needsTaskCommit() Task attempt_202105142203212368839571186625675_0002_m_000137_141
[2021-05-14 19:04:23,562] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203212368839571186625675_0002_m_000137_141: needsTaskCommit() Task attempt_202105142203212368839571186625675_0002_m_000137_141: duration 0:00.001s
21/05/14 22:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212368839571186625675_0002_m_000137_141
[2021-05-14 19:04:23,563] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Finished task 137.0 in stage 2.0 (TID 141). 4544 bytes result sent to driver
[2021-05-14 19:04:23,564] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Starting task 140.0 in stage 2.0 (TID 144) (028692ec38a6, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:23,565] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Running task 140.0 in stage 2.0 (TID 144)
[2021-05-14 19:04:23,565] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Finished task 137.0 in stage 2.0 (TID 141) in 1698 ms on 028692ec38a6 (executor driver) (137/200)
[2021-05-14 19:04:23,571] {docker.py:276} INFO - 21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:23,573] {docker.py:276} INFO - 21/05/14 22:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216314909651641236548_0002_m_000140_144, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216314909651641236548_0002_m_000140_144}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216314909651641236548_0002}; taskId=attempt_202105142203216314909651641236548_0002_m_000140_144, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51a40087}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:23,573] {docker.py:276} INFO - 21/05/14 22:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203216314909651641236548_0002_m_000140_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216314909651641236548_0002_m_000140_144
[2021-05-14 19:04:23,576] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203216314909651641236548_0002_m_000140_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216314909651641236548_0002_m_000140_144 : duration 0:00.002s
[2021-05-14 19:04:23,600] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_20210514220321184983492683589638_0002_m_000136_140: needsTaskCommit() Task attempt_20210514220321184983492683589638_0002_m_000136_140
[2021-05-14 19:04:23,601] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_20210514220321184983492683589638_0002_m_000136_140: needsTaskCommit() Task attempt_20210514220321184983492683589638_0002_m_000136_140: duration 0:00.000s
21/05/14 22:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321184983492683589638_0002_m_000136_140
[2021-05-14 19:04:23,602] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Finished task 136.0 in stage 2.0 (TID 140). 4544 bytes result sent to driver
[2021-05-14 19:04:23,603] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Starting task 141.0 in stage 2.0 (TID 145) (028692ec38a6, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:23,605] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Running task 141.0 in stage 2.0 (TID 145)
21/05/14 22:04:23 INFO TaskSetManager: Finished task 136.0 in stage 2.0 (TID 140) in 1756 ms on 028692ec38a6 (executor driver) (138/200)
[2021-05-14 19:04:23,613] {docker.py:276} INFO - 21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Getting 3 (1036.0 B) non-empty blocks including 3 (1036.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:23,615] {docker.py:276} INFO - 21/05/14 22:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203211410303656587097641_0002_m_000141_145, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211410303656587097641_0002_m_000141_145}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203211410303656587097641_0002}; taskId=attempt_202105142203211410303656587097641_0002_m_000141_145, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bfe6cad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203211410303656587097641_0002_m_000141_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211410303656587097641_0002_m_000141_145
[2021-05-14 19:04:23,618] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203211410303656587097641_0002_m_000141_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203211410303656587097641_0002_m_000141_145 : duration 0:00.002s
[2021-05-14 19:04:23,785] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203214893891866036744630_0002_m_000138_142: needsTaskCommit() Task attempt_202105142203214893891866036744630_0002_m_000138_142
[2021-05-14 19:04:23,786] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203214893891866036744630_0002_m_000138_142: needsTaskCommit() Task attempt_202105142203214893891866036744630_0002_m_000138_142: duration 0:00.000s
21/05/14 22:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214893891866036744630_0002_m_000138_142
[2021-05-14 19:04:23,788] {docker.py:276} INFO - 21/05/14 22:04:23 INFO Executor: Finished task 138.0 in stage 2.0 (TID 142). 4544 bytes result sent to driver
[2021-05-14 19:04:23,789] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Starting task 142.0 in stage 2.0 (TID 146) (028692ec38a6, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:23,789] {docker.py:276} INFO - 21/05/14 22:04:23 INFO TaskSetManager: Finished task 138.0 in stage 2.0 (TID 142) in 1720 ms on 028692ec38a6 (executor driver) (139/200)
21/05/14 22:04:23 INFO Executor: Running task 142.0 in stage 2.0 (TID 146)
[2021-05-14 19:04:23,810] {docker.py:276} INFO - 21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Getting 3 (831.0 B) non-empty blocks including 3 (831.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:23,812] {docker.py:276} INFO - 21/05/14 22:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:23,813] {docker.py:276} INFO - 21/05/14 22:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218401235644664885156_0002_m_000142_146, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218401235644664885156_0002_m_000142_146}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218401235644664885156_0002}; taskId=attempt_202105142203218401235644664885156_0002_m_000142_146, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@314ba016}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:23,813] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105142203218401235644664885156_0002_m_000142_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218401235644664885156_0002_m_000142_146
[2021-05-14 19:04:23,816] {docker.py:276} INFO - 21/05/14 22:04:23 INFO StagingCommitter: Task committer attempt_202105142203218401235644664885156_0002_m_000142_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218401235644664885156_0002_m_000142_146 : duration 0:00.003s
[2021-05-14 19:04:25,216] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203217054583322826594941_0002_m_000139_143: needsTaskCommit() Task attempt_202105142203217054583322826594941_0002_m_000139_143
[2021-05-14 19:04:25,217] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203217054583322826594941_0002_m_000139_143: needsTaskCommit() Task attempt_202105142203217054583322826594941_0002_m_000139_143: duration 0:00.001s
21/05/14 22:04:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203217054583322826594941_0002_m_000139_143
[2021-05-14 19:04:25,219] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Finished task 139.0 in stage 2.0 (TID 143). 4587 bytes result sent to driver
[2021-05-14 19:04:25,222] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Starting task 143.0 in stage 2.0 (TID 147) (028692ec38a6, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:25,224] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Running task 143.0 in stage 2.0 (TID 147)
[2021-05-14 19:04:25,225] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Finished task 139.0 in stage 2.0 (TID 143) in 1707 ms on 028692ec38a6 (executor driver) (140/200)
[2021-05-14 19:04:25,233] {docker.py:276} INFO - 21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Getting 3 (774.0 B) non-empty blocks including 3 (774.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:25,255] {docker.py:276} INFO - 21/05/14 22:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:25,256] {docker.py:276} INFO - 21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321901907214539967306_0002_m_000143_147, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321901907214539967306_0002_m_000143_147}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321901907214539967306_0002}; taskId=attempt_20210514220321901907214539967306_0002_m_000143_147, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fb41d5b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:25,256] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_20210514220321901907214539967306_0002_m_000143_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321901907214539967306_0002_m_000143_147
[2021-05-14 19:04:25,256] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_20210514220321901907214539967306_0002_m_000143_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321901907214539967306_0002_m_000143_147 : duration 0:00.003s
[2021-05-14 19:04:25,298] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203211410303656587097641_0002_m_000141_145: needsTaskCommit() Task attempt_202105142203211410303656587097641_0002_m_000141_145
[2021-05-14 19:04:25,299] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203211410303656587097641_0002_m_000141_145: needsTaskCommit() Task attempt_202105142203211410303656587097641_0002_m_000141_145: duration 0:00.001s
21/05/14 22:04:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203211410303656587097641_0002_m_000141_145
[2021-05-14 19:04:25,301] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Finished task 141.0 in stage 2.0 (TID 145). 4587 bytes result sent to driver
[2021-05-14 19:04:25,302] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Starting task 144.0 in stage 2.0 (TID 148) (028692ec38a6, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:25,303] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Running task 144.0 in stage 2.0 (TID 148)
[2021-05-14 19:04:25,304] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Finished task 141.0 in stage 2.0 (TID 145) in 1703 ms on 028692ec38a6 (executor driver) (141/200)
[2021-05-14 19:04:25,306] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203216314909651641236548_0002_m_000140_144: needsTaskCommit() Task attempt_202105142203216314909651641236548_0002_m_000140_144
[2021-05-14 19:04:25,306] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203216314909651641236548_0002_m_000140_144: needsTaskCommit() Task attempt_202105142203216314909651641236548_0002_m_000140_144: duration 0:00.000s
21/05/14 22:04:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216314909651641236548_0002_m_000140_144
[2021-05-14 19:04:25,307] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Finished task 140.0 in stage 2.0 (TID 144). 4587 bytes result sent to driver
[2021-05-14 19:04:25,308] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Starting task 145.0 in stage 2.0 (TID 149) (028692ec38a6, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:25,309] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Running task 145.0 in stage 2.0 (TID 149)
[2021-05-14 19:04:25,310] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Finished task 140.0 in stage 2.0 (TID 144) in 1748 ms on 028692ec38a6 (executor driver) (142/200)
[2021-05-14 19:04:25,315] {docker.py:276} INFO - 21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Getting 3 (948.0 B) non-empty blocks including 3 (948.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:25,317] {docker.py:276} INFO - 21/05/14 22:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:25,318] {docker.py:276} INFO - 21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218828094826172570760_0002_m_000144_148, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218828094826172570760_0002_m_000144_148}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218828094826172570760_0002}; taskId=attempt_202105142203218828094826172570760_0002_m_000144_148, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@571363f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:25,318] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203218828094826172570760_0002_m_000144_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218828094826172570760_0002_m_000144_148
[2021-05-14 19:04:25,319] {docker.py:276} INFO - 21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:25,321] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203218828094826172570760_0002_m_000144_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218828094826172570760_0002_m_000144_148 : duration 0:00.003s
[2021-05-14 19:04:25,321] {docker.py:276} INFO - 21/05/14 22:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:04:25,322] {docker.py:276} INFO - 21/05/14 22:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:25,322] {docker.py:276} INFO - 21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:25,323] {docker.py:276} INFO - 21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214468500860440099646_0002_m_000145_149, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214468500860440099646_0002_m_000145_149}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214468500860440099646_0002}; taskId=attempt_202105142203214468500860440099646_0002_m_000145_149, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@381e9ee8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:25,323] {docker.py:276} INFO - 21/05/14 22:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203214468500860440099646_0002_m_000145_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214468500860440099646_0002_m_000145_149
[2021-05-14 19:04:25,326] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203214468500860440099646_0002_m_000145_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214468500860440099646_0002_m_000145_149 : duration 0:00.003s
[2021-05-14 19:04:25,515] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203218401235644664885156_0002_m_000142_146: needsTaskCommit() Task attempt_202105142203218401235644664885156_0002_m_000142_146
[2021-05-14 19:04:25,516] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203218401235644664885156_0002_m_000142_146: needsTaskCommit() Task attempt_202105142203218401235644664885156_0002_m_000142_146: duration 0:00.001s
[2021-05-14 19:04:25,517] {docker.py:276} INFO - 21/05/14 22:04:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218401235644664885156_0002_m_000142_146
[2021-05-14 19:04:25,519] {docker.py:276} INFO - 21/05/14 22:04:25 INFO Executor: Finished task 142.0 in stage 2.0 (TID 146). 4587 bytes result sent to driver
[2021-05-14 19:04:25,520] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Starting task 146.0 in stage 2.0 (TID 150) (028692ec38a6, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:25,522] {docker.py:276} INFO - 21/05/14 22:04:25 INFO TaskSetManager: Finished task 142.0 in stage 2.0 (TID 146) in 1735 ms on 028692ec38a6 (executor driver) (143/200)
21/05/14 22:04:25 INFO Executor: Running task 146.0 in stage 2.0 (TID 150)
[2021-05-14 19:04:25,531] {docker.py:276} INFO - 21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Getting 3 (1040.0 B) non-empty blocks including 3 (1040.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:25,532] {docker.py:276} INFO - 21/05/14 22:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203212269948745005585516_0002_m_000146_150, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212269948745005585516_0002_m_000146_150}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203212269948745005585516_0002}; taskId=attempt_202105142203212269948745005585516_0002_m_000146_150, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d51d492}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105142203212269948745005585516_0002_m_000146_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212269948745005585516_0002_m_000146_150
[2021-05-14 19:04:25,536] {docker.py:276} INFO - 21/05/14 22:04:25 INFO StagingCommitter: Task committer attempt_202105142203212269948745005585516_0002_m_000146_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203212269948745005585516_0002_m_000146_150 : duration 0:00.004s
[2021-05-14 19:04:26,495] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105142203218828094826172570760_0002_m_000144_148: needsTaskCommit() Task attempt_202105142203218828094826172570760_0002_m_000144_148
[2021-05-14 19:04:26,496] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Task committer attempt_202105142203218828094826172570760_0002_m_000144_148: needsTaskCommit() Task attempt_202105142203218828094826172570760_0002_m_000144_148: duration 0:00.000s
21/05/14 22:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218828094826172570760_0002_m_000144_148
[2021-05-14 19:04:26,498] {docker.py:276} INFO - 21/05/14 22:04:26 INFO Executor: Finished task 144.0 in stage 2.0 (TID 148). 4544 bytes result sent to driver
[2021-05-14 19:04:26,499] {docker.py:276} INFO - 21/05/14 22:04:26 INFO TaskSetManager: Starting task 147.0 in stage 2.0 (TID 151) (028692ec38a6, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:26,501] {docker.py:276} INFO - 21/05/14 22:04:26 INFO TaskSetManager: Finished task 144.0 in stage 2.0 (TID 148) in 1198 ms on 028692ec38a6 (executor driver) (144/200)
[2021-05-14 19:04:26,502] {docker.py:276} INFO - 21/05/14 22:04:26 INFO Executor: Running task 147.0 in stage 2.0 (TID 151)
[2021-05-14 19:04:26,511] {docker.py:276} INFO - 21/05/14 22:04:26 INFO ShuffleBlockFetcherIterator: Getting 3 (884.0 B) non-empty blocks including 3 (884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:26,515] {docker.py:276} INFO - 21/05/14 22:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:26,515] {docker.py:276} INFO - 21/05/14 22:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:26,516] {docker.py:276} INFO - 21/05/14 22:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203215159600877684708704_0002_m_000147_151, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215159600877684708704_0002_m_000147_151}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203215159600877684708704_0002}; taskId=attempt_202105142203215159600877684708704_0002_m_000147_151, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ea26c6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:26,516] {docker.py:276} INFO - 21/05/14 22:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:26,517] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105142203215159600877684708704_0002_m_000147_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215159600877684708704_0002_m_000147_151
[2021-05-14 19:04:26,520] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Task committer attempt_202105142203215159600877684708704_0002_m_000147_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203215159600877684708704_0002_m_000147_151 : duration 0:00.004s
[2021-05-14 19:04:26,910] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Starting: Task committer attempt_20210514220321901907214539967306_0002_m_000143_147: needsTaskCommit() Task attempt_20210514220321901907214539967306_0002_m_000143_147
[2021-05-14 19:04:26,912] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Task committer attempt_20210514220321901907214539967306_0002_m_000143_147: needsTaskCommit() Task attempt_20210514220321901907214539967306_0002_m_000143_147: duration 0:00.002s
21/05/14 22:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321901907214539967306_0002_m_000143_147
[2021-05-14 19:04:26,913] {docker.py:276} INFO - 21/05/14 22:04:26 INFO Executor: Finished task 143.0 in stage 2.0 (TID 147). 4544 bytes result sent to driver
[2021-05-14 19:04:26,917] {docker.py:276} INFO - 21/05/14 22:04:26 INFO TaskSetManager: Starting task 148.0 in stage 2.0 (TID 152) (028692ec38a6, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:26,917] {docker.py:276} INFO - 21/05/14 22:04:26 INFO TaskSetManager: Finished task 143.0 in stage 2.0 (TID 147) in 1697 ms on 028692ec38a6 (executor driver) (145/200)
[2021-05-14 19:04:26,918] {docker.py:276} INFO - 21/05/14 22:04:26 INFO Executor: Running task 148.0 in stage 2.0 (TID 152)
[2021-05-14 19:04:26,925] {docker.py:276} INFO - 21/05/14 22:04:26 INFO ShuffleBlockFetcherIterator: Getting 3 (944.0 B) non-empty blocks including 3 (944.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:26,927] {docker.py:276} INFO - 21/05/14 22:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203218977556618374939850_0002_m_000148_152, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218977556618374939850_0002_m_000148_152}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203218977556618374939850_0002}; taskId=attempt_202105142203218977556618374939850_0002_m_000148_152, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e0535c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:26,927] {docker.py:276} INFO - 21/05/14 22:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105142203218977556618374939850_0002_m_000148_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218977556618374939850_0002_m_000148_152
[2021-05-14 19:04:26,930] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Task committer attempt_202105142203218977556618374939850_0002_m_000148_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203218977556618374939850_0002_m_000148_152 : duration 0:00.003s
[2021-05-14 19:04:26,983] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105142203214468500860440099646_0002_m_000145_149: needsTaskCommit() Task attempt_202105142203214468500860440099646_0002_m_000145_149
[2021-05-14 19:04:26,985] {docker.py:276} INFO - 21/05/14 22:04:26 INFO StagingCommitter: Task committer attempt_202105142203214468500860440099646_0002_m_000145_149: needsTaskCommit() Task attempt_202105142203214468500860440099646_0002_m_000145_149: duration 0:00.001s
21/05/14 22:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203214468500860440099646_0002_m_000145_149
[2021-05-14 19:04:26,987] {docker.py:276} INFO - 21/05/14 22:04:26 INFO Executor: Finished task 145.0 in stage 2.0 (TID 149). 4544 bytes result sent to driver
[2021-05-14 19:04:26,988] {docker.py:276} INFO - 21/05/14 22:04:26 INFO TaskSetManager: Starting task 149.0 in stage 2.0 (TID 153) (028692ec38a6, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:26,989] {docker.py:276} INFO - 21/05/14 22:04:26 INFO TaskSetManager: Finished task 145.0 in stage 2.0 (TID 149) in 1682 ms on 028692ec38a6 (executor driver) (146/200)
21/05/14 22:04:26 INFO Executor: Running task 149.0 in stage 2.0 (TID 153)
[2021-05-14 19:04:26,999] {docker.py:276} INFO - 21/05/14 22:04:27 INFO ShuffleBlockFetcherIterator: Getting 3 (799.0 B) non-empty blocks including 3 (799.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:27,001] {docker.py:276} INFO - 21/05/14 22:04:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:04:27,001] {docker.py:276} INFO - 21/05/14 22:04:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:27,002] {docker.py:276} INFO - 21/05/14 22:04:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051422032171640881158992136_0002_m_000149_153, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_2021051422032171640881158992136_0002_m_000149_153}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051422032171640881158992136_0002}; taskId=attempt_2021051422032171640881158992136_0002_m_000149_153, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b002b22}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:27 INFO StagingCommitter: Starting: Task committer attempt_2021051422032171640881158992136_0002_m_000149_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_2021051422032171640881158992136_0002_m_000149_153
[2021-05-14 19:04:27,005] {docker.py:276} INFO - 21/05/14 22:04:27 INFO StagingCommitter: Task committer attempt_2021051422032171640881158992136_0002_m_000149_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_2021051422032171640881158992136_0002_m_000149_153 : duration 0:00.003s
[2021-05-14 19:04:27,201] {docker.py:276} INFO - 21/05/14 22:04:27 INFO StagingCommitter: Starting: Task committer attempt_202105142203212269948745005585516_0002_m_000146_150: needsTaskCommit() Task attempt_202105142203212269948745005585516_0002_m_000146_150
[2021-05-14 19:04:27,203] {docker.py:276} INFO - 21/05/14 22:04:27 INFO StagingCommitter: Task committer attempt_202105142203212269948745005585516_0002_m_000146_150: needsTaskCommit() Task attempt_202105142203212269948745005585516_0002_m_000146_150: duration 0:00.002s
21/05/14 22:04:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203212269948745005585516_0002_m_000146_150
[2021-05-14 19:04:27,204] {docker.py:276} INFO - 21/05/14 22:04:27 INFO Executor: Finished task 146.0 in stage 2.0 (TID 150). 4544 bytes result sent to driver
[2021-05-14 19:04:27,206] {docker.py:276} INFO - 21/05/14 22:04:27 INFO TaskSetManager: Starting task 150.0 in stage 2.0 (TID 154) (028692ec38a6, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:27,207] {docker.py:276} INFO - 21/05/14 22:04:27 INFO TaskSetManager: Finished task 146.0 in stage 2.0 (TID 150) in 1688 ms on 028692ec38a6 (executor driver) (147/200)
[2021-05-14 19:04:27,208] {docker.py:276} INFO - 21/05/14 22:04:27 INFO Executor: Running task 150.0 in stage 2.0 (TID 154)
[2021-05-14 19:04:27,216] {docker.py:276} INFO - 21/05/14 22:04:27 INFO ShuffleBlockFetcherIterator: Getting 3 (914.0 B) non-empty blocks including 3 (914.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:27,218] {docker.py:276} INFO - 21/05/14 22:04:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321550490101895898707_0002_m_000150_154, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321550490101895898707_0002_m_000150_154}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321550490101895898707_0002}; taskId=attempt_20210514220321550490101895898707_0002_m_000150_154, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@da2efd8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:27 INFO StagingCommitter: Starting: Task committer attempt_20210514220321550490101895898707_0002_m_000150_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321550490101895898707_0002_m_000150_154
[2021-05-14 19:04:27,221] {docker.py:276} INFO - 21/05/14 22:04:27 INFO StagingCommitter: Task committer attempt_20210514220321550490101895898707_0002_m_000150_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321550490101895898707_0002_m_000150_154 : duration 0:00.003s
[2021-05-14 19:04:28,166] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105142203215159600877684708704_0002_m_000147_151: needsTaskCommit() Task attempt_202105142203215159600877684708704_0002_m_000147_151
[2021-05-14 19:04:28,167] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_202105142203215159600877684708704_0002_m_000147_151: needsTaskCommit() Task attempt_202105142203215159600877684708704_0002_m_000147_151: duration 0:00.001s
21/05/14 22:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203215159600877684708704_0002_m_000147_151
[2021-05-14 19:04:28,167] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Finished task 147.0 in stage 2.0 (TID 151). 4544 bytes result sent to driver
[2021-05-14 19:04:28,168] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Starting task 151.0 in stage 2.0 (TID 155) (028692ec38a6, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:28,169] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Finished task 147.0 in stage 2.0 (TID 151) in 1674 ms on 028692ec38a6 (executor driver) (148/200)
[2021-05-14 19:04:28,170] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Running task 151.0 in stage 2.0 (TID 155)
[2021-05-14 19:04:28,178] {docker.py:276} INFO - 21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Getting 3 (1013.0 B) non-empty blocks including 3 (1013.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:28,181] {docker.py:276} INFO - 21/05/14 22:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:28,181] {docker.py:276} INFO - 21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216439745936786048247_0002_m_000151_155, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216439745936786048247_0002_m_000151_155}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216439745936786048247_0002}; taskId=attempt_202105142203216439745936786048247_0002_m_000151_155, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5714ba71}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105142203216439745936786048247_0002_m_000151_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216439745936786048247_0002_m_000151_155
[2021-05-14 19:04:28,185] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_202105142203216439745936786048247_0002_m_000151_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216439745936786048247_0002_m_000151_155 : duration 0:00.004s
[2021-05-14 19:04:28,537] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105142203218977556618374939850_0002_m_000148_152: needsTaskCommit() Task attempt_202105142203218977556618374939850_0002_m_000148_152
[2021-05-14 19:04:28,538] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_202105142203218977556618374939850_0002_m_000148_152: needsTaskCommit() Task attempt_202105142203218977556618374939850_0002_m_000148_152: duration 0:00.001s
21/05/14 22:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203218977556618374939850_0002_m_000148_152
[2021-05-14 19:04:28,539] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Finished task 148.0 in stage 2.0 (TID 152). 4544 bytes result sent to driver
[2021-05-14 19:04:28,540] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Starting task 152.0 in stage 2.0 (TID 156) (028692ec38a6, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:28,541] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Running task 152.0 in stage 2.0 (TID 156)
[2021-05-14 19:04:28,542] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Finished task 148.0 in stage 2.0 (TID 152) in 1630 ms on 028692ec38a6 (executor driver) (149/200)
[2021-05-14 19:04:28,553] {docker.py:276} INFO - 21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Getting 3 (836.0 B) non-empty blocks including 3 (836.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:28,555] {docker.py:276} INFO - 21/05/14 22:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:28,555] {docker.py:276} INFO - 21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203214175910852787676260_0002_m_000152_156, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214175910852787676260_0002_m_000152_156}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203214175910852787676260_0002}; taskId=attempt_202105142203214175910852787676260_0002_m_000152_156, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5916d12c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:28,555] {docker.py:276} INFO - 21/05/14 22:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:28,556] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105142203214175910852787676260_0002_m_000152_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214175910852787676260_0002_m_000152_156
[2021-05-14 19:04:28,558] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_202105142203214175910852787676260_0002_m_000152_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203214175910852787676260_0002_m_000152_156 : duration 0:00.003s
[2021-05-14 19:04:28,739] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_2021051422032171640881158992136_0002_m_000149_153: needsTaskCommit() Task attempt_2021051422032171640881158992136_0002_m_000149_153
[2021-05-14 19:04:28,739] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_2021051422032171640881158992136_0002_m_000149_153: needsTaskCommit() Task attempt_2021051422032171640881158992136_0002_m_000149_153: duration 0:00.000s
21/05/14 22:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051422032171640881158992136_0002_m_000149_153
[2021-05-14 19:04:28,742] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Finished task 149.0 in stage 2.0 (TID 153). 4544 bytes result sent to driver
[2021-05-14 19:04:28,742] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Starting task 153.0 in stage 2.0 (TID 157) (028692ec38a6, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:28,743] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Finished task 149.0 in stage 2.0 (TID 153) in 1758 ms on 028692ec38a6 (executor driver) (150/200)
[2021-05-14 19:04:28,744] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Running task 153.0 in stage 2.0 (TID 157)
[2021-05-14 19:04:28,754] {docker.py:276} INFO - 21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Getting 3 (948.0 B) non-empty blocks including 3 (948.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:28,755] {docker.py:276} INFO - 21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:28,758] {docker.py:276} INFO - 21/05/14 22:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203213802963186336224302_0002_m_000153_157, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213802963186336224302_0002_m_000153_157}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203213802963186336224302_0002}; taskId=attempt_202105142203213802963186336224302_0002_m_000153_157, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49a67d26}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:28,758] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105142203213802963186336224302_0002_m_000153_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213802963186336224302_0002_m_000153_157
[2021-05-14 19:04:28,760] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_202105142203213802963186336224302_0002_m_000153_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203213802963186336224302_0002_m_000153_157 : duration 0:00.003s
[2021-05-14 19:04:28,855] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_20210514220321550490101895898707_0002_m_000150_154: needsTaskCommit() Task attempt_20210514220321550490101895898707_0002_m_000150_154
[2021-05-14 19:04:28,856] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_20210514220321550490101895898707_0002_m_000150_154: needsTaskCommit() Task attempt_20210514220321550490101895898707_0002_m_000150_154: duration 0:00.001s
21/05/14 22:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220321550490101895898707_0002_m_000150_154
[2021-05-14 19:04:28,857] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Finished task 150.0 in stage 2.0 (TID 154). 4544 bytes result sent to driver
[2021-05-14 19:04:28,859] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Starting task 154.0 in stage 2.0 (TID 158) (028692ec38a6, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:28,860] {docker.py:276} INFO - 21/05/14 22:04:28 INFO Executor: Running task 154.0 in stage 2.0 (TID 158)
[2021-05-14 19:04:28,860] {docker.py:276} INFO - 21/05/14 22:04:28 INFO TaskSetManager: Finished task 150.0 in stage 2.0 (TID 154) in 1656 ms on 028692ec38a6 (executor driver) (151/200)
[2021-05-14 19:04:28,878] {docker.py:276} INFO - 21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Getting 3 (919.0 B) non-empty blocks including 3 (919.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:04:28,878] {docker.py:276} INFO - 21/05/14 22:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:28,880] {docker.py:276} INFO - 21/05/14 22:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220321686103849366847558_0002_m_000154_158, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321686103849366847558_0002_m_000154_158}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220321686103849366847558_0002}; taskId=attempt_20210514220321686103849366847558_0002_m_000154_158, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b9813d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:28,880] {docker.py:276} INFO - 21/05/14 22:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:04:28,881] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Starting: Task committer attempt_20210514220321686103849366847558_0002_m_000154_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321686103849366847558_0002_m_000154_158
[2021-05-14 19:04:28,884] {docker.py:276} INFO - 21/05/14 22:04:28 INFO StagingCommitter: Task committer attempt_20210514220321686103849366847558_0002_m_000154_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_20210514220321686103849366847558_0002_m_000154_158 : duration 0:00.003s
[2021-05-14 19:04:29,396] {docker.py:276} INFO - 21/05/14 22:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203216439745936786048247_0002_m_000151_155: needsTaskCommit() Task attempt_202105142203216439745936786048247_0002_m_000151_155
21/05/14 22:04:29 INFO StagingCommitter: Task committer attempt_202105142203216439745936786048247_0002_m_000151_155: needsTaskCommit() Task attempt_202105142203216439745936786048247_0002_m_000151_155: duration 0:00.001s
21/05/14 22:04:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142203216439745936786048247_0002_m_000151_155
21/05/14 22:04:29 INFO Executor: Finished task 151.0 in stage 2.0 (TID 155). 4587 bytes result sent to driver
[2021-05-14 19:04:29,398] {docker.py:276} INFO - 21/05/14 22:04:29 INFO TaskSetManager: Starting task 155.0 in stage 2.0 (TID 159) (028692ec38a6, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:04:29,398] {docker.py:276} INFO - 21/05/14 22:04:29 INFO Executor: Running task 155.0 in stage 2.0 (TID 159)
[2021-05-14 19:04:29,399] {docker.py:276} INFO - 21/05/14 22:04:29 INFO TaskSetManager: Finished task 151.0 in stage 2.0 (TID 155) in 1232 ms on 028692ec38a6 (executor driver) (152/200)
[2021-05-14 19:04:29,408] {docker.py:276} INFO - 21/05/14 22:04:29 INFO ShuffleBlockFetcherIterator: Getting 3 (978.0 B) non-empty blocks including 3 (978.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:04:29,410] {docker.py:276} INFO - 21/05/14 22:04:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:04:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:04:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:04:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142203216564481843217266641_0002_m_000155_159, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216564481843217266641_0002_m_000155_159}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142203216564481843217266641_0002}; taskId=attempt_202105142203216564481843217266641_0002_m_000155_159, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78a2827c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:04:29,410] {docker.py:276} INFO - 21/05/14 22:04:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105142203216564481843217266641_0002_m_000155_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216564481843217266641_0002_m_000155_159
[2021-05-14 19:04:29,415] {docker.py:276} INFO - 21/05/14 22:04:29 INFO StagingCommitter: Task committer attempt_202105142203216564481843217266641_0002_m_000155_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5c3bca0c-1a4d-4e24-9270-9e96fad8d0ac/_temporary/0/_temporary/attempt_202105142203216564481843217266641_0002_m_000155_159 : duration 0:00.005s
[2021-05-14 19:04:29,988] {local_task_job.py:188} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2021-05-14 19:04:30,000] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 25355
[2021-05-14 19:04:30,001] {taskinstance.py:1265} ERROR - Received SIGTERM. Terminating subprocesses.
[2021-05-14 19:04:30,001] {docker.py:347} INFO - Stopping docker container
[2021-05-14 19:04:30,818] {process_utils.py:66} INFO - Process psutil.Process(pid=25355, status='terminated', exitcode=0, started='19:02:54') (25355) terminated with exit code 0
