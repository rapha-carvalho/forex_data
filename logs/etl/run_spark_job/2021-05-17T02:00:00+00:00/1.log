[2021-05-17 00:10:12,051] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T02:00:00+00:00 [queued]>
[2021-05-17 00:10:12,060] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T02:00:00+00:00 [queued]>
[2021-05-17 00:10:12,060] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 00:10:12,060] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-17 00:10:12,060] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-17 00:10:12,070] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-17T02:00:00+00:00
[2021-05-17 00:10:12,074] {standard_task_runner.py:52} INFO - Started process 33406 to run task
[2021-05-17 00:10:12,084] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-17T02:00:00+00:00', '--job-id', '870', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpos8fomt3', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpzn9to63k']
[2021-05-17 00:10:12,087] {standard_task_runner.py:77} INFO - Job 870: Subtask run_spark_job
[2021-05-17 00:10:12,138] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-17T02:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-17 00:10:12,178] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-17T02:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-17T02:00:00+00:00
[2021-05-17 00:10:12,182] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-17 00:10:15,288] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-17 00:10:15,294] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-17 00:10:15,294] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-17 00:10:15,302] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-17 00:10:18,231] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-17 00:10:18,986] {docker.py:276} INFO - 21/05/17 03:10:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-17 00:10:21,451] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-17 00:10:21,467] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SparkContext: Running Spark version 3.1.1
[2021-05-17 00:10:21,545] {docker.py:276} INFO - 21/05/17 03:10:21 INFO ResourceUtils: ==============================================================
[2021-05-17 00:10:21,546] {docker.py:276} INFO - 21/05/17 03:10:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-17 00:10:21,548] {docker.py:276} INFO - 21/05/17 03:10:21 INFO ResourceUtils: ==============================================================
[2021-05-17 00:10:21,549] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SparkContext: Submitted application: spark.py
[2021-05-17 00:10:21,591] {docker.py:276} INFO - 21/05/17 03:10:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-17 00:10:21,611] {docker.py:276} INFO - 21/05/17 03:10:21 INFO ResourceProfile: Limiting resource is cpu
[2021-05-17 00:10:21,612] {docker.py:276} INFO - 21/05/17 03:10:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-17 00:10:21,700] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-17 00:10:21,700] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-17 00:10:21,701] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SecurityManager: Changing view acls groups to:
[2021-05-17 00:10:21,702] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SecurityManager: Changing modify acls groups to:
[2021-05-17 00:10:21,702] {docker.py:276} INFO - 21/05/17 03:10:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-17 00:10:22,137] {docker.py:276} INFO - 21/05/17 03:10:22 INFO Utils: Successfully started service 'sparkDriver' on port 38471.
[2021-05-17 00:10:22,192] {docker.py:276} INFO - 21/05/17 03:10:22 INFO SparkEnv: Registering MapOutputTracker
[2021-05-17 00:10:22,254] {docker.py:276} INFO - 21/05/17 03:10:22 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-17 00:10:22,296] {docker.py:276} INFO - 21/05/17 03:10:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-17 00:10:22,298] {docker.py:276} INFO - 21/05/17 03:10:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-17 00:10:22,308] {docker.py:276} INFO - 21/05/17 03:10:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-17 00:10:22,335] {docker.py:276} INFO - 21/05/17 03:10:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6509dac8-be0d-4aa1-bf05-bcc9a90eadf5
[2021-05-17 00:10:22,370] {docker.py:276} INFO - 21/05/17 03:10:22 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-17 00:10:22,401] {docker.py:276} INFO - 21/05/17 03:10:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-17 00:10:22,777] {docker.py:276} INFO - 21/05/17 03:10:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-17 00:10:22,878] {docker.py:276} INFO - 21/05/17 03:10:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://50e6ccadb8f1:4040
[2021-05-17 00:10:23,203] {docker.py:276} INFO - 21/05/17 03:10:23 INFO Executor: Starting executor ID driver on host 50e6ccadb8f1
[2021-05-17 00:10:23,256] {docker.py:276} INFO - 21/05/17 03:10:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45201.
21/05/17 03:10:23 INFO NettyBlockTransferService: Server created on 50e6ccadb8f1:45201
[2021-05-17 00:10:23,259] {docker.py:276} INFO - 21/05/17 03:10:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-17 00:10:23,275] {docker.py:276} INFO - 21/05/17 03:10:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
[2021-05-17 00:10:23,286] {docker.py:276} INFO - 21/05/17 03:10:23 INFO BlockManagerMasterEndpoint: Registering block manager 50e6ccadb8f1:45201 with 934.4 MiB RAM, BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
[2021-05-17 00:10:23,293] {docker.py:276} INFO - 21/05/17 03:10:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
[2021-05-17 00:10:23,295] {docker.py:276} INFO - 21/05/17 03:10:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
[2021-05-17 00:10:23,980] {docker.py:276} INFO - 21/05/17 03:10:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-17 00:10:23,981] {docker.py:276} INFO - 21/05/17 03:10:23 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-17 00:10:25,327] {docker.py:276} INFO - 21/05/17 03:10:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-17 00:10:25,382] {docker.py:276} INFO - 21/05/17 03:10:25 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-17 00:10:25,383] {docker.py:276} INFO - 21/05/17 03:10:25 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-17 00:10:33,309] {docker.py:276} INFO - 21/05/17 03:10:33 INFO InMemoryFileIndex: It took 1710 ms to list leaf files for 9 paths.
[2021-05-17 00:10:34,985] {docker.py:276} INFO - 21/05/17 03:10:35 INFO InMemoryFileIndex: It took 1557 ms to list leaf files for 9 paths.
[2021-05-17 00:10:38,446] {docker.py:276} INFO - 21/05/17 03:10:38 INFO FileSourceStrategy: Pushed Filters:
[2021-05-17 00:10:38,455] {docker.py:276} INFO - 21/05/17 03:10:38 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-17 00:10:38,462] {docker.py:276} INFO - 21/05/17 03:10:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-17 00:10:39,664] {docker.py:276} INFO - 21/05/17 03:10:39 INFO CodeGenerator: Code generated in 446.0472 ms
[2021-05-17 00:10:39,779] {docker.py:276} INFO - 21/05/17 03:10:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-17 00:10:39,932] {docker.py:276} INFO - 21/05/17 03:10:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-17 00:10:39,938] {docker.py:276} INFO - 21/05/17 03:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 50e6ccadb8f1:45201 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 00:10:39,945] {docker.py:276} INFO - 21/05/17 03:10:39 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 00:10:39,989] {docker.py:276} INFO - 21/05/17 03:10:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9610970 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 00:10:40,236] {docker.py:276} INFO - 21/05/17 03:10:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 00:10:40,273] {docker.py:276} INFO - 21/05/17 03:10:40 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-17 00:10:40,274] {docker.py:276} INFO - 21/05/17 03:10:40 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 00:10:40,276] {docker.py:276} INFO - 21/05/17 03:10:40 INFO DAGScheduler: Parents of final stage: List()
[2021-05-17 00:10:40,280] {docker.py:276} INFO - 21/05/17 03:10:40 INFO DAGScheduler: Missing parents: List()
[2021-05-17 00:10:40,291] {docker.py:276} INFO - 21/05/17 03:10:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 00:10:40,432] {docker.py:276} INFO - 21/05/17 03:10:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-17 00:10:40,457] {docker.py:276} INFO - 21/05/17 03:10:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-17 00:10:40,459] {docker.py:276} INFO - 21/05/17 03:10:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 50e6ccadb8f1:45201 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-17 00:10:40,463] {docker.py:276} INFO - 21/05/17 03:10:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-17 00:10:40,493] {docker.py:276} INFO - 21/05/17 03:10:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-17 00:10:40,497] {docker.py:276} INFO - 21/05/17 03:10:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-05-17 00:10:40,635] {docker.py:276} INFO - 21/05/17 03:10:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (50e6ccadb8f1, executor driver, partition 0, PROCESS_LOCAL, 5125 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:40,676] {docker.py:276} INFO - 21/05/17 03:10:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-17 00:10:40,939] {docker.py:276} INFO - 21/05/17 03:10:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104224, partition values: [empty row]
[2021-05-17 00:10:40,982] {docker.py:276} INFO - 21/05/17 03:10:40 INFO CodeGenerator: Code generated in 30.3222 ms
[2021-05-17 00:10:41,991] {docker.py:276} INFO - 21/05/17 03:10:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1607 bytes result sent to driver
[2021-05-17 00:10:42,009] {docker.py:276} INFO - 21/05/17 03:10:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1392 ms on 50e6ccadb8f1 (executor driver) (1/1)
[2021-05-17 00:10:42,014] {docker.py:276} INFO - 21/05/17 03:10:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-17 00:10:42,027] {docker.py:276} INFO - 21/05/17 03:10:42 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.706 s
[2021-05-17 00:10:42,036] {docker.py:276} INFO - 21/05/17 03:10:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-17 00:10:42,038] {docker.py:276} INFO - 21/05/17 03:10:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-17 00:10:42,043] {docker.py:276} INFO - 21/05/17 03:10:42 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.807598 s
[2021-05-17 00:10:42,090] {docker.py:276} INFO - 21/05/17 03:10:42 INFO CodeGenerator: Code generated in 22.32 ms
[2021-05-17 00:10:42,198] {docker.py:276} INFO - 21/05/17 03:10:42 INFO FileSourceStrategy: Pushed Filters:
[2021-05-17 00:10:42,198] {docker.py:276} INFO - 21/05/17 03:10:42 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-17 00:10:42,199] {docker.py:276} INFO - 21/05/17 03:10:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-17 00:10:42,212] {docker.py:276} INFO - 21/05/17 03:10:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-17 00:10:42,258] {docker.py:276} INFO - 21/05/17 03:10:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-17 00:10:42,275] {docker.py:276} INFO - 21/05/17 03:10:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 50e6ccadb8f1:45201 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 00:10:42,277] {docker.py:276} INFO - 21/05/17 03:10:42 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 00:10:42,280] {docker.py:276} INFO - 21/05/17 03:10:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9610970 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 00:10:42,295] {docker.py:276} INFO - 21/05/17 03:10:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 50e6ccadb8f1:45201 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 00:10:42,396] {docker.py:276} INFO - 21/05/17 03:10:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 50e6ccadb8f1:45201 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-17 00:10:43,195] {docker.py:276} INFO - 21/05/17 03:10:43 INFO FileSourceStrategy: Pushed Filters:
[2021-05-17 00:10:43,196] {docker.py:276} INFO - 21/05/17 03:10:43 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-17 00:10:43,197] {docker.py:276} INFO - 21/05/17 03:10:43 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-17 00:10:44,406] {docker.py:276} INFO - 21/05/17 03:10:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:44,412] {docker.py:276} INFO - 21/05/17 03:10:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:44,413] {docker.py:276} INFO - 21/05/17 03:10:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044670033147968055892_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044670033147968055892_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044670033147968055892_0000}; taskId=attempt_20210517031044670033147968055892_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73ced1f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:44,414] {docker.py:276} INFO - 21/05/17 03:10:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:44,462] {docker.py:276} INFO - 21/05/17 03:10:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-17 00:10:44,586] {docker.py:276} INFO - 21/05/17 03:10:44 INFO CodeGenerator: Code generated in 79.9653 ms
[2021-05-17 00:10:44,589] {docker.py:276} INFO - 21/05/17 03:10:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-17 00:10:44,654] {docker.py:276} INFO - 21/05/17 03:10:44 INFO CodeGenerator: Code generated in 52.4982 ms
[2021-05-17 00:10:44,660] {docker.py:276} INFO - 21/05/17 03:10:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.5 KiB, free 934.0 MiB)
[2021-05-17 00:10:44,697] {docker.py:276} INFO - 21/05/17 03:10:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-17 00:10:44,699] {docker.py:276} INFO - 21/05/17 03:10:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 50e6ccadb8f1:45201 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-17 00:10:44,702] {docker.py:276} INFO - 21/05/17 03:10:44 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0
[2021-05-17 00:10:44,709] {docker.py:276} INFO - 21/05/17 03:10:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9610970 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-17 00:10:44,907] {docker.py:276} INFO - 21/05/17 03:10:44 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-17 00:10:44,914] {docker.py:276} INFO - 21/05/17 03:10:44 INFO DAGScheduler: Registering RDD 13 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-17 00:10:44,918] {docker.py:276} INFO - 21/05/17 03:10:44 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/17 03:10:44 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-17 00:10:44,919] {docker.py:276} INFO - 21/05/17 03:10:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2021-05-17 00:10:44,922] {docker.py:276} INFO - 21/05/17 03:10:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
[2021-05-17 00:10:44,928] {docker.py:276} INFO - 21/05/17 03:10:44 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 00:10:44,949] {docker.py:276} INFO - 21/05/17 03:10:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-17 00:10:44,976] {docker.py:276} INFO - 21/05/17 03:10:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-17 00:10:44,978] {docker.py:276} INFO - 21/05/17 03:10:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 50e6ccadb8f1:45201 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-17 00:10:44,980] {docker.py:276} INFO - 21/05/17 03:10:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
[2021-05-17 00:10:44,983] {docker.py:276} INFO - 21/05/17 03:10:44 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
21/05/17 03:10:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0
[2021-05-17 00:10:44,987] {docker.py:276} INFO - 21/05/17 03:10:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (50e6ccadb8f1, executor driver, partition 0, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:44,988] {docker.py:276} INFO - 21/05/17 03:10:44 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (50e6ccadb8f1, executor driver, partition 1, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:44,990] {docker.py:276} INFO - 21/05/17 03:10:44 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (50e6ccadb8f1, executor driver, partition 2, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:44,991] {docker.py:276} INFO - 21/05/17 03:10:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2021-05-17 00:10:45,013] {docker.py:276} INFO - 21/05/17 03:10:45 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2021-05-17 00:10:45,014] {docker.py:276} INFO - 21/05/17 03:10:45 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[2021-05-17 00:10:45,156] {docker.py:276} INFO - 21/05/17 03:10:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 50e6ccadb8f1:45201 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-17 00:10:45,211] {docker.py:276} INFO - 21/05/17 03:10:45 INFO CodeGenerator: Code generated in 86.6446 ms
[2021-05-17 00:10:45,258] {docker.py:276} INFO - 21/05/17 03:10:45 INFO CodeGenerator: Code generated in 20.3936 ms
[2021-05-17 00:10:45,303] {docker.py:276} INFO - 21/05/17 03:10:45 INFO CodeGenerator: Code generated in 33.2471 ms
[2021-05-17 00:10:45,328] {docker.py:276} INFO - 21/05/17 03:10:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_00_10_00/from_1621215193_to_1621215600.csv, range: 0-23658, partition values: [empty row]
[2021-05-17 00:10:45,331] {docker.py:276} INFO - 21/05/17 03:10:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104224, partition values: [empty row]
[2021-05-17 00:10:45,336] {docker.py:276} INFO - 21/05/17 03:10:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_00_10_00/from_1621217400_to_1621219200.csv, range: 0-104040, partition values: [empty row]
[2021-05-17 00:10:46,194] {docker.py:276} INFO - 21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104004, partition values: [empty row]
[2021-05-17 00:10:46,371] {docker.py:276} INFO - 21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621215193_to_1621215600.csv, range: 0-23634, partition values: [empty row]
[2021-05-17 00:10:46,669] {docker.py:276} INFO - 21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104211, partition values: [empty row]
[2021-05-17 00:10:46,691] {docker.py:276} INFO - 21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_00_10_00/from_1621217400_to_1621219200.csv, range: 0-103631, partition values: [empty row]
[2021-05-17 00:10:46,705] {docker.py:276} INFO - 21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_00_10_00/from_1621215193_to_1621215600.csv, range: 0-23627, partition values: [empty row]
[2021-05-17 00:10:47,158] {docker.py:276} INFO - 21/05/17 03:10:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621217400_to_1621219200.csv, range: 0-104117, partition values: [empty row]
[2021-05-17 00:10:47,434] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2722 bytes result sent to driver
[2021-05-17 00:10:47,440] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2722 bytes result sent to driver
[2021-05-17 00:10:47,442] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2457 ms on 50e6ccadb8f1 (executor driver) (1/3)
[2021-05-17 00:10:47,449] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2463 ms on 50e6ccadb8f1 (executor driver) (2/3)
[2021-05-17 00:10:47,726] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2679 bytes result sent to driver
[2021-05-17 00:10:47,730] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2747 ms on 50e6ccadb8f1 (executor driver) (3/3)
[2021-05-17 00:10:47,732] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.798 s
[2021-05-17 00:10:47,733] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: looking for newly runnable stages
[2021-05-17 00:10:47,734] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: running: Set()
[2021-05-17 00:10:47,735] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2021-05-17 00:10:47,736] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: failed: Set()
[2021-05-17 00:10:47,739] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-17 00:10:47,743] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-17 00:10:47,820] {docker.py:276} INFO - 21/05/17 03:10:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-17 00:10:47,828] {docker.py:276} INFO - 21/05/17 03:10:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-17 00:10:47,829] {docker.py:276} INFO - 21/05/17 03:10:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 50e6ccadb8f1:45201 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-17 00:10:47,831] {docker.py:276} INFO - 21/05/17 03:10:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
[2021-05-17 00:10:47,835] {docker.py:276} INFO - 21/05/17 03:10:47 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-17 00:10:47,835] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2021-05-17 00:10:47,846] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (50e6ccadb8f1, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:47,847] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (50e6ccadb8f1, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:47,849] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 6) (50e6ccadb8f1, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:47,850] {docker.py:276} INFO - 21/05/17 03:10:47 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 7) (50e6ccadb8f1, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:47,852] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)
[2021-05-17 00:10:47,852] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Running task 2.0 in stage 2.0 (TID 6)
[2021-05-17 00:10:47,853] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)
[2021-05-17 00:10:47,857] {docker.py:276} INFO - 21/05/17 03:10:47 INFO Executor: Running task 3.0 in stage 2.0 (TID 7)
[2021-05-17 00:10:48,059] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:48,061] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:48,062] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:48,063] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:48,063] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
[2021-05-17 00:10:48,064] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
[2021-05-17 00:10:48,064] {docker.py:276} INFO - 21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
[2021-05-17 00:10:48,086] {docker.py:276} INFO - 21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:10:48,086] {docker.py:276} INFO - 21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:48,087] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,088] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446699488705347514612_0002_m_000002_6, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446699488705347514612_0002_m_000002_6}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446699488705347514612_0002}; taskId=attempt_202105170310446699488705347514612_0002_m_000002_6, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b076477}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,089] {docker.py:276} INFO - 21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:48,090] {docker.py:276} INFO - 21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:48,091] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,092] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443473590844231302139_0002_m_000003_7, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443473590844231302139_0002_m_000003_7}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443473590844231302139_0002}; taskId=attempt_202105170310443473590844231302139_0002_m_000003_7, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51c9c43c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,093] {docker.py:276} INFO - 21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:48,096] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446699488705347514612_0002_m_000002_6
[2021-05-17 00:10:48,097] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443473590844231302139_0002_m_000003_7
[2021-05-17 00:10:48,098] {docker.py:276} INFO - 21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:48,099] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,099] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044683771020940094180_0002_m_000001_5, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044683771020940094180_0002_m_000001_5}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044683771020940094180_0002}; taskId=attempt_20210517031044683771020940094180_0002_m_000001_5, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a0c8011}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044683771020940094180_0002_m_000001_5
[2021-05-17 00:10:48,101] {docker.py:276} INFO - 21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:10:48,101] {docker.py:276} INFO - 21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:48,102] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,102] {docker.py:276} INFO - 21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446510374913935670487_0002_m_000000_4, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446510374913935670487_0002_m_000000_4}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446510374913935670487_0002}; taskId=attempt_202105170310446510374913935670487_0002_m_000000_4, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7832dbf2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:48,103] {docker.py:276} INFO - 21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:48,103] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446510374913935670487_0002_m_000000_4
[2021-05-17 00:10:48,136] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446510374913935670487_0002_m_000000_4 : duration 0:00.030s
[2021-05-17 00:10:48,140] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044683771020940094180_0002_m_000001_5 : duration 0:00.041s
[2021-05-17 00:10:48,156] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446699488705347514612_0002_m_000002_6 : duration 0:00.061s
[2021-05-17 00:10:48,165] {docker.py:276} INFO - 21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443473590844231302139_0002_m_000003_7 : duration 0:00.068s
[2021-05-17 00:10:50,067] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: needsTaskCommit() Task attempt_202105170310443473590844231302139_0002_m_000003_7
[2021-05-17 00:10:50,067] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: needsTaskCommit() Task attempt_202105170310443473590844231302139_0002_m_000003_7: duration 0:00.001s
[2021-05-17 00:10:50,069] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443473590844231302139_0002_m_000003_7
[2021-05-17 00:10:50,076] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: needsTaskCommit() Task attempt_202105170310446699488705347514612_0002_m_000002_6
[2021-05-17 00:10:50,077] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: needsTaskCommit() Task attempt_202105170310446699488705347514612_0002_m_000002_6: duration 0:00.001s
[2021-05-17 00:10:50,079] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446699488705347514612_0002_m_000002_6
[2021-05-17 00:10:50,086] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: needsTaskCommit() Task attempt_202105170310446510374913935670487_0002_m_000000_4
[2021-05-17 00:10:50,086] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: needsTaskCommit() Task attempt_202105170310446510374913935670487_0002_m_000000_4: duration 0:00.001s
21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446510374913935670487_0002_m_000000_4
[2021-05-17 00:10:50,090] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Finished task 2.0 in stage 2.0 (TID 6). 4630 bytes result sent to driver
[2021-05-17 00:10:50,092] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Finished task 3.0 in stage 2.0 (TID 7). 4630 bytes result sent to driver
[2021-05-17 00:10:50,094] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 8) (50e6ccadb8f1, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:50,097] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 9) (50e6ccadb8f1, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:50,099] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Running task 4.0 in stage 2.0 (TID 8)
[2021-05-17 00:10:50,100] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 6) in 2253 ms on 50e6ccadb8f1 (executor driver) (1/200)
[2021-05-17 00:10:50,101] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 7) in 2253 ms on 50e6ccadb8f1 (executor driver) (2/200)
[2021-05-17 00:10:50,101] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Running task 5.0 in stage 2.0 (TID 9)
[2021-05-17 00:10:50,107] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 4587 bytes result sent to driver
[2021-05-17 00:10:50,110] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 10) (50e6ccadb8f1, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:50,111] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2271 ms on 50e6ccadb8f1 (executor driver) (3/200)
[2021-05-17 00:10:50,113] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Running task 6.0 in stage 2.0 (TID 10)
[2021-05-17 00:10:50,133] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:50,133] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 00:10:50,137] {docker.py:276} INFO - 21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:10:50,137] {docker.py:276} INFO - 21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:50,138] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,139] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044846538821421497868_0002_m_000005_9, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044846538821421497868_0002_m_000005_9}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044846538821421497868_0002}; taskId=attempt_20210517031044846538821421497868_0002_m_000005_9, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19344e40}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,140] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:50,140] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044846538821421497868_0002_m_000005_9
[2021-05-17 00:10:50,147] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (1983.0 B) non-empty blocks including 3 (1983.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:50,149] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-17 00:10:50,157] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.0 KiB) non-empty blocks including 3 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:10:50,160] {docker.py:276} INFO - 21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:50,161] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,162] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446963036895015102514_0002_m_000006_10, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446963036895015102514_0002_m_000006_10}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446963036895015102514_0002}; taskId=attempt_202105170310446963036895015102514_0002_m_000006_10, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13b7740e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,163] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:50,163] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446963036895015102514_0002_m_000006_10
[2021-05-17 00:10:50,166] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044846538821421497868_0002_m_000005_9 : duration 0:00.026s
[2021-05-17 00:10:50,183] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446963036895015102514_0002_m_000006_10 : duration 0:00.021s
[2021-05-17 00:10:50,184] {docker.py:276} INFO - 21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:50,185] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,185] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444522578803307905687_0002_m_000004_8, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444522578803307905687_0002_m_000004_8}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444522578803307905687_0002}; taskId=attempt_202105170310444522578803307905687_0002_m_000004_8, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b39a080}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,186] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:50,189] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444522578803307905687_0002_m_000004_8
[2021-05-17 00:10:50,204] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444522578803307905687_0002_m_000004_8 : duration 0:00.018s
[2021-05-17 00:10:50,322] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: needsTaskCommit() Task attempt_20210517031044683771020940094180_0002_m_000001_5
[2021-05-17 00:10:50,323] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: needsTaskCommit() Task attempt_20210517031044683771020940094180_0002_m_000001_5: duration 0:00.001s
[2021-05-17 00:10:50,324] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044683771020940094180_0002_m_000001_5
[2021-05-17 00:10:50,326] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 4587 bytes result sent to driver
[2021-05-17 00:10:50,328] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 11) (50e6ccadb8f1, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:50,330] {docker.py:276} INFO - 21/05/17 03:10:50 INFO Executor: Running task 7.0 in stage 2.0 (TID 11)
[2021-05-17 00:10:50,330] {docker.py:276} INFO - 21/05/17 03:10:50 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2486 ms on 50e6ccadb8f1 (executor driver) (4/200)
[2021-05-17 00:10:50,364] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:10:50,366] {docker.py:276} INFO - 21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2021-05-17 00:10:50,374] {docker.py:276} INFO - 21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:10:50,375] {docker.py:276} INFO - 21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:50,376] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,377] {docker.py:276} INFO - 21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310449213256912346936367_0002_m_000007_11, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310449213256912346936367_0002_m_000007_11}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310449213256912346936367_0002}; taskId=attempt_202105170310449213256912346936367_0002_m_000007_11, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@741f395a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:50,378] {docker.py:276} INFO - 21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:50,379] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310449213256912346936367_0002_m_000007_11
[2021-05-17 00:10:50,389] {docker.py:276} INFO - 21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310449213256912346936367_0002_m_000007_11 : duration 0:00.011s
[2021-05-17 00:10:58,328] {docker.py:276} INFO - 21/05/17 03:10:54 INFO StagingCommitter: Starting: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: needsTaskCommit() Task attempt_202105170310449213256912346936367_0002_m_000007_11
[2021-05-17 00:10:58,329] {docker.py:276} INFO - 21/05/17 03:10:54 INFO StagingCommitter: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: needsTaskCommit() Task attempt_202105170310449213256912346936367_0002_m_000007_11: duration 0:00.001s
[2021-05-17 00:10:58,330] {docker.py:276} INFO - 21/05/17 03:10:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310449213256912346936367_0002_m_000007_11
[2021-05-17 00:10:58,331] {docker.py:276} INFO - 21/05/17 03:10:54 INFO Executor: Finished task 7.0 in stage 2.0 (TID 11). 4544 bytes result sent to driver
[2021-05-17 00:10:58,336] {docker.py:276} INFO - 21/05/17 03:10:54 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 12) (50e6ccadb8f1, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:10:58,338] {docker.py:276} INFO - 21/05/17 03:10:54 INFO Executor: Running task 8.0 in stage 2.0 (TID 12)
[2021-05-17 00:10:58,339] {docker.py:276} INFO - 21/05/17 03:10:54 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 11) in 4094 ms on 50e6ccadb8f1 (executor driver) (5/200)
[2021-05-17 00:10:58,360] {docker.py:276} INFO - 21/05/17 03:10:54 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:10:58,363] {docker.py:276} INFO - 21/05/17 03:10:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:10:58,363] {docker.py:276} INFO - 21/05/17 03:10:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:10:58,363] {docker.py:276} INFO - 21/05/17 03:10:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044800075857028101322_0002_m_000008_12, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044800075857028101322_0002_m_000008_12}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044800075857028101322_0002}; taskId=attempt_20210517031044800075857028101322_0002_m_000008_12, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e17f256}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:10:58,364] {docker.py:276} INFO - 21/05/17 03:10:54 INFO StagingCommitter: Starting: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044800075857028101322_0002_m_000008_12
[2021-05-17 00:10:58,367] {docker.py:276} INFO - 21/05/17 03:10:54 INFO StagingCommitter: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044800075857028101322_0002_m_000008_12 : duration 0:00.004s
[2021-05-17 00:11:00,082] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: needsTaskCommit() Task attempt_20210517031044846538821421497868_0002_m_000005_9
[2021-05-17 00:11:00,083] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: needsTaskCommit() Task attempt_20210517031044846538821421497868_0002_m_000005_9: duration 0:00.004s
21/05/17 03:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044846538821421497868_0002_m_000005_9
[2021-05-17 00:11:00,089] {docker.py:276} INFO - 21/05/17 03:10:56 INFO Executor: Finished task 5.0 in stage 2.0 (TID 9). 4544 bytes result sent to driver
[2021-05-17 00:11:00,090] {docker.py:276} INFO - 21/05/17 03:10:56 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 13) (50e6ccadb8f1, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:00,092] {docker.py:276} INFO - 21/05/17 03:10:56 INFO Executor: Running task 9.0 in stage 2.0 (TID 13)
[2021-05-17 00:11:00,092] {docker.py:276} INFO - 21/05/17 03:10:56 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 9) in 6081 ms on 50e6ccadb8f1 (executor driver) (6/200)
[2021-05-17 00:11:00,124] {docker.py:276} INFO - 21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:00,125] {docker.py:276} INFO - 21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:00,127] {docker.py:276} INFO - 21/05/17 03:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:00,128] {docker.py:276} INFO - 21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:00,128] {docker.py:276} INFO - 21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446257759387510137200_0002_m_000009_13, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446257759387510137200_0002_m_000009_13}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446257759387510137200_0002}; taskId=attempt_202105170310446257759387510137200_0002_m_000009_13, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@764c3386}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446257759387510137200_0002_m_000009_13
[2021-05-17 00:11:00,132] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446257759387510137200_0002_m_000009_13 : duration 0:00.004s
[2021-05-17 00:11:00,595] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: needsTaskCommit() Task attempt_202105170310446963036895015102514_0002_m_000006_10
[2021-05-17 00:11:00,596] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: needsTaskCommit() Task attempt_202105170310446963036895015102514_0002_m_000006_10: duration 0:00.002s
[2021-05-17 00:11:00,596] {docker.py:276} INFO - 21/05/17 03:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446963036895015102514_0002_m_000006_10
[2021-05-17 00:11:00,599] {docker.py:276} INFO - 21/05/17 03:10:56 INFO Executor: Finished task 6.0 in stage 2.0 (TID 10). 4587 bytes result sent to driver
[2021-05-17 00:11:00,600] {docker.py:276} INFO - 21/05/17 03:10:56 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 14) (50e6ccadb8f1, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:00,601] {docker.py:276} INFO - 21/05/17 03:10:56 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 10) in 6580 ms on 50e6ccadb8f1 (executor driver) (7/200)
[2021-05-17 00:11:00,603] {docker.py:276} INFO - 21/05/17 03:10:56 INFO Executor: Running task 10.0 in stage 2.0 (TID 14)
[2021-05-17 00:11:00,615] {docker.py:276} INFO - 21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:00,617] {docker.py:276} INFO - 21/05/17 03:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:00,618] {docker.py:276} INFO - 21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:00,618] {docker.py:276} INFO - 21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445821211218635200570_0002_m_000010_14, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445821211218635200570_0002_m_000010_14}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445821211218635200570_0002}; taskId=attempt_202105170310445821211218635200570_0002_m_000010_14, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d736806}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:00,618] {docker.py:276} INFO - 21/05/17 03:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:00,619] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445821211218635200570_0002_m_000010_14
[2021-05-17 00:11:00,622] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445821211218635200570_0002_m_000010_14 : duration 0:00.004s
[2021-05-17 00:11:00,661] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: needsTaskCommit() Task attempt_202105170310444522578803307905687_0002_m_000004_8
[2021-05-17 00:11:00,662] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: needsTaskCommit() Task attempt_202105170310444522578803307905687_0002_m_000004_8: duration 0:00.001s
[2021-05-17 00:11:00,662] {docker.py:276} INFO - 21/05/17 03:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444522578803307905687_0002_m_000004_8
[2021-05-17 00:11:00,665] {docker.py:276} INFO - 21/05/17 03:10:56 INFO Executor: Finished task 4.0 in stage 2.0 (TID 8). 4587 bytes result sent to driver
[2021-05-17 00:11:00,666] {docker.py:276} INFO - 21/05/17 03:10:56 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 15) (50e6ccadb8f1, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:00,668] {docker.py:276} INFO - 21/05/17 03:10:56 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 8) in 6662 ms on 50e6ccadb8f1 (executor driver) (8/200)
[2021-05-17 00:11:00,668] {docker.py:276} INFO - 21/05/17 03:10:56 INFO Executor: Running task 11.0 in stage 2.0 (TID 15)
[2021-05-17 00:11:00,680] {docker.py:276} INFO - 21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:00,680] {docker.py:276} INFO - 21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:00,683] {docker.py:276} INFO - 21/05/17 03:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:00,683] {docker.py:276} INFO - 21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:00,683] {docker.py:276} INFO - 21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044650065206039659478_0002_m_000011_15, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044650065206039659478_0002_m_000011_15}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044650065206039659478_0002}; taskId=attempt_20210517031044650065206039659478_0002_m_000011_15, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24b9dd54}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:00,684] {docker.py:276} INFO - 21/05/17 03:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:00,684] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044650065206039659478_0002_m_000011_15
[2021-05-17 00:11:00,689] {docker.py:276} INFO - 21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044650065206039659478_0002_m_000011_15 : duration 0:00.006s
[2021-05-17 00:11:00,949] {docker.py:276} INFO - 21/05/17 03:10:57 INFO StagingCommitter: Starting: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: needsTaskCommit() Task attempt_20210517031044800075857028101322_0002_m_000008_12
[2021-05-17 00:11:00,953] {docker.py:276} INFO - 21/05/17 03:10:57 INFO StagingCommitter: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: needsTaskCommit() Task attempt_20210517031044800075857028101322_0002_m_000008_12: duration 0:00.006s
[2021-05-17 00:11:00,955] {docker.py:276} INFO - 21/05/17 03:10:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044800075857028101322_0002_m_000008_12
[2021-05-17 00:11:00,960] {docker.py:276} INFO - 21/05/17 03:10:57 INFO Executor: Finished task 8.0 in stage 2.0 (TID 12). 4587 bytes result sent to driver
[2021-05-17 00:11:00,961] {docker.py:276} INFO - 21/05/17 03:10:57 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 16) (50e6ccadb8f1, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:00,965] {docker.py:276} INFO - 21/05/17 03:10:57 INFO Executor: Running task 12.0 in stage 2.0 (TID 16)
21/05/17 03:10:57 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 12) in 2634 ms on 50e6ccadb8f1 (executor driver) (9/200)
[2021-05-17 00:11:00,977] {docker.py:276} INFO - 21/05/17 03:10:57 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:00,980] {docker.py:276} INFO - 21/05/17 03:10:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:00,980] {docker.py:276} INFO - 21/05/17 03:10:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442577202771616352006_0002_m_000012_16, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442577202771616352006_0002_m_000012_16}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442577202771616352006_0002}; taskId=attempt_202105170310442577202771616352006_0002_m_000012_16, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68c3ec05}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:00,981] {docker.py:276} INFO - 21/05/17 03:10:57 INFO StagingCommitter: Starting: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442577202771616352006_0002_m_000012_16
[2021-05-17 00:11:00,985] {docker.py:276} INFO - 21/05/17 03:10:57 INFO StagingCommitter: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442577202771616352006_0002_m_000012_16 : duration 0:00.005s
[2021-05-17 00:11:02,520] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: needsTaskCommit() Task attempt_202105170310445821211218635200570_0002_m_000010_14
[2021-05-17 00:11:02,521] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: needsTaskCommit() Task attempt_202105170310445821211218635200570_0002_m_000010_14: duration 0:00.003s
21/05/17 03:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445821211218635200570_0002_m_000010_14
[2021-05-17 00:11:02,526] {docker.py:276} INFO - 21/05/17 03:10:58 INFO Executor: Finished task 10.0 in stage 2.0 (TID 14). 4587 bytes result sent to driver
[2021-05-17 00:11:02,529] {docker.py:276} INFO - 21/05/17 03:10:58 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 17) (50e6ccadb8f1, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:02,531] {docker.py:276} INFO - 21/05/17 03:10:58 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 14) in 1935 ms on 50e6ccadb8f1 (executor driver) (10/200)
[2021-05-17 00:11:02,533] {docker.py:276} INFO - 21/05/17 03:10:58 INFO Executor: Running task 13.0 in stage 2.0 (TID 17)
[2021-05-17 00:11:02,545] {docker.py:276} INFO - 21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:02,545] {docker.py:276} INFO - 21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:02,547] {docker.py:276} INFO - 21/05/17 03:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:02,548] {docker.py:276} INFO - 21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441697200031450029390_0002_m_000013_17, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441697200031450029390_0002_m_000013_17}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441697200031450029390_0002}; taskId=attempt_202105170310441697200031450029390_0002_m_000013_17, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3435a569}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441697200031450029390_0002_m_000013_17
[2021-05-17 00:11:02,552] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441697200031450029390_0002_m_000013_17 : duration 0:00.004s
[2021-05-17 00:11:02,695] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: needsTaskCommit() Task attempt_202105170310442577202771616352006_0002_m_000012_16
[2021-05-17 00:11:02,696] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: needsTaskCommit() Task attempt_202105170310442577202771616352006_0002_m_000012_16: duration 0:00.002s
[2021-05-17 00:11:02,697] {docker.py:276} INFO - 21/05/17 03:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442577202771616352006_0002_m_000012_16
[2021-05-17 00:11:02,700] {docker.py:276} INFO - 21/05/17 03:10:58 INFO Executor: Finished task 12.0 in stage 2.0 (TID 16). 4544 bytes result sent to driver
[2021-05-17 00:11:02,702] {docker.py:276} INFO - 21/05/17 03:10:58 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 18) (50e6ccadb8f1, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:02,703] {docker.py:276} INFO - 21/05/17 03:10:58 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 16) in 1746 ms on 50e6ccadb8f1 (executor driver) (11/200)
[2021-05-17 00:11:02,704] {docker.py:276} INFO - 21/05/17 03:10:58 INFO Executor: Running task 14.0 in stage 2.0 (TID 18)
[2021-05-17 00:11:02,714] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: needsTaskCommit() Task attempt_202105170310446257759387510137200_0002_m_000009_13
[2021-05-17 00:11:02,714] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: needsTaskCommit() Task attempt_202105170310446257759387510137200_0002_m_000009_13: duration 0:00.000s
[2021-05-17 00:11:02,715] {docker.py:276} INFO - 21/05/17 03:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446257759387510137200_0002_m_000009_13
[2021-05-17 00:11:02,716] {docker.py:276} INFO - 21/05/17 03:10:58 INFO Executor: Finished task 9.0 in stage 2.0 (TID 13). 4587 bytes result sent to driver
[2021-05-17 00:11:02,718] {docker.py:276} INFO - 21/05/17 03:10:58 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 19) (50e6ccadb8f1, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:02,719] {docker.py:276} INFO - 21/05/17 03:10:58 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 13) in 2634 ms on 50e6ccadb8f1 (executor driver) (12/200)
[2021-05-17 00:11:02,720] {docker.py:276} INFO - 21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:02,721] {docker.py:276} INFO - 21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/05/17 03:10:58 INFO Executor: Running task 15.0 in stage 2.0 (TID 19)
[2021-05-17 00:11:02,725] {docker.py:276} INFO - 21/05/17 03:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:02,725] {docker.py:276} INFO - 21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:02,726] {docker.py:276} INFO - 21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448378238263607005253_0002_m_000014_18, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448378238263607005253_0002_m_000014_18}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448378238263607005253_0002}; taskId=attempt_202105170310448378238263607005253_0002_m_000014_18, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a454850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448378238263607005253_0002_m_000014_18
[2021-05-17 00:11:02,730] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448378238263607005253_0002_m_000014_18 : duration 0:00.004s
[2021-05-17 00:11:02,734] {docker.py:276} INFO - 21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:02,735] {docker.py:276} INFO - 21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-17 00:11:02,736] {docker.py:276} INFO - 21/05/17 03:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:02,737] {docker.py:276} INFO - 21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044898926361917508960_0002_m_000015_19, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044898926361917508960_0002_m_000015_19}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044898926361917508960_0002}; taskId=attempt_20210517031044898926361917508960_0002_m_000015_19, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ca2db31}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:02,737] {docker.py:276} INFO - 21/05/17 03:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044898926361917508960_0002_m_000015_19
[2021-05-17 00:11:02,742] {docker.py:276} INFO - 21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044898926361917508960_0002_m_000015_19 : duration 0:00.005s
[2021-05-17 00:11:03,237] {docker.py:276} INFO - 21/05/17 03:10:59 INFO StagingCommitter: Starting: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: needsTaskCommit() Task attempt_20210517031044650065206039659478_0002_m_000011_15
[2021-05-17 00:11:03,238] {docker.py:276} INFO - 21/05/17 03:10:59 INFO StagingCommitter: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: needsTaskCommit() Task attempt_20210517031044650065206039659478_0002_m_000011_15: duration 0:00.002s
21/05/17 03:10:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044650065206039659478_0002_m_000011_15
[2021-05-17 00:11:03,241] {docker.py:276} INFO - 21/05/17 03:10:59 INFO Executor: Finished task 11.0 in stage 2.0 (TID 15). 4544 bytes result sent to driver
[2021-05-17 00:11:03,244] {docker.py:276} INFO - 21/05/17 03:10:59 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 20) (50e6ccadb8f1, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:03,245] {docker.py:276} INFO - 21/05/17 03:10:59 INFO Executor: Running task 16.0 in stage 2.0 (TID 20)
[2021-05-17 00:11:03,246] {docker.py:276} INFO - 21/05/17 03:10:59 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 15) in 2584 ms on 50e6ccadb8f1 (executor driver) (13/200)
[2021-05-17 00:11:03,257] {docker.py:276} INFO - 21/05/17 03:10:59 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:03,257] {docker.py:276} INFO - 21/05/17 03:10:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:03,260] {docker.py:276} INFO - 21/05/17 03:10:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:03,260] {docker.py:276} INFO - 21/05/17 03:10:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:03,261] {docker.py:276} INFO - 21/05/17 03:10:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448415062236824146034_0002_m_000016_20, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448415062236824146034_0002_m_000016_20}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448415062236824146034_0002}; taskId=attempt_202105170310448415062236824146034_0002_m_000016_20, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@125a1959}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:03,262] {docker.py:276} INFO - 21/05/17 03:10:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:03,262] {docker.py:276} INFO - 21/05/17 03:10:59 INFO StagingCommitter: Starting: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448415062236824146034_0002_m_000016_20
[2021-05-17 00:11:03,266] {docker.py:276} INFO - 21/05/17 03:10:59 INFO StagingCommitter: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448415062236824146034_0002_m_000016_20 : duration 0:00.004s
[2021-05-17 00:11:04,828] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: needsTaskCommit() Task attempt_202105170310448378238263607005253_0002_m_000014_18
[2021-05-17 00:11:04,829] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: needsTaskCommit() Task attempt_202105170310448378238263607005253_0002_m_000014_18: duration 0:00.001s
[2021-05-17 00:11:04,829] {docker.py:276} INFO - 21/05/17 03:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448378238263607005253_0002_m_000014_18
[2021-05-17 00:11:04,831] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: needsTaskCommit() Task attempt_20210517031044898926361917508960_0002_m_000015_19
[2021-05-17 00:11:04,832] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: needsTaskCommit() Task attempt_20210517031044898926361917508960_0002_m_000015_19: duration 0:00.000s
[2021-05-17 00:11:04,832] {docker.py:276} INFO - 21/05/17 03:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044898926361917508960_0002_m_000015_19
[2021-05-17 00:11:04,833] {docker.py:276} INFO - 21/05/17 03:11:00 INFO Executor: Finished task 14.0 in stage 2.0 (TID 18). 4544 bytes result sent to driver
[2021-05-17 00:11:04,838] {docker.py:276} INFO - 21/05/17 03:11:00 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 21) (50e6ccadb8f1, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:04,839] {docker.py:276} INFO - 21/05/17 03:11:00 INFO Executor: Running task 17.0 in stage 2.0 (TID 21)
[2021-05-17 00:11:04,839] {docker.py:276} INFO - 21/05/17 03:11:00 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 18) in 2142 ms on 50e6ccadb8f1 (executor driver) (14/200)
[2021-05-17 00:11:04,842] {docker.py:276} INFO - 21/05/17 03:11:00 INFO Executor: Finished task 15.0 in stage 2.0 (TID 19). 4544 bytes result sent to driver
[2021-05-17 00:11:04,843] {docker.py:276} INFO - 21/05/17 03:11:00 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 22) (50e6ccadb8f1, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:04,844] {docker.py:276} INFO - 21/05/17 03:11:00 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 19) in 2130 ms on 50e6ccadb8f1 (executor driver) (15/200)
21/05/17 03:11:00 INFO Executor: Running task 18.0 in stage 2.0 (TID 22)
[2021-05-17 00:11:04,850] {docker.py:276} INFO - 21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Getting 3 (1961.0 B) non-empty blocks including 3 (1961.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:04,850] {docker.py:276} INFO - 21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:04,853] {docker.py:276} INFO - 21/05/17 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:04,853] {docker.py:276} INFO - 21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:04,854] {docker.py:276} INFO - 21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446833103702532721328_0002_m_000017_21, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446833103702532721328_0002_m_000017_21}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446833103702532721328_0002}; taskId=attempt_202105170310446833103702532721328_0002_m_000017_21, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@580dbff2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:04,854] {docker.py:276} INFO - 21/05/17 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:04,854] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446833103702532721328_0002_m_000017_21
[2021-05-17 00:11:04,858] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446833103702532721328_0002_m_000017_21 : duration 0:00.005s
[2021-05-17 00:11:04,860] {docker.py:276} INFO - 21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-17 00:11:04,863] {docker.py:276} INFO - 21/05/17 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442450223603098137813_0002_m_000018_22, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442450223603098137813_0002_m_000018_22}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442450223603098137813_0002}; taskId=attempt_202105170310442450223603098137813_0002_m_000018_22, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a1057a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:04,863] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442450223603098137813_0002_m_000018_22
[2021-05-17 00:11:04,869] {docker.py:276} INFO - 21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442450223603098137813_0002_m_000018_22 : duration 0:00.007s
[2021-05-17 00:11:05,334] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: needsTaskCommit() Task attempt_202105170310448415062236824146034_0002_m_000016_20
[2021-05-17 00:11:05,334] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: needsTaskCommit() Task attempt_202105170310448415062236824146034_0002_m_000016_20: duration 0:00.003s
21/05/17 03:11:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448415062236824146034_0002_m_000016_20
[2021-05-17 00:11:05,337] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: needsTaskCommit() Task attempt_202105170310441697200031450029390_0002_m_000013_17
[2021-05-17 00:11:05,337] {docker.py:276} INFO - 21/05/17 03:11:01 INFO Executor: Finished task 16.0 in stage 2.0 (TID 20). 4544 bytes result sent to driver
21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: needsTaskCommit() Task attempt_202105170310441697200031450029390_0002_m_000013_17: duration 0:00.000s
[2021-05-17 00:11:05,338] {docker.py:276} INFO - 21/05/17 03:11:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441697200031450029390_0002_m_000013_17
[2021-05-17 00:11:05,340] {docker.py:276} INFO - 21/05/17 03:11:01 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 23) (50e6ccadb8f1, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:05,342] {docker.py:276} INFO - 21/05/17 03:11:01 INFO Executor: Finished task 13.0 in stage 2.0 (TID 17). 4544 bytes result sent to driver
[2021-05-17 00:11:05,343] {docker.py:276} INFO - 21/05/17 03:11:01 INFO Executor: Running task 19.0 in stage 2.0 (TID 23)
21/05/17 03:11:01 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 20) in 2105 ms on 50e6ccadb8f1 (executor driver) (16/200)
[2021-05-17 00:11:05,345] {docker.py:276} INFO - 21/05/17 03:11:01 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 24) (50e6ccadb8f1, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:05,346] {docker.py:276} INFO - 21/05/17 03:11:01 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 17) in 2822 ms on 50e6ccadb8f1 (executor driver) (17/200)
[2021-05-17 00:11:05,347] {docker.py:276} INFO - 21/05/17 03:11:01 INFO Executor: Running task 20.0 in stage 2.0 (TID 24)
[2021-05-17 00:11:05,355] {docker.py:276} INFO - 21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:05,356] {docker.py:276} INFO - 21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:11:05,358] {docker.py:276} INFO - 21/05/17 03:11:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:05,358] {docker.py:276} INFO - 21/05/17 03:11:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:05,360] {docker.py:276} INFO - 21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448441300862858158598_0002_m_000020_24, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448441300862858158598_0002_m_000020_24}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448441300862858158598_0002}; taskId=attempt_202105170310448441300862858158598_0002_m_000020_24, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70753b5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444905782509601915382_0002_m_000019_23, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444905782509601915382_0002_m_000019_23}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444905782509601915382_0002}; taskId=attempt_202105170310444905782509601915382_0002_m_000019_23, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71969e0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:05,360] {docker.py:276} INFO - 21/05/17 03:11:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:05,360] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448441300862858158598_0002_m_000020_24
[2021-05-17 00:11:05,361] {docker.py:276} INFO - 21/05/17 03:11:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:05,361] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444905782509601915382_0002_m_000019_23
[2021-05-17 00:11:05,364] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448441300862858158598_0002_m_000020_24 : duration 0:00.004s
[2021-05-17 00:11:05,370] {docker.py:276} INFO - 21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444905782509601915382_0002_m_000019_23 : duration 0:00.010s
[2021-05-17 00:11:07,256] {docker.py:276} INFO - 21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: needsTaskCommit() Task attempt_202105170310446833103702532721328_0002_m_000017_21
[2021-05-17 00:11:07,262] {docker.py:276} INFO - 21/05/17 03:11:03 INFO StagingCommitter: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: needsTaskCommit() Task attempt_202105170310446833103702532721328_0002_m_000017_21: duration 0:00.003s
[2021-05-17 00:11:07,264] {docker.py:276} INFO - 21/05/17 03:11:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446833103702532721328_0002_m_000017_21
[2021-05-17 00:11:07,268] {docker.py:276} INFO - 21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: needsTaskCommit() Task attempt_202105170310442450223603098137813_0002_m_000018_22
[2021-05-17 00:11:07,278] {docker.py:276} INFO - 21/05/17 03:11:03 INFO Executor: Finished task 17.0 in stage 2.0 (TID 21). 4630 bytes result sent to driver
[2021-05-17 00:11:07,291] {docker.py:276} INFO - 21/05/17 03:11:03 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 25) (50e6ccadb8f1, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:07,302] {docker.py:276} INFO - 21/05/17 03:11:03 INFO StagingCommitter: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: needsTaskCommit() Task attempt_202105170310442450223603098137813_0002_m_000018_22: duration 0:00.044s
[2021-05-17 00:11:07,312] {docker.py:276} INFO - 21/05/17 03:11:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442450223603098137813_0002_m_000018_22
[2021-05-17 00:11:07,317] {docker.py:276} INFO - 21/05/17 03:11:03 INFO Executor: Running task 21.0 in stage 2.0 (TID 25)
[2021-05-17 00:11:07,321] {docker.py:276} INFO - 21/05/17 03:11:03 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 21) in 2467 ms on 50e6ccadb8f1 (executor driver) (18/200)
[2021-05-17 00:11:07,348] {docker.py:276} INFO - 21/05/17 03:11:03 INFO Executor: Finished task 18.0 in stage 2.0 (TID 22). 4630 bytes result sent to driver
[2021-05-17 00:11:07,381] {docker.py:276} INFO - 21/05/17 03:11:03 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 26) (50e6ccadb8f1, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:07,397] {docker.py:276} INFO - 21/05/17 03:11:03 INFO Executor: Running task 22.0 in stage 2.0 (TID 26)
[2021-05-17 00:11:07,404] {docker.py:276} INFO - 21/05/17 03:11:03 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 22) in 2550 ms on 50e6ccadb8f1 (executor driver) (19/200)
[2021-05-17 00:11:07,653] {docker.py:276} INFO - 21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:07,697] {docker.py:276} INFO - 21/05/17 03:11:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:07,701] {docker.py:276} INFO - 21/05/17 03:11:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:07,706] {docker.py:276} INFO - 21/05/17 03:11:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445003124214055647070_0002_m_000021_25, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445003124214055647070_0002_m_000021_25}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445003124214055647070_0002}; taskId=attempt_202105170310445003124214055647070_0002_m_000021_25, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5effd0f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:07,710] {docker.py:276} INFO - 21/05/17 03:11:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:07,736] {docker.py:276} INFO - 21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:07,741] {docker.py:276} INFO - 21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 38 ms
21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445003124214055647070_0002_m_000021_25
[2021-05-17 00:11:07,911] {docker.py:276} INFO - 21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: needsTaskCommit() Task attempt_202105170310448441300862858158598_0002_m_000020_24
[2021-05-17 00:11:07,925] {docker.py:276} INFO - 21/05/17 03:11:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:07,935] {docker.py:276} INFO - 21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443285973931464533167_0002_m_000022_26, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443285973931464533167_0002_m_000022_26}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443285973931464533167_0002}; taskId=attempt_202105170310443285973931464533167_0002_m_000022_26, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30e3c31}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: needsTaskCommit() Task attempt_202105170310448441300862858158598_0002_m_000020_24: duration 0:00.035s
[2021-05-17 00:11:07,941] {docker.py:276} INFO - 21/05/17 03:11:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:07,950] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445003124214055647070_0002_m_000021_25 : duration 0:00.201s
[2021-05-17 00:11:07,955] {docker.py:276} INFO - 21/05/17 03:11:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448441300862858158598_0002_m_000020_24
[2021-05-17 00:11:07,960] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443285973931464533167_0002_m_000022_26
[2021-05-17 00:11:07,990] {docker.py:276} INFO - 21/05/17 03:11:04 INFO Executor: Finished task 20.0 in stage 2.0 (TID 24). 4630 bytes result sent to driver
[2021-05-17 00:11:08,002] {docker.py:276} INFO - 21/05/17 03:11:04 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 27) (50e6ccadb8f1, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:08,023] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: needsTaskCommit() Task attempt_202105170310444905782509601915382_0002_m_000019_23
[2021-05-17 00:11:08,027] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: needsTaskCommit() Task attempt_202105170310444905782509601915382_0002_m_000019_23: duration 0:00.003s
[2021-05-17 00:11:08,040] {docker.py:276} INFO - 21/05/17 03:11:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444905782509601915382_0002_m_000019_23
[2021-05-17 00:11:08,056] {docker.py:276} INFO - 21/05/17 03:11:04 INFO Executor: Finished task 19.0 in stage 2.0 (TID 23). 4630 bytes result sent to driver
[2021-05-17 00:11:08,081] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443285973931464533167_0002_m_000022_26 : duration 0:00.128s
[2021-05-17 00:11:08,091] {docker.py:276} INFO - 21/05/17 03:11:04 INFO Executor: Running task 23.0 in stage 2.0 (TID 27)
[2021-05-17 00:11:08,109] {docker.py:276} INFO - 21/05/17 03:11:04 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 28) (50e6ccadb8f1, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:08,131] {docker.py:276} INFO - 21/05/17 03:11:04 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 24) in 2784 ms on 50e6ccadb8f1 (executor driver) (20/200)
[2021-05-17 00:11:08,137] {docker.py:276} INFO - 21/05/17 03:11:04 INFO Executor: Running task 24.0 in stage 2.0 (TID 28)
[2021-05-17 00:11:08,158] {docker.py:276} INFO - 21/05/17 03:11:04 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 23) in 2810 ms on 50e6ccadb8f1 (executor driver) (21/200)
[2021-05-17 00:11:08,385] {docker.py:276} INFO - 21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2021-05-17 00:11:08,423] {docker.py:276} INFO - 21/05/17 03:11:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:08,429] {docker.py:276} INFO - 21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:08,434] {docker.py:276} INFO - 21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444135220769137305057_0002_m_000023_27, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444135220769137305057_0002_m_000023_27}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444135220769137305057_0002}; taskId=attempt_202105170310444135220769137305057_0002_m_000023_27, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c089bd2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:08,440] {docker.py:276} INFO - 21/05/17 03:11:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:08,444] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444135220769137305057_0002_m_000023_27
[2021-05-17 00:11:08,524] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444135220769137305057_0002_m_000023_27 : duration 0:00.084s
[2021-05-17 00:11:08,571] {docker.py:276} INFO - 21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-17 00:11:08,593] {docker.py:276} INFO - 21/05/17 03:11:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:08,598] {docker.py:276} INFO - 21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:08,602] {docker.py:276} INFO - 21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442943056473930753721_0002_m_000024_28, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442943056473930753721_0002_m_000024_28}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442943056473930753721_0002}; taskId=attempt_202105170310442943056473930753721_0002_m_000024_28, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32592203}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:08,607] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442943056473930753721_0002_m_000024_28
[2021-05-17 00:11:08,667] {docker.py:276} INFO - 21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442943056473930753721_0002_m_000024_28 : duration 0:00.065s
[2021-05-17 00:11:10,865] {docker.py:276} INFO - 21/05/17 03:11:06 INFO StagingCommitter: Starting: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: needsTaskCommit() Task attempt_202105170310443285973931464533167_0002_m_000022_26
21/05/17 03:11:06 INFO StagingCommitter: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: needsTaskCommit() Task attempt_202105170310443285973931464533167_0002_m_000022_26: duration 0:00.010s
21/05/17 03:11:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443285973931464533167_0002_m_000022_26
[2021-05-17 00:11:10,891] {docker.py:276} INFO - 21/05/17 03:11:06 INFO Executor: Finished task 22.0 in stage 2.0 (TID 26). 4544 bytes result sent to driver
[2021-05-17 00:11:10,908] {docker.py:276} INFO - 21/05/17 03:11:07 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 29) (50e6ccadb8f1, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:10,938] {docker.py:276} INFO - 21/05/17 03:11:07 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 26) in 3581 ms on 50e6ccadb8f1 (executor driver) (22/200)
21/05/17 03:11:07 INFO Executor: Running task 25.0 in stage 2.0 (TID 29)
[2021-05-17 00:11:11,015] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: needsTaskCommit() Task attempt_202105170310445003124214055647070_0002_m_000021_25
[2021-05-17 00:11:11,022] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: needsTaskCommit() Task attempt_202105170310445003124214055647070_0002_m_000021_25: duration 0:00.015s
21/05/17 03:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445003124214055647070_0002_m_000021_25
[2021-05-17 00:11:11,071] {docker.py:276} INFO - 21/05/17 03:11:07 INFO Executor: Finished task 21.0 in stage 2.0 (TID 25). 4587 bytes result sent to driver
[2021-05-17 00:11:11,083] {docker.py:276} INFO - 21/05/17 03:11:07 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 30) (50e6ccadb8f1, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:11,104] {docker.py:276} INFO - 21/05/17 03:11:07 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 25) in 3820 ms on 50e6ccadb8f1 (executor driver) (23/200)
[2021-05-17 00:11:11,139] {docker.py:276} INFO - 21/05/17 03:11:07 INFO Executor: Running task 26.0 in stage 2.0 (TID 30)
[2021-05-17 00:11:11,224] {docker.py:276} INFO - 21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Getting 3 (2.9 KiB) non-empty blocks including 3 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-17 00:11:11,254] {docker.py:276} INFO - 21/05/17 03:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:11,259] {docker.py:276} INFO - 21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:11,261] {docker.py:276} INFO - 21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447809294174389448069_0002_m_000025_29, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447809294174389448069_0002_m_000025_29}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447809294174389448069_0002}; taskId=attempt_202105170310447809294174389448069_0002_m_000025_29, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c30afd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:11,264] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447809294174389448069_0002_m_000025_29
[2021-05-17 00:11:11,327] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447809294174389448069_0002_m_000025_29 : duration 0:00.065s
[2021-05-17 00:11:11,368] {docker.py:276} INFO - 21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:11,372] {docker.py:276} INFO - 21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-17 00:11:11,409] {docker.py:276} INFO - 21/05/17 03:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:11,412] {docker.py:276} INFO - 21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:11,418] {docker.py:276} INFO - 21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044526516834364116976_0002_m_000026_30, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044526516834364116976_0002_m_000026_30}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044526516834364116976_0002}; taskId=attempt_20210517031044526516834364116976_0002_m_000026_30, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33e3b08b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:11,424] {docker.py:276} INFO - 21/05/17 03:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044526516834364116976_0002_m_000026_30
[2021-05-17 00:11:11,491] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044526516834364116976_0002_m_000026_30 : duration 0:00.072s
[2021-05-17 00:11:11,680] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: needsTaskCommit() Task attempt_202105170310442943056473930753721_0002_m_000024_28
[2021-05-17 00:11:11,690] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: needsTaskCommit() Task attempt_202105170310442943056473930753721_0002_m_000024_28: duration 0:00.020s
[2021-05-17 00:11:11,700] {docker.py:276} INFO - 21/05/17 03:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442943056473930753721_0002_m_000024_28
[2021-05-17 00:11:11,720] {docker.py:276} INFO - 21/05/17 03:11:07 INFO Executor: Finished task 24.0 in stage 2.0 (TID 28). 4544 bytes result sent to driver
[2021-05-17 00:11:11,736] {docker.py:276} INFO - 21/05/17 03:11:07 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 31) (50e6ccadb8f1, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:11,753] {docker.py:276} INFO - 21/05/17 03:11:07 INFO Executor: Running task 27.0 in stage 2.0 (TID 31)
[2021-05-17 00:11:11,759] {docker.py:276} INFO - 21/05/17 03:11:07 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 28) in 3653 ms on 50e6ccadb8f1 (executor driver) (24/200)
[2021-05-17 00:11:11,841] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: needsTaskCommit() Task attempt_202105170310444135220769137305057_0002_m_000023_27
[2021-05-17 00:11:11,849] {docker.py:276} INFO - 21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: needsTaskCommit() Task attempt_202105170310444135220769137305057_0002_m_000023_27: duration 0:00.006s
21/05/17 03:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444135220769137305057_0002_m_000023_27
[2021-05-17 00:11:11,891] {docker.py:276} INFO - 21/05/17 03:11:07 INFO Executor: Finished task 23.0 in stage 2.0 (TID 27). 4544 bytes result sent to driver
[2021-05-17 00:11:11,956] {docker.py:276} INFO - 21/05/17 03:11:08 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 32) (50e6ccadb8f1, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:11,982] {docker.py:276} INFO - 21/05/17 03:11:08 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 27) in 3986 ms on 50e6ccadb8f1 (executor driver) (25/200)
[2021-05-17 00:11:12,018] {docker.py:276} INFO - 21/05/17 03:11:08 INFO Executor: Running task 28.0 in stage 2.0 (TID 32)
[2021-05-17 00:11:12,105] {docker.py:276} INFO - 21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Getting 3 (2.0 KiB) non-empty blocks including 3 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-17 00:11:12,154] {docker.py:276} INFO - 21/05/17 03:11:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:12,164] {docker.py:276} INFO - 21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445095766092571174203_0002_m_000027_31, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445095766092571174203_0002_m_000027_31}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445095766092571174203_0002}; taskId=attempt_202105170310445095766092571174203_0002_m_000027_31, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70af5c26}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:12,170] {docker.py:276} INFO - 21/05/17 03:11:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:12,176] {docker.py:276} INFO - 21/05/17 03:11:08 INFO StagingCommitter: Starting: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445095766092571174203_0002_m_000027_31
[2021-05-17 00:11:12,223] {docker.py:276} INFO - 21/05/17 03:11:08 INFO StagingCommitter: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445095766092571174203_0002_m_000027_31 : duration 0:00.048s
[2021-05-17 00:11:12,273] {docker.py:276} INFO - 21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:12,279] {docker.py:276} INFO - 21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms
[2021-05-17 00:11:12,314] {docker.py:276} INFO - 21/05/17 03:11:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:12,319] {docker.py:276} INFO - 21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444544091199301428286_0002_m_000028_32, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444544091199301428286_0002_m_000028_32}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444544091199301428286_0002}; taskId=attempt_202105170310444544091199301428286_0002_m_000028_32, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5afbc3c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:08 INFO StagingCommitter: Starting: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444544091199301428286_0002_m_000028_32
[2021-05-17 00:11:12,359] {docker.py:276} INFO - 21/05/17 03:11:08 INFO StagingCommitter: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444544091199301428286_0002_m_000028_32 : duration 0:00.044s
[2021-05-17 00:11:13,559] {docker.py:276} INFO - 21/05/17 03:11:13 INFO StagingCommitter: Starting: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: needsTaskCommit() Task attempt_20210517031044526516834364116976_0002_m_000026_30
[2021-05-17 00:11:13,564] {docker.py:276} INFO - 21/05/17 03:11:13 INFO StagingCommitter: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: needsTaskCommit() Task attempt_20210517031044526516834364116976_0002_m_000026_30: duration 0:00.023s
[2021-05-17 00:11:13,570] {docker.py:276} INFO - 21/05/17 03:11:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044526516834364116976_0002_m_000026_30
[2021-05-17 00:11:13,578] {docker.py:276} INFO - 21/05/17 03:11:13 INFO Executor: Finished task 26.0 in stage 2.0 (TID 30). 4587 bytes result sent to driver
[2021-05-17 00:11:13,598] {docker.py:276} INFO - 21/05/17 03:11:13 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 33) (50e6ccadb8f1, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:13,623] {docker.py:276} INFO - 21/05/17 03:11:13 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 30) in 6428 ms on 50e6ccadb8f1 (executor driver) (26/200)
21/05/17 03:11:13 INFO Executor: Running task 29.0 in stage 2.0 (TID 33)
[2021-05-17 00:11:13,812] {docker.py:276} INFO - 21/05/17 03:11:13 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:13,815] {docker.py:276} INFO - 21/05/17 03:11:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:13,836] {docker.py:276} INFO - 21/05/17 03:11:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:13,841] {docker.py:276} INFO - 21/05/17 03:11:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:13,845] {docker.py:276} INFO - 21/05/17 03:11:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044253485623106593207_0002_m_000029_33, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044253485623106593207_0002_m_000029_33}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044253485623106593207_0002}; taskId=attempt_20210517031044253485623106593207_0002_m_000029_33, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54cfb093}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:13 INFO StagingCommitter: Starting: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044253485623106593207_0002_m_000029_33
[2021-05-17 00:11:13,885] {docker.py:276} INFO - 21/05/17 03:11:13 INFO StagingCommitter: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044253485623106593207_0002_m_000029_33 : duration 0:00.042s
[2021-05-17 00:11:14,225] {docker.py:276} INFO - 21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: needsTaskCommit() Task attempt_202105170310444544091199301428286_0002_m_000028_32
[2021-05-17 00:11:14,236] {docker.py:276} INFO - 21/05/17 03:11:14 INFO StagingCommitter: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: needsTaskCommit() Task attempt_202105170310444544091199301428286_0002_m_000028_32: duration 0:00.017s
[2021-05-17 00:11:14,244] {docker.py:276} INFO - 21/05/17 03:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444544091199301428286_0002_m_000028_32
[2021-05-17 00:11:14,260] {docker.py:276} INFO - 21/05/17 03:11:14 INFO Executor: Finished task 28.0 in stage 2.0 (TID 32). 4587 bytes result sent to driver
[2021-05-17 00:11:14,278] {docker.py:276} INFO - 21/05/17 03:11:14 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 34) (50e6ccadb8f1, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:14,304] {docker.py:276} INFO - 21/05/17 03:11:14 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 32) in 6279 ms on 50e6ccadb8f1 (executor driver) (27/200)
[2021-05-17 00:11:14,330] {docker.py:276} INFO - 21/05/17 03:11:14 INFO Executor: Running task 30.0 in stage 2.0 (TID 34)
[2021-05-17 00:11:14,480] {docker.py:276} INFO - 21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2021-05-17 00:11:14,526] {docker.py:276} INFO - 21/05/17 03:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:14,536] {docker.py:276} INFO - 21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441628843165832546118_0002_m_000030_34, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441628843165832546118_0002_m_000030_34}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441628843165832546118_0002}; taskId=attempt_202105170310441628843165832546118_0002_m_000030_34, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@733f3d6e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:14,541] {docker.py:276} INFO - 21/05/17 03:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441628843165832546118_0002_m_000030_34
[2021-05-17 00:11:14,595] {docker.py:276} INFO - 21/05/17 03:11:14 INFO StagingCommitter: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441628843165832546118_0002_m_000030_34 : duration 0:00.057s
[2021-05-17 00:11:14,612] {docker.py:276} INFO - 21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: needsTaskCommit() Task attempt_202105170310447809294174389448069_0002_m_000025_29
[2021-05-17 00:11:14,633] {docker.py:276} INFO - 21/05/17 03:11:14 INFO StagingCommitter: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: needsTaskCommit() Task attempt_202105170310447809294174389448069_0002_m_000025_29: duration 0:00.039s
[2021-05-17 00:11:14,638] {docker.py:276} INFO - 21/05/17 03:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447809294174389448069_0002_m_000025_29
[2021-05-17 00:11:14,659] {docker.py:276} INFO - 21/05/17 03:11:14 INFO Executor: Finished task 25.0 in stage 2.0 (TID 29). 4587 bytes result sent to driver
[2021-05-17 00:11:14,670] {docker.py:276} INFO - 21/05/17 03:11:14 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 35) (50e6ccadb8f1, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:14,683] {docker.py:276} INFO - 21/05/17 03:11:14 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 29) in 7665 ms on 50e6ccadb8f1 (executor driver) (28/200)
[2021-05-17 00:11:14,692] {docker.py:276} INFO - 21/05/17 03:11:14 INFO Executor: Running task 31.0 in stage 2.0 (TID 35)
[2021-05-17 00:11:14,934] {docker.py:276} INFO - 21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:14,938] {docker.py:276} INFO - 21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-17 00:11:14,965] {docker.py:276} INFO - 21/05/17 03:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:14,969] {docker.py:276} INFO - 21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:14,980] {docker.py:276} INFO - 21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444551762255917598986_0002_m_000031_35, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444551762255917598986_0002_m_000031_35}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444551762255917598986_0002}; taskId=attempt_202105170310444551762255917598986_0002_m_000031_35, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7427859c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:14,983] {docker.py:276} INFO - 21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444551762255917598986_0002_m_000031_35
[2021-05-17 00:11:15,043] {docker.py:276} INFO - 21/05/17 03:11:15 INFO StagingCommitter: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444551762255917598986_0002_m_000031_35 : duration 0:00.062s
[2021-05-17 00:11:15,294] {docker.py:276} INFO - 21/05/17 03:11:15 INFO StagingCommitter: Starting: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: needsTaskCommit() Task attempt_202105170310445095766092571174203_0002_m_000027_31
[2021-05-17 00:11:15,299] {docker.py:276} INFO - 21/05/17 03:11:15 INFO StagingCommitter: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: needsTaskCommit() Task attempt_202105170310445095766092571174203_0002_m_000027_31: duration 0:00.009s
21/05/17 03:11:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445095766092571174203_0002_m_000027_31
[2021-05-17 00:11:15,313] {docker.py:276} INFO - 21/05/17 03:11:15 INFO Executor: Finished task 27.0 in stage 2.0 (TID 31). 4587 bytes result sent to driver
[2021-05-17 00:11:15,333] {docker.py:276} INFO - 21/05/17 03:11:15 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 36) (50e6ccadb8f1, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:15,339] {docker.py:276} INFO - 21/05/17 03:11:15 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 31) in 7493 ms on 50e6ccadb8f1 (executor driver) (29/200)
[2021-05-17 00:11:15,346] {docker.py:276} INFO - 21/05/17 03:11:15 INFO Executor: Running task 32.0 in stage 2.0 (TID 36)
[2021-05-17 00:11:15,487] {docker.py:276} INFO - 21/05/17 03:11:15 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:15,516] {docker.py:276} INFO - 21/05/17 03:11:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:15,520] {docker.py:276} INFO - 21/05/17 03:11:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444935604236885678976_0002_m_000032_36, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444935604236885678976_0002_m_000032_36}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444935604236885678976_0002}; taskId=attempt_202105170310444935604236885678976_0002_m_000032_36, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43aadf1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:15 INFO StagingCommitter: Starting: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444935604236885678976_0002_m_000032_36
[2021-05-17 00:11:15,565] {docker.py:276} INFO - 21/05/17 03:11:15 INFO StagingCommitter: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444935604236885678976_0002_m_000032_36 : duration 0:00.044s
[2021-05-17 00:11:17,442] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: needsTaskCommit() Task attempt_202105170310441628843165832546118_0002_m_000030_34
[2021-05-17 00:11:17,449] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: needsTaskCommit() Task attempt_202105170310441628843165832546118_0002_m_000030_34: duration 0:00.014s
21/05/17 03:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441628843165832546118_0002_m_000030_34
[2021-05-17 00:11:17,472] {docker.py:276} INFO - 21/05/17 03:11:17 INFO Executor: Finished task 30.0 in stage 2.0 (TID 34). 4630 bytes result sent to driver
[2021-05-17 00:11:17,493] {docker.py:276} INFO - 21/05/17 03:11:17 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 37) (50e6ccadb8f1, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:17,522] {docker.py:276} INFO - 21/05/17 03:11:17 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 34) in 3250 ms on 50e6ccadb8f1 (executor driver) (30/200)
[2021-05-17 00:11:17,527] {docker.py:276} INFO - 21/05/17 03:11:17 INFO Executor: Running task 33.0 in stage 2.0 (TID 37)
[2021-05-17 00:11:17,660] {docker.py:276} INFO - 21/05/17 03:11:17 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:17,687] {docker.py:276} INFO - 21/05/17 03:11:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:17,693] {docker.py:276} INFO - 21/05/17 03:11:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448967553897272536447_0002_m_000033_37, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448967553897272536447_0002_m_000033_37}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448967553897272536447_0002}; taskId=attempt_202105170310448967553897272536447_0002_m_000033_37, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17bfdc44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:17,695] {docker.py:276} INFO - 21/05/17 03:11:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448967553897272536447_0002_m_000033_37
[2021-05-17 00:11:17,754] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448967553897272536447_0002_m_000033_37 : duration 0:00.062s
[2021-05-17 00:11:17,932] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: needsTaskCommit() Task attempt_20210517031044253485623106593207_0002_m_000029_33
[2021-05-17 00:11:17,936] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: needsTaskCommit() Task attempt_20210517031044253485623106593207_0002_m_000029_33: duration 0:00.015s
[2021-05-17 00:11:17,941] {docker.py:276} INFO - 21/05/17 03:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044253485623106593207_0002_m_000029_33
[2021-05-17 00:11:17,951] {docker.py:276} INFO - 21/05/17 03:11:17 INFO Executor: Finished task 29.0 in stage 2.0 (TID 33). 4630 bytes result sent to driver
[2021-05-17 00:11:17,967] {docker.py:276} INFO - 21/05/17 03:11:17 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 38) (50e6ccadb8f1, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:17,973] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: needsTaskCommit() Task attempt_202105170310444551762255917598986_0002_m_000031_35
[2021-05-17 00:11:17,981] {docker.py:276} INFO - 21/05/17 03:11:17 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 33) in 4396 ms on 50e6ccadb8f1 (executor driver) (31/200)
[2021-05-17 00:11:17,986] {docker.py:276} INFO - 21/05/17 03:11:17 INFO Executor: Running task 34.0 in stage 2.0 (TID 38)
[2021-05-17 00:11:17,989] {docker.py:276} INFO - 21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: needsTaskCommit() Task attempt_202105170310444551762255917598986_0002_m_000031_35: duration 0:00.015s
21/05/17 03:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444551762255917598986_0002_m_000031_35
[2021-05-17 00:11:18,023] {docker.py:276} INFO - 21/05/17 03:11:17 INFO Executor: Finished task 31.0 in stage 2.0 (TID 35). 4587 bytes result sent to driver
[2021-05-17 00:11:18,034] {docker.py:276} INFO - 21/05/17 03:11:18 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 39) (50e6ccadb8f1, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:18,041] {docker.py:276} INFO - 21/05/17 03:11:18 INFO Executor: Running task 35.0 in stage 2.0 (TID 39)
[2021-05-17 00:11:18,057] {docker.py:276} INFO - 21/05/17 03:11:18 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 35) in 3386 ms on 50e6ccadb8f1 (executor driver) (32/200)
[2021-05-17 00:11:18,220] {docker.py:276} INFO - 21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:18,225] {docker.py:276} INFO - 21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:18,257] {docker.py:276} INFO - 21/05/17 03:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:18,263] {docker.py:276} INFO - 21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446846625391595280355_0002_m_000034_38, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446846625391595280355_0002_m_000034_38}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446846625391595280355_0002}; taskId=attempt_202105170310446846625391595280355_0002_m_000034_38, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@227717f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:18,268] {docker.py:276} INFO - 21/05/17 03:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446846625391595280355_0002_m_000034_38
[2021-05-17 00:11:18,314] {docker.py:276} INFO - 21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: needsTaskCommit() Task attempt_202105170310444935604236885678976_0002_m_000032_36
[2021-05-17 00:11:18,319] {docker.py:276} INFO - 21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: needsTaskCommit() Task attempt_202105170310444935604236885678976_0002_m_000032_36: duration 0:00.016s
[2021-05-17 00:11:18,336] {docker.py:276} INFO - 21/05/17 03:11:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444935604236885678976_0002_m_000032_36
[2021-05-17 00:11:18,342] {docker.py:276} INFO - 21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446846625391595280355_0002_m_000034_38 : duration 0:00.073s
[2021-05-17 00:11:18,347] {docker.py:276} INFO - 21/05/17 03:11:18 INFO Executor: Finished task 32.0 in stage 2.0 (TID 36). 4630 bytes result sent to driver
[2021-05-17 00:11:18,353] {docker.py:276} INFO - 21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:18,358] {docker.py:276} INFO - 21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 56 ms
[2021-05-17 00:11:18,369] {docker.py:276} INFO - 21/05/17 03:11:18 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 40) (50e6ccadb8f1, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:18,374] {docker.py:276} INFO - 21/05/17 03:11:18 INFO Executor: Running task 36.0 in stage 2.0 (TID 40)
[2021-05-17 00:11:18,383] {docker.py:276} INFO - 21/05/17 03:11:18 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 36) in 3057 ms on 50e6ccadb8f1 (executor driver) (33/200)
[2021-05-17 00:11:18,426] {docker.py:276} INFO - 21/05/17 03:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:18,436] {docker.py:276} INFO - 21/05/17 03:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:18,439] {docker.py:276} INFO - 21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444392254441304600869_0002_m_000035_39, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444392254441304600869_0002_m_000035_39}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444392254441304600869_0002}; taskId=attempt_202105170310444392254441304600869_0002_m_000035_39, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58aff1bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:18,447] {docker.py:276} INFO - 21/05/17 03:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444392254441304600869_0002_m_000035_39
[2021-05-17 00:11:18,482] {docker.py:276} INFO - 21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444392254441304600869_0002_m_000035_39 : duration 0:00.033s
[2021-05-17 00:11:18,670] {docker.py:276} INFO - 21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:18,674] {docker.py:276} INFO - 21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
[2021-05-17 00:11:18,699] {docker.py:276} INFO - 21/05/17 03:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:18,705] {docker.py:276} INFO - 21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443347006655096946966_0002_m_000036_40, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443347006655096946966_0002_m_000036_40}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443347006655096946966_0002}; taskId=attempt_202105170310443347006655096946966_0002_m_000036_40, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd7e32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443347006655096946966_0002_m_000036_40
[2021-05-17 00:11:18,745] {docker.py:276} INFO - 21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443347006655096946966_0002_m_000036_40 : duration 0:00.040s
[2021-05-17 00:11:20,489] {docker.py:276} INFO - 21/05/17 03:11:20 INFO StagingCommitter: Starting: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: needsTaskCommit() Task attempt_202105170310444392254441304600869_0002_m_000035_39
[2021-05-17 00:11:20,507] {docker.py:276} INFO - 21/05/17 03:11:20 INFO StagingCommitter: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: needsTaskCommit() Task attempt_202105170310444392254441304600869_0002_m_000035_39: duration 0:00.016s
21/05/17 03:11:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444392254441304600869_0002_m_000035_39
[2021-05-17 00:11:20,596] {docker.py:276} INFO - 21/05/17 03:11:20 INFO Executor: Finished task 35.0 in stage 2.0 (TID 39). 4587 bytes result sent to driver
[2021-05-17 00:11:20,612] {docker.py:276} INFO - 21/05/17 03:11:20 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 41) (50e6ccadb8f1, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:20,671] {docker.py:276} INFO - 21/05/17 03:11:20 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 39) in 2619 ms on 50e6ccadb8f1 (executor driver) (34/200)
[2021-05-17 00:11:20,680] {docker.py:276} INFO - 21/05/17 03:11:20 INFO Executor: Running task 37.0 in stage 2.0 (TID 41)
[2021-05-17 00:11:20,858] {docker.py:276} INFO - 21/05/17 03:11:20 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:20,863] {docker.py:276} INFO - 21/05/17 03:11:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2021-05-17 00:11:20,901] {docker.py:276} INFO - 21/05/17 03:11:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:20,907] {docker.py:276} INFO - 21/05/17 03:11:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:20,915] {docker.py:276} INFO - 21/05/17 03:11:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447060217971280555645_0002_m_000037_41, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447060217971280555645_0002_m_000037_41}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447060217971280555645_0002}; taskId=attempt_202105170310447060217971280555645_0002_m_000037_41, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a006df9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:20,921] {docker.py:276} INFO - 21/05/17 03:11:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:20,926] {docker.py:276} INFO - 21/05/17 03:11:20 INFO StagingCommitter: Starting: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447060217971280555645_0002_m_000037_41
[2021-05-17 00:11:20,990] {docker.py:276} INFO - 21/05/17 03:11:20 INFO StagingCommitter: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447060217971280555645_0002_m_000037_41 : duration 0:00.069s
[2021-05-17 00:11:21,403] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: needsTaskCommit() Task attempt_202105170310443347006655096946966_0002_m_000036_40
[2021-05-17 00:11:21,411] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: needsTaskCommit() Task attempt_202105170310443347006655096946966_0002_m_000036_40: duration 0:00.022s
21/05/17 03:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443347006655096946966_0002_m_000036_40
[2021-05-17 00:11:21,449] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: needsTaskCommit() Task attempt_202105170310446846625391595280355_0002_m_000034_38
[2021-05-17 00:11:21,454] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: needsTaskCommit() Task attempt_202105170310446846625391595280355_0002_m_000034_38: duration 0:00.019s
[2021-05-17 00:11:21,462] {docker.py:276} INFO - 21/05/17 03:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446846625391595280355_0002_m_000034_38
[2021-05-17 00:11:21,469] {docker.py:276} INFO - 21/05/17 03:11:21 INFO Executor: Finished task 36.0 in stage 2.0 (TID 40). 4587 bytes result sent to driver
[2021-05-17 00:11:21,482] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: needsTaskCommit() Task attempt_202105170310448967553897272536447_0002_m_000033_37
21/05/17 03:11:21 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 42) (50e6ccadb8f1, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:21,504] {docker.py:276} INFO - 21/05/17 03:11:21 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 40) in 3143 ms on 50e6ccadb8f1 (executor driver) (35/200)
[2021-05-17 00:11:21,523] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: needsTaskCommit() Task attempt_202105170310448967553897272536447_0002_m_000033_37: duration 0:00.039s
21/05/17 03:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448967553897272536447_0002_m_000033_37
[2021-05-17 00:11:21,532] {docker.py:276} INFO - 21/05/17 03:11:21 INFO Executor: Running task 38.0 in stage 2.0 (TID 42)
21/05/17 03:11:21 INFO Executor: Finished task 34.0 in stage 2.0 (TID 38). 4587 bytes result sent to driver
[2021-05-17 00:11:21,559] {docker.py:276} INFO - 21/05/17 03:11:21 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 43) (50e6ccadb8f1, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:21,581] {docker.py:276} INFO - 21/05/17 03:11:21 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 38) in 3625 ms on 50e6ccadb8f1 (executor driver) (36/200)
[2021-05-17 00:11:21,586] {docker.py:276} INFO - 21/05/17 03:11:21 INFO Executor: Running task 39.0 in stage 2.0 (TID 43)
[2021-05-17 00:11:21,618] {docker.py:276} INFO - 21/05/17 03:11:21 INFO Executor: Finished task 33.0 in stage 2.0 (TID 37). 4587 bytes result sent to driver
[2021-05-17 00:11:21,632] {docker.py:276} INFO - 21/05/17 03:11:21 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 44) (50e6ccadb8f1, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:21,644] {docker.py:276} INFO - 21/05/17 03:11:21 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 37) in 4169 ms on 50e6ccadb8f1 (executor driver) (37/200)
[2021-05-17 00:11:21,661] {docker.py:276} INFO - 21/05/17 03:11:21 INFO Executor: Running task 40.0 in stage 2.0 (TID 44)
[2021-05-17 00:11:21,757] {docker.py:276} INFO - 21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:21,762] {docker.py:276} INFO - 21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2021-05-17 00:11:21,789] {docker.py:276} INFO - 21/05/17 03:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:21,795] {docker.py:276} INFO - 21/05/17 03:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:21,803] {docker.py:276} INFO - 21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:21,811] {docker.py:276} INFO - 21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445840534874267510790_0002_m_000038_42, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445840534874267510790_0002_m_000038_42}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445840534874267510790_0002}; taskId=attempt_202105170310445840534874267510790_0002_m_000038_42, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65d05dfe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:21,816] {docker.py:276} INFO - 21/05/17 03:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:21,825] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445840534874267510790_0002_m_000038_42
[2021-05-17 00:11:21,880] {docker.py:276} INFO - 21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445840534874267510790_0002_m_000038_42 : duration 0:00.056s
[2021-05-17 00:11:21,935] {docker.py:276} INFO - 21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:21,940] {docker.py:276} INFO - 21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:21,963] {docker.py:276} INFO - 21/05/17 03:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:21,967] {docker.py:276} INFO - 21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445434069748180308351_0002_m_000040_44, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445434069748180308351_0002_m_000040_44}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445434069748180308351_0002}; taskId=attempt_202105170310445434069748180308351_0002_m_000040_44, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31d67170}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445434069748180308351_0002_m_000040_44
[2021-05-17 00:11:22,000] {docker.py:276} INFO - 21/05/17 03:11:22 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-17 00:11:22,018] {docker.py:276} INFO - 21/05/17 03:11:22 INFO StagingCommitter: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445434069748180308351_0002_m_000040_44 : duration 0:00.049s
[2021-05-17 00:11:22,052] {docker.py:276} INFO - 21/05/17 03:11:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:22,059] {docker.py:276} INFO - 21/05/17 03:11:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:22,072] {docker.py:276} INFO - 21/05/17 03:11:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:22,075] {docker.py:276} INFO - 21/05/17 03:11:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441759493548102664980_0002_m_000039_43, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441759493548102664980_0002_m_000039_43}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441759493548102664980_0002}; taskId=attempt_202105170310441759493548102664980_0002_m_000039_43, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fe30d16}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:22,079] {docker.py:276} INFO - 21/05/17 03:11:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:22 INFO StagingCommitter: Starting: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441759493548102664980_0002_m_000039_43
[2021-05-17 00:11:22,176] {docker.py:276} INFO - 21/05/17 03:11:22 INFO StagingCommitter: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441759493548102664980_0002_m_000039_43 : duration 0:00.094s
[2021-05-17 00:11:23,503] {docker.py:276} INFO - 21/05/17 03:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: needsTaskCommit() Task attempt_202105170310445840534874267510790_0002_m_000038_42
[2021-05-17 00:11:23,511] {docker.py:276} INFO - 21/05/17 03:11:23 INFO StagingCommitter: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: needsTaskCommit() Task attempt_202105170310445840534874267510790_0002_m_000038_42: duration 0:00.004s
21/05/17 03:11:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445840534874267510790_0002_m_000038_42
[2021-05-17 00:11:23,535] {docker.py:276} INFO - 21/05/17 03:11:23 INFO Executor: Finished task 38.0 in stage 2.0 (TID 42). 4587 bytes result sent to driver
[2021-05-17 00:11:23,552] {docker.py:276} INFO - 21/05/17 03:11:23 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 45) (50e6ccadb8f1, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:23,576] {docker.py:276} INFO - 21/05/17 03:11:23 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 42) in 2092 ms on 50e6ccadb8f1 (executor driver) (38/200)
[2021-05-17 00:11:23,610] {docker.py:276} INFO - 21/05/17 03:11:23 INFO Executor: Running task 41.0 in stage 2.0 (TID 45)
[2021-05-17 00:11:23,770] {docker.py:276} INFO - 21/05/17 03:11:23 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:23,775] {docker.py:276} INFO - 21/05/17 03:11:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
[2021-05-17 00:11:23,798] {docker.py:276} INFO - 21/05/17 03:11:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:23,806] {docker.py:276} INFO - 21/05/17 03:11:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:23,812] {docker.py:276} INFO - 21/05/17 03:11:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444073032738344709967_0002_m_000041_45, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444073032738344709967_0002_m_000041_45}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444073032738344709967_0002}; taskId=attempt_202105170310444073032738344709967_0002_m_000041_45, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c381c9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:23,817] {docker.py:276} INFO - 21/05/17 03:11:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444073032738344709967_0002_m_000041_45
[2021-05-17 00:11:23,881] {docker.py:276} INFO - 21/05/17 03:11:23 INFO StagingCommitter: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444073032738344709967_0002_m_000041_45 : duration 0:00.068s
[2021-05-17 00:11:24,100] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: needsTaskCommit() Task attempt_202105170310447060217971280555645_0002_m_000037_41
[2021-05-17 00:11:24,104] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: needsTaskCommit() Task attempt_202105170310447060217971280555645_0002_m_000037_41: duration 0:00.004s
[2021-05-17 00:11:24,110] {docker.py:276} INFO - 21/05/17 03:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447060217971280555645_0002_m_000037_41
[2021-05-17 00:11:24,126] {docker.py:276} INFO - 21/05/17 03:11:24 INFO Executor: Finished task 37.0 in stage 2.0 (TID 41). 4544 bytes result sent to driver
[2021-05-17 00:11:24,149] {docker.py:276} INFO - 21/05/17 03:11:24 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 46) (50e6ccadb8f1, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:24,163] {docker.py:276} INFO - 21/05/17 03:11:24 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 41) in 3557 ms on 50e6ccadb8f1 (executor driver) (39/200)
[2021-05-17 00:11:24,168] {docker.py:276} INFO - 21/05/17 03:11:24 INFO Executor: Running task 42.0 in stage 2.0 (TID 46)
[2021-05-17 00:11:24,333] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: needsTaskCommit() Task attempt_202105170310445434069748180308351_0002_m_000040_44
[2021-05-17 00:11:24,341] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: needsTaskCommit() Task attempt_202105170310445434069748180308351_0002_m_000040_44: duration 0:00.010s
[2021-05-17 00:11:24,348] {docker.py:276} INFO - 21/05/17 03:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445434069748180308351_0002_m_000040_44
[2021-05-17 00:11:24,375] {docker.py:276} INFO - 21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:24,382] {docker.py:276} INFO - 21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[2021-05-17 00:11:24,412] {docker.py:276} INFO - 21/05/17 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:24,418] {docker.py:276} INFO - 21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044186632743283096761_0002_m_000042_46, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044186632743283096761_0002_m_000042_46}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044186632743283096761_0002}; taskId=attempt_20210517031044186632743283096761_0002_m_000042_46, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@294aa386}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:24,422] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044186632743283096761_0002_m_000042_46
[2021-05-17 00:11:24,455] {docker.py:276} INFO - 21/05/17 03:11:24 INFO Executor: Finished task 40.0 in stage 2.0 (TID 44). 4587 bytes result sent to driver
[2021-05-17 00:11:24,469] {docker.py:276} INFO - 21/05/17 03:11:24 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 47) (50e6ccadb8f1, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:24,482] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044186632743283096761_0002_m_000042_46 : duration 0:00.055s
[2021-05-17 00:11:24,500] {docker.py:276} INFO - 21/05/17 03:11:24 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 44) in 2872 ms on 50e6ccadb8f1 (executor driver) (40/200)
[2021-05-17 00:11:24,504] {docker.py:276} INFO - 21/05/17 03:11:24 INFO Executor: Running task 43.0 in stage 2.0 (TID 47)
[2021-05-17 00:11:24,509] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: needsTaskCommit() Task attempt_202105170310441759493548102664980_0002_m_000039_43
[2021-05-17 00:11:24,520] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: needsTaskCommit() Task attempt_202105170310441759493548102664980_0002_m_000039_43: duration 0:00.021s
21/05/17 03:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441759493548102664980_0002_m_000039_43
[2021-05-17 00:11:24,560] {docker.py:276} INFO - 21/05/17 03:11:24 INFO Executor: Finished task 39.0 in stage 2.0 (TID 43). 4587 bytes result sent to driver
[2021-05-17 00:11:24,582] {docker.py:276} INFO - 21/05/17 03:11:24 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 48) (50e6ccadb8f1, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:24,602] {docker.py:276} INFO - 21/05/17 03:11:24 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 43) in 3049 ms on 50e6ccadb8f1 (executor driver) (41/200)
[2021-05-17 00:11:24,607] {docker.py:276} INFO - 21/05/17 03:11:24 INFO Executor: Running task 44.0 in stage 2.0 (TID 48)
[2021-05-17 00:11:24,735] {docker.py:276} INFO - 21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:24,738] {docker.py:276} INFO - 21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:24,786] {docker.py:276} INFO - 21/05/17 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:24,794] {docker.py:276} INFO - 21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441969056317441669391_0002_m_000043_47, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441969056317441669391_0002_m_000043_47}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441969056317441669391_0002}; taskId=attempt_202105170310441969056317441669391_0002_m_000043_47, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f491a1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441969056317441669391_0002_m_000043_47
[2021-05-17 00:11:24,840] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441969056317441669391_0002_m_000043_47 : duration 0:00.046s
[2021-05-17 00:11:24,850] {docker.py:276} INFO - 21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:24,858] {docker.py:276} INFO - 21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2021-05-17 00:11:24,890] {docker.py:276} INFO - 21/05/17 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:24,898] {docker.py:276} INFO - 21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044286766208191874772_0002_m_000044_48, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044286766208191874772_0002_m_000044_48}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044286766208191874772_0002}; taskId=attempt_20210517031044286766208191874772_0002_m_000044_48, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11e0293f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044286766208191874772_0002_m_000044_48
[2021-05-17 00:11:24,928] {docker.py:276} INFO - 21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044286766208191874772_0002_m_000044_48 : duration 0:00.039s
[2021-05-17 00:11:26,837] {docker.py:276} INFO - 21/05/17 03:11:26 INFO StagingCommitter: Starting: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: needsTaskCommit() Task attempt_202105170310444073032738344709967_0002_m_000041_45
[2021-05-17 00:11:26,843] {docker.py:276} INFO - 21/05/17 03:11:26 INFO StagingCommitter: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: needsTaskCommit() Task attempt_202105170310444073032738344709967_0002_m_000041_45: duration 0:00.015s
[2021-05-17 00:11:26,853] {docker.py:276} INFO - 21/05/17 03:11:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444073032738344709967_0002_m_000041_45
[2021-05-17 00:11:26,874] {docker.py:276} INFO - 21/05/17 03:11:26 INFO Executor: Finished task 41.0 in stage 2.0 (TID 45). 4587 bytes result sent to driver
[2021-05-17 00:11:26,889] {docker.py:276} INFO - 21/05/17 03:11:26 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 49) (50e6ccadb8f1, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:26,903] {docker.py:276} INFO - 21/05/17 03:11:26 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 45) in 3359 ms on 50e6ccadb8f1 (executor driver) (42/200)
[2021-05-17 00:11:26,916] {docker.py:276} INFO - 21/05/17 03:11:26 INFO Executor: Running task 45.0 in stage 2.0 (TID 49)
[2021-05-17 00:11:27,072] {docker.py:276} INFO - 21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:27,077] {docker.py:276} INFO - 21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-17 00:11:27,108] {docker.py:276} INFO - 21/05/17 03:11:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448730539564982436309_0002_m_000045_49, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448730539564982436309_0002_m_000045_49}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448730539564982436309_0002}; taskId=attempt_202105170310448730539564982436309_0002_m_000045_49, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c1ed19d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:27,120] {docker.py:276} INFO - 21/05/17 03:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448730539564982436309_0002_m_000045_49
[2021-05-17 00:11:27,202] {docker.py:276} INFO - 21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: needsTaskCommit() Task attempt_202105170310441969056317441669391_0002_m_000043_47
[2021-05-17 00:11:27,213] {docker.py:276} INFO - 21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: needsTaskCommit() Task attempt_202105170310441969056317441669391_0002_m_000043_47: duration 0:00.014s
[2021-05-17 00:11:27,217] {docker.py:276} INFO - 21/05/17 03:11:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441969056317441669391_0002_m_000043_47
[2021-05-17 00:11:27,241] {docker.py:276} INFO - 21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448730539564982436309_0002_m_000045_49 : duration 0:00.121s
[2021-05-17 00:11:27,268] {docker.py:276} INFO - 21/05/17 03:11:27 INFO Executor: Finished task 43.0 in stage 2.0 (TID 47). 4630 bytes result sent to driver
[2021-05-17 00:11:27,295] {docker.py:276} INFO - 21/05/17 03:11:27 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 50) (50e6ccadb8f1, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:27,307] {docker.py:276} INFO - 21/05/17 03:11:27 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 47) in 2848 ms on 50e6ccadb8f1 (executor driver) (43/200)
[2021-05-17 00:11:27,325] {docker.py:276} INFO - 21/05/17 03:11:27 INFO Executor: Running task 46.0 in stage 2.0 (TID 50)
[2021-05-17 00:11:27,653] {docker.py:276} INFO - 21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:27,661] {docker.py:276} INFO - 21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:27,718] {docker.py:276} INFO - 21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: needsTaskCommit() Task attempt_20210517031044186632743283096761_0002_m_000042_46
[2021-05-17 00:11:27,729] {docker.py:276} INFO - 21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: needsTaskCommit() Task attempt_20210517031044186632743283096761_0002_m_000042_46: duration 0:00.011s
21/05/17 03:11:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044186632743283096761_0002_m_000042_46
[2021-05-17 00:11:27,747] {docker.py:276} INFO - 21/05/17 03:11:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:27,762] {docker.py:276} INFO - 21/05/17 03:11:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:27,772] {docker.py:276} INFO - 21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441529610456988542853_0002_m_000046_50, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441529610456988542853_0002_m_000046_50}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441529610456988542853_0002}; taskId=attempt_202105170310441529610456988542853_0002_m_000046_50, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1399ab1b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:27,793] {docker.py:276} INFO - 21/05/17 03:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441529610456988542853_0002_m_000046_50
[2021-05-17 00:11:27,800] {docker.py:276} INFO - 21/05/17 03:11:27 INFO Executor: Finished task 42.0 in stage 2.0 (TID 46). 4630 bytes result sent to driver
[2021-05-17 00:11:27,833] {docker.py:276} INFO - 21/05/17 03:11:27 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 51) (50e6ccadb8f1, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:27,856] {docker.py:276} INFO - 21/05/17 03:11:27 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 46) in 3719 ms on 50e6ccadb8f1 (executor driver) (44/200)
[2021-05-17 00:11:27,888] {docker.py:276} INFO - 21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441529610456988542853_0002_m_000046_50 : duration 0:00.088s
[2021-05-17 00:11:27,893] {docker.py:276} INFO - 21/05/17 03:11:27 INFO Executor: Running task 47.0 in stage 2.0 (TID 51)
[2021-05-17 00:11:28,227] {docker.py:276} INFO - 21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:28,231] {docker.py:276} INFO - 21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-17 00:11:28,246] {docker.py:276} INFO - 21/05/17 03:11:28 INFO StagingCommitter: Starting: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: needsTaskCommit() Task attempt_20210517031044286766208191874772_0002_m_000044_48
[2021-05-17 00:11:28,257] {docker.py:276} INFO - 21/05/17 03:11:28 INFO StagingCommitter: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: needsTaskCommit() Task attempt_20210517031044286766208191874772_0002_m_000044_48: duration 0:00.009s
[2021-05-17 00:11:28,280] {docker.py:276} INFO - 21/05/17 03:11:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044286766208191874772_0002_m_000044_48
[2021-05-17 00:11:28,305] {docker.py:276} INFO - 21/05/17 03:11:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:28,315] {docker.py:276} INFO - 21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443791610810071741603_0002_m_000047_51, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443791610810071741603_0002_m_000047_51}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443791610810071741603_0002}; taskId=attempt_202105170310443791610810071741603_0002_m_000047_51, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4da9bf24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:28,327] {docker.py:276} INFO - 21/05/17 03:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443791610810071741603_0002_m_000047_51
[2021-05-17 00:11:28,334] {docker.py:276} INFO - 21/05/17 03:11:28 INFO Executor: Finished task 44.0 in stage 2.0 (TID 48). 4630 bytes result sent to driver
[2021-05-17 00:11:28,368] {docker.py:276} INFO - 21/05/17 03:11:28 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 52) (50e6ccadb8f1, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:28,381] {docker.py:276} INFO - 21/05/17 03:11:28 INFO Executor: Running task 48.0 in stage 2.0 (TID 52)
[2021-05-17 00:11:28,434] {docker.py:276} INFO - 21/05/17 03:11:28 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 48) in 3821 ms on 50e6ccadb8f1 (executor driver) (45/200)
[2021-05-17 00:11:28,452] {docker.py:276} INFO - 21/05/17 03:11:28 INFO StagingCommitter: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443791610810071741603_0002_m_000047_51 : duration 0:00.131s
[2021-05-17 00:11:28,769] {docker.py:276} INFO - 21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:28,776] {docker.py:276} INFO - 21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2021-05-17 00:11:28,816] {docker.py:276} INFO - 21/05/17 03:11:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:28,827] {docker.py:276} INFO - 21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:28,832] {docker.py:276} INFO - 21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444213259486931209798_0002_m_000048_52, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444213259486931209798_0002_m_000048_52}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444213259486931209798_0002}; taskId=attempt_202105170310444213259486931209798_0002_m_000048_52, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4514cd46}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:28,843] {docker.py:276} INFO - 21/05/17 03:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444213259486931209798_0002_m_000048_52
[2021-05-17 00:11:28,924] {docker.py:276} INFO - 21/05/17 03:11:28 INFO StagingCommitter: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444213259486931209798_0002_m_000048_52 : duration 0:00.082s
[2021-05-17 00:11:31,389] {docker.py:276} INFO - 21/05/17 03:11:31 INFO StagingCommitter: Starting: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: needsTaskCommit() Task attempt_202105170310441529610456988542853_0002_m_000046_50
[2021-05-17 00:11:31,399] {docker.py:276} INFO - 21/05/17 03:11:31 INFO StagingCommitter: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: needsTaskCommit() Task attempt_202105170310441529610456988542853_0002_m_000046_50: duration 0:00.021s
21/05/17 03:11:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441529610456988542853_0002_m_000046_50
[2021-05-17 00:11:31,470] {docker.py:276} INFO - 21/05/17 03:11:31 INFO Executor: Finished task 46.0 in stage 2.0 (TID 50). 4587 bytes result sent to driver
[2021-05-17 00:11:31,514] {docker.py:276} INFO - 21/05/17 03:11:31 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 53) (50e6ccadb8f1, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:31,531] {docker.py:276} INFO - 21/05/17 03:11:31 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 50) in 4252 ms on 50e6ccadb8f1 (executor driver) (46/200)
[2021-05-17 00:11:31,538] {docker.py:276} INFO - 21/05/17 03:11:31 INFO Executor: Running task 49.0 in stage 2.0 (TID 53)
[2021-05-17 00:11:31,881] {docker.py:276} INFO - 21/05/17 03:11:31 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:31,886] {docker.py:276} INFO - 21/05/17 03:11:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2021-05-17 00:11:31,932] {docker.py:276} INFO - 21/05/17 03:11:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448383472073194543640_0002_m_000049_53, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448383472073194543640_0002_m_000049_53}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448383472073194543640_0002}; taskId=attempt_202105170310448383472073194543640_0002_m_000049_53, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@439bba12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:31,944] {docker.py:276} INFO - 21/05/17 03:11:31 INFO StagingCommitter: Starting: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448383472073194543640_0002_m_000049_53
[2021-05-17 00:11:31,954] {docker.py:276} INFO - 21/05/17 03:11:31 INFO StagingCommitter: Starting: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: needsTaskCommit() Task attempt_202105170310448730539564982436309_0002_m_000045_49
[2021-05-17 00:11:31,965] {docker.py:276} INFO - 21/05/17 03:11:31 INFO StagingCommitter: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: needsTaskCommit() Task attempt_202105170310448730539564982436309_0002_m_000045_49: duration 0:00.019s
21/05/17 03:11:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448730539564982436309_0002_m_000045_49
[2021-05-17 00:11:32,065] {docker.py:276} INFO - 21/05/17 03:11:32 INFO Executor: Finished task 45.0 in stage 2.0 (TID 49). 4587 bytes result sent to driver
[2021-05-17 00:11:32,074] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448383472073194543640_0002_m_000049_53 : duration 0:00.131s
[2021-05-17 00:11:32,096] {docker.py:276} INFO - 21/05/17 03:11:32 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 54) (50e6ccadb8f1, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:32,122] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: needsTaskCommit() Task attempt_202105170310443791610810071741603_0002_m_000047_51
[2021-05-17 00:11:32,128] {docker.py:276} INFO - 21/05/17 03:11:32 INFO Executor: Running task 50.0 in stage 2.0 (TID 54)
[2021-05-17 00:11:32,138] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: needsTaskCommit() Task attempt_202105170310443791610810071741603_0002_m_000047_51: duration 0:00.013s
21/05/17 03:11:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443791610810071741603_0002_m_000047_51
[2021-05-17 00:11:32,144] {docker.py:276} INFO - 21/05/17 03:11:32 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 49) in 5236 ms on 50e6ccadb8f1 (executor driver) (47/200)
[2021-05-17 00:11:32,187] {docker.py:276} INFO - 21/05/17 03:11:32 INFO Executor: Finished task 47.0 in stage 2.0 (TID 51). 4587 bytes result sent to driver
[2021-05-17 00:11:32,223] {docker.py:276} INFO - 21/05/17 03:11:32 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 55) (50e6ccadb8f1, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:32,240] {docker.py:276} INFO - 21/05/17 03:11:32 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 51) in 4430 ms on 50e6ccadb8f1 (executor driver) (48/200)
21/05/17 03:11:32 INFO Executor: Running task 51.0 in stage 2.0 (TID 55)
[2021-05-17 00:11:32,467] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: needsTaskCommit() Task attempt_202105170310444213259486931209798_0002_m_000048_52
[2021-05-17 00:11:32,476] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: needsTaskCommit() Task attempt_202105170310444213259486931209798_0002_m_000048_52: duration 0:00.006s
21/05/17 03:11:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444213259486931209798_0002_m_000048_52
[2021-05-17 00:11:32,503] {docker.py:276} INFO - 21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:32,507] {docker.py:276} INFO - 21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2021-05-17 00:11:32,533] {docker.py:276} INFO - 21/05/17 03:11:32 INFO Executor: Finished task 48.0 in stage 2.0 (TID 52). 4587 bytes result sent to driver
[2021-05-17 00:11:32,578] {docker.py:276} INFO - 21/05/17 03:11:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:32 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 56) (50e6ccadb8f1, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442705697827878854901_0002_m_000050_54, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442705697827878854901_0002_m_000050_54}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442705697827878854901_0002}; taskId=attempt_202105170310442705697827878854901_0002_m_000050_54, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@485242d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:32,593] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442705697827878854901_0002_m_000050_54
[2021-05-17 00:11:32,620] {docker.py:276} INFO - 21/05/17 03:11:32 INFO Executor: Running task 52.0 in stage 2.0 (TID 56)
[2021-05-17 00:11:32,635] {docker.py:276} INFO - 21/05/17 03:11:32 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 52) in 4287 ms on 50e6ccadb8f1 (executor driver) (49/200)
[2021-05-17 00:11:32,647] {docker.py:276} INFO - 21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:32,657] {docker.py:276} INFO - 21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2021-05-17 00:11:32,687] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442705697827878854901_0002_m_000050_54 : duration 0:00.110s
[2021-05-17 00:11:32,727] {docker.py:276} INFO - 21/05/17 03:11:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:32,730] {docker.py:276} INFO - 21/05/17 03:11:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:32,735] {docker.py:276} INFO - 21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:32,741] {docker.py:276} INFO - 21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444829623036195558816_0002_m_000051_55, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444829623036195558816_0002_m_000051_55}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444829623036195558816_0002}; taskId=attempt_202105170310444829623036195558816_0002_m_000051_55, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c848a24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:32,745] {docker.py:276} INFO - 21/05/17 03:11:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:32,749] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444829623036195558816_0002_m_000051_55
[2021-05-17 00:11:32,859] {docker.py:276} INFO - 21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444829623036195558816_0002_m_000051_55 : duration 0:00.104s
[2021-05-17 00:11:32,915] {docker.py:276} INFO - 21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:32,921] {docker.py:276} INFO - 21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-17 00:11:32,947] {docker.py:276} INFO - 21/05/17 03:11:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:32,952] {docker.py:276} INFO - 21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:32,956] {docker.py:276} INFO - 21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445139164563728988308_0002_m_000052_56, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445139164563728988308_0002_m_000052_56}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445139164563728988308_0002}; taskId=attempt_202105170310445139164563728988308_0002_m_000052_56, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@733943e1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445139164563728988308_0002_m_000052_56
[2021-05-17 00:11:32,986] {docker.py:276} INFO - 21/05/17 03:11:33 INFO StagingCommitter: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445139164563728988308_0002_m_000052_56 : duration 0:00.034s
[2021-05-17 00:11:34,325] {docker.py:276} INFO - 21/05/17 03:11:34 INFO StagingCommitter: Starting: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: needsTaskCommit() Task attempt_202105170310442705697827878854901_0002_m_000050_54
[2021-05-17 00:11:34,332] {docker.py:276} INFO - 21/05/17 03:11:34 INFO StagingCommitter: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: needsTaskCommit() Task attempt_202105170310442705697827878854901_0002_m_000050_54: duration 0:00.004s
[2021-05-17 00:11:34,335] {docker.py:276} INFO - 21/05/17 03:11:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442705697827878854901_0002_m_000050_54
[2021-05-17 00:11:34,365] {docker.py:276} INFO - 21/05/17 03:11:34 INFO Executor: Finished task 50.0 in stage 2.0 (TID 54). 4587 bytes result sent to driver
[2021-05-17 00:11:34,378] {docker.py:276} INFO - 21/05/17 03:11:34 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 57) (50e6ccadb8f1, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:34,401] {docker.py:276} INFO - 21/05/17 03:11:34 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 54) in 2327 ms on 50e6ccadb8f1 (executor driver) (50/200)
[2021-05-17 00:11:34,407] {docker.py:276} INFO - 21/05/17 03:11:34 INFO Executor: Running task 53.0 in stage 2.0 (TID 57)
[2021-05-17 00:11:34,567] {docker.py:276} INFO - 21/05/17 03:11:34 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:34,570] {docker.py:276} INFO - 21/05/17 03:11:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-17 00:11:34,640] {docker.py:276} INFO - 21/05/17 03:11:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:34,648] {docker.py:276} INFO - 21/05/17 03:11:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:34,653] {docker.py:276} INFO - 21/05/17 03:11:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448295762992238359232_0002_m_000053_57, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448295762992238359232_0002_m_000053_57}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448295762992238359232_0002}; taskId=attempt_202105170310448295762992238359232_0002_m_000053_57, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ffb361d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:34,657] {docker.py:276} INFO - 21/05/17 03:11:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:34,663] {docker.py:276} INFO - 21/05/17 03:11:34 INFO StagingCommitter: Starting: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448295762992238359232_0002_m_000053_57
[2021-05-17 00:11:34,741] {docker.py:276} INFO - 21/05/17 03:11:34 INFO StagingCommitter: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448295762992238359232_0002_m_000053_57 : duration 0:00.087s
[2021-05-17 00:11:35,153] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: needsTaskCommit() Task attempt_202105170310444829623036195558816_0002_m_000051_55
[2021-05-17 00:11:35,162] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: needsTaskCommit() Task attempt_202105170310444829623036195558816_0002_m_000051_55: duration 0:00.006s
[2021-05-17 00:11:35,170] {docker.py:276} INFO - 21/05/17 03:11:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444829623036195558816_0002_m_000051_55
[2021-05-17 00:11:35,190] {docker.py:276} INFO - 21/05/17 03:11:35 INFO Executor: Finished task 51.0 in stage 2.0 (TID 55). 4544 bytes result sent to driver
[2021-05-17 00:11:35,204] {docker.py:276} INFO - 21/05/17 03:11:35 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 58) (50e6ccadb8f1, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:35,212] {docker.py:276} INFO - 21/05/17 03:11:35 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 55) in 3019 ms on 50e6ccadb8f1 (executor driver) (51/200)
[2021-05-17 00:11:35,222] {docker.py:276} INFO - 21/05/17 03:11:35 INFO Executor: Running task 54.0 in stage 2.0 (TID 58)
[2021-05-17 00:11:35,237] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: needsTaskCommit() Task attempt_202105170310445139164563728988308_0002_m_000052_56
[2021-05-17 00:11:35,253] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: needsTaskCommit() Task attempt_202105170310445139164563728988308_0002_m_000052_56: duration 0:00.016s
[2021-05-17 00:11:35,274] {docker.py:276} INFO - 21/05/17 03:11:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445139164563728988308_0002_m_000052_56
[2021-05-17 00:11:35,302] {docker.py:276} INFO - 21/05/17 03:11:35 INFO Executor: Finished task 52.0 in stage 2.0 (TID 56). 4587 bytes result sent to driver
[2021-05-17 00:11:35,321] {docker.py:276} INFO - 21/05/17 03:11:35 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 59) (50e6ccadb8f1, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:35,332] {docker.py:276} INFO - 21/05/17 03:11:35 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 56) in 2784 ms on 50e6ccadb8f1 (executor driver) (52/200)
21/05/17 03:11:35 INFO Executor: Running task 55.0 in stage 2.0 (TID 59)
[2021-05-17 00:11:35,377] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: needsTaskCommit() Task attempt_202105170310448383472073194543640_0002_m_000049_53
[2021-05-17 00:11:35,390] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: needsTaskCommit() Task attempt_202105170310448383472073194543640_0002_m_000049_53: duration 0:00.012s
21/05/17 03:11:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448383472073194543640_0002_m_000049_53
[2021-05-17 00:11:35,438] {docker.py:276} INFO - 21/05/17 03:11:35 INFO Executor: Finished task 49.0 in stage 2.0 (TID 53). 4587 bytes result sent to driver
[2021-05-17 00:11:35,454] {docker.py:276} INFO - 21/05/17 03:11:35 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 60) (50e6ccadb8f1, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:35,473] {docker.py:276} INFO - 21/05/17 03:11:35 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 53) in 3983 ms on 50e6ccadb8f1 (executor driver) (53/200)
21/05/17 03:11:35 INFO Executor: Running task 56.0 in stage 2.0 (TID 60)
[2021-05-17 00:11:35,507] {docker.py:276} INFO - 21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:35,515] {docker.py:276} INFO - 21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2021-05-17 00:11:35,551] {docker.py:276} INFO - 21/05/17 03:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:35,563] {docker.py:276} INFO - 21/05/17 03:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:35,572] {docker.py:276} INFO - 21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:35,577] {docker.py:276} INFO - 21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms
[2021-05-17 00:11:35,590] {docker.py:276} INFO - 21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443516653645533783773_0002_m_000055_59, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443516653645533783773_0002_m_000055_59}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443516653645533783773_0002}; taskId=attempt_202105170310443516653645533783773_0002_m_000055_59, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1424573d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:35,593] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443516653645533783773_0002_m_000055_59
[2021-05-17 00:11:35,648] {docker.py:276} INFO - 21/05/17 03:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:35,655] {docker.py:276} INFO - 21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448368011798834432846_0002_m_000054_58, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448368011798834432846_0002_m_000054_58}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448368011798834432846_0002}; taskId=attempt_202105170310448368011798834432846_0002_m_000054_58, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a490d36}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448368011798834432846_0002_m_000054_58
[2021-05-17 00:11:35,665] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443516653645533783773_0002_m_000055_59 : duration 0:00.075s
[2021-05-17 00:11:35,745] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448368011798834432846_0002_m_000054_58 : duration 0:00.094s
[2021-05-17 00:11:35,835] {docker.py:276} INFO - 21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:35,848] {docker.py:276} INFO - 21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
[2021-05-17 00:11:35,895] {docker.py:276} INFO - 21/05/17 03:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:35,931] {docker.py:276} INFO - 21/05/17 03:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:35,947] {docker.py:276} INFO - 21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:35,953] {docker.py:276} INFO - 21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441308718270171367657_0002_m_000056_60, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441308718270171367657_0002_m_000056_60}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441308718270171367657_0002}; taskId=attempt_202105170310441308718270171367657_0002_m_000056_60, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@687804cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:35,959] {docker.py:276} INFO - 21/05/17 03:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:35,963] {docker.py:276} INFO - 21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441308718270171367657_0002_m_000056_60
[2021-05-17 00:11:35,994] {docker.py:276} INFO - 21/05/17 03:11:36 INFO StagingCommitter: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441308718270171367657_0002_m_000056_60 : duration 0:00.031s
[2021-05-17 00:11:37,520] {docker.py:276} INFO - 21/05/17 03:11:37 INFO StagingCommitter: Starting: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: needsTaskCommit() Task attempt_202105170310448295762992238359232_0002_m_000053_57
[2021-05-17 00:11:37,526] {docker.py:276} INFO - 21/05/17 03:11:37 INFO StagingCommitter: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: needsTaskCommit() Task attempt_202105170310448295762992238359232_0002_m_000053_57: duration 0:00.004s
[2021-05-17 00:11:37,531] {docker.py:276} INFO - 21/05/17 03:11:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448295762992238359232_0002_m_000053_57
[2021-05-17 00:11:37,549] {docker.py:276} INFO - 21/05/17 03:11:37 INFO Executor: Finished task 53.0 in stage 2.0 (TID 57). 4587 bytes result sent to driver
[2021-05-17 00:11:37,573] {docker.py:276} INFO - 21/05/17 03:11:37 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 61) (50e6ccadb8f1, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:37,589] {docker.py:276} INFO - 21/05/17 03:11:37 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 57) in 3216 ms on 50e6ccadb8f1 (executor driver) (54/200)
21/05/17 03:11:37 INFO Executor: Running task 57.0 in stage 2.0 (TID 61)
[2021-05-17 00:11:37,776] {docker.py:276} INFO - 21/05/17 03:11:37 INFO ShuffleBlockFetcherIterator: Getting 3 (2.0 KiB) non-empty blocks including 3 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:37,787] {docker.py:276} INFO - 21/05/17 03:11:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2021-05-17 00:11:37,819] {docker.py:276} INFO - 21/05/17 03:11:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:37,828] {docker.py:276} INFO - 21/05/17 03:11:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:37,837] {docker.py:276} INFO - 21/05/17 03:11:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446517098664823693945_0002_m_000057_61, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446517098664823693945_0002_m_000057_61}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446517098664823693945_0002}; taskId=attempt_202105170310446517098664823693945_0002_m_000057_61, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a723c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:37 INFO StagingCommitter: Starting: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446517098664823693945_0002_m_000057_61
[2021-05-17 00:11:37,902] {docker.py:276} INFO - 21/05/17 03:11:37 INFO StagingCommitter: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446517098664823693945_0002_m_000057_61 : duration 0:00.067s
[2021-05-17 00:11:38,052] {docker.py:276} INFO - 21/05/17 03:11:38 INFO StagingCommitter: Starting: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: needsTaskCommit() Task attempt_202105170310448368011798834432846_0002_m_000054_58
[2021-05-17 00:11:38,077] {docker.py:276} INFO - 21/05/17 03:11:38 INFO StagingCommitter: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: needsTaskCommit() Task attempt_202105170310448368011798834432846_0002_m_000054_58: duration 0:00.022s
21/05/17 03:11:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448368011798834432846_0002_m_000054_58
[2021-05-17 00:11:38,096] {docker.py:276} INFO - 21/05/17 03:11:38 INFO Executor: Finished task 54.0 in stage 2.0 (TID 58). 4587 bytes result sent to driver
[2021-05-17 00:11:38,326] {docker.py:276} INFO - 21/05/17 03:11:38 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 62) (50e6ccadb8f1, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:38,364] {docker.py:276} INFO - 21/05/17 03:11:38 INFO Executor: Running task 58.0 in stage 2.0 (TID 62)
[2021-05-17 00:11:38,370] {docker.py:276} INFO - 21/05/17 03:11:38 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 58) in 3162 ms on 50e6ccadb8f1 (executor driver) (55/200)
[2021-05-17 00:11:38,570] {docker.py:276} INFO - 21/05/17 03:11:38 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:38,575] {docker.py:276} INFO - 21/05/17 03:11:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-17 00:11:38,603] {docker.py:276} INFO - 21/05/17 03:11:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:38,607] {docker.py:276} INFO - 21/05/17 03:11:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447174991347084826617_0002_m_000058_62, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447174991347084826617_0002_m_000058_62}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447174991347084826617_0002}; taskId=attempt_202105170310447174991347084826617_0002_m_000058_62, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c5c1d21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:38 INFO StagingCommitter: Starting: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447174991347084826617_0002_m_000058_62
[2021-05-17 00:11:38,646] {docker.py:276} INFO - 21/05/17 03:11:38 INFO StagingCommitter: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447174991347084826617_0002_m_000058_62 : duration 0:00.038s
[2021-05-17 00:11:39,550] {docker.py:276} INFO - 21/05/17 03:11:39 INFO StagingCommitter: Starting: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: needsTaskCommit() Task attempt_202105170310443516653645533783773_0002_m_000055_59
[2021-05-17 00:11:39,557] {docker.py:276} INFO - 21/05/17 03:11:39 INFO StagingCommitter: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: needsTaskCommit() Task attempt_202105170310443516653645533783773_0002_m_000055_59: duration 0:00.010s
21/05/17 03:11:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443516653645533783773_0002_m_000055_59
[2021-05-17 00:11:39,579] {docker.py:276} INFO - 21/05/17 03:11:39 INFO Executor: Finished task 55.0 in stage 2.0 (TID 59). 4630 bytes result sent to driver
[2021-05-17 00:11:39,606] {docker.py:276} INFO - 21/05/17 03:11:39 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 63) (50e6ccadb8f1, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:39,618] {docker.py:276} INFO - 21/05/17 03:11:39 INFO Executor: Running task 59.0 in stage 2.0 (TID 63)
[2021-05-17 00:11:39,635] {docker.py:276} INFO - 21/05/17 03:11:39 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 59) in 4324 ms on 50e6ccadb8f1 (executor driver) (56/200)
[2021-05-17 00:11:39,875] {docker.py:276} INFO - 21/05/17 03:11:39 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2021-05-17 00:11:39,911] {docker.py:276} INFO - 21/05/17 03:11:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:39,917] {docker.py:276} INFO - 21/05/17 03:11:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:39,922] {docker.py:276} INFO - 21/05/17 03:11:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444999287016091372016_0002_m_000059_63, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444999287016091372016_0002_m_000059_63}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444999287016091372016_0002}; taskId=attempt_202105170310444999287016091372016_0002_m_000059_63, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18ed7cbd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:39,926] {docker.py:276} INFO - 21/05/17 03:11:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:39,934] {docker.py:276} INFO - 21/05/17 03:11:39 INFO StagingCommitter: Starting: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444999287016091372016_0002_m_000059_63
[2021-05-17 00:11:39,975] {docker.py:276} INFO - 21/05/17 03:11:39 INFO StagingCommitter: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444999287016091372016_0002_m_000059_63 : duration 0:00.048s
[2021-05-17 00:11:40,825] {docker.py:276} INFO - 21/05/17 03:11:40 INFO StagingCommitter: Starting: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: needsTaskCommit() Task attempt_202105170310441308718270171367657_0002_m_000056_60
[2021-05-17 00:11:40,851] {docker.py:276} INFO - 21/05/17 03:11:40 INFO StagingCommitter: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: needsTaskCommit() Task attempt_202105170310441308718270171367657_0002_m_000056_60: duration 0:00.024s
21/05/17 03:11:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441308718270171367657_0002_m_000056_60
[2021-05-17 00:11:40,897] {docker.py:276} INFO - 21/05/17 03:11:40 INFO Executor: Finished task 56.0 in stage 2.0 (TID 60). 4630 bytes result sent to driver
[2021-05-17 00:11:40,911] {docker.py:276} INFO - 21/05/17 03:11:40 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 64) (50e6ccadb8f1, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:40,935] {docker.py:276} INFO - 21/05/17 03:11:40 INFO Executor: Running task 60.0 in stage 2.0 (TID 64)
[2021-05-17 00:11:40,955] {docker.py:276} INFO - 21/05/17 03:11:40 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 60) in 5494 ms on 50e6ccadb8f1 (executor driver) (57/200)
[2021-05-17 00:11:41,219] {docker.py:276} INFO - 21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-17 00:11:41,261] {docker.py:276} INFO - 21/05/17 03:11:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:41,268] {docker.py:276} INFO - 21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:41,273] {docker.py:276} INFO - 21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445699233081199584384_0002_m_000060_64, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445699233081199584384_0002_m_000060_64}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445699233081199584384_0002}; taskId=attempt_202105170310445699233081199584384_0002_m_000060_64, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3941d38c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:41,285] {docker.py:276} INFO - 21/05/17 03:11:41 INFO StagingCommitter: Starting: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445699233081199584384_0002_m_000060_64
[2021-05-17 00:11:41,331] {docker.py:276} INFO - 21/05/17 03:11:41 INFO StagingCommitter: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445699233081199584384_0002_m_000060_64 : duration 0:00.046s
[2021-05-17 00:11:41,355] {docker.py:276} INFO - 21/05/17 03:11:41 INFO StagingCommitter: Starting: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: needsTaskCommit() Task attempt_202105170310447174991347084826617_0002_m_000058_62
[2021-05-17 00:11:41,374] {docker.py:276} INFO - 21/05/17 03:11:41 INFO StagingCommitter: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: needsTaskCommit() Task attempt_202105170310447174991347084826617_0002_m_000058_62: duration 0:00.016s
21/05/17 03:11:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447174991347084826617_0002_m_000058_62
[2021-05-17 00:11:41,408] {docker.py:276} INFO - 21/05/17 03:11:41 INFO Executor: Finished task 58.0 in stage 2.0 (TID 62). 4544 bytes result sent to driver
[2021-05-17 00:11:41,430] {docker.py:276} INFO - 21/05/17 03:11:41 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 65) (50e6ccadb8f1, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:41,460] {docker.py:276} INFO - 21/05/17 03:11:41 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 62) in 3359 ms on 50e6ccadb8f1 (executor driver) (58/200)
21/05/17 03:11:41 INFO Executor: Running task 61.0 in stage 2.0 (TID 65)
[2021-05-17 00:11:41,657] {docker.py:276} INFO - 21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-17 00:11:41,691] {docker.py:276} INFO - 21/05/17 03:11:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:41,697] {docker.py:276} INFO - 21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:41,700] {docker.py:276} INFO - 21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447231433932774290797_0002_m_000061_65, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447231433932774290797_0002_m_000061_65}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447231433932774290797_0002}; taskId=attempt_202105170310447231433932774290797_0002_m_000061_65, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5519a5e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:41 INFO StagingCommitter: Starting: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447231433932774290797_0002_m_000061_65
[2021-05-17 00:11:41,736] {docker.py:276} INFO - 21/05/17 03:11:41 INFO StagingCommitter: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447231433932774290797_0002_m_000061_65 : duration 0:00.036s
[2021-05-17 00:11:42,345] {docker.py:276} INFO - 21/05/17 03:11:42 INFO StagingCommitter: Starting: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: needsTaskCommit() Task attempt_202105170310446517098664823693945_0002_m_000057_61
[2021-05-17 00:11:42,351] {docker.py:276} INFO - 21/05/17 03:11:42 INFO StagingCommitter: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: needsTaskCommit() Task attempt_202105170310446517098664823693945_0002_m_000057_61: duration 0:00.005s
[2021-05-17 00:11:42,353] {docker.py:276} INFO - 21/05/17 03:11:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446517098664823693945_0002_m_000057_61
[2021-05-17 00:11:42,368] {docker.py:276} INFO - 21/05/17 03:11:42 INFO Executor: Finished task 57.0 in stage 2.0 (TID 61). 4630 bytes result sent to driver
[2021-05-17 00:11:42,387] {docker.py:276} INFO - 21/05/17 03:11:42 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 66) (50e6ccadb8f1, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:42,411] {docker.py:276} INFO - 21/05/17 03:11:42 INFO Executor: Running task 62.0 in stage 2.0 (TID 66)
[2021-05-17 00:11:42,416] {docker.py:276} INFO - 21/05/17 03:11:42 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 61) in 4855 ms on 50e6ccadb8f1 (executor driver) (59/200)
[2021-05-17 00:11:42,637] {docker.py:276} INFO - 21/05/17 03:11:42 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-17 00:11:42,679] {docker.py:276} INFO - 21/05/17 03:11:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:42,684] {docker.py:276} INFO - 21/05/17 03:11:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:42,694] {docker.py:276} INFO - 21/05/17 03:11:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:42,702] {docker.py:276} INFO - 21/05/17 03:11:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446122932560106960702_0002_m_000062_66, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446122932560106960702_0002_m_000062_66}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446122932560106960702_0002}; taskId=attempt_202105170310446122932560106960702_0002_m_000062_66, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@560dbb1d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:42,713] {docker.py:276} INFO - 21/05/17 03:11:42 INFO StagingCommitter: Starting: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446122932560106960702_0002_m_000062_66
[2021-05-17 00:11:42,956] {docker.py:276} INFO - 21/05/17 03:11:42 INFO StagingCommitter: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446122932560106960702_0002_m_000062_66 : duration 0:00.250s
[2021-05-17 00:11:43,233] {docker.py:276} INFO - 21/05/17 03:11:43 INFO StagingCommitter: Starting: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: needsTaskCommit() Task attempt_202105170310444999287016091372016_0002_m_000059_63
[2021-05-17 00:11:43,251] {docker.py:276} INFO - 21/05/17 03:11:43 INFO StagingCommitter: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: needsTaskCommit() Task attempt_202105170310444999287016091372016_0002_m_000059_63: duration 0:00.012s
[2021-05-17 00:11:43,254] {docker.py:276} INFO - 21/05/17 03:11:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444999287016091372016_0002_m_000059_63
[2021-05-17 00:11:43,304] {docker.py:276} INFO - 21/05/17 03:11:43 INFO Executor: Finished task 59.0 in stage 2.0 (TID 63). 4544 bytes result sent to driver
[2021-05-17 00:11:43,352] {docker.py:276} INFO - 21/05/17 03:11:43 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 67) (50e6ccadb8f1, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:43,403] {docker.py:276} INFO - 21/05/17 03:11:43 INFO Executor: Running task 63.0 in stage 2.0 (TID 67)
[2021-05-17 00:11:43,438] {docker.py:276} INFO - 21/05/17 03:11:43 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 63) in 3838 ms on 50e6ccadb8f1 (executor driver) (60/200)
[2021-05-17 00:11:43,708] {docker.py:276} INFO - 21/05/17 03:11:43 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:43,712] {docker.py:276} INFO - 21/05/17 03:11:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2021-05-17 00:11:43,757] {docker.py:276} INFO - 21/05/17 03:11:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:43,765] {docker.py:276} INFO - 21/05/17 03:11:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:43,769] {docker.py:276} INFO - 21/05/17 03:11:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044579943512634877918_0002_m_000063_67, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044579943512634877918_0002_m_000063_67}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044579943512634877918_0002}; taskId=attempt_20210517031044579943512634877918_0002_m_000063_67, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a214336}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:43,799] {docker.py:276} INFO - 21/05/17 03:11:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:43,803] {docker.py:276} INFO - 21/05/17 03:11:43 INFO StagingCommitter: Starting: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044579943512634877918_0002_m_000063_67
[2021-05-17 00:11:43,843] {docker.py:276} INFO - 21/05/17 03:11:43 INFO StagingCommitter: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044579943512634877918_0002_m_000063_67 : duration 0:00.066s
[2021-05-17 00:11:43,907] {docker.py:276} INFO - 21/05/17 03:11:43 INFO StagingCommitter: Starting: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: needsTaskCommit() Task attempt_202105170310445699233081199584384_0002_m_000060_64
21/05/17 03:11:43 INFO StagingCommitter: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: needsTaskCommit() Task attempt_202105170310445699233081199584384_0002_m_000060_64: duration 0:00.003s
21/05/17 03:11:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445699233081199584384_0002_m_000060_64
[2021-05-17 00:11:43,943] {docker.py:276} INFO - 21/05/17 03:11:43 INFO Executor: Finished task 60.0 in stage 2.0 (TID 64). 4587 bytes result sent to driver
[2021-05-17 00:11:43,957] {docker.py:276} INFO - 21/05/17 03:11:43 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 68) (50e6ccadb8f1, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:43,971] {docker.py:276} INFO - 21/05/17 03:11:43 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 64) in 3023 ms on 50e6ccadb8f1 (executor driver) (61/200)
[2021-05-17 00:11:43,988] {docker.py:276} INFO - 21/05/17 03:11:43 INFO Executor: Running task 64.0 in stage 2.0 (TID 68)
[2021-05-17 00:11:44,190] {docker.py:276} INFO - 21/05/17 03:11:44 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:44,195] {docker.py:276} INFO - 21/05/17 03:11:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2021-05-17 00:11:44,230] {docker.py:276} INFO - 21/05/17 03:11:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:44,237] {docker.py:276} INFO - 21/05/17 03:11:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441289891283949382873_0002_m_000064_68, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441289891283949382873_0002_m_000064_68}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441289891283949382873_0002}; taskId=attempt_202105170310441289891283949382873_0002_m_000064_68, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f295042}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:44 INFO StagingCommitter: Starting: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441289891283949382873_0002_m_000064_68
[2021-05-17 00:11:44,330] {docker.py:276} INFO - 21/05/17 03:11:44 INFO StagingCommitter: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441289891283949382873_0002_m_000064_68 : duration 0:00.092s
[2021-05-17 00:11:45,847] {docker.py:276} INFO - 21/05/17 03:11:45 INFO StagingCommitter: Starting: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: needsTaskCommit() Task attempt_202105170310446122932560106960702_0002_m_000062_66
[2021-05-17 00:11:45,861] {docker.py:276} INFO - 21/05/17 03:11:45 INFO StagingCommitter: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: needsTaskCommit() Task attempt_202105170310446122932560106960702_0002_m_000062_66: duration 0:00.010s
21/05/17 03:11:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446122932560106960702_0002_m_000062_66
[2021-05-17 00:11:45,895] {docker.py:276} INFO - 21/05/17 03:11:45 INFO Executor: Finished task 62.0 in stage 2.0 (TID 66). 4587 bytes result sent to driver
[2021-05-17 00:11:45,934] {docker.py:276} INFO - 21/05/17 03:11:45 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 69) (50e6ccadb8f1, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:45,954] {docker.py:276} INFO - 21/05/17 03:11:45 INFO Executor: Running task 65.0 in stage 2.0 (TID 69)
21/05/17 03:11:45 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 66) in 3531 ms on 50e6ccadb8f1 (executor driver) (62/200)
[2021-05-17 00:11:46,023] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: needsTaskCommit() Task attempt_202105170310447231433932774290797_0002_m_000061_65
[2021-05-17 00:11:46,046] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: needsTaskCommit() Task attempt_202105170310447231433932774290797_0002_m_000061_65: duration 0:00.022s
[2021-05-17 00:11:46,053] {docker.py:276} INFO - 21/05/17 03:11:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447231433932774290797_0002_m_000061_65
[2021-05-17 00:11:46,089] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: needsTaskCommit() Task attempt_20210517031044579943512634877918_0002_m_000063_67
[2021-05-17 00:11:46,103] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: needsTaskCommit() Task attempt_20210517031044579943512634877918_0002_m_000063_67: duration 0:00.015s
[2021-05-17 00:11:46,108] {docker.py:276} INFO - 21/05/17 03:11:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044579943512634877918_0002_m_000063_67
[2021-05-17 00:11:46,142] {docker.py:276} INFO - 21/05/17 03:11:46 INFO Executor: Finished task 63.0 in stage 2.0 (TID 67). 4587 bytes result sent to driver
21/05/17 03:11:46 INFO Executor: Finished task 61.0 in stage 2.0 (TID 65). 4587 bytes result sent to driver
[2021-05-17 00:11:46,167] {docker.py:276} INFO - 21/05/17 03:11:46 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 70) (50e6ccadb8f1, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:46,238] {docker.py:276} INFO - 21/05/17 03:11:46 INFO Executor: Running task 66.0 in stage 2.0 (TID 70)
[2021-05-17 00:11:46,259] {docker.py:276} INFO - 21/05/17 03:11:46 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 65) in 4788 ms on 50e6ccadb8f1 (executor driver) (63/200)
[2021-05-17 00:11:46,335] {docker.py:276} INFO - 21/05/17 03:11:46 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 71) (50e6ccadb8f1, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:46,404] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: needsTaskCommit() Task attempt_202105170310441289891283949382873_0002_m_000064_68
[2021-05-17 00:11:46,442] {docker.py:276} INFO - 21/05/17 03:11:46 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 67) in 3068 ms on 50e6ccadb8f1 (executor driver) (64/200)
[2021-05-17 00:11:46,451] {docker.py:276} INFO - 21/05/17 03:11:46 INFO Executor: Running task 67.0 in stage 2.0 (TID 71)
21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: needsTaskCommit() Task attempt_202105170310441289891283949382873_0002_m_000064_68: duration 0:00.019s
[2021-05-17 00:11:46,457] {docker.py:276} INFO - 21/05/17 03:11:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441289891283949382873_0002_m_000064_68
[2021-05-17 00:11:46,478] {docker.py:276} INFO - 21/05/17 03:11:46 INFO Executor: Finished task 64.0 in stage 2.0 (TID 68). 4587 bytes result sent to driver
[2021-05-17 00:11:46,513] {docker.py:276} INFO - 21/05/17 03:11:46 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:11:46,530] {docker.py:276} INFO - 21/05/17 03:11:46 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 68) in 2571 ms on 50e6ccadb8f1 (executor driver) (65/200)
[2021-05-17 00:11:46,535] {docker.py:276} INFO - 21/05/17 03:11:46 INFO Executor: Running task 68.0 in stage 2.0 (TID 72)
[2021-05-17 00:11:46,654] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:46,659] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
[2021-05-17 00:11:46,722] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:46,736] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:46,746] {docker.py:276} INFO - 21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447291783761476396147_0002_m_000065_69, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447291783761476396147_0002_m_000065_69}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447291783761476396147_0002}; taskId=attempt_202105170310447291783761476396147_0002_m_000065_69, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e7184a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:46,764] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310447291783761476396147_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447291783761476396147_0002_m_000065_69
[2021-05-17 00:11:46,813] {docker.py:276} INFO - 21/05/17 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:46,827] {docker.py:276} INFO - 21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441401521357486393300_0002_m_000066_70, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441401521357486393300_0002_m_000066_70}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441401521357486393300_0002}; taskId=attempt_202105170310441401521357486393300_0002_m_000066_70, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@781fab98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:46,837] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310441401521357486393300_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441401521357486393300_0002_m_000066_70
[2021-05-17 00:11:46,867] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310447291783761476396147_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447291783761476396147_0002_m_000065_69 : duration 0:00.120s
[2021-05-17 00:11:46,888] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:46,894] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
[2021-05-17 00:11:46,922] {docker.py:276} INFO - 21/05/17 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:46,946] {docker.py:276} INFO - 21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445248365142681796192_0002_m_000067_71, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445248365142681796192_0002_m_000067_71}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445248365142681796192_0002}; taskId=attempt_202105170310445248365142681796192_0002_m_000067_71, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@136cd93d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:46,951] {docker.py:276} INFO - 21/05/17 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:11:46,960] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310445248365142681796192_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445248365142681796192_0002_m_000067_71
[2021-05-17 00:11:46,997] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:11:47,003] {docker.py:276} INFO - 21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2021-05-17 00:11:47,010] {docker.py:276} INFO - 21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310441401521357486393300_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441401521357486393300_0002_m_000066_70 : duration 0:00.166s
[2021-05-17 00:11:47,045] {docker.py:276} INFO - 21/05/17 03:11:47 INFO StagingCommitter: Task committer attempt_202105170310445248365142681796192_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445248365142681796192_0002_m_000067_71 : duration 0:00.100s
[2021-05-17 00:11:47,065] {docker.py:276} INFO - 21/05/17 03:11:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:11:47,071] {docker.py:276} INFO - 21/05/17 03:11:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:11:47,115] {docker.py:276} INFO - 21/05/17 03:11:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:47,135] {docker.py:276} INFO - 21/05/17 03:11:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446051227697220378960_0002_m_000068_72, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446051227697220378960_0002_m_000068_72}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446051227697220378960_0002}; taskId=attempt_202105170310446051227697220378960_0002_m_000068_72, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@598e4e88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-17 00:11:47,146] {docker.py:276} INFO - 21/05/17 03:11:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:47 INFO StagingCommitter: Starting: Task committer attempt_202105170310446051227697220378960_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446051227697220378960_0002_m_000068_72
[2021-05-17 00:11:47,229] {docker.py:276} INFO - 21/05/17 03:11:47 INFO StagingCommitter: Task committer attempt_202105170310446051227697220378960_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446051227697220378960_0002_m_000068_72 : duration 0:00.102s
[2021-05-17 00:28:27,097] {docker.py:276} INFO - 21/05/17 03:11:50 ERROR Executor: Exception in task 68.0 in stage 2.0 (TID 72)
java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
[2021-05-17 00:28:27,182] {docker.py:276} INFO - 21/05/17 03:11:50 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 73) (50e6ccadb8f1, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-17 00:28:27,185] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Running task 69.0 in stage 2.0 (TID 73)
[2021-05-17 00:28:27,192] {docker.py:276} INFO - 21/05/17 03:11:50 WARN TaskSetManager: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
[2021-05-17 00:28:27,197] {docker.py:276} INFO - 21/05/17 03:11:50 ERROR TaskSetManager: Task 68 in stage 2.0 failed 1 times; aborting job
[2021-05-17 00:28:27,201] {docker.py:276} INFO - 21/05/17 03:11:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-17 00:28:27,201] {docker.py:276} INFO - 21/05/17 03:11:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-17 00:28:27,204] {docker.py:276} INFO - 21/05/17 03:11:50 INFO TaskSchedulerImpl: Cancelling stage 2
21/05/17 03:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled
[2021-05-17 00:28:27,204] {docker.py:276} INFO - 21/05/17 03:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-17 00:28:27,205] {docker.py:276} INFO - 21/05/17 03:11:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-17 00:28:27,205] {docker.py:276} INFO - 21/05/17 03:11:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-17 00:28:27,206] {docker.py:276} INFO - 21/05/17 03:11:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443683335013466727905_0002_m_000069_73, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443683335013466727905_0002_m_000069_73}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443683335013466727905_0002}; taskId=attempt_202105170310443683335013466727905_0002_m_000069_73, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c92e12f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-17 00:28:27,206] {docker.py:276} INFO - 21/05/17 03:11:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310443683335013466727905_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443683335013466727905_0002_m_000069_73
[2021-05-17 00:28:27,211] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 66.0 in stage 2.0 (TID 70), reason: Stage cancelled
21/05/17 03:11:50 INFO StagingCommitter: Task committer attempt_202105170310443683335013466727905_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443683335013466727905_0002_m_000069_73 : duration 0:00.006s
[2021-05-17 00:28:27,212] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 67.0 in stage 2.0 (TID 71), reason: Stage cancelled
[2021-05-17 00:28:27,213] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 65.0 in stage 2.0 (TID 69), reason: Stage cancelled
21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 69.0 in stage 2.0 (TID 73), reason: Stage cancelled
[2021-05-17 00:28:27,216] {docker.py:276} INFO - 21/05/17 03:11:50 INFO TaskSchedulerImpl: Stage 2 was cancelled
[2021-05-17 00:28:27,217] {docker.py:276} INFO - 21/05/17 03:11:50 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) failed in 62.953 s due to Job aborted due to stage failure: Task 68 in stage 2.0 failed 1 times, most recent failure: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$R
[2021-05-17 00:28:27,218] {docker.py:276} INFO - equestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more

Driver stacktrace:
[2021-05-17 00:28:27,221] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Executor killed task 69.0 in stage 2.0 (TID 73), reason: Stage cancelled
21/05/17 03:11:50 INFO DAGScheduler: Job 1 failed: csv at NativeMethodAccessorImpl.java:0, took 62.015651 s
[2021-05-17 00:28:27,224] {docker.py:276} INFO - 21/05/17 03:11:50 ERROR FileFormatWriter: Aborting job 5e113974-9aaf-4add-adcc-306105605440.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 68 in stage 2.0 failed 1 times, most recent failure: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttp
[2021-05-17 00:28:27,227] {docker.py:276} INFO - Client$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception
[2021-05-17 00:28:27,227] {docker.py:276} INFO - : Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
[2021-05-17 00:28:27,229] {docker.py:276} INFO - 21/05/17 03:11:50 INFO AbstractS3ACommitter: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: aborting job (no job ID) in state FAILED
[2021-05-17 00:28:27,229] {docker.py:276} INFO - 21/05/17 03:11:50 INFO StagingCommitter: Starting: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: aborting job in state (no job ID)
[2021-05-17 00:28:27,230] {docker.py:276} INFO - 21/05/17 03:11:50 WARN TaskSetManager: Lost task 69.0 in stage 2.0 (TID 73) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
[2021-05-17 00:28:27,233] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Executor interrupted and killed task 65.0 in stage 2.0 (TID 69), reason: Stage cancelled
[2021-05-17 00:28:27,237] {docker.py:276} INFO - 21/05/17 03:11:50 INFO Executor: Executor interrupted and killed task 66.0 in stage 2.0 (TID 70), reason: Stage cancelled
[2021-05-17 00:28:27,241] {docker.py:276} INFO - 21/05/17 03:11:50 WARN TaskSetManager: Lost task 66.0 in stage 2.0 (TID 70) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
[2021-05-17 00:28:27,242] {docker.py:276} INFO - 21/05/17 03:11:50 WARN TaskSetManager: Lost task 65.0 in stage 2.0 (TID 69) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
[2021-05-17 00:28:27,251] {docker.py:276} INFO - 21/05/17 03:11:50 INFO AbstractS3ACommitter: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: no pending commits to abort
21/05/17 03:11:50 INFO StagingCommitter: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: aborting job in state (no job ID) : duration 0:00.023s
[2021-05-17 00:28:27,769] {docker.py:276} INFO - 21/05/17 03:11:51 INFO Executor: Executor interrupted and killed task 67.0 in stage 2.0 (TID 71), reason: Stage cancelled
[2021-05-17 00:28:27,775] {docker.py:276} INFO - 21/05/17 03:11:51 WARN TaskSetManager: Lost task 67.0 in stage 2.0 (TID 71) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
[2021-05-17 00:28:27,776] {docker.py:276} INFO - 21/05/17 03:11:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-17 00:28:28,101] {docker.py:276} INFO - 21/05/17 03:11:51 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
[2021-05-17 00:28:28,102] {docker.py:276} INFO - 21/05/17 03:11:51 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-17 00:28:28,758] {docker.py:276} INFO - 21/05/17 03:11:52 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.657s
21/05/17 03:11:52 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.658s
[2021-05-17 00:28:29,922] {docker.py:276} INFO - Traceback (most recent call last):
  File "/home/jovyan/spark.py", line 72, in <module>
    df \
  File "/usr/local/spark/python/pyspark/sql/readwriter.py", line 1371, in csv
[2021-05-17 00:28:29,923] {docker.py:276} INFO - self._jwrite.csv(path)
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2021-05-17 00:28:29,923] {docker.py:276} INFO - File "/usr/local/spark/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
[2021-05-17 00:28:29,924] {docker.py:276} INFO - py4j.protocol.Py4JJavaError
[2021-05-17 00:28:29,965] {docker.py:276} INFO - : An error occurred while calling o89.csv.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 68 in stage 2.0 failed 1 times, most recent failure: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
	at org.apache.spark.scheduler.DAGSchedul
[2021-05-17 00:28:29,966] {docker.py:276} INFO - er.$anon
[2021-05-17 00:28:29,966] {docker.py:276} INFO - fun$abortStage$2(DAGScheduler.scala:2202)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
	at org.apache.spark.scheduler.DAGSchedulerEv
[2021-05-17 00:28:29,967] {docker.py:276} INFO - entProcessLoop.onReceive(DAGScheduler.scala:2371)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)
	... 33 more
Caused by: java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidde
[2021-05-17 00:28:29,967] {docker.py:276} INFO - n
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.da
[2021-05-17 00:28:29,968] {docker.py:276} INFO - tasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor
[2021-05-17 00:28:29,968] {docker.py:276} INFO - $Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(Amazo
[2021-05-17 00:28:29,969] {docker.py:276} INFO - nHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.jav
[2021-05-17 00:28:29,969] {docker.py:276} INFO - a:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
[2021-05-17 00:28:30,097] {docker.py:276} INFO - 21/05/17 03:11:53 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-17 00:28:30,116] {docker.py:276} INFO - 21/05/17 03:11:53 INFO SparkUI: Stopped Spark web UI at http://50e6ccadb8f1:4040
[2021-05-17 00:28:30,144] {docker.py:276} INFO - 21/05/17 03:11:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-17 00:28:30,163] {docker.py:276} INFO - 21/05/17 03:11:53 INFO MemoryStore: MemoryStore cleared
[2021-05-17 00:28:30,163] {docker.py:276} INFO - 21/05/17 03:11:53 INFO BlockManager: BlockManager stopped
[2021-05-17 00:28:30,168] {docker.py:276} INFO - 21/05/17 03:11:53 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-17 00:28:30,173] {docker.py:276} INFO - 21/05/17 03:11:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-17 00:28:30,182] {docker.py:276} INFO - 21/05/17 03:11:53 INFO SparkContext: Successfully stopped SparkContext
[2021-05-17 00:28:30,182] {docker.py:276} INFO - 21/05/17 03:11:53 INFO ShutdownHookManager: Shutdown hook called
[2021-05-17 00:28:30,184] {docker.py:276} INFO - 21/05/17 03:11:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-030bae49-8de0-442a-83a2-278e8d90d048
[2021-05-17 00:28:30,188] {docker.py:276} INFO - 21/05/17 03:11:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-a332d460-94bf-4cba-8e4d-eeac4a022c39
[2021-05-17 00:28:30,190] {docker.py:276} INFO - 21/05/17 03:11:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-030bae49-8de0-442a-83a2-278e8d90d048/pyspark-d5bd71c4-2b20-4dfe-816e-81023b3b705c
[2021-05-17 00:28:30,201] {docker.py:276} INFO - 21/05/17 03:11:53 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-17 00:28:30,202] {docker.py:276} INFO - 21/05/17 03:11:53 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-17 00:28:30,203] {docker.py:276} INFO - 21/05/17 03:11:53 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-17 00:28:30,523] {taskinstance.py:1482} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1138, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 321, in execute
    return self._run_image()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 281, in _run_image
    raise AirflowException('docker container failed: ' + repr(result) + f"lines {res_lines}")
airflow.exceptions.AirflowException: docker container failed: {'Error': None, 'StatusCode': 1}lines WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
21/05/17 03:10:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/05/17 03:10:21 INFO SparkContext: Running Spark version 3.1.1
21/05/17 03:10:21 INFO ResourceUtils: ==============================================================
21/05/17 03:10:21 INFO ResourceUtils: No custom resources configured for spark.driver.
21/05/17 03:10:21 INFO ResourceUtils: ==============================================================
21/05/17 03:10:21 INFO SparkContext: Submitted application: spark.py
21/05/17 03:10:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/05/17 03:10:21 INFO ResourceProfile: Limiting resource is cpu
21/05/17 03:10:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/05/17 03:10:21 INFO SecurityManager: Changing view acls to: jovyan
21/05/17 03:10:21 INFO SecurityManager: Changing modify acls to: jovyan
21/05/17 03:10:21 INFO SecurityManager: Changing view acls groups to:
21/05/17 03:10:21 INFO SecurityManager: Changing modify acls groups to:
21/05/17 03:10:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
21/05/17 03:10:22 INFO Utils: Successfully started service 'sparkDriver' on port 38471.
21/05/17 03:10:22 INFO SparkEnv: Registering MapOutputTracker
21/05/17 03:10:22 INFO SparkEnv: Registering BlockManagerMaster
21/05/17 03:10:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/05/17 03:10:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/05/17 03:10:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/05/17 03:10:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6509dac8-be0d-4aa1-bf05-bcc9a90eadf5
21/05/17 03:10:22 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
21/05/17 03:10:22 INFO SparkEnv: Registering OutputCommitCoordinator
21/05/17 03:10:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/05/17 03:10:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://50e6ccadb8f1:4040
21/05/17 03:10:23 INFO Executor: Starting executor ID driver on host 50e6ccadb8f1
21/05/17 03:10:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45201.
21/05/17 03:10:23 INFO NettyBlockTransferService: Server created on 50e6ccadb8f1:45201
21/05/17 03:10:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/05/17 03:10:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
21/05/17 03:10:23 INFO BlockManagerMasterEndpoint: Registering block manager 50e6ccadb8f1:45201 with 934.4 MiB RAM, BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
21/05/17 03:10:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
21/05/17 03:10:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 50e6ccadb8f1, 45201, None)
21/05/17 03:10:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
21/05/17 03:10:23 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
21/05/17 03:10:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
21/05/17 03:10:25 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/17 03:10:25 INFO MetricsSystemImpl: s3a-file-system metrics system started
21/05/17 03:10:33 INFO InMemoryFileIndex: It took 1710 ms to list leaf files for 9 paths.
21/05/17 03:10:35 INFO InMemoryFileIndex: It took 1557 ms to list leaf files for 9 paths.
21/05/17 03:10:38 INFO FileSourceStrategy: Pushed Filters:
21/05/17 03:10:38 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
21/05/17 03:10:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
21/05/17 03:10:39 INFO CodeGenerator: Code generated in 446.0472 ms
21/05/17 03:10:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
21/05/17 03:10:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
21/05/17 03:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 50e6ccadb8f1:45201 (size: 28.2 KiB, free: 934.4 MiB)
21/05/17 03:10:39 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
21/05/17 03:10:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9610970 bytes, open cost is considered as scanning 4194304 bytes.
21/05/17 03:10:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
21/05/17 03:10:40 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/17 03:10:40 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
21/05/17 03:10:40 INFO DAGScheduler: Parents of final stage: List()
21/05/17 03:10:40 INFO DAGScheduler: Missing parents: List()
21/05/17 03:10:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
21/05/17 03:10:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
21/05/17 03:10:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
21/05/17 03:10:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 50e6ccadb8f1:45201 (size: 5.4 KiB, free: 934.4 MiB)
21/05/17 03:10:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
21/05/17 03:10:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/17 03:10:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
21/05/17 03:10:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (50e6ccadb8f1, executor driver, partition 0, PROCESS_LOCAL, 5125 bytes) taskResourceAssignments Map()
21/05/17 03:10:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/05/17 03:10:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104224, partition values: [empty row]
21/05/17 03:10:40 INFO CodeGenerator: Code generated in 30.3222 ms
21/05/17 03:10:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1607 bytes result sent to driver
21/05/17 03:10:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1392 ms on 50e6ccadb8f1 (executor driver) (1/1)
21/05/17 03:10:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
21/05/17 03:10:42 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.706 s
21/05/17 03:10:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 03:10:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/05/17 03:10:42 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.807598 s
21/05/17 03:10:42 INFO CodeGenerator: Code generated in 22.32 ms
21/05/17 03:10:42 INFO FileSourceStrategy: Pushed Filters:
21/05/17 03:10:42 INFO FileSourceStrategy: Post-Scan Filters:
21/05/17 03:10:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
21/05/17 03:10:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
21/05/17 03:10:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
21/05/17 03:10:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 50e6ccadb8f1:45201 (size: 28.2 KiB, free: 934.3 MiB)
21/05/17 03:10:42 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
21/05/17 03:10:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9610970 bytes, open cost is considered as scanning 4194304 bytes.
21/05/17 03:10:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 50e6ccadb8f1:45201 in memory (size: 28.2 KiB, free: 934.4 MiB)
21/05/17 03:10:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 50e6ccadb8f1:45201 in memory (size: 5.4 KiB, free: 934.4 MiB)
21/05/17 03:10:43 INFO FileSourceStrategy: Pushed Filters:
21/05/17 03:10:43 INFO FileSourceStrategy: Post-Scan Filters:
21/05/17 03:10:43 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
21/05/17 03:10:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044670033147968055892_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044670033147968055892_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044670033147968055892_0000}; taskId=attempt_20210517031044670033147968055892_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73ced1f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
21/05/17 03:10:44 INFO CodeGenerator: Code generated in 79.9653 ms
21/05/17 03:10:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
21/05/17 03:10:44 INFO CodeGenerator: Code generated in 52.4982 ms
21/05/17 03:10:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.5 KiB, free 934.0 MiB)
21/05/17 03:10:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
21/05/17 03:10:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 50e6ccadb8f1:45201 (size: 28.2 KiB, free: 934.3 MiB)
21/05/17 03:10:44 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0
21/05/17 03:10:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9610970 bytes, open cost is considered as scanning 4194304 bytes.
21/05/17 03:10:44 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
21/05/17 03:10:44 INFO DAGScheduler: Registering RDD 13 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
21/05/17 03:10:44 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/17 03:10:44 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/17 03:10:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
21/05/17 03:10:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
21/05/17 03:10:44 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
21/05/17 03:10:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
21/05/17 03:10:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
21/05/17 03:10:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 50e6ccadb8f1:45201 (size: 12.0 KiB, free: 934.3 MiB)
21/05/17 03:10:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
21/05/17 03:10:44 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
21/05/17 03:10:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0
21/05/17 03:10:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (50e6ccadb8f1, executor driver, partition 0, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
21/05/17 03:10:44 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (50e6ccadb8f1, executor driver, partition 1, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
21/05/17 03:10:44 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (50e6ccadb8f1, executor driver, partition 2, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
21/05/17 03:10:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/05/17 03:10:45 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
21/05/17 03:10:45 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
21/05/17 03:10:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 50e6ccadb8f1:45201 in memory (size: 28.2 KiB, free: 934.4 MiB)
21/05/17 03:10:45 INFO CodeGenerator: Code generated in 86.6446 ms
21/05/17 03:10:45 INFO CodeGenerator: Code generated in 20.3936 ms
21/05/17 03:10:45 INFO CodeGenerator: Code generated in 33.2471 ms
21/05/17 03:10:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_00_10_00/from_1621215193_to_1621215600.csv, range: 0-23658, partition values: [empty row]
21/05/17 03:10:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104224, partition values: [empty row]
21/05/17 03:10:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_00_10_00/from_1621217400_to_1621219200.csv, range: 0-104040, partition values: [empty row]
21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104004, partition values: [empty row]
21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621215193_to_1621215600.csv, range: 0-23634, partition values: [empty row]
21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-17_00_10_00/from_1621215600_to_1621217400.csv, range: 0-104211, partition values: [empty row]
21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_00_10_00/from_1621217400_to_1621219200.csv, range: 0-103631, partition values: [empty row]
21/05/17 03:10:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-17_00_10_00/from_1621215193_to_1621215600.csv, range: 0-23627, partition values: [empty row]
21/05/17 03:10:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-17_00_10_00/from_1621217400_to_1621219200.csv, range: 0-104117, partition values: [empty row]
21/05/17 03:10:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2722 bytes result sent to driver
21/05/17 03:10:47 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2722 bytes result sent to driver
21/05/17 03:10:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2457 ms on 50e6ccadb8f1 (executor driver) (1/3)
21/05/17 03:10:47 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2463 ms on 50e6ccadb8f1 (executor driver) (2/3)
21/05/17 03:10:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2679 bytes result sent to driver
21/05/17 03:10:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2747 ms on 50e6ccadb8f1 (executor driver) (3/3)
21/05/17 03:10:47 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.798 s
21/05/17 03:10:47 INFO DAGScheduler: looking for newly runnable stages
21/05/17 03:10:47 INFO DAGScheduler: running: Set()
21/05/17 03:10:47 INFO DAGScheduler: waiting: Set(ResultStage 2)
21/05/17 03:10:47 INFO DAGScheduler: failed: Set()
21/05/17 03:10:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
21/05/17 03:10:47 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
21/05/17 03:10:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
21/05/17 03:10:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
21/05/17 03:10:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 50e6ccadb8f1:45201 (size: 73.8 KiB, free: 934.3 MiB)
21/05/17 03:10:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
21/05/17 03:10:47 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/17 03:10:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
21/05/17 03:10:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (50e6ccadb8f1, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:47 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (50e6ccadb8f1, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:47 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 6) (50e6ccadb8f1, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:47 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 7) (50e6ccadb8f1, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)
21/05/17 03:10:47 INFO Executor: Running task 2.0 in stage 2.0 (TID 6)
21/05/17 03:10:47 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)
21/05/17 03:10:47 INFO Executor: Running task 3.0 in stage 2.0 (TID 7)
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
21/05/17 03:10:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446699488705347514612_0002_m_000002_6, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446699488705347514612_0002_m_000002_6}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446699488705347514612_0002}; taskId=attempt_202105170310446699488705347514612_0002_m_000002_6, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b076477}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443473590844231302139_0002_m_000003_7, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443473590844231302139_0002_m_000003_7}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443473590844231302139_0002}; taskId=attempt_202105170310443473590844231302139_0002_m_000003_7, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51c9c43c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446699488705347514612_0002_m_000002_6
21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443473590844231302139_0002_m_000003_7
21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044683771020940094180_0002_m_000001_5, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044683771020940094180_0002_m_000001_5}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044683771020940094180_0002}; taskId=attempt_20210517031044683771020940094180_0002_m_000001_5, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a0c8011}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044683771020940094180_0002_m_000001_5
21/05/17 03:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446510374913935670487_0002_m_000000_4, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446510374913935670487_0002_m_000000_4}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446510374913935670487_0002}; taskId=attempt_202105170310446510374913935670487_0002_m_000000_4, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7832dbf2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:48 INFO StagingCommitter: Starting: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446510374913935670487_0002_m_000000_4
21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446510374913935670487_0002_m_000000_4 : duration 0:00.030s
21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044683771020940094180_0002_m_000001_5 : duration 0:00.041s
21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446699488705347514612_0002_m_000002_6 : duration 0:00.061s
21/05/17 03:10:48 INFO StagingCommitter: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443473590844231302139_0002_m_000003_7 : duration 0:00.068s
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: needsTaskCommit() Task attempt_202105170310443473590844231302139_0002_m_000003_7
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310443473590844231302139_0002_m_000003_7: needsTaskCommit() Task attempt_202105170310443473590844231302139_0002_m_000003_7: duration 0:00.001s
21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443473590844231302139_0002_m_000003_7
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: needsTaskCommit() Task attempt_202105170310446699488705347514612_0002_m_000002_6
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310446699488705347514612_0002_m_000002_6: needsTaskCommit() Task attempt_202105170310446699488705347514612_0002_m_000002_6: duration 0:00.001s
21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446699488705347514612_0002_m_000002_6
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: needsTaskCommit() Task attempt_202105170310446510374913935670487_0002_m_000000_4
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310446510374913935670487_0002_m_000000_4: needsTaskCommit() Task attempt_202105170310446510374913935670487_0002_m_000000_4: duration 0:00.001s
21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446510374913935670487_0002_m_000000_4
21/05/17 03:10:50 INFO Executor: Finished task 2.0 in stage 2.0 (TID 6). 4630 bytes result sent to driver
21/05/17 03:10:50 INFO Executor: Finished task 3.0 in stage 2.0 (TID 7). 4630 bytes result sent to driver
21/05/17 03:10:50 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 8) (50e6ccadb8f1, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:50 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 9) (50e6ccadb8f1, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:50 INFO Executor: Running task 4.0 in stage 2.0 (TID 8)
21/05/17 03:10:50 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 6) in 2253 ms on 50e6ccadb8f1 (executor driver) (1/200)
21/05/17 03:10:50 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 7) in 2253 ms on 50e6ccadb8f1 (executor driver) (2/200)
21/05/17 03:10:50 INFO Executor: Running task 5.0 in stage 2.0 (TID 9)
21/05/17 03:10:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 4587 bytes result sent to driver
21/05/17 03:10:50 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 10) (50e6ccadb8f1, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2271 ms on 50e6ccadb8f1 (executor driver) (3/200)
21/05/17 03:10:50 INFO Executor: Running task 6.0 in stage 2.0 (TID 10)
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044846538821421497868_0002_m_000005_9, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044846538821421497868_0002_m_000005_9}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044846538821421497868_0002}; taskId=attempt_20210517031044846538821421497868_0002_m_000005_9, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19344e40}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044846538821421497868_0002_m_000005_9
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (1983.0 B) non-empty blocks including 3 (1983.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.0 KiB) non-empty blocks including 3 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446963036895015102514_0002_m_000006_10, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446963036895015102514_0002_m_000006_10}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446963036895015102514_0002}; taskId=attempt_202105170310446963036895015102514_0002_m_000006_10, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13b7740e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446963036895015102514_0002_m_000006_10
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044846538821421497868_0002_m_000005_9 : duration 0:00.026s
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446963036895015102514_0002_m_000006_10 : duration 0:00.021s
21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444522578803307905687_0002_m_000004_8, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444522578803307905687_0002_m_000004_8}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444522578803307905687_0002}; taskId=attempt_202105170310444522578803307905687_0002_m_000004_8, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b39a080}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444522578803307905687_0002_m_000004_8
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444522578803307905687_0002_m_000004_8 : duration 0:00.018s
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: needsTaskCommit() Task attempt_20210517031044683771020940094180_0002_m_000001_5
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_20210517031044683771020940094180_0002_m_000001_5: needsTaskCommit() Task attempt_20210517031044683771020940094180_0002_m_000001_5: duration 0:00.001s
21/05/17 03:10:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044683771020940094180_0002_m_000001_5
21/05/17 03:10:50 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 4587 bytes result sent to driver
21/05/17 03:10:50 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 11) (50e6ccadb8f1, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:50 INFO Executor: Running task 7.0 in stage 2.0 (TID 11)
21/05/17 03:10:50 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2486 ms on 50e6ccadb8f1 (executor driver) (4/200)
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
21/05/17 03:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310449213256912346936367_0002_m_000007_11, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310449213256912346936367_0002_m_000007_11}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310449213256912346936367_0002}; taskId=attempt_202105170310449213256912346936367_0002_m_000007_11, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@741f395a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310449213256912346936367_0002_m_000007_11
21/05/17 03:10:50 INFO StagingCommitter: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310449213256912346936367_0002_m_000007_11 : duration 0:00.011s
21/05/17 03:10:54 INFO StagingCommitter: Starting: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: needsTaskCommit() Task attempt_202105170310449213256912346936367_0002_m_000007_11
21/05/17 03:10:54 INFO StagingCommitter: Task committer attempt_202105170310449213256912346936367_0002_m_000007_11: needsTaskCommit() Task attempt_202105170310449213256912346936367_0002_m_000007_11: duration 0:00.001s
21/05/17 03:10:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310449213256912346936367_0002_m_000007_11
21/05/17 03:10:54 INFO Executor: Finished task 7.0 in stage 2.0 (TID 11). 4544 bytes result sent to driver
21/05/17 03:10:54 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 12) (50e6ccadb8f1, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:54 INFO Executor: Running task 8.0 in stage 2.0 (TID 12)
21/05/17 03:10:54 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 11) in 4094 ms on 50e6ccadb8f1 (executor driver) (5/200)
21/05/17 03:10:54 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044800075857028101322_0002_m_000008_12, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044800075857028101322_0002_m_000008_12}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044800075857028101322_0002}; taskId=attempt_20210517031044800075857028101322_0002_m_000008_12, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e17f256}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:54 INFO StagingCommitter: Starting: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044800075857028101322_0002_m_000008_12
21/05/17 03:10:54 INFO StagingCommitter: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044800075857028101322_0002_m_000008_12 : duration 0:00.004s
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: needsTaskCommit() Task attempt_20210517031044846538821421497868_0002_m_000005_9
21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_20210517031044846538821421497868_0002_m_000005_9: needsTaskCommit() Task attempt_20210517031044846538821421497868_0002_m_000005_9: duration 0:00.004s
21/05/17 03:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044846538821421497868_0002_m_000005_9
21/05/17 03:10:56 INFO Executor: Finished task 5.0 in stage 2.0 (TID 9). 4544 bytes result sent to driver
21/05/17 03:10:56 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 13) (50e6ccadb8f1, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:56 INFO Executor: Running task 9.0 in stage 2.0 (TID 13)
21/05/17 03:10:56 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 9) in 6081 ms on 50e6ccadb8f1 (executor driver) (6/200)
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446257759387510137200_0002_m_000009_13, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446257759387510137200_0002_m_000009_13}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446257759387510137200_0002}; taskId=attempt_202105170310446257759387510137200_0002_m_000009_13, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@764c3386}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446257759387510137200_0002_m_000009_13
21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446257759387510137200_0002_m_000009_13 : duration 0:00.004s
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: needsTaskCommit() Task attempt_202105170310446963036895015102514_0002_m_000006_10
21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310446963036895015102514_0002_m_000006_10: needsTaskCommit() Task attempt_202105170310446963036895015102514_0002_m_000006_10: duration 0:00.002s
21/05/17 03:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446963036895015102514_0002_m_000006_10
21/05/17 03:10:56 INFO Executor: Finished task 6.0 in stage 2.0 (TID 10). 4587 bytes result sent to driver
21/05/17 03:10:56 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 14) (50e6ccadb8f1, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:56 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 10) in 6580 ms on 50e6ccadb8f1 (executor driver) (7/200)
21/05/17 03:10:56 INFO Executor: Running task 10.0 in stage 2.0 (TID 14)
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445821211218635200570_0002_m_000010_14, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445821211218635200570_0002_m_000010_14}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445821211218635200570_0002}; taskId=attempt_202105170310445821211218635200570_0002_m_000010_14, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d736806}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445821211218635200570_0002_m_000010_14
21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445821211218635200570_0002_m_000010_14 : duration 0:00.004s
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: needsTaskCommit() Task attempt_202105170310444522578803307905687_0002_m_000004_8
21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_202105170310444522578803307905687_0002_m_000004_8: needsTaskCommit() Task attempt_202105170310444522578803307905687_0002_m_000004_8: duration 0:00.001s
21/05/17 03:10:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444522578803307905687_0002_m_000004_8
21/05/17 03:10:56 INFO Executor: Finished task 4.0 in stage 2.0 (TID 8). 4587 bytes result sent to driver
21/05/17 03:10:56 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 15) (50e6ccadb8f1, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:56 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 8) in 6662 ms on 50e6ccadb8f1 (executor driver) (8/200)
21/05/17 03:10:56 INFO Executor: Running task 11.0 in stage 2.0 (TID 15)
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044650065206039659478_0002_m_000011_15, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044650065206039659478_0002_m_000011_15}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044650065206039659478_0002}; taskId=attempt_20210517031044650065206039659478_0002_m_000011_15, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24b9dd54}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:56 INFO StagingCommitter: Starting: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044650065206039659478_0002_m_000011_15
21/05/17 03:10:56 INFO StagingCommitter: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044650065206039659478_0002_m_000011_15 : duration 0:00.006s
21/05/17 03:10:57 INFO StagingCommitter: Starting: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: needsTaskCommit() Task attempt_20210517031044800075857028101322_0002_m_000008_12
21/05/17 03:10:57 INFO StagingCommitter: Task committer attempt_20210517031044800075857028101322_0002_m_000008_12: needsTaskCommit() Task attempt_20210517031044800075857028101322_0002_m_000008_12: duration 0:00.006s
21/05/17 03:10:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044800075857028101322_0002_m_000008_12
21/05/17 03:10:57 INFO Executor: Finished task 8.0 in stage 2.0 (TID 12). 4587 bytes result sent to driver
21/05/17 03:10:57 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 16) (50e6ccadb8f1, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:57 INFO Executor: Running task 12.0 in stage 2.0 (TID 16)
21/05/17 03:10:57 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 12) in 2634 ms on 50e6ccadb8f1 (executor driver) (9/200)
21/05/17 03:10:57 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442577202771616352006_0002_m_000012_16, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442577202771616352006_0002_m_000012_16}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442577202771616352006_0002}; taskId=attempt_202105170310442577202771616352006_0002_m_000012_16, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68c3ec05}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:57 INFO StagingCommitter: Starting: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442577202771616352006_0002_m_000012_16
21/05/17 03:10:57 INFO StagingCommitter: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442577202771616352006_0002_m_000012_16 : duration 0:00.005s
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: needsTaskCommit() Task attempt_202105170310445821211218635200570_0002_m_000010_14
21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310445821211218635200570_0002_m_000010_14: needsTaskCommit() Task attempt_202105170310445821211218635200570_0002_m_000010_14: duration 0:00.003s
21/05/17 03:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445821211218635200570_0002_m_000010_14
21/05/17 03:10:58 INFO Executor: Finished task 10.0 in stage 2.0 (TID 14). 4587 bytes result sent to driver
21/05/17 03:10:58 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 17) (50e6ccadb8f1, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:58 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 14) in 1935 ms on 50e6ccadb8f1 (executor driver) (10/200)
21/05/17 03:10:58 INFO Executor: Running task 13.0 in stage 2.0 (TID 17)
21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441697200031450029390_0002_m_000013_17, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441697200031450029390_0002_m_000013_17}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441697200031450029390_0002}; taskId=attempt_202105170310441697200031450029390_0002_m_000013_17, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3435a569}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441697200031450029390_0002_m_000013_17
21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441697200031450029390_0002_m_000013_17 : duration 0:00.004s
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: needsTaskCommit() Task attempt_202105170310442577202771616352006_0002_m_000012_16
21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310442577202771616352006_0002_m_000012_16: needsTaskCommit() Task attempt_202105170310442577202771616352006_0002_m_000012_16: duration 0:00.002s
21/05/17 03:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442577202771616352006_0002_m_000012_16
21/05/17 03:10:58 INFO Executor: Finished task 12.0 in stage 2.0 (TID 16). 4544 bytes result sent to driver
21/05/17 03:10:58 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 18) (50e6ccadb8f1, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:58 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 16) in 1746 ms on 50e6ccadb8f1 (executor driver) (11/200)
21/05/17 03:10:58 INFO Executor: Running task 14.0 in stage 2.0 (TID 18)
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: needsTaskCommit() Task attempt_202105170310446257759387510137200_0002_m_000009_13
21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310446257759387510137200_0002_m_000009_13: needsTaskCommit() Task attempt_202105170310446257759387510137200_0002_m_000009_13: duration 0:00.000s
21/05/17 03:10:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446257759387510137200_0002_m_000009_13
21/05/17 03:10:58 INFO Executor: Finished task 9.0 in stage 2.0 (TID 13). 4587 bytes result sent to driver
21/05/17 03:10:58 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 19) (50e6ccadb8f1, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:58 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 13) in 2634 ms on 50e6ccadb8f1 (executor driver) (12/200)
21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/05/17 03:10:58 INFO Executor: Running task 15.0 in stage 2.0 (TID 19)
21/05/17 03:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448378238263607005253_0002_m_000014_18, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448378238263607005253_0002_m_000014_18}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448378238263607005253_0002}; taskId=attempt_202105170310448378238263607005253_0002_m_000014_18, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a454850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448378238263607005253_0002_m_000014_18
21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448378238263607005253_0002_m_000014_18 : duration 0:00.004s
21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
21/05/17 03:10:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044898926361917508960_0002_m_000015_19, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044898926361917508960_0002_m_000015_19}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044898926361917508960_0002}; taskId=attempt_20210517031044898926361917508960_0002_m_000015_19, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ca2db31}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:58 INFO StagingCommitter: Starting: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044898926361917508960_0002_m_000015_19
21/05/17 03:10:58 INFO StagingCommitter: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044898926361917508960_0002_m_000015_19 : duration 0:00.005s
21/05/17 03:10:59 INFO StagingCommitter: Starting: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: needsTaskCommit() Task attempt_20210517031044650065206039659478_0002_m_000011_15
21/05/17 03:10:59 INFO StagingCommitter: Task committer attempt_20210517031044650065206039659478_0002_m_000011_15: needsTaskCommit() Task attempt_20210517031044650065206039659478_0002_m_000011_15: duration 0:00.002s
21/05/17 03:10:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044650065206039659478_0002_m_000011_15
21/05/17 03:10:59 INFO Executor: Finished task 11.0 in stage 2.0 (TID 15). 4544 bytes result sent to driver
21/05/17 03:10:59 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 20) (50e6ccadb8f1, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:10:59 INFO Executor: Running task 16.0 in stage 2.0 (TID 20)
21/05/17 03:10:59 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 15) in 2584 ms on 50e6ccadb8f1 (executor driver) (13/200)
21/05/17 03:10:59 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:10:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:10:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:10:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:10:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:10:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448415062236824146034_0002_m_000016_20, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448415062236824146034_0002_m_000016_20}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448415062236824146034_0002}; taskId=attempt_202105170310448415062236824146034_0002_m_000016_20, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@125a1959}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:10:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:10:59 INFO StagingCommitter: Starting: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448415062236824146034_0002_m_000016_20
21/05/17 03:10:59 INFO StagingCommitter: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448415062236824146034_0002_m_000016_20 : duration 0:00.004s
21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: needsTaskCommit() Task attempt_202105170310448378238263607005253_0002_m_000014_18
21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_202105170310448378238263607005253_0002_m_000014_18: needsTaskCommit() Task attempt_202105170310448378238263607005253_0002_m_000014_18: duration 0:00.001s
21/05/17 03:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448378238263607005253_0002_m_000014_18
21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: needsTaskCommit() Task attempt_20210517031044898926361917508960_0002_m_000015_19
21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_20210517031044898926361917508960_0002_m_000015_19: needsTaskCommit() Task attempt_20210517031044898926361917508960_0002_m_000015_19: duration 0:00.000s
21/05/17 03:11:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044898926361917508960_0002_m_000015_19
21/05/17 03:11:00 INFO Executor: Finished task 14.0 in stage 2.0 (TID 18). 4544 bytes result sent to driver
21/05/17 03:11:00 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 21) (50e6ccadb8f1, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:00 INFO Executor: Running task 17.0 in stage 2.0 (TID 21)
21/05/17 03:11:00 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 18) in 2142 ms on 50e6ccadb8f1 (executor driver) (14/200)
21/05/17 03:11:00 INFO Executor: Finished task 15.0 in stage 2.0 (TID 19). 4544 bytes result sent to driver
21/05/17 03:11:00 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 22) (50e6ccadb8f1, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:00 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 19) in 2130 ms on 50e6ccadb8f1 (executor driver) (15/200)
21/05/17 03:11:00 INFO Executor: Running task 18.0 in stage 2.0 (TID 22)
21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Getting 3 (1961.0 B) non-empty blocks including 3 (1961.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446833103702532721328_0002_m_000017_21, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446833103702532721328_0002_m_000017_21}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446833103702532721328_0002}; taskId=attempt_202105170310446833103702532721328_0002_m_000017_21, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@580dbff2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446833103702532721328_0002_m_000017_21
21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446833103702532721328_0002_m_000017_21 : duration 0:00.005s
21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
21/05/17 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442450223603098137813_0002_m_000018_22, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442450223603098137813_0002_m_000018_22}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442450223603098137813_0002}; taskId=attempt_202105170310442450223603098137813_0002_m_000018_22, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a1057a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:00 INFO StagingCommitter: Starting: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442450223603098137813_0002_m_000018_22
21/05/17 03:11:00 INFO StagingCommitter: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442450223603098137813_0002_m_000018_22 : duration 0:00.007s
21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: needsTaskCommit() Task attempt_202105170310448415062236824146034_0002_m_000016_20
21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310448415062236824146034_0002_m_000016_20: needsTaskCommit() Task attempt_202105170310448415062236824146034_0002_m_000016_20: duration 0:00.003s
21/05/17 03:11:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448415062236824146034_0002_m_000016_20
21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: needsTaskCommit() Task attempt_202105170310441697200031450029390_0002_m_000013_17
21/05/17 03:11:01 INFO Executor: Finished task 16.0 in stage 2.0 (TID 20). 4544 bytes result sent to driver
21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310441697200031450029390_0002_m_000013_17: needsTaskCommit() Task attempt_202105170310441697200031450029390_0002_m_000013_17: duration 0:00.000s
21/05/17 03:11:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441697200031450029390_0002_m_000013_17
21/05/17 03:11:01 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 23) (50e6ccadb8f1, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:01 INFO Executor: Finished task 13.0 in stage 2.0 (TID 17). 4544 bytes result sent to driver
21/05/17 03:11:01 INFO Executor: Running task 19.0 in stage 2.0 (TID 23)
21/05/17 03:11:01 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 20) in 2105 ms on 50e6ccadb8f1 (executor driver) (16/200)
21/05/17 03:11:01 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 24) (50e6ccadb8f1, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:01 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 17) in 2822 ms on 50e6ccadb8f1 (executor driver) (17/200)
21/05/17 03:11:01 INFO Executor: Running task 20.0 in stage 2.0 (TID 24)
21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:11:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448441300862858158598_0002_m_000020_24, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448441300862858158598_0002_m_000020_24}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448441300862858158598_0002}; taskId=attempt_202105170310448441300862858158598_0002_m_000020_24, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70753b5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444905782509601915382_0002_m_000019_23, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444905782509601915382_0002_m_000019_23}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444905782509601915382_0002}; taskId=attempt_202105170310444905782509601915382_0002_m_000019_23, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71969e0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448441300862858158598_0002_m_000020_24
21/05/17 03:11:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:01 INFO StagingCommitter: Starting: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444905782509601915382_0002_m_000019_23
21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448441300862858158598_0002_m_000020_24 : duration 0:00.004s
21/05/17 03:11:01 INFO StagingCommitter: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444905782509601915382_0002_m_000019_23 : duration 0:00.010s
21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: needsTaskCommit() Task attempt_202105170310446833103702532721328_0002_m_000017_21
21/05/17 03:11:03 INFO StagingCommitter: Task committer attempt_202105170310446833103702532721328_0002_m_000017_21: needsTaskCommit() Task attempt_202105170310446833103702532721328_0002_m_000017_21: duration 0:00.003s
21/05/17 03:11:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446833103702532721328_0002_m_000017_21
21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: needsTaskCommit() Task attempt_202105170310442450223603098137813_0002_m_000018_22
21/05/17 03:11:03 INFO Executor: Finished task 17.0 in stage 2.0 (TID 21). 4630 bytes result sent to driver
21/05/17 03:11:03 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 25) (50e6ccadb8f1, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:03 INFO StagingCommitter: Task committer attempt_202105170310442450223603098137813_0002_m_000018_22: needsTaskCommit() Task attempt_202105170310442450223603098137813_0002_m_000018_22: duration 0:00.044s
21/05/17 03:11:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442450223603098137813_0002_m_000018_22
21/05/17 03:11:03 INFO Executor: Running task 21.0 in stage 2.0 (TID 25)
21/05/17 03:11:03 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 21) in 2467 ms on 50e6ccadb8f1 (executor driver) (18/200)
21/05/17 03:11:03 INFO Executor: Finished task 18.0 in stage 2.0 (TID 22). 4630 bytes result sent to driver
21/05/17 03:11:03 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 26) (50e6ccadb8f1, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:03 INFO Executor: Running task 22.0 in stage 2.0 (TID 26)
21/05/17 03:11:03 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 22) in 2550 ms on 50e6ccadb8f1 (executor driver) (19/200)
21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445003124214055647070_0002_m_000021_25, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445003124214055647070_0002_m_000021_25}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445003124214055647070_0002}; taskId=attempt_202105170310445003124214055647070_0002_m_000021_25, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5effd0f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 38 ms
21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445003124214055647070_0002_m_000021_25
21/05/17 03:11:03 INFO StagingCommitter: Starting: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: needsTaskCommit() Task attempt_202105170310448441300862858158598_0002_m_000020_24
21/05/17 03:11:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443285973931464533167_0002_m_000022_26, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443285973931464533167_0002_m_000022_26}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443285973931464533167_0002}; taskId=attempt_202105170310443285973931464533167_0002_m_000022_26, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30e3c31}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310448441300862858158598_0002_m_000020_24: needsTaskCommit() Task attempt_202105170310448441300862858158598_0002_m_000020_24: duration 0:00.035s
21/05/17 03:11:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445003124214055647070_0002_m_000021_25 : duration 0:00.201s
21/05/17 03:11:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448441300862858158598_0002_m_000020_24
21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443285973931464533167_0002_m_000022_26
21/05/17 03:11:04 INFO Executor: Finished task 20.0 in stage 2.0 (TID 24). 4630 bytes result sent to driver
21/05/17 03:11:04 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 27) (50e6ccadb8f1, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: needsTaskCommit() Task attempt_202105170310444905782509601915382_0002_m_000019_23
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310444905782509601915382_0002_m_000019_23: needsTaskCommit() Task attempt_202105170310444905782509601915382_0002_m_000019_23: duration 0:00.003s
21/05/17 03:11:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444905782509601915382_0002_m_000019_23
21/05/17 03:11:04 INFO Executor: Finished task 19.0 in stage 2.0 (TID 23). 4630 bytes result sent to driver
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443285973931464533167_0002_m_000022_26 : duration 0:00.128s
21/05/17 03:11:04 INFO Executor: Running task 23.0 in stage 2.0 (TID 27)
21/05/17 03:11:04 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 28) (50e6ccadb8f1, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:04 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 24) in 2784 ms on 50e6ccadb8f1 (executor driver) (20/200)
21/05/17 03:11:04 INFO Executor: Running task 24.0 in stage 2.0 (TID 28)
21/05/17 03:11:04 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 23) in 2810 ms on 50e6ccadb8f1 (executor driver) (21/200)
21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
21/05/17 03:11:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444135220769137305057_0002_m_000023_27, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444135220769137305057_0002_m_000023_27}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444135220769137305057_0002}; taskId=attempt_202105170310444135220769137305057_0002_m_000023_27, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c089bd2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444135220769137305057_0002_m_000023_27
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444135220769137305057_0002_m_000023_27 : duration 0:00.084s
21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
21/05/17 03:11:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442943056473930753721_0002_m_000024_28, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442943056473930753721_0002_m_000024_28}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442943056473930753721_0002}; taskId=attempt_202105170310442943056473930753721_0002_m_000024_28, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32592203}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:04 INFO StagingCommitter: Starting: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442943056473930753721_0002_m_000024_28
21/05/17 03:11:04 INFO StagingCommitter: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442943056473930753721_0002_m_000024_28 : duration 0:00.065s
21/05/17 03:11:06 INFO StagingCommitter: Starting: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: needsTaskCommit() Task attempt_202105170310443285973931464533167_0002_m_000022_26
21/05/17 03:11:06 INFO StagingCommitter: Task committer attempt_202105170310443285973931464533167_0002_m_000022_26: needsTaskCommit() Task attempt_202105170310443285973931464533167_0002_m_000022_26: duration 0:00.010s
21/05/17 03:11:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443285973931464533167_0002_m_000022_26
21/05/17 03:11:06 INFO Executor: Finished task 22.0 in stage 2.0 (TID 26). 4544 bytes result sent to driver
21/05/17 03:11:07 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 29) (50e6ccadb8f1, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:07 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 26) in 3581 ms on 50e6ccadb8f1 (executor driver) (22/200)
21/05/17 03:11:07 INFO Executor: Running task 25.0 in stage 2.0 (TID 29)
21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: needsTaskCommit() Task attempt_202105170310445003124214055647070_0002_m_000021_25
21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310445003124214055647070_0002_m_000021_25: needsTaskCommit() Task attempt_202105170310445003124214055647070_0002_m_000021_25: duration 0:00.015s
21/05/17 03:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445003124214055647070_0002_m_000021_25
21/05/17 03:11:07 INFO Executor: Finished task 21.0 in stage 2.0 (TID 25). 4587 bytes result sent to driver
21/05/17 03:11:07 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 30) (50e6ccadb8f1, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:07 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 25) in 3820 ms on 50e6ccadb8f1 (executor driver) (23/200)
21/05/17 03:11:07 INFO Executor: Running task 26.0 in stage 2.0 (TID 30)
21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Getting 3 (2.9 KiB) non-empty blocks including 3 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
21/05/17 03:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447809294174389448069_0002_m_000025_29, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447809294174389448069_0002_m_000025_29}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447809294174389448069_0002}; taskId=attempt_202105170310447809294174389448069_0002_m_000025_29, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c30afd5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447809294174389448069_0002_m_000025_29
21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447809294174389448069_0002_m_000025_29 : duration 0:00.065s
21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
21/05/17 03:11:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044526516834364116976_0002_m_000026_30, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044526516834364116976_0002_m_000026_30}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044526516834364116976_0002}; taskId=attempt_20210517031044526516834364116976_0002_m_000026_30, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33e3b08b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044526516834364116976_0002_m_000026_30
21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044526516834364116976_0002_m_000026_30 : duration 0:00.072s
21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: needsTaskCommit() Task attempt_202105170310442943056473930753721_0002_m_000024_28
21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310442943056473930753721_0002_m_000024_28: needsTaskCommit() Task attempt_202105170310442943056473930753721_0002_m_000024_28: duration 0:00.020s
21/05/17 03:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442943056473930753721_0002_m_000024_28
21/05/17 03:11:07 INFO Executor: Finished task 24.0 in stage 2.0 (TID 28). 4544 bytes result sent to driver
21/05/17 03:11:07 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 31) (50e6ccadb8f1, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:07 INFO Executor: Running task 27.0 in stage 2.0 (TID 31)
21/05/17 03:11:07 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 28) in 3653 ms on 50e6ccadb8f1 (executor driver) (24/200)
21/05/17 03:11:07 INFO StagingCommitter: Starting: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: needsTaskCommit() Task attempt_202105170310444135220769137305057_0002_m_000023_27
21/05/17 03:11:07 INFO StagingCommitter: Task committer attempt_202105170310444135220769137305057_0002_m_000023_27: needsTaskCommit() Task attempt_202105170310444135220769137305057_0002_m_000023_27: duration 0:00.006s
21/05/17 03:11:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444135220769137305057_0002_m_000023_27
21/05/17 03:11:07 INFO Executor: Finished task 23.0 in stage 2.0 (TID 27). 4544 bytes result sent to driver
21/05/17 03:11:08 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 32) (50e6ccadb8f1, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:08 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 27) in 3986 ms on 50e6ccadb8f1 (executor driver) (25/200)
21/05/17 03:11:08 INFO Executor: Running task 28.0 in stage 2.0 (TID 32)
21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Getting 3 (2.0 KiB) non-empty blocks including 3 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
21/05/17 03:11:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445095766092571174203_0002_m_000027_31, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445095766092571174203_0002_m_000027_31}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445095766092571174203_0002}; taskId=attempt_202105170310445095766092571174203_0002_m_000027_31, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70af5c26}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:08 INFO StagingCommitter: Starting: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445095766092571174203_0002_m_000027_31
21/05/17 03:11:08 INFO StagingCommitter: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445095766092571174203_0002_m_000027_31 : duration 0:00.048s
21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms
21/05/17 03:11:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444544091199301428286_0002_m_000028_32, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444544091199301428286_0002_m_000028_32}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444544091199301428286_0002}; taskId=attempt_202105170310444544091199301428286_0002_m_000028_32, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5afbc3c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:08 INFO StagingCommitter: Starting: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444544091199301428286_0002_m_000028_32
21/05/17 03:11:08 INFO StagingCommitter: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444544091199301428286_0002_m_000028_32 : duration 0:00.044s
21/05/17 03:11:13 INFO StagingCommitter: Starting: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: needsTaskCommit() Task attempt_20210517031044526516834364116976_0002_m_000026_30
21/05/17 03:11:13 INFO StagingCommitter: Task committer attempt_20210517031044526516834364116976_0002_m_000026_30: needsTaskCommit() Task attempt_20210517031044526516834364116976_0002_m_000026_30: duration 0:00.023s
21/05/17 03:11:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044526516834364116976_0002_m_000026_30
21/05/17 03:11:13 INFO Executor: Finished task 26.0 in stage 2.0 (TID 30). 4587 bytes result sent to driver
21/05/17 03:11:13 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 33) (50e6ccadb8f1, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:13 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 30) in 6428 ms on 50e6ccadb8f1 (executor driver) (26/200)
21/05/17 03:11:13 INFO Executor: Running task 29.0 in stage 2.0 (TID 33)
21/05/17 03:11:13 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044253485623106593207_0002_m_000029_33, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044253485623106593207_0002_m_000029_33}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044253485623106593207_0002}; taskId=attempt_20210517031044253485623106593207_0002_m_000029_33, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54cfb093}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:13 INFO StagingCommitter: Starting: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044253485623106593207_0002_m_000029_33
21/05/17 03:11:13 INFO StagingCommitter: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044253485623106593207_0002_m_000029_33 : duration 0:00.042s
21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: needsTaskCommit() Task attempt_202105170310444544091199301428286_0002_m_000028_32
21/05/17 03:11:14 INFO StagingCommitter: Task committer attempt_202105170310444544091199301428286_0002_m_000028_32: needsTaskCommit() Task attempt_202105170310444544091199301428286_0002_m_000028_32: duration 0:00.017s
21/05/17 03:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444544091199301428286_0002_m_000028_32
21/05/17 03:11:14 INFO Executor: Finished task 28.0 in stage 2.0 (TID 32). 4587 bytes result sent to driver
21/05/17 03:11:14 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 34) (50e6ccadb8f1, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:14 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 32) in 6279 ms on 50e6ccadb8f1 (executor driver) (27/200)
21/05/17 03:11:14 INFO Executor: Running task 30.0 in stage 2.0 (TID 34)
21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
21/05/17 03:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441628843165832546118_0002_m_000030_34, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441628843165832546118_0002_m_000030_34}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441628843165832546118_0002}; taskId=attempt_202105170310441628843165832546118_0002_m_000030_34, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@733f3d6e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441628843165832546118_0002_m_000030_34
21/05/17 03:11:14 INFO StagingCommitter: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441628843165832546118_0002_m_000030_34 : duration 0:00.057s
21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: needsTaskCommit() Task attempt_202105170310447809294174389448069_0002_m_000025_29
21/05/17 03:11:14 INFO StagingCommitter: Task committer attempt_202105170310447809294174389448069_0002_m_000025_29: needsTaskCommit() Task attempt_202105170310447809294174389448069_0002_m_000025_29: duration 0:00.039s
21/05/17 03:11:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447809294174389448069_0002_m_000025_29
21/05/17 03:11:14 INFO Executor: Finished task 25.0 in stage 2.0 (TID 29). 4587 bytes result sent to driver
21/05/17 03:11:14 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 35) (50e6ccadb8f1, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:14 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 29) in 7665 ms on 50e6ccadb8f1 (executor driver) (28/200)
21/05/17 03:11:14 INFO Executor: Running task 31.0 in stage 2.0 (TID 35)
21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
21/05/17 03:11:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444551762255917598986_0002_m_000031_35, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444551762255917598986_0002_m_000031_35}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444551762255917598986_0002}; taskId=attempt_202105170310444551762255917598986_0002_m_000031_35, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7427859c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:14 INFO StagingCommitter: Starting: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444551762255917598986_0002_m_000031_35
21/05/17 03:11:15 INFO StagingCommitter: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444551762255917598986_0002_m_000031_35 : duration 0:00.062s
21/05/17 03:11:15 INFO StagingCommitter: Starting: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: needsTaskCommit() Task attempt_202105170310445095766092571174203_0002_m_000027_31
21/05/17 03:11:15 INFO StagingCommitter: Task committer attempt_202105170310445095766092571174203_0002_m_000027_31: needsTaskCommit() Task attempt_202105170310445095766092571174203_0002_m_000027_31: duration 0:00.009s
21/05/17 03:11:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445095766092571174203_0002_m_000027_31
21/05/17 03:11:15 INFO Executor: Finished task 27.0 in stage 2.0 (TID 31). 4587 bytes result sent to driver
21/05/17 03:11:15 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 36) (50e6ccadb8f1, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:15 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 31) in 7493 ms on 50e6ccadb8f1 (executor driver) (29/200)
21/05/17 03:11:15 INFO Executor: Running task 32.0 in stage 2.0 (TID 36)
21/05/17 03:11:15 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444935604236885678976_0002_m_000032_36, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444935604236885678976_0002_m_000032_36}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444935604236885678976_0002}; taskId=attempt_202105170310444935604236885678976_0002_m_000032_36, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43aadf1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:15 INFO StagingCommitter: Starting: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444935604236885678976_0002_m_000032_36
21/05/17 03:11:15 INFO StagingCommitter: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444935604236885678976_0002_m_000032_36 : duration 0:00.044s
21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: needsTaskCommit() Task attempt_202105170310441628843165832546118_0002_m_000030_34
21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_202105170310441628843165832546118_0002_m_000030_34: needsTaskCommit() Task attempt_202105170310441628843165832546118_0002_m_000030_34: duration 0:00.014s
21/05/17 03:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441628843165832546118_0002_m_000030_34
21/05/17 03:11:17 INFO Executor: Finished task 30.0 in stage 2.0 (TID 34). 4630 bytes result sent to driver
21/05/17 03:11:17 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 37) (50e6ccadb8f1, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:17 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 34) in 3250 ms on 50e6ccadb8f1 (executor driver) (30/200)
21/05/17 03:11:17 INFO Executor: Running task 33.0 in stage 2.0 (TID 37)
21/05/17 03:11:17 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448967553897272536447_0002_m_000033_37, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448967553897272536447_0002_m_000033_37}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448967553897272536447_0002}; taskId=attempt_202105170310448967553897272536447_0002_m_000033_37, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17bfdc44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448967553897272536447_0002_m_000033_37
21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448967553897272536447_0002_m_000033_37 : duration 0:00.062s
21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: needsTaskCommit() Task attempt_20210517031044253485623106593207_0002_m_000029_33
21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_20210517031044253485623106593207_0002_m_000029_33: needsTaskCommit() Task attempt_20210517031044253485623106593207_0002_m_000029_33: duration 0:00.015s
21/05/17 03:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044253485623106593207_0002_m_000029_33
21/05/17 03:11:17 INFO Executor: Finished task 29.0 in stage 2.0 (TID 33). 4630 bytes result sent to driver
21/05/17 03:11:17 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 38) (50e6ccadb8f1, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:17 INFO StagingCommitter: Starting: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: needsTaskCommit() Task attempt_202105170310444551762255917598986_0002_m_000031_35
21/05/17 03:11:17 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 33) in 4396 ms on 50e6ccadb8f1 (executor driver) (31/200)
21/05/17 03:11:17 INFO Executor: Running task 34.0 in stage 2.0 (TID 38)
21/05/17 03:11:17 INFO StagingCommitter: Task committer attempt_202105170310444551762255917598986_0002_m_000031_35: needsTaskCommit() Task attempt_202105170310444551762255917598986_0002_m_000031_35: duration 0:00.015s
21/05/17 03:11:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444551762255917598986_0002_m_000031_35
21/05/17 03:11:17 INFO Executor: Finished task 31.0 in stage 2.0 (TID 35). 4587 bytes result sent to driver
21/05/17 03:11:18 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 39) (50e6ccadb8f1, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:18 INFO Executor: Running task 35.0 in stage 2.0 (TID 39)
21/05/17 03:11:18 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 35) in 3386 ms on 50e6ccadb8f1 (executor driver) (32/200)
21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446846625391595280355_0002_m_000034_38, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446846625391595280355_0002_m_000034_38}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446846625391595280355_0002}; taskId=attempt_202105170310446846625391595280355_0002_m_000034_38, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@227717f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446846625391595280355_0002_m_000034_38
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: needsTaskCommit() Task attempt_202105170310444935604236885678976_0002_m_000032_36
21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310444935604236885678976_0002_m_000032_36: needsTaskCommit() Task attempt_202105170310444935604236885678976_0002_m_000032_36: duration 0:00.016s
21/05/17 03:11:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444935604236885678976_0002_m_000032_36
21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446846625391595280355_0002_m_000034_38 : duration 0:00.073s
21/05/17 03:11:18 INFO Executor: Finished task 32.0 in stage 2.0 (TID 36). 4630 bytes result sent to driver
21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 56 ms
21/05/17 03:11:18 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 40) (50e6ccadb8f1, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:18 INFO Executor: Running task 36.0 in stage 2.0 (TID 40)
21/05/17 03:11:18 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 36) in 3057 ms on 50e6ccadb8f1 (executor driver) (33/200)
21/05/17 03:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444392254441304600869_0002_m_000035_39, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444392254441304600869_0002_m_000035_39}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444392254441304600869_0002}; taskId=attempt_202105170310444392254441304600869_0002_m_000035_39, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58aff1bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444392254441304600869_0002_m_000035_39
21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444392254441304600869_0002_m_000035_39 : duration 0:00.033s
21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
21/05/17 03:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443347006655096946966_0002_m_000036_40, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443347006655096946966_0002_m_000036_40}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443347006655096946966_0002}; taskId=attempt_202105170310443347006655096946966_0002_m_000036_40, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd7e32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:18 INFO StagingCommitter: Starting: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443347006655096946966_0002_m_000036_40
21/05/17 03:11:18 INFO StagingCommitter: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443347006655096946966_0002_m_000036_40 : duration 0:00.040s
21/05/17 03:11:20 INFO StagingCommitter: Starting: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: needsTaskCommit() Task attempt_202105170310444392254441304600869_0002_m_000035_39
21/05/17 03:11:20 INFO StagingCommitter: Task committer attempt_202105170310444392254441304600869_0002_m_000035_39: needsTaskCommit() Task attempt_202105170310444392254441304600869_0002_m_000035_39: duration 0:00.016s
21/05/17 03:11:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444392254441304600869_0002_m_000035_39
21/05/17 03:11:20 INFO Executor: Finished task 35.0 in stage 2.0 (TID 39). 4587 bytes result sent to driver
21/05/17 03:11:20 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 41) (50e6ccadb8f1, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:20 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 39) in 2619 ms on 50e6ccadb8f1 (executor driver) (34/200)
21/05/17 03:11:20 INFO Executor: Running task 37.0 in stage 2.0 (TID 41)
21/05/17 03:11:20 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
21/05/17 03:11:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447060217971280555645_0002_m_000037_41, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447060217971280555645_0002_m_000037_41}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447060217971280555645_0002}; taskId=attempt_202105170310447060217971280555645_0002_m_000037_41, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a006df9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:20 INFO StagingCommitter: Starting: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447060217971280555645_0002_m_000037_41
21/05/17 03:11:20 INFO StagingCommitter: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447060217971280555645_0002_m_000037_41 : duration 0:00.069s
21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: needsTaskCommit() Task attempt_202105170310443347006655096946966_0002_m_000036_40
21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310443347006655096946966_0002_m_000036_40: needsTaskCommit() Task attempt_202105170310443347006655096946966_0002_m_000036_40: duration 0:00.022s
21/05/17 03:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443347006655096946966_0002_m_000036_40
21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: needsTaskCommit() Task attempt_202105170310446846625391595280355_0002_m_000034_38
21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310446846625391595280355_0002_m_000034_38: needsTaskCommit() Task attempt_202105170310446846625391595280355_0002_m_000034_38: duration 0:00.019s
21/05/17 03:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446846625391595280355_0002_m_000034_38
21/05/17 03:11:21 INFO Executor: Finished task 36.0 in stage 2.0 (TID 40). 4587 bytes result sent to driver
21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: needsTaskCommit() Task attempt_202105170310448967553897272536447_0002_m_000033_37
21/05/17 03:11:21 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 42) (50e6ccadb8f1, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:21 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 40) in 3143 ms on 50e6ccadb8f1 (executor driver) (35/200)
21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310448967553897272536447_0002_m_000033_37: needsTaskCommit() Task attempt_202105170310448967553897272536447_0002_m_000033_37: duration 0:00.039s
21/05/17 03:11:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448967553897272536447_0002_m_000033_37
21/05/17 03:11:21 INFO Executor: Running task 38.0 in stage 2.0 (TID 42)
21/05/17 03:11:21 INFO Executor: Finished task 34.0 in stage 2.0 (TID 38). 4587 bytes result sent to driver
21/05/17 03:11:21 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 43) (50e6ccadb8f1, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:21 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 38) in 3625 ms on 50e6ccadb8f1 (executor driver) (36/200)
21/05/17 03:11:21 INFO Executor: Running task 39.0 in stage 2.0 (TID 43)
21/05/17 03:11:21 INFO Executor: Finished task 33.0 in stage 2.0 (TID 37). 4587 bytes result sent to driver
21/05/17 03:11:21 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 44) (50e6ccadb8f1, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:21 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 37) in 4169 ms on 50e6ccadb8f1 (executor driver) (37/200)
21/05/17 03:11:21 INFO Executor: Running task 40.0 in stage 2.0 (TID 44)
21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
21/05/17 03:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445840534874267510790_0002_m_000038_42, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445840534874267510790_0002_m_000038_42}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445840534874267510790_0002}; taskId=attempt_202105170310445840534874267510790_0002_m_000038_42, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65d05dfe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445840534874267510790_0002_m_000038_42
21/05/17 03:11:21 INFO StagingCommitter: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445840534874267510790_0002_m_000038_42 : duration 0:00.056s
21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445434069748180308351_0002_m_000040_44, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445434069748180308351_0002_m_000040_44}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445434069748180308351_0002}; taskId=attempt_202105170310445434069748180308351_0002_m_000040_44, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31d67170}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:21 INFO StagingCommitter: Starting: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445434069748180308351_0002_m_000040_44
21/05/17 03:11:22 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
21/05/17 03:11:22 INFO StagingCommitter: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445434069748180308351_0002_m_000040_44 : duration 0:00.049s
21/05/17 03:11:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441759493548102664980_0002_m_000039_43, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441759493548102664980_0002_m_000039_43}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441759493548102664980_0002}; taskId=attempt_202105170310441759493548102664980_0002_m_000039_43, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fe30d16}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:22 INFO StagingCommitter: Starting: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441759493548102664980_0002_m_000039_43
21/05/17 03:11:22 INFO StagingCommitter: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441759493548102664980_0002_m_000039_43 : duration 0:00.094s
21/05/17 03:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: needsTaskCommit() Task attempt_202105170310445840534874267510790_0002_m_000038_42
21/05/17 03:11:23 INFO StagingCommitter: Task committer attempt_202105170310445840534874267510790_0002_m_000038_42: needsTaskCommit() Task attempt_202105170310445840534874267510790_0002_m_000038_42: duration 0:00.004s
21/05/17 03:11:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445840534874267510790_0002_m_000038_42
21/05/17 03:11:23 INFO Executor: Finished task 38.0 in stage 2.0 (TID 42). 4587 bytes result sent to driver
21/05/17 03:11:23 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 45) (50e6ccadb8f1, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:23 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 42) in 2092 ms on 50e6ccadb8f1 (executor driver) (38/200)
21/05/17 03:11:23 INFO Executor: Running task 41.0 in stage 2.0 (TID 45)
21/05/17 03:11:23 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
21/05/17 03:11:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444073032738344709967_0002_m_000041_45, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444073032738344709967_0002_m_000041_45}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444073032738344709967_0002}; taskId=attempt_202105170310444073032738344709967_0002_m_000041_45, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c381c9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:23 INFO StagingCommitter: Starting: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444073032738344709967_0002_m_000041_45
21/05/17 03:11:23 INFO StagingCommitter: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444073032738344709967_0002_m_000041_45 : duration 0:00.068s
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: needsTaskCommit() Task attempt_202105170310447060217971280555645_0002_m_000037_41
21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310447060217971280555645_0002_m_000037_41: needsTaskCommit() Task attempt_202105170310447060217971280555645_0002_m_000037_41: duration 0:00.004s
21/05/17 03:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447060217971280555645_0002_m_000037_41
21/05/17 03:11:24 INFO Executor: Finished task 37.0 in stage 2.0 (TID 41). 4544 bytes result sent to driver
21/05/17 03:11:24 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 46) (50e6ccadb8f1, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:24 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 41) in 3557 ms on 50e6ccadb8f1 (executor driver) (39/200)
21/05/17 03:11:24 INFO Executor: Running task 42.0 in stage 2.0 (TID 46)
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: needsTaskCommit() Task attempt_202105170310445434069748180308351_0002_m_000040_44
21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310445434069748180308351_0002_m_000040_44: needsTaskCommit() Task attempt_202105170310445434069748180308351_0002_m_000040_44: duration 0:00.010s
21/05/17 03:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445434069748180308351_0002_m_000040_44
21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
21/05/17 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044186632743283096761_0002_m_000042_46, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044186632743283096761_0002_m_000042_46}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044186632743283096761_0002}; taskId=attempt_20210517031044186632743283096761_0002_m_000042_46, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@294aa386}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044186632743283096761_0002_m_000042_46
21/05/17 03:11:24 INFO Executor: Finished task 40.0 in stage 2.0 (TID 44). 4587 bytes result sent to driver
21/05/17 03:11:24 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 47) (50e6ccadb8f1, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044186632743283096761_0002_m_000042_46 : duration 0:00.055s
21/05/17 03:11:24 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 44) in 2872 ms on 50e6ccadb8f1 (executor driver) (40/200)
21/05/17 03:11:24 INFO Executor: Running task 43.0 in stage 2.0 (TID 47)
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: needsTaskCommit() Task attempt_202105170310441759493548102664980_0002_m_000039_43
21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310441759493548102664980_0002_m_000039_43: needsTaskCommit() Task attempt_202105170310441759493548102664980_0002_m_000039_43: duration 0:00.021s
21/05/17 03:11:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441759493548102664980_0002_m_000039_43
21/05/17 03:11:24 INFO Executor: Finished task 39.0 in stage 2.0 (TID 43). 4587 bytes result sent to driver
21/05/17 03:11:24 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 48) (50e6ccadb8f1, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:24 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 43) in 3049 ms on 50e6ccadb8f1 (executor driver) (41/200)
21/05/17 03:11:24 INFO Executor: Running task 44.0 in stage 2.0 (TID 48)
21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441969056317441669391_0002_m_000043_47, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441969056317441669391_0002_m_000043_47}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441969056317441669391_0002}; taskId=attempt_202105170310441969056317441669391_0002_m_000043_47, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f491a1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441969056317441669391_0002_m_000043_47
21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441969056317441669391_0002_m_000043_47 : duration 0:00.046s
21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
21/05/17 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044286766208191874772_0002_m_000044_48, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044286766208191874772_0002_m_000044_48}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044286766208191874772_0002}; taskId=attempt_20210517031044286766208191874772_0002_m_000044_48, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11e0293f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:24 INFO StagingCommitter: Starting: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044286766208191874772_0002_m_000044_48
21/05/17 03:11:24 INFO StagingCommitter: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044286766208191874772_0002_m_000044_48 : duration 0:00.039s
21/05/17 03:11:26 INFO StagingCommitter: Starting: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: needsTaskCommit() Task attempt_202105170310444073032738344709967_0002_m_000041_45
21/05/17 03:11:26 INFO StagingCommitter: Task committer attempt_202105170310444073032738344709967_0002_m_000041_45: needsTaskCommit() Task attempt_202105170310444073032738344709967_0002_m_000041_45: duration 0:00.015s
21/05/17 03:11:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444073032738344709967_0002_m_000041_45
21/05/17 03:11:26 INFO Executor: Finished task 41.0 in stage 2.0 (TID 45). 4587 bytes result sent to driver
21/05/17 03:11:26 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 49) (50e6ccadb8f1, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:26 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 45) in 3359 ms on 50e6ccadb8f1 (executor driver) (42/200)
21/05/17 03:11:26 INFO Executor: Running task 45.0 in stage 2.0 (TID 49)
21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
21/05/17 03:11:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448730539564982436309_0002_m_000045_49, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448730539564982436309_0002_m_000045_49}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448730539564982436309_0002}; taskId=attempt_202105170310448730539564982436309_0002_m_000045_49, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c1ed19d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448730539564982436309_0002_m_000045_49
21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: needsTaskCommit() Task attempt_202105170310441969056317441669391_0002_m_000043_47
21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_202105170310441969056317441669391_0002_m_000043_47: needsTaskCommit() Task attempt_202105170310441969056317441669391_0002_m_000043_47: duration 0:00.014s
21/05/17 03:11:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441969056317441669391_0002_m_000043_47
21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448730539564982436309_0002_m_000045_49 : duration 0:00.121s
21/05/17 03:11:27 INFO Executor: Finished task 43.0 in stage 2.0 (TID 47). 4630 bytes result sent to driver
21/05/17 03:11:27 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 50) (50e6ccadb8f1, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:27 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 47) in 2848 ms on 50e6ccadb8f1 (executor driver) (43/200)
21/05/17 03:11:27 INFO Executor: Running task 46.0 in stage 2.0 (TID 50)
21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: needsTaskCommit() Task attempt_20210517031044186632743283096761_0002_m_000042_46
21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_20210517031044186632743283096761_0002_m_000042_46: needsTaskCommit() Task attempt_20210517031044186632743283096761_0002_m_000042_46: duration 0:00.011s
21/05/17 03:11:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044186632743283096761_0002_m_000042_46
21/05/17 03:11:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441529610456988542853_0002_m_000046_50, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441529610456988542853_0002_m_000046_50}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441529610456988542853_0002}; taskId=attempt_202105170310441529610456988542853_0002_m_000046_50, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1399ab1b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:27 INFO StagingCommitter: Starting: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441529610456988542853_0002_m_000046_50
21/05/17 03:11:27 INFO Executor: Finished task 42.0 in stage 2.0 (TID 46). 4630 bytes result sent to driver
21/05/17 03:11:27 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 51) (50e6ccadb8f1, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:27 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 46) in 3719 ms on 50e6ccadb8f1 (executor driver) (44/200)
21/05/17 03:11:27 INFO StagingCommitter: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441529610456988542853_0002_m_000046_50 : duration 0:00.088s
21/05/17 03:11:27 INFO Executor: Running task 47.0 in stage 2.0 (TID 51)
21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
21/05/17 03:11:28 INFO StagingCommitter: Starting: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: needsTaskCommit() Task attempt_20210517031044286766208191874772_0002_m_000044_48
21/05/17 03:11:28 INFO StagingCommitter: Task committer attempt_20210517031044286766208191874772_0002_m_000044_48: needsTaskCommit() Task attempt_20210517031044286766208191874772_0002_m_000044_48: duration 0:00.009s
21/05/17 03:11:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044286766208191874772_0002_m_000044_48
21/05/17 03:11:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443791610810071741603_0002_m_000047_51, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443791610810071741603_0002_m_000047_51}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443791610810071741603_0002}; taskId=attempt_202105170310443791610810071741603_0002_m_000047_51, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4da9bf24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443791610810071741603_0002_m_000047_51
21/05/17 03:11:28 INFO Executor: Finished task 44.0 in stage 2.0 (TID 48). 4630 bytes result sent to driver
21/05/17 03:11:28 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 52) (50e6ccadb8f1, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:28 INFO Executor: Running task 48.0 in stage 2.0 (TID 52)
21/05/17 03:11:28 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 48) in 3821 ms on 50e6ccadb8f1 (executor driver) (45/200)
21/05/17 03:11:28 INFO StagingCommitter: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443791610810071741603_0002_m_000047_51 : duration 0:00.131s
21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
21/05/17 03:11:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444213259486931209798_0002_m_000048_52, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444213259486931209798_0002_m_000048_52}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444213259486931209798_0002}; taskId=attempt_202105170310444213259486931209798_0002_m_000048_52, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4514cd46}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:28 INFO StagingCommitter: Starting: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444213259486931209798_0002_m_000048_52
21/05/17 03:11:28 INFO StagingCommitter: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444213259486931209798_0002_m_000048_52 : duration 0:00.082s
21/05/17 03:11:31 INFO StagingCommitter: Starting: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: needsTaskCommit() Task attempt_202105170310441529610456988542853_0002_m_000046_50
21/05/17 03:11:31 INFO StagingCommitter: Task committer attempt_202105170310441529610456988542853_0002_m_000046_50: needsTaskCommit() Task attempt_202105170310441529610456988542853_0002_m_000046_50: duration 0:00.021s
21/05/17 03:11:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441529610456988542853_0002_m_000046_50
21/05/17 03:11:31 INFO Executor: Finished task 46.0 in stage 2.0 (TID 50). 4587 bytes result sent to driver
21/05/17 03:11:31 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 53) (50e6ccadb8f1, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:31 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 50) in 4252 ms on 50e6ccadb8f1 (executor driver) (46/200)
21/05/17 03:11:31 INFO Executor: Running task 49.0 in stage 2.0 (TID 53)
21/05/17 03:11:31 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
21/05/17 03:11:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448383472073194543640_0002_m_000049_53, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448383472073194543640_0002_m_000049_53}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448383472073194543640_0002}; taskId=attempt_202105170310448383472073194543640_0002_m_000049_53, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@439bba12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:31 INFO StagingCommitter: Starting: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448383472073194543640_0002_m_000049_53
21/05/17 03:11:31 INFO StagingCommitter: Starting: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: needsTaskCommit() Task attempt_202105170310448730539564982436309_0002_m_000045_49
21/05/17 03:11:31 INFO StagingCommitter: Task committer attempt_202105170310448730539564982436309_0002_m_000045_49: needsTaskCommit() Task attempt_202105170310448730539564982436309_0002_m_000045_49: duration 0:00.019s
21/05/17 03:11:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448730539564982436309_0002_m_000045_49
21/05/17 03:11:32 INFO Executor: Finished task 45.0 in stage 2.0 (TID 49). 4587 bytes result sent to driver
21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448383472073194543640_0002_m_000049_53 : duration 0:00.131s
21/05/17 03:11:32 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 54) (50e6ccadb8f1, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: needsTaskCommit() Task attempt_202105170310443791610810071741603_0002_m_000047_51
21/05/17 03:11:32 INFO Executor: Running task 50.0 in stage 2.0 (TID 54)
21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310443791610810071741603_0002_m_000047_51: needsTaskCommit() Task attempt_202105170310443791610810071741603_0002_m_000047_51: duration 0:00.013s
21/05/17 03:11:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443791610810071741603_0002_m_000047_51
21/05/17 03:11:32 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 49) in 5236 ms on 50e6ccadb8f1 (executor driver) (47/200)
21/05/17 03:11:32 INFO Executor: Finished task 47.0 in stage 2.0 (TID 51). 4587 bytes result sent to driver
21/05/17 03:11:32 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 55) (50e6ccadb8f1, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:32 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 51) in 4430 ms on 50e6ccadb8f1 (executor driver) (48/200)
21/05/17 03:11:32 INFO Executor: Running task 51.0 in stage 2.0 (TID 55)
21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: needsTaskCommit() Task attempt_202105170310444213259486931209798_0002_m_000048_52
21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310444213259486931209798_0002_m_000048_52: needsTaskCommit() Task attempt_202105170310444213259486931209798_0002_m_000048_52: duration 0:00.006s
21/05/17 03:11:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444213259486931209798_0002_m_000048_52
21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
21/05/17 03:11:32 INFO Executor: Finished task 48.0 in stage 2.0 (TID 52). 4587 bytes result sent to driver
21/05/17 03:11:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:32 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 56) (50e6ccadb8f1, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310442705697827878854901_0002_m_000050_54, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442705697827878854901_0002_m_000050_54}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310442705697827878854901_0002}; taskId=attempt_202105170310442705697827878854901_0002_m_000050_54, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@485242d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442705697827878854901_0002_m_000050_54
21/05/17 03:11:32 INFO Executor: Running task 52.0 in stage 2.0 (TID 56)
21/05/17 03:11:32 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 52) in 4287 ms on 50e6ccadb8f1 (executor driver) (49/200)
21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310442705697827878854901_0002_m_000050_54 : duration 0:00.110s
21/05/17 03:11:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444829623036195558816_0002_m_000051_55, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444829623036195558816_0002_m_000051_55}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444829623036195558816_0002}; taskId=attempt_202105170310444829623036195558816_0002_m_000051_55, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c848a24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444829623036195558816_0002_m_000051_55
21/05/17 03:11:32 INFO StagingCommitter: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444829623036195558816_0002_m_000051_55 : duration 0:00.104s
21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
21/05/17 03:11:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445139164563728988308_0002_m_000052_56, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445139164563728988308_0002_m_000052_56}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445139164563728988308_0002}; taskId=attempt_202105170310445139164563728988308_0002_m_000052_56, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@733943e1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:32 INFO StagingCommitter: Starting: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445139164563728988308_0002_m_000052_56
21/05/17 03:11:33 INFO StagingCommitter: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445139164563728988308_0002_m_000052_56 : duration 0:00.034s
21/05/17 03:11:34 INFO StagingCommitter: Starting: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: needsTaskCommit() Task attempt_202105170310442705697827878854901_0002_m_000050_54
21/05/17 03:11:34 INFO StagingCommitter: Task committer attempt_202105170310442705697827878854901_0002_m_000050_54: needsTaskCommit() Task attempt_202105170310442705697827878854901_0002_m_000050_54: duration 0:00.004s
21/05/17 03:11:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310442705697827878854901_0002_m_000050_54
21/05/17 03:11:34 INFO Executor: Finished task 50.0 in stage 2.0 (TID 54). 4587 bytes result sent to driver
21/05/17 03:11:34 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 57) (50e6ccadb8f1, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:34 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 54) in 2327 ms on 50e6ccadb8f1 (executor driver) (50/200)
21/05/17 03:11:34 INFO Executor: Running task 53.0 in stage 2.0 (TID 57)
21/05/17 03:11:34 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
21/05/17 03:11:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448295762992238359232_0002_m_000053_57, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448295762992238359232_0002_m_000053_57}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448295762992238359232_0002}; taskId=attempt_202105170310448295762992238359232_0002_m_000053_57, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ffb361d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:34 INFO StagingCommitter: Starting: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448295762992238359232_0002_m_000053_57
21/05/17 03:11:34 INFO StagingCommitter: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448295762992238359232_0002_m_000053_57 : duration 0:00.087s
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: needsTaskCommit() Task attempt_202105170310444829623036195558816_0002_m_000051_55
21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310444829623036195558816_0002_m_000051_55: needsTaskCommit() Task attempt_202105170310444829623036195558816_0002_m_000051_55: duration 0:00.006s
21/05/17 03:11:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444829623036195558816_0002_m_000051_55
21/05/17 03:11:35 INFO Executor: Finished task 51.0 in stage 2.0 (TID 55). 4544 bytes result sent to driver
21/05/17 03:11:35 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 58) (50e6ccadb8f1, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:35 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 55) in 3019 ms on 50e6ccadb8f1 (executor driver) (51/200)
21/05/17 03:11:35 INFO Executor: Running task 54.0 in stage 2.0 (TID 58)
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: needsTaskCommit() Task attempt_202105170310445139164563728988308_0002_m_000052_56
21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310445139164563728988308_0002_m_000052_56: needsTaskCommit() Task attempt_202105170310445139164563728988308_0002_m_000052_56: duration 0:00.016s
21/05/17 03:11:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445139164563728988308_0002_m_000052_56
21/05/17 03:11:35 INFO Executor: Finished task 52.0 in stage 2.0 (TID 56). 4587 bytes result sent to driver
21/05/17 03:11:35 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 59) (50e6ccadb8f1, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:35 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 56) in 2784 ms on 50e6ccadb8f1 (executor driver) (52/200)
21/05/17 03:11:35 INFO Executor: Running task 55.0 in stage 2.0 (TID 59)
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: needsTaskCommit() Task attempt_202105170310448383472073194543640_0002_m_000049_53
21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310448383472073194543640_0002_m_000049_53: needsTaskCommit() Task attempt_202105170310448383472073194543640_0002_m_000049_53: duration 0:00.012s
21/05/17 03:11:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448383472073194543640_0002_m_000049_53
21/05/17 03:11:35 INFO Executor: Finished task 49.0 in stage 2.0 (TID 53). 4587 bytes result sent to driver
21/05/17 03:11:35 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 60) (50e6ccadb8f1, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:35 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 53) in 3983 ms on 50e6ccadb8f1 (executor driver) (53/200)
21/05/17 03:11:35 INFO Executor: Running task 56.0 in stage 2.0 (TID 60)
21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443516653645533783773_0002_m_000055_59, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443516653645533783773_0002_m_000055_59}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443516653645533783773_0002}; taskId=attempt_202105170310443516653645533783773_0002_m_000055_59, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1424573d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443516653645533783773_0002_m_000055_59
21/05/17 03:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310448368011798834432846_0002_m_000054_58, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448368011798834432846_0002_m_000054_58}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310448368011798834432846_0002}; taskId=attempt_202105170310448368011798834432846_0002_m_000054_58, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a490d36}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448368011798834432846_0002_m_000054_58
21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443516653645533783773_0002_m_000055_59 : duration 0:00.075s
21/05/17 03:11:35 INFO StagingCommitter: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310448368011798834432846_0002_m_000054_58 : duration 0:00.094s
21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Getting 3 (2.3 KiB) non-empty blocks including 3 (2.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
21/05/17 03:11:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441308718270171367657_0002_m_000056_60, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441308718270171367657_0002_m_000056_60}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441308718270171367657_0002}; taskId=attempt_202105170310441308718270171367657_0002_m_000056_60, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@687804cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:35 INFO StagingCommitter: Starting: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441308718270171367657_0002_m_000056_60
21/05/17 03:11:36 INFO StagingCommitter: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441308718270171367657_0002_m_000056_60 : duration 0:00.031s
21/05/17 03:11:37 INFO StagingCommitter: Starting: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: needsTaskCommit() Task attempt_202105170310448295762992238359232_0002_m_000053_57
21/05/17 03:11:37 INFO StagingCommitter: Task committer attempt_202105170310448295762992238359232_0002_m_000053_57: needsTaskCommit() Task attempt_202105170310448295762992238359232_0002_m_000053_57: duration 0:00.004s
21/05/17 03:11:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448295762992238359232_0002_m_000053_57
21/05/17 03:11:37 INFO Executor: Finished task 53.0 in stage 2.0 (TID 57). 4587 bytes result sent to driver
21/05/17 03:11:37 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 61) (50e6ccadb8f1, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:37 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 57) in 3216 ms on 50e6ccadb8f1 (executor driver) (54/200)
21/05/17 03:11:37 INFO Executor: Running task 57.0 in stage 2.0 (TID 61)
21/05/17 03:11:37 INFO ShuffleBlockFetcherIterator: Getting 3 (2.0 KiB) non-empty blocks including 3 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
21/05/17 03:11:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446517098664823693945_0002_m_000057_61, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446517098664823693945_0002_m_000057_61}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446517098664823693945_0002}; taskId=attempt_202105170310446517098664823693945_0002_m_000057_61, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a723c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:37 INFO StagingCommitter: Starting: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446517098664823693945_0002_m_000057_61
21/05/17 03:11:37 INFO StagingCommitter: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446517098664823693945_0002_m_000057_61 : duration 0:00.067s
21/05/17 03:11:38 INFO StagingCommitter: Starting: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: needsTaskCommit() Task attempt_202105170310448368011798834432846_0002_m_000054_58
21/05/17 03:11:38 INFO StagingCommitter: Task committer attempt_202105170310448368011798834432846_0002_m_000054_58: needsTaskCommit() Task attempt_202105170310448368011798834432846_0002_m_000054_58: duration 0:00.022s
21/05/17 03:11:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310448368011798834432846_0002_m_000054_58
21/05/17 03:11:38 INFO Executor: Finished task 54.0 in stage 2.0 (TID 58). 4587 bytes result sent to driver
21/05/17 03:11:38 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 62) (50e6ccadb8f1, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:38 INFO Executor: Running task 58.0 in stage 2.0 (TID 62)
21/05/17 03:11:38 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 58) in 3162 ms on 50e6ccadb8f1 (executor driver) (55/200)
21/05/17 03:11:38 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/05/17 03:11:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447174991347084826617_0002_m_000058_62, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447174991347084826617_0002_m_000058_62}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447174991347084826617_0002}; taskId=attempt_202105170310447174991347084826617_0002_m_000058_62, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c5c1d21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:38 INFO StagingCommitter: Starting: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447174991347084826617_0002_m_000058_62
21/05/17 03:11:38 INFO StagingCommitter: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447174991347084826617_0002_m_000058_62 : duration 0:00.038s
21/05/17 03:11:39 INFO StagingCommitter: Starting: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: needsTaskCommit() Task attempt_202105170310443516653645533783773_0002_m_000055_59
21/05/17 03:11:39 INFO StagingCommitter: Task committer attempt_202105170310443516653645533783773_0002_m_000055_59: needsTaskCommit() Task attempt_202105170310443516653645533783773_0002_m_000055_59: duration 0:00.010s
21/05/17 03:11:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310443516653645533783773_0002_m_000055_59
21/05/17 03:11:39 INFO Executor: Finished task 55.0 in stage 2.0 (TID 59). 4630 bytes result sent to driver
21/05/17 03:11:39 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 63) (50e6ccadb8f1, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:39 INFO Executor: Running task 59.0 in stage 2.0 (TID 63)
21/05/17 03:11:39 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 59) in 4324 ms on 50e6ccadb8f1 (executor driver) (56/200)
21/05/17 03:11:39 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
21/05/17 03:11:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310444999287016091372016_0002_m_000059_63, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444999287016091372016_0002_m_000059_63}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310444999287016091372016_0002}; taskId=attempt_202105170310444999287016091372016_0002_m_000059_63, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18ed7cbd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:39 INFO StagingCommitter: Starting: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444999287016091372016_0002_m_000059_63
21/05/17 03:11:39 INFO StagingCommitter: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310444999287016091372016_0002_m_000059_63 : duration 0:00.048s
21/05/17 03:11:40 INFO StagingCommitter: Starting: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: needsTaskCommit() Task attempt_202105170310441308718270171367657_0002_m_000056_60
21/05/17 03:11:40 INFO StagingCommitter: Task committer attempt_202105170310441308718270171367657_0002_m_000056_60: needsTaskCommit() Task attempt_202105170310441308718270171367657_0002_m_000056_60: duration 0:00.024s
21/05/17 03:11:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441308718270171367657_0002_m_000056_60
21/05/17 03:11:40 INFO Executor: Finished task 56.0 in stage 2.0 (TID 60). 4630 bytes result sent to driver
21/05/17 03:11:40 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 64) (50e6ccadb8f1, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:40 INFO Executor: Running task 60.0 in stage 2.0 (TID 64)
21/05/17 03:11:40 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 60) in 5494 ms on 50e6ccadb8f1 (executor driver) (57/200)
21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
21/05/17 03:11:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445699233081199584384_0002_m_000060_64, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445699233081199584384_0002_m_000060_64}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445699233081199584384_0002}; taskId=attempt_202105170310445699233081199584384_0002_m_000060_64, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3941d38c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:41 INFO StagingCommitter: Starting: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445699233081199584384_0002_m_000060_64
21/05/17 03:11:41 INFO StagingCommitter: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445699233081199584384_0002_m_000060_64 : duration 0:00.046s
21/05/17 03:11:41 INFO StagingCommitter: Starting: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: needsTaskCommit() Task attempt_202105170310447174991347084826617_0002_m_000058_62
21/05/17 03:11:41 INFO StagingCommitter: Task committer attempt_202105170310447174991347084826617_0002_m_000058_62: needsTaskCommit() Task attempt_202105170310447174991347084826617_0002_m_000058_62: duration 0:00.016s
21/05/17 03:11:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447174991347084826617_0002_m_000058_62
21/05/17 03:11:41 INFO Executor: Finished task 58.0 in stage 2.0 (TID 62). 4544 bytes result sent to driver
21/05/17 03:11:41 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 65) (50e6ccadb8f1, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:41 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 62) in 3359 ms on 50e6ccadb8f1 (executor driver) (58/200)
21/05/17 03:11:41 INFO Executor: Running task 61.0 in stage 2.0 (TID 65)
21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Getting 3 (2.6 KiB) non-empty blocks including 3 (2.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/05/17 03:11:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447231433932774290797_0002_m_000061_65, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447231433932774290797_0002_m_000061_65}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447231433932774290797_0002}; taskId=attempt_202105170310447231433932774290797_0002_m_000061_65, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5519a5e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:41 INFO StagingCommitter: Starting: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447231433932774290797_0002_m_000061_65
21/05/17 03:11:41 INFO StagingCommitter: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447231433932774290797_0002_m_000061_65 : duration 0:00.036s
21/05/17 03:11:42 INFO StagingCommitter: Starting: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: needsTaskCommit() Task attempt_202105170310446517098664823693945_0002_m_000057_61
21/05/17 03:11:42 INFO StagingCommitter: Task committer attempt_202105170310446517098664823693945_0002_m_000057_61: needsTaskCommit() Task attempt_202105170310446517098664823693945_0002_m_000057_61: duration 0:00.005s
21/05/17 03:11:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446517098664823693945_0002_m_000057_61
21/05/17 03:11:42 INFO Executor: Finished task 57.0 in stage 2.0 (TID 61). 4630 bytes result sent to driver
21/05/17 03:11:42 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 66) (50e6ccadb8f1, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:42 INFO Executor: Running task 62.0 in stage 2.0 (TID 66)
21/05/17 03:11:42 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 61) in 4855 ms on 50e6ccadb8f1 (executor driver) (59/200)
21/05/17 03:11:42 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/05/17 03:11:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446122932560106960702_0002_m_000062_66, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446122932560106960702_0002_m_000062_66}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446122932560106960702_0002}; taskId=attempt_202105170310446122932560106960702_0002_m_000062_66, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@560dbb1d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:42 INFO StagingCommitter: Starting: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446122932560106960702_0002_m_000062_66
21/05/17 03:11:42 INFO StagingCommitter: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446122932560106960702_0002_m_000062_66 : duration 0:00.250s
21/05/17 03:11:43 INFO StagingCommitter: Starting: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: needsTaskCommit() Task attempt_202105170310444999287016091372016_0002_m_000059_63
21/05/17 03:11:43 INFO StagingCommitter: Task committer attempt_202105170310444999287016091372016_0002_m_000059_63: needsTaskCommit() Task attempt_202105170310444999287016091372016_0002_m_000059_63: duration 0:00.012s
21/05/17 03:11:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310444999287016091372016_0002_m_000059_63
21/05/17 03:11:43 INFO Executor: Finished task 59.0 in stage 2.0 (TID 63). 4544 bytes result sent to driver
21/05/17 03:11:43 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 67) (50e6ccadb8f1, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:43 INFO Executor: Running task 63.0 in stage 2.0 (TID 67)
21/05/17 03:11:43 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 63) in 3838 ms on 50e6ccadb8f1 (executor driver) (60/200)
21/05/17 03:11:43 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
21/05/17 03:11:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517031044579943512634877918_0002_m_000063_67, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044579943512634877918_0002_m_000063_67}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517031044579943512634877918_0002}; taskId=attempt_20210517031044579943512634877918_0002_m_000063_67, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a214336}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:43 INFO StagingCommitter: Starting: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044579943512634877918_0002_m_000063_67
21/05/17 03:11:43 INFO StagingCommitter: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_20210517031044579943512634877918_0002_m_000063_67 : duration 0:00.066s
21/05/17 03:11:43 INFO StagingCommitter: Starting: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: needsTaskCommit() Task attempt_202105170310445699233081199584384_0002_m_000060_64
21/05/17 03:11:43 INFO StagingCommitter: Task committer attempt_202105170310445699233081199584384_0002_m_000060_64: needsTaskCommit() Task attempt_202105170310445699233081199584384_0002_m_000060_64: duration 0:00.003s
21/05/17 03:11:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310445699233081199584384_0002_m_000060_64
21/05/17 03:11:43 INFO Executor: Finished task 60.0 in stage 2.0 (TID 64). 4587 bytes result sent to driver
21/05/17 03:11:43 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 68) (50e6ccadb8f1, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:43 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 64) in 3023 ms on 50e6ccadb8f1 (executor driver) (61/200)
21/05/17 03:11:43 INFO Executor: Running task 64.0 in stage 2.0 (TID 68)
21/05/17 03:11:44 INFO ShuffleBlockFetcherIterator: Getting 3 (2.5 KiB) non-empty blocks including 3 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/05/17 03:11:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441289891283949382873_0002_m_000064_68, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441289891283949382873_0002_m_000064_68}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441289891283949382873_0002}; taskId=attempt_202105170310441289891283949382873_0002_m_000064_68, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f295042}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:44 INFO StagingCommitter: Starting: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441289891283949382873_0002_m_000064_68
21/05/17 03:11:44 INFO StagingCommitter: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441289891283949382873_0002_m_000064_68 : duration 0:00.092s
21/05/17 03:11:45 INFO StagingCommitter: Starting: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: needsTaskCommit() Task attempt_202105170310446122932560106960702_0002_m_000062_66
21/05/17 03:11:45 INFO StagingCommitter: Task committer attempt_202105170310446122932560106960702_0002_m_000062_66: needsTaskCommit() Task attempt_202105170310446122932560106960702_0002_m_000062_66: duration 0:00.010s
21/05/17 03:11:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310446122932560106960702_0002_m_000062_66
21/05/17 03:11:45 INFO Executor: Finished task 62.0 in stage 2.0 (TID 66). 4587 bytes result sent to driver
21/05/17 03:11:45 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 69) (50e6ccadb8f1, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:45 INFO Executor: Running task 65.0 in stage 2.0 (TID 69)
21/05/17 03:11:45 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 66) in 3531 ms on 50e6ccadb8f1 (executor driver) (62/200)
21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: needsTaskCommit() Task attempt_202105170310447231433932774290797_0002_m_000061_65
21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310447231433932774290797_0002_m_000061_65: needsTaskCommit() Task attempt_202105170310447231433932774290797_0002_m_000061_65: duration 0:00.022s
21/05/17 03:11:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310447231433932774290797_0002_m_000061_65
21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: needsTaskCommit() Task attempt_20210517031044579943512634877918_0002_m_000063_67
21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_20210517031044579943512634877918_0002_m_000063_67: needsTaskCommit() Task attempt_20210517031044579943512634877918_0002_m_000063_67: duration 0:00.015s
21/05/17 03:11:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517031044579943512634877918_0002_m_000063_67
21/05/17 03:11:46 INFO Executor: Finished task 63.0 in stage 2.0 (TID 67). 4587 bytes result sent to driver
21/05/17 03:11:46 INFO Executor: Finished task 61.0 in stage 2.0 (TID 65). 4587 bytes result sent to driver
21/05/17 03:11:46 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 70) (50e6ccadb8f1, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:46 INFO Executor: Running task 66.0 in stage 2.0 (TID 70)
21/05/17 03:11:46 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 65) in 4788 ms on 50e6ccadb8f1 (executor driver) (63/200)
21/05/17 03:11:46 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 71) (50e6ccadb8f1, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: needsTaskCommit() Task attempt_202105170310441289891283949382873_0002_m_000064_68
21/05/17 03:11:46 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 67) in 3068 ms on 50e6ccadb8f1 (executor driver) (64/200)
21/05/17 03:11:46 INFO Executor: Running task 67.0 in stage 2.0 (TID 71)
21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310441289891283949382873_0002_m_000064_68: needsTaskCommit() Task attempt_202105170310441289891283949382873_0002_m_000064_68: duration 0:00.019s
21/05/17 03:11:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170310441289891283949382873_0002_m_000064_68
21/05/17 03:11:46 INFO Executor: Finished task 64.0 in stage 2.0 (TID 68). 4587 bytes result sent to driver
21/05/17 03:11:46 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:46 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 68) in 2571 ms on 50e6ccadb8f1 (executor driver) (65/200)
21/05/17 03:11:46 INFO Executor: Running task 68.0 in stage 2.0 (TID 72)
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.4 KiB) non-empty blocks including 3 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/05/17 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310447291783761476396147_0002_m_000065_69, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447291783761476396147_0002_m_000065_69}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310447291783761476396147_0002}; taskId=attempt_202105170310447291783761476396147_0002_m_000065_69, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e7184a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310447291783761476396147_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447291783761476396147_0002_m_000065_69
21/05/17 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310441401521357486393300_0002_m_000066_70, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441401521357486393300_0002_m_000066_70}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310441401521357486393300_0002}; taskId=attempt_202105170310441401521357486393300_0002_m_000066_70, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@781fab98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310441401521357486393300_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441401521357486393300_0002_m_000066_70
21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310447291783761476396147_0002_m_000065_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310447291783761476396147_0002_m_000065_69 : duration 0:00.120s
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.7 KiB) non-empty blocks including 3 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
21/05/17 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310445248365142681796192_0002_m_000067_71, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445248365142681796192_0002_m_000067_71}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310445248365142681796192_0002}; taskId=attempt_202105170310445248365142681796192_0002_m_000067_71, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@136cd93d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:46 INFO StagingCommitter: Starting: Task committer attempt_202105170310445248365142681796192_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445248365142681796192_0002_m_000067_71
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
21/05/17 03:11:46 INFO StagingCommitter: Task committer attempt_202105170310441401521357486393300_0002_m_000066_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310441401521357486393300_0002_m_000066_70 : duration 0:00.166s
21/05/17 03:11:47 INFO StagingCommitter: Task committer attempt_202105170310445248365142681796192_0002_m_000067_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310445248365142681796192_0002_m_000067_71 : duration 0:00.100s
21/05/17 03:11:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310446051227697220378960_0002_m_000068_72, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446051227697220378960_0002_m_000068_72}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310446051227697220378960_0002}; taskId=attempt_202105170310446051227697220378960_0002_m_000068_72, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@598e4e88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:47 INFO StagingCommitter: Starting: Task committer attempt_202105170310446051227697220378960_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446051227697220378960_0002_m_000068_72
21/05/17 03:11:47 INFO StagingCommitter: Task committer attempt_202105170310446051227697220378960_0002_m_000068_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310446051227697220378960_0002_m_000068_72 : duration 0:00.102s
21/05/17 03:11:50 ERROR Executor: Exception in task 68.0 in stage 2.0 (TID 72)
java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
21/05/17 03:11:50 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 73) (50e6ccadb8f1, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 03:11:50 INFO Executor: Running task 69.0 in stage 2.0 (TID 73)
21/05/17 03:11:50 WARN TaskSetManager: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
21/05/17 03:11:50 ERROR TaskSetManager: Task 68 in stage 2.0 failed 1 times; aborting job
21/05/17 03:11:50 INFO ShuffleBlockFetcherIterator: Getting 3 (2.1 KiB) non-empty blocks including 3 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 03:11:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 03:11:50 INFO TaskSchedulerImpl: Cancelling stage 2
21/05/17 03:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled
21/05/17 03:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 03:11:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 03:11:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 03:11:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170310443683335013466727905_0002_m_000069_73, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443683335013466727905_0002_m_000069_73}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170310443683335013466727905_0002}; taskId=attempt_202105170310443683335013466727905_0002_m_000069_73, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c92e12f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/5e113974-9aaf-4add-adcc-306105605440/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 03:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 03:11:50 INFO StagingCommitter: Starting: Task committer attempt_202105170310443683335013466727905_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443683335013466727905_0002_m_000069_73
21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 66.0 in stage 2.0 (TID 70), reason: Stage cancelled
21/05/17 03:11:50 INFO StagingCommitter: Task committer attempt_202105170310443683335013466727905_0002_m_000069_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/5e113974-9aaf-4add-adcc-306105605440/_temporary/0/_temporary/attempt_202105170310443683335013466727905_0002_m_000069_73 : duration 0:00.006s
21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 67.0 in stage 2.0 (TID 71), reason: Stage cancelled
21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 65.0 in stage 2.0 (TID 69), reason: Stage cancelled
21/05/17 03:11:50 INFO Executor: Executor is trying to kill task 69.0 in stage 2.0 (TID 73), reason: Stage cancelled
21/05/17 03:11:50 INFO TaskSchedulerImpl: Stage 2 was cancelled
21/05/17 03:11:50 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) failed in 62.953 s due to Job aborted due to stage failure: Task 68 in stage 2.0 failed 1 times, most recent failure: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$R
equestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more

Driver stacktrace:
21/05/17 03:11:50 INFO Executor: Executor killed task 69.0 in stage 2.0 (TID 73), reason: Stage cancelled
21/05/17 03:11:50 INFO DAGScheduler: Job 1 failed: csv at NativeMethodAccessorImpl.java:0, took 62.015651 s
21/05/17 03:11:50 ERROR FileFormatWriter: Aborting job 5e113974-9aaf-4add-adcc-306105605440.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 68 in stage 2.0 failed 1 times, most recent failure: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttp
Client$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception
: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
21/05/17 03:11:50 INFO AbstractS3ACommitter: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: aborting job (no job ID) in state FAILED
21/05/17 03:11:50 INFO StagingCommitter: Starting: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: aborting job in state (no job ID)
21/05/17 03:11:50 WARN TaskSetManager: Lost task 69.0 in stage 2.0 (TID 73) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
21/05/17 03:11:50 INFO Executor: Executor interrupted and killed task 65.0 in stage 2.0 (TID 69), reason: Stage cancelled
21/05/17 03:11:50 INFO Executor: Executor interrupted and killed task 66.0 in stage 2.0 (TID 70), reason: Stage cancelled
21/05/17 03:11:50 WARN TaskSetManager: Lost task 66.0 in stage 2.0 (TID 70) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
21/05/17 03:11:50 WARN TaskSetManager: Lost task 65.0 in stage 2.0 (TID 69) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
21/05/17 03:11:50 INFO AbstractS3ACommitter: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: no pending commits to abort
21/05/17 03:11:50 INFO StagingCommitter: Task committer attempt_20210517031044670033147968055892_0000_m_000000_0: aborting job in state (no job ID) : duration 0:00.023s
21/05/17 03:11:51 INFO Executor: Executor interrupted and killed task 67.0 in stage 2.0 (TID 71), reason: Stage cancelled
21/05/17 03:11:51 WARN TaskSetManager: Lost task 67.0 in stage 2.0 (TID 71) (50e6ccadb8f1 executor driver): TaskKilled (Stage cancelled)
21/05/17 03:11:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
21/05/17 03:11:51 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/17 03:11:51 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
21/05/17 03:11:52 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.657s
21/05/17 03:11:52 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.658s
Traceback (most recent call last):
  File "/home/jovyan/spark.py", line 72, in <module>
    df \
  File "/usr/local/spark/python/pyspark/sql/readwriter.py", line 1371, in csv
self._jwrite.csv(path)
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
File "/usr/local/spark/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError
: An error occurred while calling o89.csv.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 68 in stage 2.0 failed 1 times, most recent failure: Lost task 68.0 in stage 2.0 (TID 72) (50e6ccadb8f1 executor driver): java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidden
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
	at org.apache.spark.scheduler.DAGSchedul
er.$anon
fun$abortStage$2(DAGScheduler.scala:2202)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
	at org.apache.spark.scheduler.DAGSchedulerEv
entProcessLoop.onReceive(DAGScheduler.scala:2371)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)
	... 33 more
Caused by: java.nio.file.AccessDeniedException: s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: getFileStatus on s3a://udac-forex-project/consolidated_data/part-00068-8cbbad88-025f-419d-b7be-35140a7a2908-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=:403 Forbidde
n
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2198)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:752)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
	at org.apache.spark.sql.execution.da
tasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor
$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F6K85N8VTQF6V53P; S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=; Proxy: null), S3 Extended Request ID: Q94VZNrpbsdIv27MbTshH6E8qTVGmwrwTIvZAJ3LI4QweI+bO6mEczE7sS/fcRVHjFsCQcU27aI=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(Amazo
nHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1249)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.jav
a:1246)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2183)
	... 22 more
21/05/17 03:11:53 INFO SparkContext: Invoking stop() from shutdown hook
21/05/17 03:11:53 INFO SparkUI: Stopped Spark web UI at http://50e6ccadb8f1:4040
21/05/17 03:11:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/05/17 03:11:53 INFO MemoryStore: MemoryStore cleared
21/05/17 03:11:53 INFO BlockManager: BlockManager stopped
21/05/17 03:11:53 INFO BlockManagerMaster: BlockManagerMaster stopped
21/05/17 03:11:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/05/17 03:11:53 INFO SparkContext: Successfully stopped SparkContext
21/05/17 03:11:53 INFO ShutdownHookManager: Shutdown hook called
21/05/17 03:11:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-030bae49-8de0-442a-83a2-278e8d90d048
21/05/17 03:11:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-a332d460-94bf-4cba-8e4d-eeac4a022c39
21/05/17 03:11:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-030bae49-8de0-442a-83a2-278e8d90d048/pyspark-d5bd71c4-2b20-4dfe-816e-81023b3b705c
21/05/17 03:11:53 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
21/05/17 03:11:53 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
21/05/17 03:11:53 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-17 00:28:30,530] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=etl, task_id=run_spark_job, execution_date=20210517T020000, start_date=20210517T031012, end_date=20210517T032830
[2021-05-17 00:28:30,554] {local_task_job.py:146} INFO - Task exited with return code 1
